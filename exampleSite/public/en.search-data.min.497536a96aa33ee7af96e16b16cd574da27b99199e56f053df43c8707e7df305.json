[{"id":0,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.1-%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/","title":"1.1.1 三次握手","section":"1.1 传输层： Tcp和 UDP","content":"三次握手 #  1 什么是三次握手 #    第一次握手：Client 将SYN 置为 1，随机产生一个初始序列号seq 发送给Server，进入SYN_SENT 状态。 第二次握手：Server 收到Client 的SYN=1 后，知道客户端请求建立连接，将自己的SYN 置为 1，ACK 置为 1，产生一个acknowledge number = seq + 1，并随机产生一个自己的初始序列号，发送给客户端，进入SYN_RCVD 状态。 第三次握手： 客户端检查acknowledge number 是否为序列号 +1，ACK 是否为 1，检查正确之后，将自己的ACK 置为 1，产生一个acknowledge number = 服务器发的序列号 + 1，发送给服务器，进入ESTBLISHED 状态；服务器检查ACK 为 1，acknowledge number = 序列号 + 1 之后，也进入ESTBLISHED 状态，完成三次握手，连接建立。  2 三次握手的必要性 #   第一次握手：客户端发送网络包，服务端收到了，这样服务端就能得出结论，客户端的发送能力、服务端的接收能力是正常的。 第二次握手：服务端发送网络包，客户端收到了，这样客户端就能得出结论，服务端的接收能力、发送能力、客户端的接收、发送能力是正常的，不过此时服务器并不能确认客户端的接收能力是否正常。 第三次握手：客户端发送网络包，服务端收到了，这样服务器就能得出结论，客户端的接收能力、服务端的接收能力是正常的。  3 TCP 连接可以两次握手吗 #  不可以，主要有两个原因：\n 可能会出现已失效的连接请求报文段又传到了服务器端。  Client 发出的第一个连接请求报文段并没有丢失，而是在某个网络节点长时间的滞留了，以延误到连接释放以后的某个时间才到达Server。 本来这是一个早已失效的报文段，但Server 收到此失效的连接请求报文段后，就误认为是Client 再次发出的一个新的连接请求，于是就向Client 发出确认报文段，同意建立连接。 如果不采用三次握手，那么只要Server 发出确认，新的连接就建立了。 由于现在Client 并没有发出建立连接的请求，因此不会理睬Server 的确认，也不会向Server 发送数据，但Server 却认为新的连接已经建立，并一直等待Client 发来数据，这样，Server 的很多资源就白白浪费掉了。 采用三次握手的办法就可以防止上述现象发生，就像刚才那种情况，Client 不会向Server 的确认发出确认，Server 由于收不到确认，就知道Client 并没有要求建立连接。   无法保证 Client 正确接收第二次握手的报文（Server 无法确认 Client 是否收到），也无法保证 Client 和 Server 之间成功互换初始序列号。  4 可以采用四次握手吗 #  可以，但是会降低传输的效率。\n 四次握手是指：第二次握手Server 只发送ACK 和acknowledge number，而Server 的SYN 和初始序列号在第三次握手时发送，原来协议中的第三次握手变为第四次握手。 出于优化的目的，四次握手中的二、三可以合并。  5 第三次握手中，如果客户端的 ACK 未送达服务器，会怎样 #   Server 端： 由于没有收到ACK 确认，因此会重发之前的SYN+ACK（默认重发 5 次，之后自动关闭连接进入CLOSED 状态），Client 收到后会重新传ACK 给Server。 Client 端：  在Server 进行超时重发的过程中，如果Client 向服务器发送数据，数据头部的ACK 是为 1 的，所以服务器收到数据之后会读取ACK Number，进入ESTABLISHED 状态。 在Server 进入CLOSED 状态之后，如果Client 向服务器发送数据，服务器会以RST 包应答。    6 如果已经建立了连接，但客户端出现了故障怎么办 #   服务器每收到一次客户端的请求后都会重新复位一个计数器，时间通常是设置为 2 小时，若 2 小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔 75 秒钟发送一次。 若一连发送 10 个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。  7 初始序列号是什么 #   TCP 连接的一方A，随机选择一个 32 为序列号作为发送数据的初始序列号，比如为 1000，然后以该序列号为原点，对要传送的数据进行编号：1001、1002\u0026hellip;。 三次握手时，把这个序列号传送给另一方B，以便在数据传输时，B 可以确认什么样的数据编号是合法的。 同时在进行数据传输时，A 还可以确认 B 收到的每一个字节，如果A 收到了B 的确认编号是 2001，就说明编号为 1001-2000 的数据已经被B 成功接受。  参考文献 #    什么是三次握手 (three-way handshake)？。  关于三次握手与四次挥手你要知道这些。  "},{"id":1,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.1-HTTP%E5%92%8CHTTPS%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"1.2.1 Http和 HTTP S的区别","section":"1.2 应用层：HTTP和HTTPS","content":"HTTP和HTTPS的区别 #  1 HTTP和HTTPS有什么区别 #   端口不同：HTTP使用80端口，HTTPS使用443端口。 HTTP（超文本传输协议）信息是明文传输，HTTPS运行在SSL（Secure Socket Layer）之上，添加了加密和认证机制，更加安全。 HTTPS由于加密解密会带来更大的CPU和内存开销。 HTTPS通信需要证书，一般需向证书颁发机构购买。  2 HTTPS的连接过程 #   客户端向服务器发出请求，同时发送客户端支持的一套加密规则（包括对称加密、非对称加密、摘要算法）。 服务器从中选取一种加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址、加密公钥（用于非对称加密）以及证书的颁发机构等信息（证书中的私钥只能用于服务器端进行解密）。 客户端验证服务器的合法性，包括证书是否过期、CA是否可靠、发行者证书的公钥能否正确解开服务器证书的发行者的数字签名、服务器证书的域名是否和服务器的实际域名相匹配。 如果证书受信任，或者用户接受了不受信任的证书，浏览器会生成一个随机密钥，并用于服务器提供的公钥加密，然后使用HASH算法对握手消息进行摘要计算，并对摘要使用之前产生的密钥加密，最后将加密后的随机密钥和摘要一起发送给服务器。 服务器使用自己的私钥解密，得到对称加密的密钥，用这个密钥解密出HASH摘要值，并验证握手消息是否一致，如果一致，服务器使用对称加密的密钥加密握手消息发送给浏览器。 浏览器解密并验证摘要，若一致，则握手结束，之后的数据传送都使用对称加密的密钥进行加密。    非对称加密算法用于在握手过程中加密生成的密码。 对称加密算法用于对真正传输的数据进行加密。 HASH算法主要用于验证数据的完整性。   3 输入www.baidu.com，怎么变成https://www.baidu.com，怎么确定用HTTP还是HTTPS #   一种是原始的302跳转，服务器把所有的HTTP流量跳转到HTTPS，但这样有一个漏洞，就是中间人可能在第一次访问站点的时候就劫持。 解决方法是引入HSTS机制，返回的302报文中有这样一条Strict-Transport-Security: max-age=31536000，其含义是强制浏览器在max-age到期之前，把所有的http://www.baidu.com自动转换成https://www.baidu.com，这是浏览器实现的URL转换，不用每次访问两次服务器，避免了302跳转到80再跳转到443的中间人劫持的问题，所以只有第一次的时候会出现302，以后访问就不会出现了。  4 什么是对称加密、非对称加密，区别是什么 #   对称加密：  加密和解密采用相同的密钥，如DES、RC2、RC4。   非对称加密：  需要两个密钥：公钥和私钥。 如果用公钥加密，需要用私钥才能解密，如RSA。   区别：  对称加密速度更快，通常用于大量数据的加密。 非对称加密安全性更高（不需要传送私钥）。    5 数字签名、报文摘要的原理 #   发送者A用私钥进行签名，接收者B用公钥验证签名，因为除A外没有人有私钥，所以B相信签名是来自A，A不可抵赖，B也不能伪造报文。 摘要算法： MD5、SHA。  6 参考文献 #    HTTP和HTTPS有什么区别？  输入网址的时候，浏览器是如何判断你是http协议还是https协议的。  "},{"id":2,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.4-%E7%BD%91%E7%BB%9C%E5%B1%82%E5%8D%8F%E8%AE%AE/1.4.1-IP%E5%9C%B0%E5%9D%80%E7%9A%84%E5%88%86%E7%B1%BB/","title":"1.4.1 Ip地址的分类","section":"1.4 网络层协议","content":"IP地址的分类 #   两级的 IP 地址通常由网络号和主机号组成：  第一个字段是网络号，标志着主机所连接到的网络，一个网络号在整个因特网范围内必须是唯一的。 第二个字段是主机号，它标志着主机，一个主机号在他前面所指明的网络范围内必须是唯一的。   IP 地址分为五大类，分别为 A 类、B 类、C 类、D 类和 E 类，具体如下：  A 类地址： 以0开头，第一个字节范围是1~126（1.0.0.0 - 126.255.255.255）。 B 类地址： 以10开头，第一个字节范围是128~191（128.0.0.0 - 191.255.255.255）。 C 类地址： 以110开头，第一个字节范围是192~223（192.0.0.0 - 223.255.255.255）。 D 类地址： 以1110开头，第一个字节范围是224~239（224.0.0.0 - 239.255.255.255）（作为多播使用）。 E 类地址： 保留。   A、B、C 是基本类，D、E 作为多播和保留使用。 留用的内部私有地址如下：  A 类地址：10.0.0.0\u0026ndash;10.255.255.255。 B 类地址：172.16.0.0\u0026ndash;172.31.255.255。 C 类地址： 192.168.0.0\u0026ndash;192.168.255.255。   IP 地址与子网掩码相与得到网络号。 主机号全为 0的是网络号（例如：192.168.2.0），主机号全为 1的是广播地址（例如：192.168.2.255）。   参考文献 #    IP 地址的分类？  IP 地址分类详解。  IP地址的分类。  "},{"id":3,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"2.1.1 进程和线程的区别","section":"2.1 进程和线程","content":"进程和线程的区别 #  1 进程和线程有哪些区别 #   进程（Process）是系统进行资源分配和调度的基本单位，线程是CPU 调度和分配的基本单位。 线程依赖于进程而存在，一个进程至少有一个线程。 进程有自己的独立地址空间，线程共享所属进程的地址空间。 进程是拥有系统资源的一个独立单位，而线程基本上不拥有系统资源，只拥有一点在运行中必不可少的资源（如程序计数器、一组寄存器和栈），和其他线程共享本进程的相关资源如内存、I/O、CPU 等。 在进程切换时，涉及到整个当前进程 CPU 环境的设置以及新被调度运行的 CPU 环境的设置，而线程切换只需保存和设置少量的寄存器的内容，并不涉及存储器管理方面的操作，可见，进程切换时的开销远大于线程切换的开销。 线程间的通信更方便，同一进程下的线程共享全局变量等数据，而进程间的通信需要以进程间通信（IPC）的方式进行。 多线程程序只要有一个线程崩溃，整个程序就崩溃了，但多进程程序中一个进程崩溃并不会对其他进程造成影响，因为进程有自己独立的地址空间，因此，多进程更加健壮。  2 同一进程中的线程可以共享哪些变量 #   进程代码块。 进程的公有数据（全局变量、静态变量\u0026hellip;）。 进程打开的文件描述符。 进程的当前目录。 信号处理器/信号处理函数：对收到的信号的处理方式。 进程 ID与进程组 ID。  3 线程独占哪些资源 #   线程 ID。 一组寄存器的值。 线程自身的栈（堆是共享的）。 错误返回码：线程可能会产生不同的错误返回码，一个线程的错误返回码不应该被其他线程修改。 信号掩码/信号屏蔽字：表示是否屏蔽/阻塞相应的信号（SIGKILL 和 SIGSTOP 除外）。  参考文献 #    进程和线程有什么区别？  "},{"id":4,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/2.3.1-%E5%88%86%E9%A1%B5%E5%92%8C%E5%88%86%E6%AE%B5/","title":"2.3.1 分页和分段","section":"2.3 内存管理","content":"分页和分段 #  1 分页存储 #   分页存储管理是将一个进程的逻辑地址空间分成若干个大小相等的片，称为页面或页，并为页面加以编号，从0开始，如第0页、第1页等。 同样，也把内存空间分成与页面相同大小的若干个存储块，称为块或页框，也为他们加以编号，如0#块、1#块等。 在为进程分配内存时，以块为单位将进程中的若干页分别装入到多个可以不相邻接的物理块中。 由于进程的最后一页经常装不满一块而形成了不可利用的碎片，称之为页内碎片。  2 分段存储 #   在分段存储管理方式中，作业的地址空间被划分为若干个段，每个段定义了一组逻辑信息，如主程序段、子程序段、数据段、栈段等，每个段都从0开始编址，并采用一段连续的地址空间。 段的长度由相应的逻辑信息组的长度决定，因而各段长度不等。 整个作业的地址空间由于是分成多个段，因而是二维的，其逻辑地址由段号和段内地址所组成。  3 段页式存储 #   用户进程先按段划分，段内再按页划分。 内存划分和分配按页划分。  4 分页存储和分段存储的区别 #   目的不同： 分页的目的是管理内存，用于虚拟内存以获得更大的地址空间；分段的目的是满足用户的需求，使程序和数据可以被划分为逻辑上独立的地址空间。 大小不同： 段的大小不固定，由其所完成的功能决定；页的大小固定，由系统决定。 地址空间维度不同： 分段是二维地址空间（段号+段内偏移），分页是一维地址空间（每个进程一个页表/多级页表，通过一个逻辑地址就能找到对应的物理地址）。 共享： 分段很方便按照逻辑模块实现信息的共享和保护；分页管理不方便按照逻辑模块实现信息的共享和保护。 碎片： 分段没有内碎片，但会产生外碎片；分页没有外碎片，但会产生内碎片。  参考文献 #    分页和分段有什么区别？  分页存储管理和分段存储管理。  操作系统——段式存储管理、段页式存储管理。  "},{"id":5,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.6-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.6.1-%E4%B8%89%E8%8C%83%E5%BC%8F/","title":"1.6.1 三范式","section":"1.6 基础知识","content":"三范式 #  数据库的设计范式是数据库设计所需要满足的规范，满足这些规范的数据库是简洁的、结构明确的，同时不会发生插入（insert）、删除（delete）、和更新（update）操作异常。\n1 第一范式（1NF） #  1.1 含义 #   符合 1NF 的关系中每个属性都不可再分。 1NF 是所有关系型数据库的最基本要求。  1.2 示例 #   下图所示的表就不符合 1NF 的要求：  为了满足 1NF 的要求，上面的表应该改为下图的形式：   1.3 存在的问题 #  如果仅仅符合 1NF 的设计，仍然会存在数据冗余过大、插入异常、删除异常、修改异常的问题，例如对于下图中表的设计：\n  数据冗余过大：  每一名学生的学号、姓名、系名、系主任这些数据重复多次。 每个系与对应的系主任的数据也重复多次。   插入异常：  假如学校新建了一个系，但是暂时还没有招收任何学生（比如 3 月份就新建了，但要等到 8 月份才招生），那么是无法将系名与系主任的数据单独地添加到数据表中去的。   删除异常：  假如将某个系中所有学生相关的记录都删除，那么所有系与系主任的数据也就随之消失了（一个系所有学生都没有了，并不表示这个系就没有了）。   修改异常：  假如李小明转系到法律系，那么为了保证数据库中数据的一致性，需要修改三条记录中系与系主任的数据。    2 第二范式（2NF） #   正因为仅符合 1NF 的数据库设计存在着上面提到的那些问题，我们需要提高设计标准，去掉导致上述四种问题的因素，使其符合更高一级的范式（2NF），这就是所谓的“规范化”。\n 2.1 相关概念 #  2.1.1 函数依赖 #  2.1.1.1 含义 #   若在一张表中，在属性或属性组 $X$ 的值确定的情况下，必定能确定属性 $Y$ 的值，那么就可以说 $Y$函数依赖于$X$，写作 $X \\rightarrow Y$。 也就是说，在数据表中，不存在任意两条记录，他们在 $X$ 上的值相同，而在 $Y$ 属性上的值不同。 类似于函数关系$y = f(x)$，在 $x$的值确定的情况下，$y$的值一定是确定的。  2.1.1.2 示例 #   例如，在上面的图中，找不到任何一条记录，他们的学号相同，而对应的姓名不同，所以我们可以说姓名函数依赖于学号，写作 $ 学号 \\rightarrow 姓名 $，但是反过来，因为可能出现同名的学生，所以有可能不同的两条学生记录，他们在姓名上的值相同，但对应的学号不同，所以我们不能说学号依赖于姓名。 上图中存在的其它函数依赖关系如下：  $ 系名 \\rightarrow 系主任 $。 $ 学号 \\rightarrow 系主任 $。 $(学号,课名) \\rightarrow 分数 $。   但如下依赖关系则不成立：  $ 学号 \\rightarrow 课名 $。 $ 学号 \\rightarrow 分数 $。 $ 课名 \\rightarrow 系主任 $。 $(学号,课名) \\rightarrow 姓名 $。    2.1.1.3 相关概念 #  2.1.1.3.1 完全函数依赖 #   在一张表中，若 $X \\rightarrow Y$，且对于 $X$ 的任意一个真子集 $X^{'}$（假如属性组 $X$ 包含超过一个属性），$X^{'} \\rightarrow Y$不成立，那么我们称 $Y$完全函数依赖于$X$，记作 $X \\xrightarrow F Y$。 例如：  $ 学号 \\xrightarrow F 姓名 $。 $(学号,课名) \\xrightarrow F 分数 $。    2.1.1.3.2 部分函数依赖 #   在一张表中，若 $X \\rightarrow Y$，且 $Y$不完全函数依赖于 $X$，那么我们就称 $Y$部分函数依赖于 $X$，记作 $X \\xrightarrow P Y$。 例如：  $(学号,课名) \\xrightarrow P 姓名 $。    2.1.1.3.3 传递函数依赖 #   假如 $Z \\rightarrow Y$，且 $Y \\rightarrow X$（$X \\notin Y$，且 $X$ 不函数依赖于 $Y$），那么我们就称 $Z$传递函数依赖于$X$，记作 $X \\xrightarrow T Z$。  2.1.2 码 #  2.1.2.1 含义 #   设 $K$ 为某表中的一个属性或属性组，若除 $K$ 之外的所有属性都完全函数依赖于 $K$，那么我们称 $K$ 为候选码，简称为码。 在实际中我们通常可以理解为假如当 $K$ 确定之后，该表除 $K$ 外的所有属性的值也就随之确定，那么 $K$ 就是码。 一张表中可以有超过一个码，实际应用中为了方便，通常选择其中的一个作为主码。  2.1.2.2 示例 #   $(学号,课名)$ 这个属性组就是码，该表中有且仅有这一个码。  2.1.3 主属性 #  2.1.3.1 含义 #   包含在任何一个码中的属性称为主属性。  2.1.3.2 示例 #   对于上图中，主属性就有两个，分别是学号和课名。  2.2 含义 #   2NF 在 1NF 的基础上，消除了非主属性对于码的部分依赖（属性完全依赖于主键）。 判断一张表是否符合2NF的方法：  第一步：找出数据表中所有的码。 第二步：根据第一步得到的码，找出所有的主属性。 第三步：数据表中，除去所有的主属性，剩下的就都是非主属性了。 第四步：查看是否存在非主属性对码的部分依赖。    2.3 示例 #    第一步：下图中表示了上面的表中所有的函数依赖关系，这一步完成后，可以得到，上面的表的码只有一个，就是$(学号,课名)$。\n   第二步：主属性有两个，分别为学号和课名。\n  第三步：非主属性有四个，分别为姓名、系名、系主任、分数。\n  第四步：\n 对于$(学号,课名) \\rightarrow 姓名$，有$学号 \\rightarrow 姓名$，存在非主属性姓名对码$(学号,课名)$的部分函数依赖。 对于$(学号,课名) \\rightarrow 系名$，有$学号 \\rightarrow 系名$，存在非主属性系名对码$(学号,课名)$的部分函数依赖。 对于$(学号,课名) \\rightarrow 系主任$，有$学号 \\rightarrow 系主任$，存在非主属性对码$(学号,课名)$的部分函数依赖。    所以上面的表存在非主属性对于码的部分函数依赖，最高只符合1NF的要求，不符合2NF的要求。\n2.4 如何使1NF的表符合2NF #   为了让上面的表符合2NF的要求，我们必须消除这些部分函数的依赖，只有一个办法，就是将大数据表拆分成两个或者更多个更小的数据表，在拆分的过程中，要达到更高一级范式的要求，这个过程叫做模式分解，模式分解的方法不是唯一的，以下是其中一种方法：  选课（学号、课名、分数）。 学生（学号、姓名、系名、系主任）。   我们先来判断一下选课表与学生表是否符合了2NF的要求：  对于选课表，其码是$(学号,课名)$，主属性是学号、课名，非主属性是分数，只有学号和课名都确定的情况下，才能唯一确定分数，因此不存在非主属性对于码的部分函数依赖，符合2NF的要求。 对于学生表，其码是$(学号)$，主属性是学号，非主属性是姓名、系名、系主任，因为码只有一个，所以不可能存在非主属性对于码的部分函数依赖，因此此表符合2NF的要求。   下图展示了模式分解以后的新的函数依赖关系：  下图展示了模式分解以后新的数据：   2.5 1NF问题解决状态 #  2.5.1 已解决 #   数据冗余过大：  学生的姓名、系名与系主任，不再像之前一样重复多次了。   修改异常：  李小明转到法律系后只需要修改一次对应的系的值即可。    2.5.2 未解决 #   插入异常：  当插入一个尚无学生的新系的新系时，因为学生表的码是学号，不能为空，所以此操作不被允许。   删除异常：  当删除某个系中所有的学生记录时，该系的信息会全部丢失。    3 第三范式（3NF） #  3.1 含义 #   3NF在2NF的基础上，消除了非主属性对于码的传递函数依赖。 符合3NF要求的数据库设计，基本上解决了数据冗余过大、插入异常、修改异常、删除异常的问题。  3.2 示例 #  我们先看一下上面符合2NF表的设计是否符合3NF的要求：\n 对于选课表，主码为$(学号,课名)$，主属性为学号和课名，非主属性只有一个，为分数，不可能存在传递函数依赖，所以选课表的设计符合3NF的要求。 对于学生表，主码为$学号$，主属性为学号，非主属性为姓名、系名、系主任，因为$姓名 \\rightarrow 系名$，$系名 \\rightarrow 系主任$，所以存在非主属性系主任对于码学号的传递函数依赖，所以学生表的设计不符合3NF的要求。  3.3 如何使2NF的表符合3NF #   为了让数据表设计达到3NF，我们必须进一步进行模式分解为以下形式：  选课（学号、课名、分数）。 学生（学号、姓名、系名）。 系（系名、系主任）。   下面我们看一下分解后的表是否符合3NF：  对于选课表，符合3NF的要求，这个我们上面已经分析过了。 对于学生表，码为学号，主属性为学号，非主属性为系名，不可能存在非主属性对于码的传递函数依赖。 对于系表，码为系名，主属性为系名，非主属性为系主任，不可能存在非主属性对于码的传递函数依赖（至少要有三个属性才可能存在传递函数依赖关系），所以符合3NF的要求。   新的函数依赖关系如下图所示：  新的数据表如下图所示：   3.4 2NF问题解决状态 #  3.4.1 已解决 #   插入异常：  当插入一个尚无学生的新系的信息时，由于系表与学生表目前是独立的两张表，所以不影响。   删除异常：  当删除某个系中所有的学生记录时，该系的信息不会丢失。    参考文献 #    如何理解关系型数据库的常见设计范式？  "},{"id":6,"href":"/school-recruitment/docs/design-pattern/1%E6%A6%82%E8%BF%B0/","title":"1、概述","section":"第四章 设计模式","content":"1 设计模式分类 #   设计模式可以分为三大类，分别是创建型模式、结构型模式、行为型模式。\n1.1 创建型模式 #  创建型模式共五种，分别是：\n  工厂模式。 抽象工厂模式。  单例模式。 建造者模式。 原型模式。  1.2 结构型模式 #  结构型模式共七种，分别是：\n 适配器模式。 装饰器模式。  代理模式。 外观模式。 桥接模式。 组合模式。 享元模式。  1.3 行为型模式 #  行为型模式共十一种，分别是：\n  策略模式。 模板模式。 观察者模式。 迭代子模式。 责任链模式。 命令模式。 备忘录模式。 状态模式。 访问者模式。 中介者模式。 解释器模式。  参考文献 #    设计模式面试题（总结最全面的面试题！！！）。  "},{"id":7,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.2-%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","title":"1.1.2 四次挥手","section":"1.1 传输层： Tcp和 UDP","content":"四次挥手 #  1 什么是四次挥手 #    第一次挥手：Client 将FIN 置为 1，发送一个序列号seq 给Server，进入FIN_WAIT_1 状态。 第二次挥手：Server 收到FIN 之后，发送一个ACK=1，acknowledge number=收到的序列号 +1，进入CLOSE_WAIT 状态，此时客户端已经没有要发送的数据了，但仍可以接受服务器发来的数据。 第三次挥手：Server 将FIN 置为 1，发送一个序列号给Client，进入LAST_ACK 状态。 第四次挥手：Client 收到服务器的FIN 之后，进入TIME_WAIT 状态，接着将ACK 置为 1，发送一个acknowledge number=序列号 +1 给服务器，服务器收到后，确认acknowledge number 后，变为CLOSED 状态，不再向客户端发送数据，客户端等待2*MSL（报文段最长寿命） 时间后，也进入CLOSED 状态，完成四次挥手。   MSL 是什么？\n MSL（Maximum Segment Lifetime），即报文段最长寿命，指一个片段在网络中最大的存活时间，2MSL 就是一个发送和一个回复所需的最大时间。 如果直到 2MSL，Client 都没有再次收到 FIN，那么 Client 推断 ACK 已经被成功接收，则结束 TCP 连接。   2 CLOSE_WAIT #  2.1 服务器端 CLOSE_WAIT 状态意义是什么（为什么不能把服务器发送的 ACK 和 FIN 合并起来，变成三次挥手） #   因为服务器收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复 ACK，表示接收到了断开连接的请求。 等到数据发完之后再发 FIN，断开服务器到客户端的数据传送。  2.2 TCP挥手时出现大量CLOSE_WAIT怎么解决 #   如果我们的服务器程序处于CLOSE_WAIT状态，说明socket是被动关闭的。 这时候服务器一直没有进行第三次挥手，导致服务器存在大量CLOSE_WAIT状态的连接，大量这种情况发生会影响服务器性能，同样可能导致套接字数量达到服务器上限。 导致这种情况的原因通常是服务端发生异常后未关闭连接，或者CLOSE_WAIT的配置时间过长，如果是MySQL，也可能存在事务开启后没有正确 rollback或者 commit的可能。 一般可以采用以下方法来进行排查：  top查看 cpu利用率和 load情况（大量CLOSE_WAIT属于IO密集型，会导致 load相比 cpu利用率高出很多）。 netstat观察 close_wait的数量变化。 perf或者火焰图定位热点函数。    3 TIME_WAIT #  3.1 客户端 TIME_WAIT 状态的意义是什么（为什么要等待2MSL） #    第四次挥手时，客户端发送给服务器的 ACK 有可能丢失，TIME_WAIT 状态就是用来重发可能丢失的 ACK 报文。\n  如果 Server 没有收到 ACK，就会重发 FIN，如果 Client 在 2*MSL 的时间内收到了 FIN，就会重新发送 ACK 并再次等待 2MSL，防止 Server 没有收到 ACK 而不断重发 FIN。\n MSL，全称为Maximum Segment Lifetime，即最长报文段寿命，他是任何报文在网络上存在的最长时间，超过这个时间的报文将被丢弃。\n   3.2 TCP挥手时出现大量TIME_WAIT怎么解决 #   TIME_WAIT是连接完全关闭前的最后一个状态，一个连接被关闭时，主动关闭的一端最后会进入TIME_WAIT状态，等待足够的时间以确保远程TCP接收到连接中断请求的确认，这个时间为2MSL。 一般来说，当系统有较大的并发短连接压力时，都会出现少量的TIME_WAIT连接，这是正常的，但是有时候系统上出现大量的TIME_WAIT状态的连接，从而导致再也没有可用端口来建立新的连接。 程序中产生大量TIME_WAIT状态的根本原因是频繁创建断开TCP连接，解决的基本思路是把频繁创建的TCP短连接改成TCP长连接。 解决方法主要有以下几种：  开启TIME_WAIT重用：  使用TIME_WAIT重用的时候需要保证下面任意一点：  初始序列号比TIME_WAIT老连接的末序列号大。 如果使用了时间戳，新到来的连接的时间戳比老连接的时间戳大。   开启TIME_WAIT重用的方法为在 /etc/sysctl.conf中加入 net.ipv4.tcp_tw_reuse = 1。   将TCP短连接改造为长连接：  如果发起连接的目标也是自己可控制的服务器时，他们自己的TCP连接通信最好采用长连接，避免大量TCP短连接每次建立/释放产生的各种开销。 如果建立连接的目标是不受自己控制的机器时，能否使用长连接就需要考虑对方机器是否支持长连接方式了。      4 如果第二次挥手时服务器的 ACK 没有送达客户端，会怎样 #   客户端没有收到ACK 确认，会重新发送FIN 请求。  参考文献 #    什么是四次挥手？  关于三次握手与四次挥手你要知道这些。  close_wait状态的产生原因及解决。  Socket 连接问题之大量 TIME_WAIT。  服务产生大量TIME_WAIT如何解决。  time_wait的快速回收和重用（转）。  "},{"id":8,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.2-GET%E5%92%8CPOST%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"1.2.2 Get和 Post的区别","section":"1.2 应用层：HTTP和HTTPS","content":"GET和POST的区别 #   GET是幂等的，即读取同一个资源，总是得到相同的数据，POST不是幂等的。 GET一般用于从服务器获取资源，而POST有可能改变服务器上的资源。 GET请求的数据附在URL之后，在HTTP请求头中；POST请求的数据在请求体中。 GET请求可被缓存、收藏、保留到历史记录，且其数据明文出现在URL中；POST请求的参数不会被保存，安全性相对较高。 GET只允许ASCII字符，POST对数据类型没有要求，也允许二进制数据。 GET长度有限制（操作系统或者浏览器），而POST数据大小无限制。  "},{"id":9,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.4-%E7%BD%91%E7%BB%9C%E5%B1%82%E5%8D%8F%E8%AE%AE/1.4.2-%E5%88%92%E5%88%86%E5%AD%90%E7%BD%91/","title":"1.4.2 划分子网","section":"1.4 网络层协议","content":"划分子网 #  1 为什么要划分子网 #   简单地说就是一个公司不可能使用 254 个公网地址，A 公司想用 6 个地址，B 公司也想用 6 个地址，如果把这两个公司的地址都放在一个大网段里，这两个公司的地址就能够直接互通。  2 什么是划分子网 #   子网划分就是通过借用 IP 地址的若干位主机位来充当子网地址，从而将原来的网络分为若干个彼此隔离的子网。 通过划分子网，我们可以按照我们的需要将网络分割成小网络，这样也有助于降低流量和隐藏网络的复杂性。 子网掩码对应的网络号和子网号都为1，主机号为0，通过将子网掩码和与操作，可以得到对应的网络地址。 划分子网后，数据报仍然先按照网络号找到目的网络，发送到路由器，路由器再按照网络号和子网号找到目的子网，然后再将子网掩码与目标地址逐位与操作，若结果为某个子网的网络地址，则送到该子网。   3 子网划分的优点 #   减少网络流量。 提高网络性能。 简化管理。 易于扩大地理范围。  4 参考文献 #    什么叫划分子网？  网络基础知识\u0026ndash;子网划分。  "},{"id":10,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.2-%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/","title":"2.1.2 进程间通信方式","section":"2.1 进程和线程","content":"进程间通信方式 #  进程间的通信方式主要有六种，分别是管道、消息队列、共享内存、信号量、信号（Signal）、套接字（Socket）。\n1 管道 #  管道分为两种，分别为匿名管道（Pipe）和命名管道（FIFO）。\n1.1 匿名管道 #  1.1.1 含义 #   匿名管道是一种半双工的方式，数据只能单向流动，也就是说，两个进程都能访问这个文件，假设进程 1 往文件内写东西，那么进程 2 就只能读取文件的内容。 只能在具有亲缘关系的进程间使用，进程的亲缘关系一般指父子关系。 当一个进程创建了一个管道，并调用fork创建自己的一个子进程后，父进程关闭读管道，子进程关闭写管道，这样就提供了两个进程间数据流动的一种方式。  1.1.2 优缺点 #  1.1.2.1 优点 #   简单方便。  1.1.2.2 缺点 #   这个管道只能在具有亲缘关系的进程之间通信。 他只能实现一个进程写另一个进程读，而如果需要两者同时进行，就得重新打开一个管道。  1.2 命名管道 #  1.2.1 含义 #   为了使任意两个进程之间通信，就提出了命名管道。 与管道的区别是提出了一个路径名与之关联，以 FIFO 的形式存储于文件系统中，能够实现任何两个进程之间通信，而匿名管道对于文件系统是不可见的，只限于在父子进程之间的通信。 FIFO 是一个设备文件，在文件系统中以文件名的形式存在，因此，即使进程与创建 FIFO 的进程不存在血缘关系也依然可以通信，前提是可以访问该路径。 FIFO 遵循先进先出原则，即第一个进来的数据会第一个被读走。  1.2.2 优缺点 #  1.2.2.1 优点 #   可以实现任意关系的进程间的通信。  1.2.2.2 缺点 #   长期存在于文件系统中，使用不当容易出错。  2 消息队列 #  2.1 含义 #   消息队列是消息的链表，存放在内核中并由消息队列标识符标识。 消息队列是 UNIX 下不同进程之间实现共享资源的一种机制，UNIX 允许不同进程将格式化的数据流以消息队列的形式发送给任意进程。 对消息队列具有操作权限的进程都可以使用 msget 完成对消息队列的操作控制。 通过使用消息类型，进程可以按任何顺序读信息，或为消息安排优先级顺序。  2.2 优缺点 #  2.2.1 优点 #   可以实现任意进程间的通信。 通过系统调用函数来实现消息发送和接收之间的同步，无需考虑同步问题。  2.2.2 缺点 #   信息的复制需要额外消耗 CPU 的时间，不适宜信息量大或操作频繁的场合。  2.3 消息队列和管道的对比 #   匿名管道是跟随进程的，消息队列是跟随内核的，也就是说，进程结束之后，匿名管道就死了，但是消息队列还会存在，除非显示调用函数销毁。 管道是文件，存放在磁盘上，访问速度慢，消息队列是数据结构，存放在内存，访问速度快。 管道是流式读取，消息队列是数据块式读取。  3 共享内存 #  3.1 含义 #    共享内存实际上就是进程通过调用 shmget（Shared Memory GET）来分配一个共享内存块，然后每个进程通过 shmat（Shared Memory Attach）将进程的逻辑虚拟地址空间指向共享内存块中。 随后需要访问这个共享内存块的进程都必须将这个共享内存块绑定到自己的地址空间中去。 这样，当一个进程往一个共享内存块中写入了数据，共享这个内存区域的所有进程就都可以看到其中的内容。 但是使用共享内存要注意的是多个进程之间对一个给定存储区访问的互斥，若一个进程正在向共享内存区写数据，则在他做完这一步操作前，别的进程不应该去读、写这些数据。  3.2 优缺点 #  3.2.1 优点 #   因为所有进程共享同一块内存，因此共享内存在各种进程间通信方式中具有最高的效率。 访问共享内存区域和访问进程独有的内存区域一样快，并不需要通过系统调用或者其他需要切入内核的过程来完成。 同时，共享内存避免了对数据的各种不必要的复制。  3.2.2 缺点 #   因为系统内核没有对访问共享内存进行同步，因此我们必须提供自己的同步措施。例如，在数据被写入之前不允许进程从共享内存中读取信息，不允许两个进程同时向同一个共享内存地址写入数据等。解决这些问题的常用方法是通过使用信号量进行同步。  4 信号量 #  4.1 含义 #   信号量实际上是一个计数器，可以用来控制多个线程对共享资源的访问，主要用于多线程之间的同步。 信号量主要分为两种：  二进制信号量： 资源数为0或1，通常用于进程互斥。 资源信号量： 资源数目为任何非负值，通常用于进程同步。   信号量只有三种操作：  初始化： 创建一个信号量并初始化他的值。 P操作： 将信号量-1，同时检测信号量，如果信号量小于0，则进程进入阻塞状态。 V操作： 将信号量+1，同时检测信号量，如果信号量小于等于0，则从队列中唤醒一个等待的进程进入就绪态。    4.2 优缺点 #  4.2.1 优点 #   可以同步进程。  4.3 缺点 #   信号量有限。  5 信号 #   信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 在操作系统中，不同信号用不同的值表示，每个信号设置相应的函数，一旦进程发送某个信号给另一个进程，另一个进程将执行相应的函数进行处理。  6 套接字 #  6.1 含义 #   Socket是在应用层和传输层之间的一个抽象层，他把TCP/IP层复杂的操作抽象为几个简单的接口，供应用层调用实现进程在网络中的通信。   6.2 优缺点 #  6.2.1 优点 #   传输数据为字节级，传输数据可自定义，数据量小，效率高。 传输数据时间短，性能高。 适用于客户端和服务器端之间信息实时交互。 可以加密，数据安全性强。  参考文献 #    进程线程面试题总结。  记一次阿里面试题：都有哪些进程间通信方式？麻烦你不要再背了。  进程间通信方式总结。  匿名管道和命名管道。  进程间通信的方式（三）：消息队列。  进程间的通信方式（一）：共享内存。  一文读懂Socket通信原理。  "},{"id":11,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/2.3.2-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/","title":"2.3.2 虚拟内存","section":"2.3 内存管理","content":"虚拟内存 #  1 什么是虚拟内存 #   每个程序都拥有自己的地址空间，这个地址空间被分成大小相等的页，然后被映射到物理内存。 但是不需要所有的页都在物理内存中，当程序引用到不在物理内存中的页时，由操作系统将缺失的部分装入物理内存。 这样，对于程序来说，逻辑上似乎有很大的的内存空间，只是实际上有一部分是存储在磁盘上，因此叫做虚拟内存。  2 如何进行地址空间到物理内存的映射 #   内存管理单元（MMU）管理着逻辑地址到物理地址的转换。 其中的页表存储着页（逻辑地址）和页框（物理地址）的映射表，页表中还包含有效位（是在内存还是在磁盘）、访问位（是否被访问过）、修改位（内存中是否被修改过）、保护位（只读还是可读写）。  3 参考文献 #    什么是虚拟内存？  "},{"id":12,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.6-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.6.2-%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F/","title":"1.6.2 多表连接方式","section":"1.6 基础知识","content":"多表连接方式 #  1 实验表 #  1.1 学生表 #  -- ---------------------------- -- Table structure for student -- ---------------------------- DROP TABLE IF EXISTS `student`; CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) COLLATE utf8_bin DEFAULT NULL, `sex` varchar(255) COLLATE utf8_bin DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; -- ---------------------------- -- Records of student -- ---------------------------- BEGIN; INSERT INTO `student` VALUES (1, \u0026#39;小强\u0026#39;, \u0026#39;男\u0026#39;, 8); INSERT INTO `student` VALUES (2, \u0026#39;小花\u0026#39;, \u0026#39;女\u0026#39;, 5); INSERT INTO `student` VALUES (3, \u0026#39;小名\u0026#39;, \u0026#39;女\u0026#39;, 6); INSERT INTO `student` VALUES (4, \u0026#39;小五\u0026#39;, \u0026#39;女\u0026#39;, 6); INSERT INTO `student` VALUES (5, \u0026#39;小星\u0026#39;, \u0026#39;男\u0026#39;, 30); COMMIT; 1.2 学分表 #  -- ---------------------------- -- Table structure for score -- ---------------------------- DROP TABLE IF EXISTS `score`; CREATE TABLE `score` ( `id` int(11) NOT NULL AUTO_INCREMENT, `class_type` varchar(255) COLLATE utf8_bin DEFAULT NULL, `teacher` varchar(255) COLLATE utf8_bin DEFAULT NULL, `number` int(11) DEFAULT NULL, `sid` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; -- ---------------------------- -- Records of score -- ---------------------------- BEGIN; INSERT INTO `score` VALUES (1, \u0026#39;物理\u0026#39;, \u0026#39;王老师\u0026#39;, 90, 1); INSERT INTO `score` VALUES (2, \u0026#39;数学\u0026#39;, \u0026#39;李老师\u0026#39;, 70, 2); INSERT INTO `score` VALUES (3, \u0026#39;英语\u0026#39;, \u0026#39;黄老师\u0026#39;, 80, 3); INSERT INTO `score` VALUES (4, \u0026#39;体育\u0026#39;, \u0026#39;易老师\u0026#39;, 99, 4); INSERT INTO `score` VALUES (5, \u0026#39;化学\u0026#39;, \u0026#39;科老师\u0026#39;, 50, 4); INSERT INTO `score` VALUES (6, \u0026#39;语文\u0026#39;, \u0026#39;张老师\u0026#39;, 88, 2); INSERT INTO `score` VALUES (7, \u0026#39;地理\u0026#39;, \u0026#39;地老师\u0026#39;, 60, 6); INSERT INTO `score` VALUES (8, \u0026#39;历史\u0026#39;, \u0026#39;历老师\u0026#39;, 65, 7); COMMIT; 2 连接方式 #  2.1 内连接 #   内连接是使用比较运算符根据每个表共有的列的值匹配两个表中的行，根据比较运算符的不同可分为等值连接、非等值连接，在等值连接中把重复的属性列去掉则称为自然连接。  内连接实际上是先将两张表进行笛卡尔积，然后再从笛卡尔积中找出符合条件的数据。 内连接查询方式主要有两种，一种是使用where，一种是使用inner join，具体如下：  select ... from ... where table1.column1 \u0026lt; 比较运算符 \u0026gt; table2.column2; select ... from table1 inner join table2 on table1.column1 \u0026lt; 比较运算符 \u0026gt; table2.column2; 2.1.1 等值连接 #   内连接中当比较运算符为 =时称为等值连接。 例如：  mysql\u0026gt; select * from test.student stu inner join test.score sco on stu.id = sco.sid; +----+--------+------+------+----+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +----+--------+------+------+----+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 4 | 小五 | 女 | 6 | 5 | 化学 | 科老师 | 50 | 4 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | +----+--------+------+------+----+------------+-----------+--------+------+ 2.1.2 非等值连接 #   内连接中当比较运算符为非 = 时称为非等值连接。 例如：  mysql\u0026gt; select * from test.student stu inner join test.score sco on stu.id \u0026gt; sco.sid; +----+--------+------+------+----+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +----+--------+------+------+----+------------+-----------+--------+------+ | 2 | 小花 | 女 | 5 | 1 | 物理 | 王老师 | 90 | 1 | | 3 | 小名 | 女 | 6 | 1 | 物理 | 王老师 | 90 | 1 | | 4 | 小五 | 女 | 6 | 1 | 物理 | 王老师 | 90 | 1 | | 5 | 小星 | 男 | 30 | 1 | 物理 | 王老师 | 90 | 1 | | 3 | 小名 | 女 | 6 | 2 | 数学 | 李老师 | 70 | 2 | | 4 | 小五 | 女 | 6 | 2 | 数学 | 李老师 | 70 | 2 | | 5 | 小星 | 男 | 30 | 2 | 数学 | 李老师 | 70 | 2 | | 4 | 小五 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 5 | 小星 | 男 | 30 | 3 | 英语 | 黄老师 | 80 | 3 | | 5 | 小星 | 男 | 30 | 4 | 体育 | 易老师 | 99 | 4 | | 5 | 小星 | 男 | 30 | 5 | 化学 | 科老师 | 50 | 4 | | 3 | 小名 | 女 | 6 | 6 | 语文 | 张老师 | 88 | 2 | | 4 | 小五 | 女 | 6 | 6 | 语文 | 张老师 | 88 | 2 | | 5 | 小星 | 男 | 30 | 6 | 语文 | 张老师 | 88 | 2 | +----+--------+------+------+----+------------+-----------+--------+------+ 2.1.3 自然连接 #   在等值连接的基础上去掉重复的属性列称为自然连接。 例如 2.1.1 等值连接中的第一列的id 和最后一列的sid 表示的都是学号，在等值连接中会将两列保存为一列：  mysql\u0026gt; select stu.*, sco.id, sco.class_type, sco.teacher, sco.number from test.student stu inner join test.score sco on stu.id = sco.sid; +----+--------+------+------+----+------------+-----------+--------+ | id | name | sex | age | id | class_type | teacher | number | +----+--------+------+------+----+------------+-----------+--------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | | 4 | 小五 | 女 | 6 | 5 | 化学 | 科老师 | 50 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | +----+--------+------+------+----+------------+-----------+--------+ 2.2 外连接 #  外连接可分为三种，分别是左连接（left outer join/left join）、右连接（right outer join/right join）、全外连接（full outer join/full join）。\n2.2.1 左连接 #    左连接是指左边表的所有数据都显示出来，右边的表数据只显示共有的部分，没有对应的部分补 NULL。   例如：\nmysql\u0026gt; select * from test.student stu left join test.score sco on stu.id = sco.sid; +----+--------+------+------+------+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +----+--------+------+------+------+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 4 | 小五 | 女 | 6 | 5 | 化学 | 科老师 | 50 | 4 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | | 5 | 小星 | 男 | 30 | NULL | NULL | NULL | NULL | NULL | +----+--------+------+------+------+------------+-----------+--------+------+   2.2.2 右连接 #    右连接是指右边表的所有数据都显示出来，左边的表数据只显示共有的部分，没有对应的部分补 NULL。\n  例如：\nmysql\u0026gt; select * from test.student stu right join test.score sco on stu.id = sco.sid; +------+--------+------+------+----+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +------+--------+------+------+----+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 4 | 小五 | 女 | 6 | 5 | 化学 | 科老师 | 50 | 4 | | NULL | NULL | NULL | NULL | 7 | 地理 | 地老师 | 60 | 6 | | NULL | NULL | NULL | NULL | 8 | 历史 | 历老师 | 65 | 7 | +------+--------+------+------+----+------------+-----------+--------+------+   2.2.3 全外连接 #   全外连接是指左边和右边表的所有数据都显示出来，如果哪一边的表没有另一边表对应的部分，则补 NULL。 MySQL 不支持全外连接，不过可以使用左连接 UNION 右连接来实现。 例如： mysql\u0026gt; select * from test.student stu left join test.score sco on stu.id = sco.sid -\u0026gt; union -\u0026gt; select * from test.student stu right join test.score sco on stu.id = sco.sid; +------+--------+------+------+------+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +------+--------+------+------+------+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 4 | 小五 | 女 | 6 | 5 | 化学 | 科老师 | 50 | 4 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | | 5 | 小星 | 男 | 30 | NULL | NULL | NULL | NULL | NULL | | NULL | NULL | NULL | NULL | 7 | 地理 | 地老师 | 60 | 6 | | NULL | NULL | NULL | NULL | 8 | 历史 | 历老师 | 65 | 7 | +------+--------+------+------+------+------------+-----------+--------+------+   2.3 交叉连接 #   交叉连接会返回两张表的笛卡尔积。 例如： mysql\u0026gt; select * from test.student stu cross join test.score sco; +----+--------+------+------+----+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +----+--------+------+------+----+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 1 | 物理 | 王老师 | 90 | 1 | | 3 | 小名 | 女 | 6 | 1 | 物理 | 王老师 | 90 | 1 | | 4 | 小五 | 女 | 6 | 1 | 物理 | 王老师 | 90 | 1 | | 5 | 小星 | 男 | 30 | 1 | 物理 | 王老师 | 90 | 1 | | 1 | 小强 | 男 | 8 | 2 | 数学 | 李老师 | 70 | 2 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 3 | 小名 | 女 | 6 | 2 | 数学 | 李老师 | 70 | 2 | | 4 | 小五 | 女 | 6 | 2 | 数学 | 李老师 | 70 | 2 | | 5 | 小星 | 男 | 30 | 2 | 数学 | 李老师 | 70 | 2 | | 1 | 小强 | 男 | 8 | 3 | 英语 | 黄老师 | 80 | 3 | | 2 | 小花 | 女 | 5 | 3 | 英语 | 黄老师 | 80 | 3 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 5 | 小星 | 男 | 30 | 3 | 英语 | 黄老师 | 80 | 3 | | 1 | 小强 | 男 | 8 | 4 | 体育 | 易老师 | 99 | 4 | | 2 | 小花 | 女 | 5 | 4 | 体育 | 易老师 | 99 | 4 | | 3 | 小名 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 5 | 小星 | 男 | 30 | 4 | 体育 | 易老师 | 99 | 4 | | 1 | 小强 | 男 | 8 | 5 | 化学 | 科老师 | 50 | 4 | | 2 | 小花 | 女 | 5 | 5 | 化学 | 科老师 | 50 | 4 | | 3 | 小名 | 女 | 6 | 5 | 化学 | 科老师 | 50 | 4 | | 4 | 小五 | 女 | 6 | 5 | 化学 | 科老师 | 50 | 4 | | 5 | 小星 | 男 | 30 | 5 | 化学 | 科老师 | 50 | 4 | | 1 | 小强 | 男 | 8 | 6 | 语文 | 张老师 | 88 | 2 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | | 3 | 小名 | 女 | 6 | 6 | 语文 | 张老师 | 88 | 2 | | 4 | 小五 | 女 | 6 | 6 | 语文 | 张老师 | 88 | 2 | | 5 | 小星 | 男 | 30 | 6 | 语文 | 张老师 | 88 | 2 | | 1 | 小强 | 男 | 8 | 7 | 地理 | 地老师 | 60 | 6 | | 2 | 小花 | 女 | 5 | 7 | 地理 | 地老师 | 60 | 6 | | 3 | 小名 | 女 | 6 | 7 | 地理 | 地老师 | 60 | 6 | | 4 | 小五 | 女 | 6 | 7 | 地理 | 地老师 | 60 | 6 | | 5 | 小星 | 男 | 30 | 7 | 地理 | 地老师 | 60 | 6 | | 1 | 小强 | 男 | 8 | 8 | 历史 | 历老师 | 65 | 7 | | 2 | 小花 | 女 | 5 | 8 | 历史 | 历老师 | 65 | 7 | | 3 | 小名 | 女 | 6 | 8 | 历史 | 历老师 | 65 | 7 | | 4 | 小五 | 女 | 6 | 8 | 历史 | 历老师 | 65 | 7 | | 5 | 小星 | 男 | 30 | 8 | 历史 | 历老师 | 65 | 7 | +----+--------+------+------+----+------------+-----------+--------+------+   2.4 UNION #    UNION 可以把两次或多次查询结果合并起来。\n  两次或多次查询的列数必须一致，列的类型可以不一样，但推荐查询的每一列，相对应的类型一样。\n  多次 SQL 语句取出的列名可以不一致，此时以第一个 SQL 语句的列名为准。\n  如果不同语句取出的行有完全相同（每个列的值都相同），那么 UNION 会将相同的行合并，最终只保留一行，即 UNION 会去掉重复的行，如果不想去掉重复的行，可以使用UNION ALL。\n  例如：\nmysql\u0026gt; (select * from test.student stu -\u0026gt; left join test.score sco on stu.id = sco.sid -\u0026gt; limit 0,4) -\u0026gt; -\u0026gt; union -\u0026gt; -\u0026gt; (select * from test.student stu -\u0026gt; right join test.score sco on stu.id = sco.sid -\u0026gt; limit 0,4); +------+--------+------+------+------+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +------+--------+------+------+------+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | +------+--------+------+------+------+------------+-----------+--------+------+ 5 rows in set (0.00 sec) mysql\u0026gt; (select * from test.student stu -\u0026gt; left join test.score sco on stu.id = sco.sid -\u0026gt; limit 0,4) -\u0026gt; -\u0026gt; union all -\u0026gt; -\u0026gt; (select * from test.student stu -\u0026gt; right join test.score sco on stu.id = sco.sid -\u0026gt; limit 0,4); +------+--------+------+------+------+------------+-----------+--------+------+ | id | name | sex | age | id | class_type | teacher | number | sid | +------+--------+------+------+------+------------+-----------+--------+------+ | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | | 4 | 小五 | 女 | 6 | 4 | 体育 | 易老师 | 99 | 4 | | 1 | 小强 | 男 | 8 | 1 | 物理 | 王老师 | 90 | 1 | | 2 | 小花 | 女 | 5 | 2 | 数学 | 李老师 | 70 | 2 | | 2 | 小花 | 女 | 5 | 6 | 语文 | 张老师 | 88 | 2 | | 3 | 小名 | 女 | 6 | 3 | 英语 | 黄老师 | 80 | 3 | +------+--------+------+------+------+------------+-----------+--------+------+ 可以看出UNION 会去除重复的行，而UNION ALL 会保留重复的行。\n  参考文献 #    列举几种表连接方式？  mysql 系列-多表连接方式。  Mysql 的 4 种表连接方式。  数据库系统概论-第 5 版_完整版（王珊，萨师煊）  "},{"id":13,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.3-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/","title":"1.1.3 流量控制","section":"1.1 传输层： Tcp和 UDP","content":"流量控制 #  1 TCP如何实现流量控制 #    TCP使用滑动窗口协议来实现流量控制，防止发送方发送速率太快，接收方缓存区不够导致溢出。 接收方会维护一个接收窗口，其大小是根据自己的资源情况动态调整的，在返回ACK时将接收窗口大小放在 TCP报文中的窗口字段告知发送方。 发送窗口的大小不能超过接受窗口的大小，只有当发送方发送并收到确认之后，才能将发送窗口左移。 发送窗口的上限为接收窗口和拥塞窗口的最小值，接收窗口表明了接收方的接收能力，拥塞窗口表明了网络的传送能力。  2 什么是零窗口（接收窗口为0时会怎样） #   如果接收方没有能力接收数据，就会将接收窗口设置为0，这时发送方必须暂停发送数据，但是会启动一个持续计时器，到期后发送一个大小为1字节的探测数据包，以查看接收窗口状态。 如果接收方能够接收数据，就会在返回的报文中更新接收窗口大小，恢复数据传送。  "},{"id":14,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.3-Session%E4%B8%8ECookie%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"1.2.3 Session与 Cookie的区别","section":"1.2 应用层：HTTP和HTTPS","content":"Session与Cookie的区别 #   Session是服务器端保持状态的方案，Cookie是客户端保持状态的方案。 Cookie保存在客户端本地，客户请求服务器时会将Cookie一起提交；Session保存在服务端，通过检索Sessionid查看状态，保存Sessionid的方式可以采用Cookie，如果禁用了Cookie，可以使用URL重写机制（把Sessionid保存在URL中）。  "},{"id":15,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.4-%E7%BD%91%E7%BB%9C%E5%B1%82%E5%8D%8F%E8%AE%AE/1.4.3-%E4%BB%80%E4%B9%88%E6%98%AFARP%E5%8D%8F%E8%AE%AE/","title":"1.4.3 什么是 Arp协议","section":"1.4 网络层协议","content":"什么是ARP协议 #   ARP协议完成了IP地址和物理地址的映射。 每一个主机都设有一个ARP高速缓存，里面有所在局域网上的各主机和路由器的IP地址到硬件地址的映射表。 当源主机要发送数据包到目的主机时，会先检查自己的ARP高速缓存中有没有目的主机的MAC地址：  如果有，就直接将数据包发送到这个MAC地址。 如果没有，就向所在局域网发起一个ARP请求的广播包（在发送自己的ARP请求时，同样会带上自己的IP地址到硬件地址的映射）。   收到请求的主机检查自己的IP地址和目的主机IP地址是否一致，如果一致，则保存源主机的映射到自己的ARP缓存，然后给源主机发送一个ARP响应数据包。 源主机收到响应数据包之后，先添加目的主机的IP地址与MAC地址的映射，再进行数据传送。 如果源主机一直没有收到响应，表示ARP查询失败。 如果所要找的主机和源主机不在同一个局域网上，那么就要通过ARP找到一个位于本局域网上的某个路由器的硬件地址，然后把分组发给这个路由器，让这个路由器把分组转发给下一个网络，剩下的工作就由下一个网络来做。  "},{"id":16,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.3-%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/","title":"2.1.3 进程同步问题","section":"2.1 进程和线程","content":"进程同步问题 #   进程的同步是目的，而进程间通信是实现进程同步的手段。\n 1 管程 #  1.1 含义 #   管程将共享变量以及对这些共享变量的操作封装起来，形成一个具有一定功能接口的功能模块，这样只能通过管程提供的某个过程才能访问管程中的资源，使用完之后必须释放管程并唤醒入口等待队列中的进程。 正在管程中的线程可临时放弃管程的互斥访问，等待事件出现时恢复。 管程使用锁确保了在任何情况下管程中只有一个活跃的线程，即确保线程互斥访问临界区。 管程使用条件变量提供的等待队列实现线程间协作，当线程暂时不能获得所需资源时，进入队列等待，当线程可以获得所需资源时，从等待队列中唤醒。  1.2 组成 #  管程主要有三部分组成，具体如下：\n 一个锁： 控制管程代码的互斥访问。 入口队列： 每次只能有一个线程进入。 条件变量： 管理数据的并发访问。   1.3 条件变量 #   条件变量是管程内的等待机制。 每个条件变量表示一种等待原因，对应一个等待队列。 条件变量有两种操作：  wait() 操作：  将自己阻塞在队列中。 唤醒一个等待者或者释放管程的互斥访问（即允许另外一个线程进入管程）。   signal() 操作：  将等待队列中的一个线程唤醒。 如果等待队列为空，这就相当于是一个空操作。      1.4 语义 #  事实上管程一共有三种语义：\n Hoare 语义。 Mesa 语义。 Hansen 语义。  1.4.1 Honare 语义 #   当一个进程试图进入管程时，会首先在入口队列等待。 如果P 进程唤醒了 Q 进程，则Q 进程先执行，P 在紧急队列中等待。 执行wait操作的进程进入条件变量链末尾，唤醒紧急等待队列或者入口队列中的进程。 执行signal操作时会唤醒条件变量链中的进程，自己进入紧急等待队列，若条件变量链为空，则继续执行。  1.4.2 Mesa 语义 #   将 Honare 中的 signal 换成了 notify（或者broadcast 通知所有满足条件的），进行通知而不是立马交换管程的所有权。 在合适的时候，条件队列首位的进程可以进入，但进入之前必须用 while 检查条件是否合适。  2 临界区的概念 #  临界区是指各个进程中对临界资源进行操作的程序片段。\n 临界资源是什么？\n临界资源是指互斥共享变量所代表的的资源，即一次只能被一个进程使用的资源。\n 3 同步与互斥的概念 #   同步： 多个进程因为合作而使得进程的执行有一定的先后顺序，比如某个进程需要另一个进程提供的消息，获得消息之前进入阻塞态。 互斥： 多个进程在同一个时刻只有一个进程能进入临界区。  4 并发和并行的区别 #   并发： 一段时间里运行多个程序或者是任务，但并不是同一时间运行多个程序或者是任务，只是这段时间里 CPU 分开安排了任务，不同的时间 CPU 执行不同的程序或任务。 并行： 当系统有一个以上的 CPU 时，每个 CPU 执行一个任务，线程之间不抢占 CPU 的资源，可以同时进行。   多线程是并发执行的一段代码，是实现异步的手段。\n 5 同步和异步的区别 #   同步： 发出一个功能调用，在没有得到结果之前，该调用就不返回或继续执行后续操作。 异步： 发出一个功能调用后，不等待返回结果，直接开始后续的操作，一般通过状态、通知和回调来通知调用者。  6 参考文献 #    进程同步问题。  《操作系统》第 18 讲：“信号量与管程”总结。  管程(Moniter) 并发编程的基本心法。  Java 并发编程中的管程（Monitor）模型。  并发并行和同步异步的概念。  "},{"id":17,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/2.3.3-%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95/","title":"2.3.3 页面置换算法","section":"2.3 内存管理","content":"页面置换算法 #  1 含义 #   在程序运行过程中，如果要访问的页面不在内存中，就会发生缺页中断从而将该页调入内存中。 此时如果内存中已无空闲空间，系统必须从内存中调出一个页面到磁盘中来腾出空间。 页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。  2 分类 #  2.1 最佳置换算法（Optimal Replacement Algorithm, OPT） #   最佳置换算法的基本思想是置换以后不需要或者最远的将来才需要的页面，这样可以保证获得最低的缺页率。 但是由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间内不再访问的，因而该算法是一种理论上的算法，也是最优的一种算法。  2.2 先进先出法（FIFO） #   先进先出法会优先置换在内存中驻留时间最长的页面。 该算法会产生贝拉迪（Belay）异常，也就是当所分配的物理块数增大而页故障数不减反增的异常现象。 该算法的缺点是将那些经常被访问的页面置换出，从而使缺页率升高。  2.3 第二次机会置换法（Second Chance Replacement, SCR） #   该算法是对 FIFO 算法的改进，每个页面访问 2 次后再淘汰。 具体实现上是设置页面访问位，每次检查队首的页面访问位：  如果该位为 0，则将该页置换出。 如果该位为 1，将该位设置为 0，然后将其移到队尾，看成新装入的页。   该算法的优点是能在一定程度上避免把经常使用的页面置换出去。   2.4 时钟置换法（Clock） #   该算法是对第二次机会置换法的改进。 第二次机会置换法需要在链表中移动页面，而时钟置换法将页面保存在环形链表中，只需要后移队头指针，就相当于是把原来的队头放队尾了。 该算法的优点是避免了移动链表节点的开销。   2.5 最近最少使用法（Least Recently Used, LRU） #  2.5.1 原理 #   优先置换最久未被访问的页面。 根据局部性原理，一个进程在一段时间内要访问的指令和数据都集中在一起，如果一个页面很久没有被访问，那么将来被访问的可能性也比较小。  2.5.2 实现 #  2.5.2.1 单链表 #   该算法最常见的实现是使用一个链表保存缓存数据，具体如下：  将新数据插入到链表头部。 每当缓存命中（即缓存数据被访问），则将数据移到链表头部。 当链表满的时候，将链表尾部的数据丢弃。    2.5.2.2 基于HashMap和双向链表 #    整体的设计思路是使用HashMap存储 key，这样可以做到 save(key)和 get(key)的时间都是$O(1)$，HashMap的 value指向双向链表实现的LRU的Node节点，如下图所示：   LRU存储是基于双向链表的，其中 head代表双向链表的表头，tail代表双向链表的尾部，首先预先设置LRU的容量，如果存储满了，可以通过$O(1)$的时间淘汰掉双向链表的尾部，每次新增和访问数据，都可以通过$O(1)$的效率把新的节点增加到头部，或者把已经存在的节点移动到头部。\n  下图展示了预设大小为3的LRU存储和访问过程中的变化，为了简化图复杂度，图中没有展示HashMap的变化，仅仅展示了LRU中双向链表的变化：  s = save，g = get\n   核心的操作步骤如下：\n save(key, value)：首先在HashMap中找到 key对应的节点，如果节点存在，更新节点的值，并把这个节点移动到队头，如果不存在，需要构造新的节点，并且尝试把节点塞到队头，如果LRU空间不足，则通过 tail淘汰尾部的节点，同时在HashMap中移除 key。 get(key)：通过HashMap找到LRU链表节点，因为根据LRU原理，这个节点是最新访问的，所以需要把节点插入到头部，然后返回缓存的值。    2.5.3 优缺点 #  2.5.3.2 优点 #   实验证明LRU 的性能较好，能够降低置换频率。  2.5.3.1 缺点 #   该算法的缺点是存在缓存污染问题，即由于偶发性或周期性的冷数据批量查询，热点数据被挤出去，导致缓存命中率下降。  2.6 LRU-K #  2.6.1 原理 #   LRU-K 中的K 代表最近的使用次数，因此 LRU 可以认为是 LRU-1。 LRU 算法中因为仅访问一次就能替代别人，可能会造成“缓存污染”问题，因此提出了 LRU-K 的概念，LRU-K 其主要目的就是为了解决 LRU 算法“缓存污染”的问题。 LRU-K 的核心思想是将“最近使用过 1 次”的判断标准扩展为“最近使用过 K 次”。 与 LRU 算法不同，LRU-K 算法需要维护两个队列，分别是历史队列和缓存队列：  历史队列：  历史队列保存着每次访问的页面，当页面访问次数达到了 K 次，该页面出栈，并保存至缓存队列。 若尚未达到 K 次则继续保存，直至历史队列也满了，那就根据一定的缓存策略（FIFO、LRU、LFU）进行淘汰。   缓存队列：  缓存队列则是保存已经访问 K 次的页面，当该队列满了之后，则淘汰最后一个页面，也就是第 K 次访问距离现在最久的那个页面。      2.6.2 实现 #    数据第一次被访问，添加到历史队列中。 当历史队列中的页面满了，根据一定的缓存策略（FIFO、LRU、LFU）淘汰老的页面。 当历史队列中的某个页面第 K 次访问时，该页面从历史队列中出栈，并存放至缓存队列。 缓存队列中的页面再次被访问 K 次时，历史队列中该页面出栈，并且更新缓存队列中该页面的位置。  2.6.3 优缺点 #  2.6.3.1 优点 #   LRU-K降低了“缓存污染”带来的问题，命中率比 LRU 要高。  2.6.3.2 缺点 #   LRU-K 是一个优先级队列，算法复杂度和代价比较高。 由于 LRU-K 还需要记录那些被访问过、但还没有放入缓存的对象，因此内存消耗会比 LRU 要多，当数据量很大的时候，内存消耗会比较可观。 LRU-K需要基于时间进行排序，CPU 消耗比 LRU 要高。  2.7 最不经常使用法（Least Frequently Used, LFU） #   最不经常使用法使用一个计数器来记录条目被访问的频率，当队列满的时候，优先淘汰使用次数最少的元素。 该算法的优点是能够避免缓存污染问题对 LRU 命中的影响，因为在淘汰元素的时候是根据一定时间内的使用次数来决定的，所以短时间的冷数据查询不一定会导致热点数据被淘汰，进而避免缓存污染问题。 LFU 的缺点：在短期的时间内，对某些缓存的访问频次很高，这些缓存会立刻晋升为热点数据，而保证不会淘汰，这样会驻留在系统内存里面，而实际上，这部分数据只是短暂的高频率访问，之后可能长期不会访问，这样就会导致一些新加入的缓存很容易被很快删除，因为他们的引用频率很低。   LRU 和 LFU 的区别？\nLRU 和 LFU 的侧重点不同，LRU 主要体现在对元素的使用时间上，LFU 主要体现在对元素的使用频次上。\n  3 颠簸现象 #  3.1 原因 #   颠簸本质上是指频繁的页调度行为。 进程发生缺页中断时必须置换某一页，然后，其他所有的页都在使用，他置换一个页，但又立刻再次需要这个页，因此会不断产生缺页中断，导致整个系统的效率急剧下降，这种现象称为颠簸。  3.2 解决方法 #   修改页面置换算法。 降低同时运行的程序的数量。 终止该进程或增加物理内存容量。  4 参考文献 #    有哪些页面置换算法？  操作系统学习(10)页面置换算法。  LRU——缓存淘汰算法。  【面试题】技术面试题汇总。  LRU 进阶之 LRU-K 和 2Q。  LRU-K 和 2Q 缓存算法介绍。  常用缓存淘汰算法（LFU、LRU、ARC、FIFO、MRU）。  LFU 的基本原理与实现。  LRU原理和Redis实现——一个今日头条的面试题。  "},{"id":18,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.6-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.6.3-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/","title":"1.6.3 存储过程","section":"1.6 基础知识","content":"存储过程 #  1 含义 #   存储过程是在数据库中定义一些完成特定功能的 SQL 语句的集合，经过编译后存储在数据库中。 存储过程可以包含流程控制语句以及各种 SQL 语句，他可以接受参数、输出参数。  2 优缺点 #  2.1 优点 #  在 MySQL 中使用存储过程，而不是用存储在客户端计算机本地的 SQL 程序，相比有以下几点优点：\n 增强了 SQL 语言的功能和灵活性：  存储过程可以使用流程控制语句编写，有很强的的灵活性，可以完成复杂的判断和较复杂的计算。   允许标准组件式变成：  存储过程被创建后，可以在程序中被多次调用，而不用重新编写该存储过程的SQL语句。 数据库专业人员可以随时对存储过程进行修改，对应用程序代码毫无影响。   能实现较快的执行速度：  如果某一操作包含大量的SQL代码或被分多次执行，那么存储过程要比批处理的执行速度快很多，因为存储过程是预编译的，在首次运行一个存储过程时，查询优化器会对其进行分析优化，并且给出最终被存储在系统表中的执行计划，而批处理的SQL语句在每次运行时都要进行编译和优化，速度相对较慢。   减少网络流量：  针对同一数据库对象的操作（比如查询、修改），如果这一操作所涉及的SQL语句被组织成存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大减少了网络流量并降低了网络负载。   可被当做一种安全机制来充分利用：  系统管理员通过对执行某一存储过程的权限进行限制，进而实现对响应数据的访问权限的限制，避免了非授权用户对数据的访问。    2.2 缺点 #   可移植性差： 存储过程将应用程序绑定到了数据库上。 开发调试复杂： 没有好的IDE。 修改复杂： 需要重新编译，有时候还需要更新程序中的代码以更新调用。  参考文献 #    什么是存储过程？有哪些优缺点？  数据库原理之存储过程和函数。  "},{"id":19,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.4-%E4%BB%8E%E8%BE%93%E5%85%A5%E7%BD%91%E5%9D%80%E5%88%B0%E8%8E%B7%E5%BE%97%E9%A1%B5%E9%9D%A2%E7%9A%84%E8%BF%87%E7%A8%8B%E8%B6%8A%E8%AF%A6%E7%BB%86%E8%B6%8A%E5%A5%BD/","title":"1.2.4 从输入网址到获得页面的过程（越详细越好）","section":"1.2 应用层：HTTP和HTTPS","content":"从输入网址到获得页面的过程（越详细越好） #   域名解析，即浏览器查询 DNS，获取域名对应的 IP 地址，具体可参考 1.2.8 DNS。 建立 TCP 连接，即浏览器获得域名对应的 IP 地址以后，向服务器请求建立链接，发起三次握手，这里使用五层协议更详细的描述如何建立这个 TCP 连接的：  使用应用层发起 HTTP 请求，这个可以根据我们本身输入的 URL 访问时，用的什么协议就发起对应协议去进行请求。 然后是传输层的 TCP 协议为传输报文提供可靠的字节流服务，这里使用了TCP 三次握手。 接着是网络层把 TCP 分隔好的各种数据包传送给接收方，而且要保证确实能传到接收方还需要接收方的 MAC 地址，也就是物理地址。 最后是链路层将数据发送到数据链路层传输。 至此，请求报文已发出，客户端发送请求的阶段结束。 然后是服务端接收请求处理阶段，将数据按照原路进行处理，即 $ 链路层 \\rightarrow 网络层 \\rightarrow 传输层 \\rightarrow 应用层 $，然后响应客户端的发送报文。   TCP/IP 链接建立起来后，浏览器向服务器发送 HTTP 请求。 服务器收到这个请求之后，根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器。 浏览器解析并渲染视图，若遇到对 js、css 及图片等静态资源的引用时，则重复上述步骤并向服务器请求这些资源。 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面  参考文献 #    HTTP 和 HTTPS 有什么区别？  （3）字节跳动面试题：从输入网址到获得页面的网络请求的过程，请详细说一下。  "},{"id":20,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.4-%E7%BD%91%E7%BB%9C%E5%B1%82%E5%8D%8F%E8%AE%AE/1.4.4-NAT%E5%8D%8F%E8%AE%AE/","title":"1.4.4 Nat协议","section":"1.4 网络层协议","content":"NAT协议 #  1 什么是NAT协议 #   NAT，Network Address Translation，即网络地址转换协议。 主要用于解决内网中的主机要和因特网上的主机通信，由NAT路由器将主机的本地IP地址转换为全球IP地址。 NAT不仅能解决IP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 NAT主要分为两种，一种是静态NAT，一种是动态NAT，具体如下：  静态NAT： 将内部网络中的每个主机都永久映射成外部网络中的某个合法的地址，多用于服务器。 动态NAT： 在外部网络中定义了一个或多个合法地址，采用动态分配的方法映射到内部网络。    2 参考文献 #    什么是NAT (网络地址转换)？  网络地址转换协议NAT功能详解及NAT基础知识介绍。  "},{"id":21,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.4-%E8%BF%9B%E7%A8%8B%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D%E7%8A%B6%E6%80%81/","title":"2.1.4 进程有哪几种状态","section":"2.1 进程和线程","content":"进程有哪几种状态 #   1 进程的三种状态 #   就绪状态： 进程获得了除处理器之外的一切所需资源，一旦得到处理器即可运行。 运行状态： 进程正在处理器上运行，在单处理器环境下，每个时刻最多只有一个进程处于运行状态。 阻塞状态： 进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理器）或等待输入输出完成，此时即使处理器空闲，该进程也不能运行。  2 进程三种状态之间的切换 #   当一个就绪进程获得处理器时，其状态就由就绪变为运行。 当一个运行进程被剥夺处理器时，如用完系统分给他的时间片、出现更高优先级别的其他进程，其状态由运行变为就绪。 当一个运行进程因为某件事情受阻时，如所申请资源被占用、启动I/O传输未完成，其状态就会由运行变为阻塞。 当一个阻塞进程所等待事情发生时，如得到申请资源、I/O传输完成，其状态就会由阻塞变为就绪。  3 参考文献 #    进程有哪几种状态？  进程的几种状态。  "},{"id":22,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/2.3.4-%E5%B1%80%E9%83%A8%E6%80%A7%E5%8E%9F%E7%90%86/","title":"2.3.4 局部性原理","section":"2.3 内存管理","content":"局部性原理 #   一个优秀的程序、优美的代码，往往具有良好的局部性。\n 1 什么是局部性原理 #  1.1 原理 #   程序局部性原理是指程序在执行时呈现出局部规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分；相应的，执行所访问的存储空间也局限于某个内存区域。 局部性通常由两种形式，分别是时间局部性和空间局部性：  时间局部性： 被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。 空间局部性： 如果一个存储器的位置被引用，那么他附近的位置也会被引用。   局部性可以提高程序的运行效率。  1.2 示例 #  //求数组元素之和，v为数组名，n为数组大小， int sum(int *v, int n) { int i = 0; int sum = 0; for (i=0; i\u0026lt;n; ++i) { sum+=v[i]; } return sum; }   数组在内存中是按照上图的方式连续存放的。 对于sum变量：  具有良好的时间局部性，因为在for循环结束之前，每次执行循环体都有对 sum的访问， 没有空间局部性，因为sum是标量，通过 sum这个地址只能得到一个值。   对于v变量：  具有良好的空间局部性，因为数组v是按照顺序存放在内存中，每次访问 v[i]总是在 v[i-1]的下一个位置。 没有时间局部性，因为在循环体中，每个元素 v[i]只会被访问一次。    2 为什么局部性可以提高程序的运行效率 #  2.1 计算机存储结构 #    在计算机系统中，存储设备都被组织成了一个存储器层次结构，如上图所示。 越往上，存储器的容量越小、成本越高、速度越快。   最开始的时候，计算机存储器层次只有三层，分别是CPU 寄存器、DRAM 主存以及磁盘存储，那么为什么后来弄得这么复杂了呢？\n 这是因为CPU 和主存之间存在着巨大速度差异，作为核心的CPU 处理数据的速度极快，内存跟不上。 因此，系统设计者被迫在CPU 寄存器和主存之间插了一个小的 SRAM 高速缓存存储器，称为L1 缓存，大约可以在 2-4 个时钟周期内访问。 后来发现L1 高速缓存和主存之间还是有较大差距，又在L1 高速缓存和主存之间插入了速度稍微慢点的 L2 缓存，大约可以在 10 个时钟周期内访问。 于是，再这样的模式下不断演变，最终形成了现在的存储体系。   2.2 缓存 #  2.2.1 什么是缓存 #  2.2.1.1 原理 #   存储器层次结构的主要思想是上一层的存储器作为低一层存储器的高速缓存，因此寄存器 L0 就是 L1 的高速缓存，L1 是 L2 的高速缓存，L2 是 L3 的高速缓存，L3 是主存的高速缓存，而主存又是磁盘的高速缓存。 也就是说，对于每个 k，位于 k 层的更快更小的存储器设备作为第 k+1 层的更大更慢存储设备的缓存，也就是说，k 层存储了 k+1 层中经常被访问的数据。 在缓存之间，数据是以块为单位传输的，不同层次的缓存，块的大小会不同，一般来说越往上，块越小。  2.2.1.2 示例 #    如上图所示，k 是 k+1 的缓存，k 中缓存了 k+1 中块编号为 4、9、14、3 的数据。 他们之间的数据传输是以块为单位的，当程序需要这些块中的数据时，可直接从缓存 k 中得到，这比从 k+1 层读数据要快。  2.2.3 缓存命中和缓存失效 #   缓存命中（Cache Hit）：  当程序需要第 k+1 层中的某个数据时，会首先在他的缓存 k 层中寻找，如果数据刚好在 k 层中时，就称为缓存命中。 如在上图中，若程序访问 k+1 层的 4，则先去其缓存层 k 去找，4 恰好在 k 层，因此发生了缓存命中。   缓存失效（Cache Invalidation）：  缓存失效也称为缓存不命中，当需要的数据对象不在缓存中时，称为缓存不命中。 当发生缓存不命中时，CPU 会直接从 k+1 层取出包含数据对象的那个块，然后需要将其再缓存到 k 层，以便下次再访问时就能直接从缓存层 k 中取到。 对缓存失效的数据被内存获取后再存入到 k 层的缓存中时，如果此时k 层的缓存已经放满的话，就会置换其中一个块，至于置换哪一个块，这是由缓存中的置换策略决定的，具体可参考 2.3.3 页面置换算法。    2.2.4 程序局部性如何影响程序性能 #  2.2.4.1 原理 #   利用时间局部性： 由于时间局部性，同一个数据对象会多次被使用，一旦一个数据对象从 k+1 层进入到 k 层的缓存中，就希望他多次被引用，这样能节省很多访问造成的时间开支。 利用空间局部性： 假设缓存 k 能存 n 个数据块，在对数组访问的时候，由于数组是连续存放的，对第一个元素访问的时候，会把第一个元素后面的一共 n 个元素（缓存以块为单位传输）拷贝到缓存 k 中，这样在对第二个元素到第 n 个元素的访问时就可以直接从缓存里获取，从而提高性能。  2.2.4.2 示例 #    为了阐述方便，作出如下假设：  缓存每次只能缓存一块，一块大小只能放 3 个 int 类型数据。 数组 a 为一个两行三列的数组，即 a = int[2][3]。   下面我们看一下按行访问和按列访问分别会发生什么：  按行访问：  开始时访问第一行，首先访问a[0][0]，直接从内存读取，然后将a[0][0] 及其后面的两个元素a[0][1]、a[0][2] 缓存到缓存中。 接着访问第一行的其他元素a[0][1] 和a[0][2]，由于这两个元素已经在缓存中存在了，所以直接从缓存中读取即可。 第二行的访问方式和第一行的访问方式一样。 因此，对于整个数组 6 个元素的访问，访问了 2 次内存，缓存命中了 4 次。   按列访问：  开始时访问第一列，首先访问a[0][0]，直接从内存读取，然后将a[0][0] 及其后面的两个元素a[0][1]、a[0][2] 缓存到缓存中。 接着访问第一列的第二个元素a[1][0]，由于其没有在缓存中，所以也需要从内存中读取，然后将a[1][0] 及其后面的两个元素a[1][1]、a[1][2] 缓存到缓存中。 第一列的第三个元素和其他两个元素的访问方式一样，第二列和第一列的访问方式一样。 因此，对于整个数组 6 个元素的访问，访问了 6 次内存，缓存命中了 0 次，效率自然比按行访问低了。      3 参考文献 #    程序局部性原理介绍。  【底层原理】从缓存来看局部性提高程序运行效率的原因。  "},{"id":23,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.6-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.6.4-DELETETRUNCATE%E5%92%8CDROP%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"1.6.4 Delete、 Truncate和 Drop的区别","section":"1.6 基础知识","content":"DELETE、TRUNCATE和DROP的区别 #  1 DELETE #   操作可以针对 table，也可以针对 view。 仅删除表中的数据。 数据库操作语言（DML），删除时是每次从表中删除一行，所以会很慢，操作会放到rollback segment，并且会将其作为事务记录在日志中保存以便进行回滚操作，事务提交之后才生效。 如果有相应的触发器，执行的时候会被触发。 不会减少表或索引所占用的空间。  2 TRUNCATE #   只能针对 table。 删除表中的所有行，但表结构、列、约束、索引等保持不变，新行标识所用的计数值重置为该列的种子。 数据库定义语言（DDL），操作立即生效，原数据不放到 rollback segment中，不能回滚。 操作不能触发触发器。 会将表和索引所占用的空间恢复到初始大小。 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 不能对以下表使用truncate：  由 FOREIGN KEY约束引用的表。 参与索引视图的表。 通过使用事务复制或合并复制发布的表。    3 DROP #   只能针对 table。 删除表结构、列、约束、触发器、索引，依赖于该表的存储过程和函数将被保留，但是会变为 invalid状态。 数据库定义语言（DDL），操作立即生效，原数据不放到 rollback segment中，不能回滚。 操作不能触发触发器。 会将表占用的空间全释放掉。  参考文献 #    Drop/Delete/Truncate的区别？  SQL中drop、truncate和delete的区别。  "},{"id":24,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.5-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","title":"1.1.5 拥塞控制","section":"1.1 传输层： Tcp和 UDP","content":"拥塞控制 #  1 TCP 的拥塞控制是怎么实现的 #   拥塞控制主要由四个算法组成：慢启动（Slow Start）、拥塞避免（Congestion Voidance）、快重传（Fast Retransmit）、快恢复（Fast Recovery）：\n 慢启动： 刚开始发送数据时，先把拥塞窗口（Congestion Window）设置为 1 个最大的报文段MSS 的数值，每收到 1 个新的确认报文之后，就把拥塞窗口加 1 个MSS，这样每经过一个传输轮次（或者说每经过一个往返时间RTT），拥塞窗口的大小就会加倍。 拥塞避免： 当拥塞窗口的大小达到慢开始门限（Slow Start Threshold）时，开始执行拥塞避免算法，拥塞窗口大小不再指数增加，而是线性增加，即每经过一个传输轮次，只增加 1 个MSS。  无论在慢开始阶段还是拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限设置为出现拥塞时的发送方窗口的一半（但不能小于 2），然后把拥塞窗口重新设置为 1，执行慢开始算法。\n  快重传： 快重传要求接收方在收到一个失序的报文段后就立即发出重复确认，为的是使发送方尽早知道有报文段没有到达对方，而不要等到自己发送数据时捎带确认，快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复： 当发送方连续收到三个重复确认时，就把慢开始门限减半，然后执行拥塞避免算法，不执行慢开始的原因是如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方认为现在网络可能没有出现拥塞。  2 参考文献 #    TCP 的拥塞控制是怎么实现的？  "},{"id":25,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.5-HTTP%E8%AF%B7%E6%B1%82%E6%9C%89%E5%93%AA%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8A%B6%E6%80%81%E7%A0%81/","title":"1.2.5 Http请求有哪些常见的状态码","section":"1.2 应用层：HTTP和HTTPS","content":"HTTP请求有哪些常见的状态码 #    2xx： 操作成功，200 OK。\n  3xx： 重定向，301 永久重定向，302 暂时重定向。\n  4xx： 客户端错误，400 Bad Request，401 Unauthorized，403 Forbidden；404 Not Found。\n 401 Unauthorized：用来表示缺失或错误的认证，可以修改后重试。\n403 Forbidden：用户认证后，权限不足，无法对该资源进行操作。\n   5xx： 服务端错误，500 服务器内部错误，501 服务不可用。\n  参考资料 #    HTTP请求有哪些常见状态码？  HTTP 状态码 401 和 403 的区别。  "},{"id":26,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.5-%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/","title":"2.1.5 进程调度策略","section":"2.1 进程和线程","content":"进程调度策略 #  1 批处理系统 #  1.1 先来先服务（first-come first-service，FCFS） #  1.1.1 含义 #   先来先服务调度算法是一种非抢占式的算法，先进入就绪队列的进程，先分配处理器运行。 一个进程一旦占有了处理器，他就一直运行下去，直到该进程完成工作或者因为等待某事件发生而不能继续运行时才释放处理器。  1.1.2 优缺点 #  1.1.2.1 优点 #   非抢占式，开销小，无饥饿问题。  1.1.2.2 缺点 #   响应时间不确定，可能很慢。 对长进程有利，对短进程不利。 对CPU 繁忙型进程有利，对I/O 繁忙型进程不利。   为什么 FCFS 对 I/O 繁忙型进程不利？\n 因为每次进行 I/O 操作都会进入阻塞态，I/O 完成之后变成就绪态，会重新进入就绪队列队尾。 因为是 FCFS，所以会重新排队。 然而每次调度这个进程只会运行很短时间就又需要 I/O，然后又到队尾，如此往复，所以效率很低。   1.2 最短作业优先（shortest job first，SJF） #  1.2.1 含义 #   最短作业优先调度算法将每个进程与其下次 CPU 执行时间关联起来，当 CPU 变为空闲时，他会被赋予给具有最短 CPU 执行的进程。 如果两个进程具有同样长度的 CPU 执行时间，那么可以由FCFS处理。 SJF 算法是对 FCFS 算法的改进，其目标是减少平均周转时间。  1.2.2 优缺点 #  1.2.2.1 优点 #   比 FCFS改善平均周转时间，缩短作业的等待时间。 非抢占式、提高系统的吞吐量。  1.2.2.2 缺点 #   对长作业非常不利，可能长时间得不到执行，会产生饥饿问题。 未能依据作业的紧迫程度来划分执行的优先级。 难以准确估计作业的执行时间，从而影响调度性能。  1.3 最短剩余时间优先（Shortest Remaining Time Next，SRTN） #  1.3.1 含义 #   该算法是 SJF 算法抢占式版本， 当一个进程正在执行时，另一个新的进程进入就绪状态，如果新进程需要的 CPU 执行时间比当前正在执行的进程剩余下来还需的 CPU 时间短，SRTF 会强行赶走当前正在执行的进程。  1.3.2 优缺点 #  1.3.2.1 优点 #   吞吐量高。 提供好的响应时间。  1.3.2.2 缺点 #   可能导致饥饿问题（长作业长期得不到调度），对长进程不利。  1.4 最高响应比优先（Highest Respnse Ratio Next） #  1.4.1 含义 #   最高响应比优先调度算法是一种对 CPU 中央控制器响应比的分配的一种算法。 他是介于先来先服务算法和短作业优先算法之间的折中算法，既考虑了作业等待时间，又考虑了作业运行时间，既照顾短作业又不使长作业等待时间过长，改进了调度性能。 响应比的计算公式如下：  $$ 响应比 = 1 + \\frac{等待时间}{处理时间} $$\n1.4.2 优缺点 #  1.4.2.1 优点 #   同时考虑了等待时间的长度和估计需要的执行时间长短，很好地平衡了长短进程。 非抢占、吞吐量高，提供好的响应时间。 无饥饿问题。  1.4.2.2 缺点 #   响应比计算可能会增加部分系统开销。  2 交互式系统 #   交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速的进行响应。\n 2.1 时间片轮转（Round Robin） #  2.1.1 含义 #   在早期的时间片轮转法中，系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把 CPU 分配给队首进程，并令其执行一个时间片，时间片的大小从几毫秒到几百毫秒。 当执行的时间片用完时，由一个计时器发出时钟中断请求，调度程序便据此信号来停止该进程的执行，并把它送往就绪队列的末尾。 然后，再把处理机分配给就绪队列中新的队首进程，同时也让他执行一个时间片，这样就可以保证就绪队列中的所有进程在一给定的时间内均能获得一时间片的处理机执行时间，换言之，系统能在给定的时间内响应所有用户的请求。  2.1.2 时间片大小的确定 #   在时间片轮转算法中，时间片的大小对系统性能有很大的影响。 如果选择很小的时间片将有利于短作业，因为他能较快的完成，但会频繁的发生中断、进程上下文的切换，从而增加系统的开销。 反之，如果选择太长的时间片，使得每个进程都能在一个时间片内完成，时间片轮转算法便退化为 FCFS 算法，无法满足交互式用户的需求。 一个较为可取的大小是，时间片略大于一次典型的交互所需的时间，这样可使大多数进程在一个时间片内完成。  2.1.3 优缺点 #  2.1.3.1 优点 #   抢占式、开销小、无饥饿问题。  2.1.3.2 缺点 #   如果时间片小，进程频繁切换吞吐量低。 如果时间片太长，实时性得不到保证。  2.2 优先级调度算法 #  2.2.1 分类 #  优先级调度算法主要分为两类，分别为非抢占式优先权调度算法和抢占式优先权调度算法。\n2.2.1.1 非抢占式优先权调度算法 #   系统一旦把处理机分配给优先权最高的进程后，便一直执行下去，直至完成。  2.2.1.2 抢占式优先权调度算法 #   只要系统出现一个新的就绪进程，就进行优先权比较。 若出现优先权更高的进程，则立即停止当前正在执行的进程，并将处理机分配给新到的优先权最高的进程。  2.2.2 优先权类型 #  2.2.2.1 静态优先权 #   静态优先权在创建进程时确定，且在进程的整个运行期间保持不变。 确定静态优先权的依据：  进程类型： 系统进程的优先权一般高于用户进程。 进程对资源的需求： 执行时间及内存需求量少的进程优先权一般更高一些。 用户要求： 紧迫程度较高的进程的优先权一般较高。    2.2.2.2 动态优先权 #   进程执行时，优先级相同的按照FCFS 算法来执行，优先级不同的按照优先级的高低来执行，优先级高的先执行。 每执行一次，优先级下降一次，每等待一次，优先级上升一次。  2.3 多级反馈队列调度算法（Multilevel Feedback Queue） #  2.3.1 含义 #    多级反馈队列算法会设置多个就绪队列，优先级逐渐递减，时间片会逐渐递增。 进程在进入待调度的队列等待时，首先进入优先级最高的队列等待。 CPU 在调度时，首先会调度优先级最高的队列中的进程，若高优先级队列中没有调度的进程，则调度次优先级队列中的进程。 对于同一队列中的各个进程，按照 FCFS 分配时间片调度，如果一个进程在该队列对应的时间片范围内还没有执行完成，则将其移动到下一个优先级队列的末尾。 在最后一个优先级队列中的各个进程，按照时间片轮转分配时间片进行调度。 在低优先级的队列的进程，又有新到达的作业，此时必须立即把正在运行的进程放回当前队列的队尾，然后把处理机分配给高优先级进程，当再次运行到当前对立的该进程时，仅分配上次还未完成的时间片，不再分配该队列对应的完整时间片。  2.3.2 优缺点 #  2.3.2.1 优点 #   兼顾长短作业，有较好的响应时间，可行性强，适用于各种作业环境。  3 优先级反转 #  3.1 含义 #   高优先级的任务被低优先级任务阻塞，导致高优先级任务迟迟得不到调度，但其他中等优先级的任务却能抢到 CPU 资源。 从现象上来看，好像是中优先级的任务比高优先级任务具有更高的优先权。  3.2 实例 #   假定一个进程中有三个线程 Thread1（高）、Thread2（中）、Thread3（低），考虑下图的执行情况。  T0 时刻，Thread3 运行，并获得同步资源SYNCH1。 T1 时刻，Thread2 开始运行，由于优先级高于Thread3，Thread3 被强占（未释放同步资源 SYNCH1）。 T2 时刻，Thread1 抢占Thread2。 T3 时刻，Thread1 需要同步资源SYNCH1，但SYNCH1 被更低优先级的Thread3 所拥有，Thread1 被挂起等待该资源。 而此时线程Thread2 和Thread3 都处于可运行状态，Thread2 的优先级大于Thread3 的优先级，Thread2 被调度执行，最终的结果是高优先级的Thread1 迟迟无法得到调度，而中优先级的Thread2 却能抢到CPU 资源。   上述现象中，优先级最高的Thread1 要得到调度，不仅需要等Thread3 释放同步资源（这个很正常），而且还需要等待另外一个毫不相关的中优先级线程Thread2 执行完成（这个就不合理了），会导致调度的实时性很差。   3.3 解决方法 #  3.3.1 优先级继承 #  3.3.1.1 含义 #   优先级继承是指当高优先级进程（$P_1$）请求一个已经被低优先级进程（$P_3$）占有的临时资源时，将低优先级进程（$P_3$）的优先级临时提升到与高优先级进程（$P_1$）一样的级别，使得低优先级进程能更快的运行，从而可以更快的释放临界资源，当优先级进程离开临界区后，其优先级恢复至原本的值。  3.3.1.2 实例 #    与上图相比，到了T3 时刻，Thread1 需要Thread3 占用的同步资源SYNCH1，操作系统检测到这种情况后，就把Thread1 的优先级提高到Thread1 的优先级。 此时处于可运行状态的进程Thread2 和Thread3 中，Thread3 的优先级大于Thread2 的优先级，Thread3 被调度执行。 Thread3 执行到Thread4 时刻，释放了同步资源SYNCH1，操作系统恢复了Thread3 的优先级，Thread1 获得了同步资源SYNCH1，重新进入可执行队列。 处于可运行状态的进程Thread1 和Thread2 中，Thread1 的优先级大于Thread2，所以Thread1 被调度执行。  3.3.1.3 存在问题 #  3.3.1.3.1 潜在死锁 #   以下图为例，高优先级任务T1 请求被低优先级任务T2 占有的临界资源S2 时，进入阻塞状态，此时会把T2 优先级提升到T1 的水平。 T2 运行一段时间后，当请求被T1 占有的邻接资源S1 时，也会进入死锁状态，此时死锁形成。   3.3.1.3.2 链阻塞 #   以下图为例，高优先级任务T1 请求临界资源S1 时，中优先级任务T2 被提升，获得CPU 执行权，执行完成之后，释放S1，T1 继续往前执行。 当请求临界资源S2 时，低优先级任务T3 优先级被提升，获得CPU 执行权，执行完成之后，释放S2，T1 继续往前执行。 在这个例子中，T1 被阻塞了两次，更极端的例子，T1 会被阻塞很多次，这个叫链阻塞。   3.3.2 优先级天花板 #  3.3.2.1 含义 #   优先级天花板是指将申请资源的任务的优先级提升到可能访问该资源的所有任务中最高优先级任务的优先级。 当一个进程获取到临界资源，就将该进程的优先级提升到优先级天花板，这样，该进程不会被其它可能使用该临界资源的进程抢占，从而得到更快执行，更快地释放临界资源，释放临界资源后，恢复优先级。   优先级继承和优先级天花板的区别？\n 优先级继承只有当高优先级任务请求一个被阻塞的低优先级占有的临界资源时才会提升低优先级任务的优先级。 优先级天花板是只要有任务去申请临界资源，就会提升该任务的优先级。   3.3.2.2 实例 #   如下图所示，进程 $[P_1,P_3]$ 会访问临界资源 $S_1$，因此 $S_1$ 的优先级天花板为 1，同理可得，$S_2$ 的优先级天花板也为 1. 首先 $P_3$ 成功占有临界资源 $S_1$，此时 $P_3$ 的优先级被提升至 1，顺利执行完临界区代码，恢复优先级。 然后 $P_2$ 抢占 $P_3$，同理，$P_2$ 的优先级被提升至 1，$P_2$ 也可以顺利执行完临界区代码，恢复优先级。 最后，$P_1$ 抢占 $P_2$，此时，临界资源 $S_1$ 和 $S_2$ 皆已释放，$P_1$ 可以一口气执行到底，可见，这里不存在链阻塞问题。   4 参考文献 #    进程调度策略有哪些？  为什么 FCFS 调度算法会不利 I/O 操作频繁的作业（进程）？  最短作业优先（SJF）调度算法（详解版）。  短作业（进程）优先调度算法具有最短的平均周转时间，因此这种算法使最好的算法。  操作系统调度算法简介。  高响应比优先调度算法（HRRN）例题详解。  【操作系统 - 2】时间片轮转 RR 进程调度算法。  优先级调度算法。  多级反馈队列调度算法。  进程调度算法的优缺点。  信号量优先级反转（翻转）与优先级继承 \u0026ndash; kummer 话你知。  优先级调度、优先级反转、优先级继承、优先级天花板。  优先级反转 + 解决方案。  "},{"id":27,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/2.3.5-%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA/","title":"2.3.5 缓冲区溢出","section":"2.3 内存管理","content":"缓冲区溢出 #  1 什么是缓冲区溢出 #   C 语言使用运行时栈来存储过程信息。 每个函数的信息存储在一个栈帧中，包括寄存器、局部变量、参数、返回地址等。 C 语言对数组引用不进行任何边界检查，因此对越界的数组元素的写操作会破坏存储在栈中的状态信息，这种现象称为缓冲区溢出。 缓冲区溢出会破坏程序运行，也可以被用来攻击计算机，如使用一个指向攻击代码的指针覆盖返回地址。  2 缓冲区溢出示例 #  /** * 测试缓冲区溢出 */ void test_cache_overflow() { char str[8]; char input[256]; gets(input); strcpy(str,input); printf(\u0026#34;%s\u0026#34;, str); }   上面这段代码存在一个经典的缓冲区溢出漏洞strcpy。 我们为str分配了8个字节的空间，输入小于等于8个字符，没问题。 但当我们输入超长的数据时，就会出现问题，程序直接终止执行了。  3 防范缓冲区溢出攻击的方法 #  防范缓冲区溢出攻击的方法有三种，分别是随机化、栈保护和限制可执行代码区域。\n3.1 随机化 #   使用缓冲区溢出进行攻击，需要知道攻击代码的地址，因此常见的方法有：  栈随机化： 程序开始时在栈上分配一段随机大小的空间。 地址空间布局随机化（Address-Space Layout Randomization, ASLR）： 每次运行时程序的不同部分，包括代码段、数据段、栈、堆等都会加载到内存空间的不同区域。   但是攻击者依然可以使用蛮力克服随机化，这种方式称为“空操作雪橇（Nop Sled）”，即在实际的攻击代码前插入很长的一段 nop指令序列，执行这条指令只会移动到下一条指令。 因此只要攻击者能够猜中这段序列的某个地址，程序就会最终经过这段序列，到达攻击代码。 因此栈随机化和ASLR只能增加攻击一个系统的难度，但不能保证安全。  3.2 栈保护 #   在发生缓冲区溢出，造成任何有害结果之前，尝试检测到他。 常用的栈破坏检测方法是栈保护机制，即在每个函数的栈帧的局部变量和栈状态之间存储一个随机产生的特殊的值，称为金丝雀值（Canary）。 在恢复寄存器状态和函数返回之前，程序检测这个金丝雀值是否被改变了，如果是，那么程序异常终止。  3.3 限制可执行代码区域 #   内存页的访问形式有三种：可读、可写、可执行。 只有编译器产生的那部分代码所处的内存才是可执行的，其它页应当限制为只允许读和写。 以前x86将读和执行视为一个标志位，可读就可执行，为了限制某些页可读但不可执行，往往会带来严重的性能损失。 现在新的处理器在硬件上引入新的位，将读和执行分开，由硬件来检查页是否可执行，效率上没有损失。  4 参考文献 #    缓冲区溢出问题。  【面试题】技术面试题汇总 🔥。  浅谈缓冲区溢出。  "},{"id":28,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.6-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.6.5-%E8%A7%A6%E5%8F%91%E5%99%A8/","title":"1.6.5 触发器","section":"1.6 基础知识","content":"触发器 #  1 含义 #   触发器是与表有关的数据库对象，在满足定义条件时触发，并执行触发器中定义的语句集合。 触发器在数据库中以独立的对象存储，用于保证数据完整性（比如在写入数据表前，强制检验或转换数据）。 触发器只能创建在永久表上，不能对临时表创建触发器。 MySQL的触发器是针对每一行执行，比如向一张表中插入500条数据，就会触发500次触发器。 MySQL中定义了NEW和OLD用来表示触发器所在表中触发了触发器的那一行数据：  NEW表示新数据，OLD表示旧数据。 NEW可以在触发器中使用 SET赋值，这样不会再次触发触发器，而OLD是只读的。    2 四要素 #   监视地点：TABLE。 监视事件：INSERT、UPDATE、DELETE。 触发时间：AFTER、BEFORE。 触发事件：INSERT、UPDATE、DELETE。  3 注意事项 #   因为在MySQL中触发器是针对行的，因此对于增、删、改非常频繁的表上切记不要使用触发器，因为他会非常消耗资源。  4 相关操作 #  4.1 创建触发器 #  4.1.1 基本语法 #  CREATE [DEFINER = { user | CURRENT_USER }] TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW [trigger_order] trigger_body trigger_time: { BEFORE | AFTER } trigger_event: { INSERT | UPDATE | DELETE } trigger_order: { FOLLOWS | PRECEDES } other_trigger_name  BEFORE和AFTER参数指定了触发执行的时间，在事件之前还是事件之后。 FOR EACH ROW表示任何一条记录上的操作满足触发事件都会触发该触发器，也就是说触发器的触发频率是针对每一行数据触发一次。 trigger_event：  INSERT型触发器：插入某一行时触发触发器，可能通过INSERT、LOAD DATA、REPLACE语句触发（LOAD DATA语句用于将一个文件装入到一个数据表中，相当于一系列的INSERT操作）。 UPDATE型触发器：更改某一行时触发触发器，可能通过UPDATE语句触发。 DELETE型触发器：删除某一行时触发触发器，可能通过DELETE、REPLACE语句触发。   trigger_order：MySQL 5.7之后的一个功能，用于定义多个触发器，使用 FOLLOWS（尾随）或 PRECEDES（在\u0026hellip;之前）来选择触发器执行的先后顺序。  4.1.2 示例 #  mysql\u0026gt; CREATE TABLE account (acct_num INT, amount DECIMAL(10,2)); mysql\u0026gt; INSERT INTO account VALUES(137,14.98),(141,1937.50),(97,-100.00); mysql\u0026gt; delimiter $$ mysql\u0026gt; CREATE TRIGGER upd_check BEFORE UPDATE ON account -\u0026gt; FOR EACH ROW -\u0026gt; BEGIN -\u0026gt; IF NEW.amount \u0026lt; 0 THEN -\u0026gt; SET NEW.amount = 0; -\u0026gt; ELSEIF NEW.amount \u0026gt; 100 THEN -\u0026gt; SET NEW.amount = 100; -\u0026gt; END IF; -\u0026gt; END$$ mysql\u0026gt; delimiter ; mysql\u0026gt; update account set amount=-10 where acct_num=137; mysql\u0026gt; select * from account; +----------+---------+ | acct_num | amount | +----------+---------+ | 137 | 0.00 | | 141 | 1937.50 | | 97 | -100.00 | +----------+---------+ mysql\u0026gt; update account set amount=200 where acct_num=137; mysql\u0026gt; select * from account; +----------+---------+ | acct_num | amount | +----------+---------+ | 137 | 100.00 | | 141 | 1937.50 | | 97 | -100.00 | +----------+---------+ 4.2 查看触发器 #    使用 show triggers查看触发器信息：\nmysql\u0026gt; show triggers;   在 information_schema.triggers中查看触发器信息：\nmysql\u0026gt; SELECT * FROM information_schema.triggers where trigger_name=\u0026#39;upd_check\u0026#39;;   4.3 删除触发器 #  删除触发器的语法如下：\ndrop trigger [if exists] [schema_name].trigger_name; 参考文献 #    Drop/Delete/Truncate的区别？  MySQL - 触发器。  一篇很棒的 MySQL 触发器学习教程。  MySQL触发器trigger的使用。  "},{"id":29,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.6-TCP%E5%92%8CUDP%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"1.1.6 Tcp和 Udp的区别","section":"1.1 传输层： Tcp和 UDP","content":"TCP和UDP的区别 #  1 TCP与UDP的区别有哪些 #    TCP是面向连接的，UDP是无连接的，即UDP发送数据之前不需要建立连接。\n  TCP是可靠的，UDP是不可靠的，这是因为UDP接收方收到报文后，不需要给出任何确认。\n  TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多；\n  TCP是面向字节流的，UDP是面向报文的：\n 面向字节流是指发送数据时以字节为单位，一个数据包可以拆分成若干组进行发送，而UDP一个报文只能一次发完。\n   TCP有拥塞控制机制，UDP没有，网络出现的拥塞不会使源主机的发送速率降低，这对某些实时应用是很重要的，比如媒体通信、游戏。\n  TCP首部开销（20字节）比UDP首部开销（8字节）要大。\n  UDP的主机不需要维持复杂的连接状态表。\n  2 什么时候选择TCP，什么时候选择UDP #   对某些实时性要求比较高的情况，选择UDP，比如游戏、媒体通信、实时视频流（直播），即使出现传输错误也可以容忍。 其他大部分情况下，HTTP都是用TCP，因为要求传输的内容可靠，不出现丢失。  3 HTTP可以使用UDP吗 #   HTTP3.0之前使用TCP协议，而HTTP3.0使用基于UDP协议的QUIC协议组成。 此变化主要是为了解决HTTP2.0中存在的对头阻塞问题，由于HTTP2.0在单个TCP连接上使用了多路复用，受到TCP拥塞控制的影响，少量的丢包就可能导致整个TCP连接上的所有流被阻塞。  4 面向连接和无连接的区别 #   面向连接的网络服务（虚电路服务）：  首先建立连接，所有的数据包经过相同的路径，服务质量有较好地保证。   无连接的网络服务（数据报服务）：  每个数据包含目的地址，数据路由相互独立，路径可能变化。 网络尽最大努力交付数据，但不保证不丢失、不保证先后顺序、不保证在时限内交付。 网络发生拥塞时，可能会将一些分组丢弃。    参考文献 #    TCP与UDP的区别。  "},{"id":30,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.6-%E4%BB%80%E4%B9%88%E6%98%AFRIP%E7%AE%97%E6%B3%95%E6%98%AF%E4%BB%80%E4%B9%88/","title":"1.2.6 什么是 Rip，算法是什么","section":"1.2 应用层：HTTP和HTTPS","content":"什么是RIP，算法是什么 #  1 简介 #   RIP全称是Routing Information Protocol，即路由信息协议，是一种较为简单的内部网关协议（Interior Gateway Protocol）。 他通过UDP报文进行路由信息的交换，使用的端口号是520。  2 原理 #   RIP是一种基于距离矢量（DIstance-Vector）算法的协议，使用跳数（Hop Count）作为度量值来衡量到达目的地址的距离。 RIP属于网络层，主要应用于规模较小的，可靠性要求较低的网络，可以通过不断的交换信息让路由器动态的适应网络连接的变化，这些信息包括每个路由器可以到达哪些网络，这些网络有多远等。 在RIP网络中，缺省情况下，设备到他直接连接网络的跳数为0，通过一个设备可达的网络跳数为1，其余以此类推，也就是说，度量值等于从本网络到达目的网络间的设备数量。 为限制收敛时间，RIP规定度量值取0~15之间的整数，大于或等于16的跳数被定义为无穷大，即目的网络或主机不可达，由于这个限制，使得RIP不可能在大型网络中得到应用。  3 RIP路由表的形成过程 #  RIP启动时的初始路由表仅包含本设备的一些直连接口路由，通过相邻设备互相学习路由表项，才能实现各网段路由互通。\n RIP路由表形成过程如上图所示：\n RIP协议启动之后，RouterA会向相邻的路由器广播一个Request报文。 当RouterB从接口接收到RouterA发送的Request报文后，把自己的RIP路由表封装在Response报文内，然后向该接口对应的网络广播。 RouterA根据RouterB发送的Response报文，形成自己的路由表。  4 优缺点 #  4.1 优点 #    实现简单，开销较小。  4.2 缺点 #   RIP限制了网路的规模，它能使用的最大距离为15（16表示不可达）。 路由器之间交换的路由信息是路由器中的完成路由表，因而随着网络规模的扩大，开销也就增加。 “坏消息传播得慢”，使更新过程的收敛时间更长，即当网络出现故障时，要经过较长的时间才能将此信息传递到所有路由器。  5 RIP协议中，为什么会出现坏消息传播的慢 #    我们可以假设三个网络通道由两个路由器互联起来，并且都已建立了各自的路由表，图中路由器的交换信息只给出了我们感兴趣的一行内容：路由器$R_1$中的 1，1，直接表示到网1的距离是1，直接交付，路由器$R_2$中的 1，2，R1表示到网1的距离是2，下一跳经过R1。 现在假定路由器$R_1$到网1的链路出现了故障，$R_1$无法到达网1，于是路由器$R_1$把到网1的距离改为16（表示到网1不可达），因而在$R_1$的路由表中的相应项目变为 1，16，直接。 但是，很可能要经过30秒中后$R_1$才把更新信息发送给$R_2$，然而$R_2$可能已经先把自己的路由表发送给了$R_1$，其中有 1，2，R1。 $R_1$收到$R_2$的更新报文之后，误认为可经过$R_2$到达网1，于是把收到的路由信息 1，2，R1修改为1，3，R2，表明我到网1的距离是3，下一条经过R2，并把更新后的信息发送给$R_2$。 同理，$R_2$接着又更新自己的路由表为 1，4，R1，以为我到网1的距离是4，下一跳经过R1。 这样的更新一直持续下去，直到$R_1$和$R_2$到网1的距离都增大到16时，$R_1$和$R_2$才知道原来网1是不可达的。 RIP协议的这一特点叫做“好消息传播得快，坏消息传播得慢”，网络出故障的传播时间往往需要较长的时间（例如数分钟）。 为了使坏消息传播得快些，可以采取多种措施，例如让路由器记录收到某特定路由信息的接口，而不让同一路由信息再通过此接口向反方向传送。  6 参考文献 #    什么是RIP (距离矢量路由协议)?  RIP基础知识。  计网学习笔记（14）- RIP 和 OSPF。  RIP协议的缺点：坏消息传播得慢。  "},{"id":31,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.6-%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/","title":"2.1.6 僵尸进程和孤儿进程","section":"2.1 进程和线程","content":"僵尸进程和孤儿进程 #  1 僵尸进程 #  1.1 含义 #   当一个子进程结束运行（一般是调用 exit、运行时发生致命错误或收到终止信号导致）时，其退出状态会汇报给操作系统，系统则以 SIGCHLD 信号将子进程被结束的事件告知父进程，此时，子进程的进程控制块（PCB）仍驻留在内存中。 一般来说，收到 SIGCHLD 后，父进程会使用 wait 系统调用以获取子进程的退出状态，然后内核就可以从内存中释放已结束的子进程的 PCB。 如果父进程没有调用 wait 或 waitpid 获取子进程的状态信息时，子进程的 PCB 就会一直驻留在内存中，成为僵尸进程。  1.2 特点 #   僵尸进程是一个死亡的进程，但是并没有真正被销毁。 他已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，仅仅在进程表中保留一个位置，记载该进程的进程 ID、终止状态以及资源利用信息（CPU 时间，内存使用量等）供父进程收集，除此之外，僵尸进程不再占用任何内存空间。 这个僵尸进程可能会一直留在系统中，直到系统重启。  1.3 危害 #   占用进程号，而系统所能使用的进程号是有限的，如果产生大量的僵尸进程，就会因为没有可用的进程号而导致系统不能产生新的进程。  1.4 如何杀死僵尸进程 #   重启服务器。 找到该僵尸进程的父进程，并将该进程的父进程杀掉，则此僵尸进程会自动消失。   查找僵尸进程的方法：ps aux | grep -w 'Z'\n 1.5 如何避免僵尸进程 #   父进程在创建子进程之前，就调用 signal(SIGCHLD,SIG_IGN) 向系统申明自己不会对这个子进程的 exit 动作进行任何关注行为，这样子进程一旦退出，系统就不会去等待父进程的操作，而是直接将该子进程的资源回收掉，也就不会出现僵尸进程了。 父进程创建子进程后调用 wait 或 waitpid 等待子进程结束，但是这会导致父进程挂起。 父进程在创建子进程的时候，连续调用两次 fork()，而且使紧跟的子进程退出，使其孙子进程成为孤儿进程，从而 init 进程将代替父进程来接手，负责清除这个孤儿进程，于是父进程就无须进行任何的清理行为，系统会自动处理。 采用信号量处理函数，父进程首先注册一个信号处理函数 signal(SIGCHLD, sig_chld_handler)，然后每当子进程退出的时候，父进程都会收到 SIGCHLD，然后在 sig_chld_handler 中调用 wait 函数等待子进程的退出。  2 孤儿进程 #  2.1 含义 #   一个父进程退出，而他的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程，孤儿进程被 init 进程（进程号为 1）所收养，并由 init 进程对他们完成状态收集工作。  参考文献 #    进程 3.0——进程状态与僵尸进程、孤儿进程。  什么是僵尸进程？  僵尸进程的产生和危害。  Kill 杀死 Linux 中的 defunct 进程(僵尸进程)。  Linux 编程中如何避免出现僵尸进程。  僵尸进程的存在意义、危害及避免方法。  "},{"id":32,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.6-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.6.6-%E8%A7%86%E5%9B%BE/","title":"1.6.6 视图","section":"1.6 基础知识","content":"视图 #  1 含义 #   视图是一种虚拟存在的逻辑表，本身并不包含数据，只是作为一个 select 语句保存在数据字典中。 视图数据来自定义视图的查询中使用的表，使用视图动态生成，因此基表数据发生了改变时，视图也会跟着改变，同时对视图数据的修改也会影响基表的数据。  2 优点 #   简单： 使用视图的用户完全不需要关心后面对应的表结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全： 使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单实现。 数据独立： 一旦视图的结构定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响，源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。  3 相关操作 #  3.1 创建视图 #  3.1.1 语法 #  -- 创建视图 CREATE VIWE 视图名称 AS SQL 语句 -- 完整 CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED|MERGE|TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement [WITH [CASCADED|LOCAL] CHECK OPTION] 3.1.2 注意事项 #   创建视图需要 CREATE VIEW 的权限，并且对于查询涉及的列有 SELECT 权限，如果使用 CREATE OR REPLACE 或者 ALTER 修改视图，那么还需要该视图的 DROP 权限。 OR REPLACE：在视图存在的情况下可对视图进行修改，视图不存在的情况下可创建视图。 ALGORITHM：表示视图选择算法：  MERGE：  将引用视图语句的文本与视图定义合并起来，使得视图定义的某一部分取代语句的对应部分。   假设有如下视图定义语句：\nCREATE ALGORITHM = MERGE VIEW v_merge (vc1, vc2) AS SELECT c1, c2 FROM t WHERE c3 \u0026gt; 100;   示例一：\n  假如我们有以下查询语句：\nSELECT * FROM v_merge;   此时 MySQL 会对上面的查询语句做如下处理：\n 将v_merge 变成t。 将* 变成vc1、vc2，对应于c1、c2。 将视图定于中的WHERE 语句添加到查询语句后面。    因此，上面的查询语句就会变成下面的形式：\nSELECT c1, c2 FROM t WHERE c3 \u0026gt; 100;     示例二：\n  假如我们有以下查询语句：\nSELECT * FROM v_merge WHERE vc1 \u0026lt; 100;   这个查询语句的处理和上面查询语句的处理类似，只不过会把 vc1 \u0026lt; 100 变成 c1 \u0026lt; 100，然后使用 AND 把视图定义语句中的 WHERE 条件和查询语句的条件连接起来，处理后的查询语句就会变成下面的形式：\nSELECT c1, c2 FROM t WHERE (c3 \u0026gt; 100) AND (c1 \u0026lt; 100);      如果视图中包含下面的结构，则不可以使用 MERGE 算法：  聚合函数（例如SUM()、MIN()、MAX()、COUNT）。 DISTINCT。 GROUP BY。 HAVING。 LIMIT。 UNION 或者是 UNION ALL。 SELECT 中包含子查询。 分配给用户的变量。 仅含有常量。   如果MERGE算法不能使用时，则会使用 TEMPTABLE 算法。   TEMPTABLE：  视图的结果将被置于临时表中，然后使用他执行语句。 该算法的一大优点是创建临时表之后，并在完成语句处理之前，能够释放基表上的锁定，与MERGE 算法相比，锁定释放的速度更快，这样，使用视图的其他客户端不会被屏蔽过长时间。   UNDEFINED：  该种情况下 MySQL 会自动选择要使用的算法。 相比于TEMPTABLE，在可以的情况下 MySQL 会更倾向于选择 MERGE 算法，因为MERGE 算法更加高效，而且如果使用了 TEMPTABLE，视图是不可更新的。 如果没有指定任何算法的话，MySQL 将会使用 UNDEFINED 算法。     select_statement：表示select语句。 WITH [CASCADED|LOCAL] CHECK OPTION：表示视图在更新时检查更新语句的范围：  CASCADED：  不仅对当前视图的定义条件进行检查，而且会对所有依赖的视图的条件进行检查。 默认为 CASCADED。 示例：   假如有如下视图：\nmysql\u0026gt; CREATE OR REPLACE VIEW payment_view AS -\u0026gt; SELECT payment_id,amount FROM payment -\u0026gt; WHERE amount \u0026lt; 10 WITH CHECK OPTION; mysql\u0026gt; CREATE OR REPLACE VIEW payment_view2 AS -\u0026gt; SELECT payment_id,amount FROM payment_view -\u0026gt; WHERE amount \u0026gt; 5 WITH CASCADED CHECK OPTION;   更新 payment_view2：\nmysql\u0026gt; UPDATE payment_view2 SET amount=10 -\u0026gt; WHERE payment_id = 3; ERROR 1369 (HY000): CHECK OPTION failed 'sakila.payment_view2'   在更新的时候会报错，因为 amount = 10虽然满足了当前视图 payment_view2的定义条件，但不满足其所依赖的 payment_view的定义条件（amount \u0026lt; 10）。\n     LOCAL：  只对当前视图的定义条件进行检查。 示例：   假如有如下视图，其中视图 payment_view的定义和上面一样：\nmysql\u0026gt; CREATE OR REPLACE VIEW payment_view1 AS -\u0026gt; SELECT payment_id,amount FROM payment_view -\u0026gt; WHERE amount \u0026lt; 5 WITH LOCAL CHECK OPTION;   更新 payment_view1：\nmysql\u0026gt; UPDATE payment_view1 SET amount=10 -\u0026gt; WHERE payment_id = 3; Query OK, 1 row affected (0.03 sec) Rows matched: 1 changed: 1 warnings: 0   更新的时候没有报错，因为 amount = 10符合当前视图 payment_view1的定义条件（amount \u0026lt; 5），而且在这种范围下不需要检查其所依赖的视图的定义条件。\n        3.1.3 示例 #  3.1.3.1 在单表上创建视图 #  mysql\u0026gt; create view v_F_players(编号,名字,性别,电话) -\u0026gt; as -\u0026gt; select PLAYERNO,NAME,SEX,PHONENO from PLAYERS -\u0026gt; where SEX=\u0026#39;F\u0026#39; -\u0026gt; with check option; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; desc v_F_players; +--------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+----------+------+-----+---------+-------+ | 编号 | int(11) | NO | | NULL | | | 名字 | char(15) | NO | | NULL | | | 性别 | char(1) | NO | | NULL | | | 电话 | char(13) | YES | | NULL | | +--------+----------+------+-----+---------+-------+ 4 rows in set (0.00 sec) mysql\u0026gt; select * from v_F_players; +--------+-----------+--------+------------+ | 编号 | 名字 | 性别 | 电话 | +--------+-----------+--------+------------+ | 8 | Newcastle | F | 070-458458 | | 27 | Collins | F | 079-234857 | | 28 | Collins | F | 010-659599 | | 104 | Moorman | F | 079-987571 | | 112 | Bailey | F | 010-548745 | +--------+-----------+--------+------------+ 5 rows in set (0.02 sec) 3.1.3.2 在多表上创建视图 #  mysql\u0026gt; create view v_match -\u0026gt; as -\u0026gt; select a.PLAYERNO,a.NAME,MATCHNO,WON,LOST,c.TEAMNO,c.DIVISION -\u0026gt; from -\u0026gt; PLAYERS a,MATCHES b,TEAMS c -\u0026gt; where a.PLAYERNO=b.PLAYERNO and b.TEAMNO=c.TEAMNO; Query OK, 0 rows affected (0.03 sec) mysql\u0026gt; select * from v_match; +----------+-----------+---------+-----+------+--------+----------+ | PLAYERNO | NAME | MATCHNO | WON | LOST | TEAMNO | DIVISION | +----------+-----------+---------+-----+------+--------+----------+ | 6 | Parmenter | 1 | 3 | 1 | 1 | first | | 44 | Baker | 4 | 3 | 2 | 1 | first | | 83 | Hope | 5 | 0 | 3 | 1 | first | | 112 | Bailey | 12 | 1 | 3 | 2 | second | | 8 | Newcastle | 13 | 0 | 3 | 2 | second | +----------+-----------+---------+-----+------+--------+----------+ 5 rows in set (0.04 sec) 3.2 查看视图 #    使用 show create view语句查看视图信息：\nmysql\u0026gt; show create view v_F_players\\G; *************************** 1. row *************************** View: v_F_players Create View: CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v_F_players` AS select `PLAYERS`.`PLAYERNO` AS `编号`,`PLAYERS`.`NAME` AS `名字`,`PLAYERS`.`SEX` AS `性别`,`PLAYERS`.`PHONENO` AS `电话` from `PLAYERS` where (`PLAYERS`.`SEX` = \u0026#39;F\u0026#39;) WITH CASCADED CHECK OPTION character_set_client: utf8 collation_connection: utf8_general_ci 1 row in set (0.00 sec)   有关视图的信息记录在 information_schema.views中：\nmysql\u0026gt; select * from information_schema.views -\u0026gt; where TABLE_NAME=\u0026#39;v_F_players\u0026#39;\\G; *************************** 1. row *************************** TABLE_CATALOG: def TABLE_SCHEMA: TENNIS TABLE_NAME: v_F_players VIEW_DEFINITION: select `TENNIS`.`PLAYERS`.`PLAYERNO` AS `编号`,`TENNIS`.`PLAYERS`.`NAME` AS `名字`,`TENNIS`.`PLAYERS`.`SEX` AS `性别`,`TENNIS`.`PLAYERS`.`PHONENO` AS `电话` from `TENNIS`.`PLAYERS` where (`TENNIS`.`PLAYERS`.`SEX` = \u0026#39;F\u0026#39;) CHECK_OPTION: CASCADED IS_UPDATABLE: YES DEFINER: root@localhost SECURITY_TYPE: DEFINER CHARACTER_SET_CLIENT: utf8 COLLATION_CONNECTION: utf8_general_ci 1 row in set (0.00 sec)   3.3 修改视图 #   当视图使用的算法为 MERGE时才可以更新视图，使用 TEMPTABLE算法的视图不可以更新，因此更新视图时需要看一下视图的定义中是否满足 使用 MERGE算法的条件，如果不满足条件的话就不可以对视图进行修改。 修改视图的方法有以下两种：   使用 CREATE OR REPLACE VIEW语句：\ncreate or replace view view_name as select_statement;   使用 ALTER语句：\nALTER [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] [DEFINER = { user | CURRENT_USER }] [SQL SECURITY { DEFINER | INVOKER }] VIEW view_name [(column_list)] AS select_statement [WITH [CASCADED | LOCAL] CHECK OPTION] 上面的部分参数的使用方式可以参考 3.1.2 注意事项。\n  DML操作，因为视图本身没有数据，所以对视图进行的 DML操作最终都体现在基表中：\nmysql\u0026gt; create view v_student as select * from student; mysql\u0026gt; select * from v_student; +--------+--------+------+ | 学号 | name | sex | +--------+--------+------+ | 1 | 张三 | M | | 2 | 李四 | F | | 5 | 王五 | NULL | +--------+--------+------+ mysql\u0026gt; update v_student set name=\u0026#39;钱六\u0026#39; where 学号=\u0026#39;1\u0026#39;; mysql\u0026gt; select * from student; +--------+--------+------+ | 学号 | name | sex | +--------+--------+------+ | 1 | 钱六 | M | | 2 | 李四 | F | | 5 | 王五 | NULL | +--------+--------+------+   使用 drop删除视图，删除视图时，只删除视图的定义，不会删除数据：\nDROP VIEW [IF EXISTS] view_name [, view_name];     参考文献 #    什么是视图？什么是游标？  深入解析 MySQL 视图 VIEW。  MySQL - 视图。  mysql 视图 algorithm_MySQL 的视图讲解。  20.5.2 View Processing Algorithms.  "},{"id":33,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.7-TCP%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E4%BC%A0%E8%BE%93%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7/","title":"1.1.7 Tcp如何保证传输的可靠性","section":"1.1 传输层： Tcp和 UDP","content":"TCP如何保证传输的可靠性 #   数据包校验。 对失序数据包重新排序（TCP报文具有序列号）。 丢弃重复数据。 应答机制：接收方收到数据之后，会发送一个确认。 超时重发：发送方发出数据之后，启动一个定时器，超时未收到接收方的确认，则重新发送这个数据。 流量控制：确保接收端能够接收发送方的数据，而不会出现缓冲区溢出的情况。  "},{"id":34,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.7-HTTP1.0HTTP1.1%E5%92%8CHTTP2.0%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB/","title":"1.2.7 Http1.0、 Http1.1和 Http2.0的主要区别","section":"1.2 应用层：HTTP和HTTPS","content":"HTTP1.0、HTTP1.1和HTTP2.0的主要区别 #  1 HTTP1.0 和 HTTP1.1 的区别 #    缓存处理：\n 在 HTTP1.0 中主要使用header 里的If-Modified-Since,Expires 来作为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，例如Entity tag、If-Unmodified-Since、If-Match、If-None-Match 等更多可供选择的缓存头来控制缓存策略。    带宽优化及网络连接的使用：\n HTTP1.0 中存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象都送过来了，并且不支持断点续传功能，HTTP1.1 则在请求头中添加了range 字段，他允许只请求资源的某个部分，这样就方便了开发者自由的选择，以便于充分的利用带宽。    错误通知的管理：\n 在 HTTP1.1 中新增了 24 个错误状态响应码，如 409（Confilct）表示请求的资源与资源的当前状态发生冲突，410（Gone）表示服务器上的某个资源被永久性的删除。    Host 头处理：\n 在 HTTP1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此请求头中的 URL 并没有传递主机名，但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机，并且他们共享一个 IP 地址，HTTP1.1 的请求消息和响应消息都应该支持 Host 字段，请求消息中如果没有 Host 字段会报 400（Bad Request）。    长连接：\n  HTTP1.1 支持长连接和请求的流水线处理（Pipeling），在一个 TCP 连接上可以传送多个 HTTP 请求和响应，减少了建立和关闭连接的消耗和延迟，在 HTTP1.1 中默认开启 Connection: keep-alive，一定程度上弥补了 HTTP1.0 每次请求都要创建连接的缺点。\n Pipeling 是什么？ Pipeling 是指若干个请求排队串行化单线程处理，后面的请求等待前面的请求的返回才能获得执行机会，一旦有某请求超时，后续请求只能被阻塞，也就是人们常说的线头阻塞。\n     2 HTTP1.1 和 HTTP2.0 的区别 #    多路复用：\n  多路复用允许同时通过单一的 HTTP/2 连接发起多重请求，在HTTP/1.1 协议中浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制，超过限制数目的请求会被阻塞，这也是为何一些站点会有多个静态资源 CDN 域名的原因之一，而HTTP/2 的多路复用则允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息，因此 HTTP/2可以很容易的去实现多流并行而不用依赖建立多个 TCP 连接。\n  这种单连接多资源的方式，减少了服务端的连接压力，内存占用更少，连接吞吐量更大，而且由于 TCP 连接的减少，使得网络拥塞状况得以改善，同时慢启动的减少，使得拥塞和丢包恢复速度更快。\n HTTP2.0 的多路复用和 HTTP1.1 中的长连接复用有什么区别？\n HTTP/1.*：一次请求一个响应，建立一个连接，用完关闭，即每一个请求都要建立一个连接。 HTTP/1.1：采用Pipeling的解决方式，若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时，后续请求只能被阻塞，也就是人们常说的线头阻塞。 HTTP/2：多个请求可同时在一个连接上并行执行，某个请求任务耗时严重，不会影响到其他连接的正常执行。       请求优先级：\n HTTP2.0 使用一个31 比特的优先值，0 表示最高优先级，$ 2^{31} - 1 $ 表示最低优先级，服务器端可以根据优先级，控制资源分配，优先处理和返回最高优先级的请求帧给客户端。    二进制分帧：\n HTTP1.1 的解析是基于文本，基于文本协议的格式解析存在天然缺陷，因为文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，而二进制则不同，只认 0 和 1 组合，因此基于这种考虑HTTP2.0 的协议解析决定采用二进制格式，实现方便且健壮。 HTTP/2 在应用层和传输层之间增加了一个二进制分帧层，在二进制分帧层中 HTTP/2 会将所有传输的信息分割为更小的消息和帧，并对他们采用二进制格式的编码，其中HTTP1.x 的首部信息会被封装在 HEADER frame 里，而相应 Request Body 则封装在 DATA frame 里，这样便可以在不改动 HTTP/1.x 的语义、方法、状态码以及首部字段的情况下，解决了 HTTP1.1 的性能限制，改进了传输性能，实现低延迟和高吞吐量。    头部压缩：\n  HTTP/1.1并不支持HTTP首部压缩，HTTP/2则使用了专门为首部压缩而设计的HPACK算法。\n 为什么需要头部压缩？\n 假设一个页面有100个资源需要加载，而每一次请求都有1KB的消息头，则至少需要多消耗100KB来获取这些消息头。 HTTP2.0可以维护一个字典，差量更新HTTP头部，大大降低因头部传输产生的流量。       服务端推送：\n 在HTTP/2中，服务器可以对客户端的一个请求发送多个响应。 如果一个请求是由我们的主页发起的，服务器很可能会响应主页内容、Logo及样式表，因为他知道客户端会用到这些东西。 同时服务器端推送还有一个很大的优势，就是可以缓存，这让在遵循同源的情况下，不同页面之间共享缓存资源成为可能。    参考文献 #    HTTP 1.0/1.1/2.0 的区别与联系。  "},{"id":35,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.7-%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5/","title":"2.1.7 线程同步","section":"2.1 进程和线程","content":"线程同步 #  1 为什么需要线程同步 #   线程有时候会和其他线程共享一些资源，比如内存、数据库等。 当多个线程同时读写一份共享资源的时候，可能会发生冲突，因此需要线程的同步，多个线程按顺序访问资源。  2 线程同步有哪些方式 #  线程同步主要有四种方式，分别是互斥量（Mutex）、信号量（Semaphore）、事件（Event）、临界区（Critical Section）。\n2.1 互斥量 #   互斥量是内核对象，只有拥有互斥对象的线程才有访问互斥资源的权限。 因为互斥对象只有一个，所以可以保证互斥资源不会被多个线程同时访问。 当前拥有互斥对象的线程处理完任务后，必须将互斥对象交出，以便其他线程访问该资源。  2.2 信号量 #   信号量是内核对象，它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。 信号量对象保存了最大资源计数和当前可用资源计数，每增加一个线程对共享资源的访问，当前可用资源计数就减1。 只要当前可用资源计数大于0，就可以发出信号量信号，如果为0，则将线程放入一个队列中等待。 线程处理完共享资源后，应在离开的同时通过ReleaseSemaphore函数将当前可用资源数加1. 如果信号量的取值只能是0或1，那么信号量就成了互斥量。  2.3 事件 #   事件允许一个线程在处理完任务后，主动唤醒另外一个线程执行任务。 事件分为手动重置事件和自动重置事件：  手动重置事件被设为激发状态后，会唤醒所有等待的线程，而且一直保持为激发状态，直到程序重新把它设置为未激发状态。 自动重置事件被设置为激发状态后，会唤醒一个等待中的线程，然后自动恢复为未激发状态。    2.4 临界区 #   临界区是一段代码，这段代码使用来访问临界资源的。 任意时刻只允许一个线程对临界资源进行访问。 拥有临界区对象的线程可以访问该临界资源，其它试图访问该资源的线程将被挂起，临界区被释放后，其它线程可继续抢占该临界区。 临界区是一种轻量级的同步机制，是用户态下的对象，即只能在同一进程中实现线程互斥，因此无需在用户态和核心态切换，工作效率比互斥来说要高很多。  3 互斥量和临界区有什么区别 #   互斥量是可以命名的，可以用于不同进程之间的同步。 临界区只能用于同一进程中线程的同步。 创建互斥量需要的资源更多，因此临界区的优势是速度快，节省资源。  参考文献 #    线程同步有哪些方式？  临界区与互斥量区别。  [多线程] 互斥量和临界区的区别。  "},{"id":36,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.1-%E4%BC%A0%E8%BE%93%E5%B1%82TCP%E5%92%8CUDP/1.1.8-TCP%E9%95%BF%E8%BF%9E%E6%8E%A5%E5%92%8C%E7%9F%AD%E8%BF%9E%E6%8E%A5/","title":"1.1.8 Tcp长连接和短连接","section":"1.1 传输层： Tcp和 UDP","content":"TCP长连接和短连接 #  TCP的具体介绍可参考 1.1.1 三次握手和 1.1.2 四次挥手，下面主要介绍TCP的短连接、长连接及其二者的优缺点。\n1 短连接 #  1.1 含义 #   短连接是指通信双方有数据交互时，就建立TCP连接，数据发送完成后，则断开此TCP连接。 短连接的模式是$连接 \\rightarrow 数据传输 \\rightarrow 关闭连接\u0026hellip;建立连接 \\rightarrow 数据传输 \\rightarrow 关闭连接$。  1.2 优缺点 #  1.2.1 优点 #   管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。  1.2.2 缺点 #   如果客户请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽。  1.3 适用场景 #   适用于像WEB网站的HTTP服务，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知，所以并发量大，但每个用户无需频繁操作情况下使用短连接比较好。  2 长连接 #  2.1 含义 #   长连接是指在一个TCP连接上可以连续发送多个数据包。 在TCP连接保持期间，如果没有数据包发送，需要双方发检测包以维持此连接，一般需要自己做在线维持。 如果一个给定的连接在两小时内没有任何动作，则服务器就向客户端发送一个探测报文段，客户端必须处于以下4个状态之一：  客户端依然正常运行，并从服务器可达，客户端的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 客户端已经崩溃，并且关闭或者正在重新启动，在任何一种情况下，客户端的TCP都没有响应，服务器将不能收到对探测的响应，并在75秒后超时，服务器总共发送10个这样的探测，每个间隔75秒，如果服务器没有收到一个响应，他就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动，服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户端正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。    2.2 优缺点 #  2.2.1 优点 #   长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。  2.2.2 缺点 #   存活功能的探测周期长，而且只是探测TCP连接的存活，遇到恶意的连接时，保活功能就不够使了。 在长连接的应用场景下，客户端一般不会主动关闭他们的连接，客户端和服务端之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，服务器早晚有扛不住的时候，这时候服务器就需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致服务器受损，如果条件再允许的话，可以以客户端为颗粒度，限制每个客户端的最大连接数，这样就可以避免某个存在问题的客户端连累后端服务。  2.3 适用场景 #   长连接多用于操作频繁，点对点的通讯，而且连接数不能太多的情况，这样每个操作完后都不断开，后面处理时直接发送数据包就可以了，不用再建立TCP连接。 例如，数据库的连接使用长连接，如果用短连接的话，频繁的通信会造成 socket错误，而且频繁的 socket创建也是对资源的浪费。  2.4 如何检测长连接是否中断 #   在应用层使用心跳来主动检测。 改变 socket的 keepalive选项，以使 socket检测连接是否中断的时间间隔更小，以满足我们的实时性需求。  参考文献 #    TCP长连接和短连接的区别。  一文搞懂 HTTP、TCP 的长连接和短连接。  "},{"id":37,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.8-DNS/","title":"1.2.8 DNS","section":"1.2 应用层：HTTP和HTTPS","content":"DNS #  1 含义 #   DNS，就是 Domain Name System，即域名系统，是互联网上作为域名和 IP 地址相互映射的一个分布式数据库。 DNS 能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的 IP 数串。  2 域名解析过程 #    网络客户端就是我们平常使用的电脑，打开浏览器，输入一个域名，比如www.163.com，这时，我们的电脑会发出一个 DNS 请求到本地 DNS 服务器，本地 DNS 服务器一般都是我们的网络接入服务器商提供，比如中国电信、中国移动。 查询www.163.com 的DNS 请求到达本地 DNS 服务器之后，本地 DNS 服务器会首先查询他的缓存记录，如果缓存中有此条记录，就可以直接返回结果，如果没有，本地 DNS 服务器还要向 DNS 根服务器进行查询。 根 DNS 服务器没有记录具体的域名和 IP 地址的对应关系，而是告诉本地 DNS 服务器，你可以到域服务器上去继续查询，并给出域服务器的地址。 本地 DNS 服务器继续向域服务器发出请求，在这个例子中，请求的对象是 .com 域服务器，.com 域服务器收到请求之后，也不会直接返回域名和 IP 地址的对应关系，而是告诉本地 DNS 服务器，你的域名的解析服务器的地址。 最后，本地 DNS 服务器向域名的解析服务器发出请求，这时就能收到一个域名和 IP 地址的对应关系，本地 DNS 服务器不仅要把 IP 地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。  3 DNS 劫持 #  3.1 含义 #   DNS 劫持即通过某种技术手段，篡改正确域名和 IP 地址的映射关系，使得域名映射到了错误的 IP 地址，因此可以认为DNS 劫持是一种 DNS 重定向攻击。 DNS 劫持通常可被用作：  域名欺诈：如在用户访问网页时显示额外的信息来赚取收入。 网络钓鱼：如显示用户访问的虚假网站版本并非法窃取用户的个人信息。    3.2 DNS 劫持的分类 #  3.2.1 本地 DNS 劫持 #   客户端发生的 DNS 劫持称为本地 DNS 劫持，本地 DNS 劫持可能是：  黑客通过木马病毒或者恶意程序入侵 PC，篡改 DNS 配置（Hosts 文件、DNS 服务器地址、DNS 缓存等）。 黑客利用路由器漏洞或者破解路由器管理账号入侵路由器并且篡改 DNS 配置。    3.2.2 DNS 解析路径劫持 #   DNS 解析过程中发生在客户端和 DNS 服务器网络通信时的 DNS 劫持统一归类为 DNS 解析路径劫持。 DNS 解析路径劫持可以划分为如下三类：  DNS 请求转发：通过技术手段将 DNS 流量重定向到其他 DNS 服务器。  DNS 请求复制：利用相关设备将 DNS 查询复制到网络设备，并先于正常应答返回 DNS 劫持的结果。  DNS 请求代答：网络设备或者软件直接代替 DNS 服务器对 DNS 查询进行应答。     3.2.3 篡改 DNS 权威记录 #   篡改 DNS 权威记录，我们这里指的是黑客非法入侵 DNS 权威记录管理账号，直接修改 DNS 记录的行为。   3.3 DNS 劫持应对策略 #   安装杀毒软件，防御木马病毒和恶意软件，定期修改路由器管理账号密码。 选择安全技术实力过硬的域名注册商，并且给自己的域名权威数据上锁，防止域名权威数据被篡改。 选择支持 DNSSEC 的域名解析服务商，并且给自己的域名实施 DNSSEC，DNSSEC 能够保证递归 DNS 服务器和权威 DNS 服务器之间的通信不被篡改。 在客户端和递归 DNS 服务器通信之间使用 DNS 加密技术，如 DNS-over-TLS、DNS-over-HTTPS。  参考文献 #    一张图看懂 DNS 域名解析全过程。  聊一聊 DNS 劫持那些事。  "},{"id":38,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.8-%E5%8D%8F%E7%A8%8B/","title":"2.1.8 协程","section":"2.1 进程和线程","content":"协程 #  1 含义 #   协程是一种轻量级线程，位于用户态，完全由用户自己控制。   2 特点 #   线程的切换由操作系统负责调度，协程由用户进行调度，因此减少了上下文切换，提高了效率。 线程的默认栈大小是1M，而协程更轻量，接近1K，因此可以在相同的内存中开启更多的协程。 由于在同一个线程上，可以避免竞争关系而使用锁。 适用于 I/O密集型的场景，不适用于计算密集型的场景。  3 注意事项 #  3.1 协程中不能调用导致线程阻塞的操作 #   假设协程运行在线程之上，并且协程调用了一个阻塞IO操作。 由于操作系统并不知道协程的存在，只知道线程，因此在协程调用阻塞IO的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度。 因此，在协程中不能调用导致线程阻塞的操作，也就是说，协程只有和异步IO结合起来，才能发挥最大的作用。  3.2 在协程中调用阻塞IO操作的处理方式 #   在调用阻塞IO操作的时候，重新启动一个线程去执行这个操作，同时将当前协程休眠（让出执行权），等执行完成之后，再唤醒协程，让协程去读取结果，这其实和多线程没有太大区别。 对系统的IO进行封装，改成异步调用的方式，这需要大量的工作，最好寄希望于编程语言原生支持。  4 线程与协程的比较 #   线程占用的资源初始时是1MB，固定不可变，协程占用的资源初始时一般为2KB，可随需求而增大。 线程的调度是由操作系统的内核完成，协程的调度是由用户来完成。 线程资源占用太高，频繁创建销毁会带来严重的性能问题，协程资源占用小，不会带来严重的性能问题。 线程需要用锁来保证数据的一致性和可见性，协程不需要多线程的锁机制，因为只有一个线程，所以不存在同时写变量冲突，所以执行效率比多线程高很多。  5 参考文献 #    什么是协程？  一文读懂什么是进程、线程、协程。  什么是协程？  "},{"id":39,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.9-HTTPS%E5%8A%A0%E5%AF%86%E5%92%8C%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B/","title":"1.2.9 HTTP S加密和认证过程","section":"1.2 应用层：HTTP和HTTPS","content":"HTTPS加密和认证过程 #  1 为什么要加密 #   因为HTTP 的内容是明文传输的，明文数据会经过中间代理服务器、路由器、WIFI 热点、通信服务运行商等多个物理节点。 如果信息在传输过程中被劫持，传输的内容就完全暴露了，同时劫持者还可以篡改传输的信息切不被对方察觉，这就是中间人攻击。 所以我们才需要对信息进行加密。  2 常见的加密算法 #  常见的加密算法可以分成三类，分别是对称加密算法、非对称加密算法和Hash 算法。\n2.1 对称加密算法 #  2.1.1 含义 #    对称加密算法指加密和解密使用相同密钥的加密算法，简单说就是有一个密钥，他可以加密一段信息，也可以对加密后的信息进行解密，和我们日常生活中用的钥匙作用差不多。 对称加密的优缺点为：  优点：  加解密的效率高、加密速度快。   缺点：  对于密钥的管理和分发上比较困难，不是非常安全，密钥管理负担很重。      2.1.2 常用算法 #  常用的对称加密算法包括DES、3DES、AES、DESX、Blowfish、RC4、RC5、RC6，下面以 DES、3DES、AES 为例进行简单介绍：\n DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合。 3DES（Triple DES）：基于 DES，对一块数据采用三个不同的密钥进行三次加密，强度更高。 AES（Advanced Encryption Standard）：高级加密标准，是下一代的加密算法标准，速度快、安全级别高。  2.1.3 对称加密用于 HTTPS 连接是否可行 #   如果通信双方都各自持有同一个密钥，且没有别人知道，这两方的通信安全当然是可以保证的。 然而最大的问题就是这个密钥怎么让传输的双方都知晓，同时不被别人知道，如果由服务器生成一个密钥并传输给浏览器，那么在这个传输过程中密钥被别人劫持到手了怎么办，因为之后他就能用密钥解开双方传输的任何内容，所以这么做当然不行。 换种思路，试想一下，如果浏览器内部就预存了网站 A 的密钥，且可以确保除了浏览器和网站 A，不会有任何外人知道该密钥，那理论上用对称加密是可以的，这样浏览器只要预存好世界上所有 HTTPS 网站的密钥就行了，这么做显然不现实，所以我们就需要 非对称加密。  2.2 非对称加密算法 #  2.2.1 含义 #    非对称加密指加、解密使用不同的密钥，一把作为公开的公钥，另一把作为私钥，公钥加密的信息，只有私钥才能解密，反之，私钥加密的信息，只有公钥才能解密。 非对称加密的优缺点为：  优点：  安全性更高，公钥是公开的，私钥是自己保存的，不需要将私钥给别人。   缺点：  加密和解密花费时间长、速度慢，只适合对少量数据进行加密。      2.2.2 常用算法 #  常用的非对称加密算法包括RSA、DSA（数字签名用）、ECC、Diffile-Hellman、EI Gamal，下面以 RSA、DSA 为例进行简单介绍：\n RSA：由 RSA 公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的。 DSA（Digital Signature Algorithm）：数字签名算法，是一种标准的数字签名标准。  2.2.3 非对称加密用于 HTTPS 连接是否可行 #   鉴于非对称加密的机制，我们可能会有这种思路，服务器先把公钥以明文方式传输给浏览器，之后浏览器向服务器传输数据前都先用这个公钥加密后再传，这条数据的安全似乎可以保障了，因为只有服务器有相应的私钥能解开公钥加密的数据。 然后反过来由服务器到浏览器的这条路怎么保障安全呢，如果服务器用他的私钥加密数据传给浏览器，那么浏览器用公钥可以解密他，而这个公钥一开始通过明文传输给浏览器的，若这个公钥被中间人劫持到了，那他也能用公钥解密服务器传来的信息了，所以目前似乎只能保证由浏览器向服务器传输数据的安全性（其实仍有漏洞，下文会说）。  2.2.4 改良的非对称加密方案，似乎可以 #   我们已经理解通过一组公钥私钥，可以保证单个方向传输的安全性，那用两组公钥私钥，是否就能保证双向传输都安全了，我们可以看一下下面的过程：  某网站服务器拥有公钥 A 与对应的私钥 A'，浏览器拥有公钥 B 与对应的私钥 B'。 浏览器把公钥 B 明文传输给服务器。 服务器把公钥明文传输给浏览器。 之后浏览器向服务器传输的内容都用公钥 A 加密，服务器收到后用私钥 A\u0026rsquo;解密，由于只有服务器拥有私钥 A'，所以能保证这条数据的安全。 同理，服务器向浏览器传输的内容都用公钥 B 加密，浏览器收到后用私钥 B\u0026rsquo;解密，同时也可以保证这条数据的安全。   的确可以，抛开这里面仍有的漏洞不谈（下文会讲），HTTPS 的加密却没有使用这种方案，很重要的原因是非对称加密算法非常耗时，而对称加密快很多，因此我们能不能运用非对称加密的特性解决前面提到的对称加密的漏洞呢。  2.2.5 非对称加密 + 对称加密 #   既然非对称加密耗时，那非对称加密 + 对称加密结合可以吗，而且得尽量减少非对称加密的次数，当然是可以的，且非对称加密、解密各只需用一次即可。 具体的过程如下：  某网站服务器拥有非对称加密的公钥 A、私钥 A'。 浏览器向网站服务器请求，服务器把公钥 A 明文传输给浏览器。 浏览器随机生成一个用于对称加密的密钥 X，用公钥 A 加密后传输给服务器。 服务器拿到后用私钥 A\u0026rsquo;解密得到密钥 X。 这样双方就都拥有密钥 X 了，且别人无法知道他，之后双方所有数据都通过密钥 X 加密解密即可。   HTTPS 基本就是采用这种方案的，但是会存在 中间人攻击。  2.3 Hash 算法 #  2.3.1 含义 #   Hash 算法的特别之处在于他是一种单向算法，用户可以通过 Hash 算法对目标信息生成一段特定长度的惟一的 Hash 值，却不能通过这个 Hash 值重新获得目标信息，因此 Hash 算法常用在不可还原的密码存储、信息完整性校验等。  2.3.2 常用算法 #  常用的 Hash 算法包括MD5、SHA、SHA1、SHA2、SHA-256、HMAC、HMAC-MD5、HMAC-SHA1，下面以 MD5、SHA-256 为例进行简单介绍：\n MD5（Message-Digest Algorithm）：一种被广泛使用的密码散列函数，可以产生出一个 128 位（16 字节）的散列值，用于确保信息传输完整一致，缺点是随着计算机运算能力的提高，有可能找到碰撞，因此，在安全要求高的场合不使用 MD5。 SHA-256（Secure Hash Algorithm 256）：是 SHA-2 细分出来的一种算法，可以产生出一个 256 位（32 字节）的散列值，安全性比 MD5 要高。  3 加密算法的选择 #   由于非对称加密算法的运行速度比对称加密算法速度慢很多，当我们需要加密大量的数据时，建议采用对称加密算法，提高加解密速度。 由于对称加密算法的密钥管理是一个复杂的过程，迷药的管理直接决定它的安全性，因此当数据量很小时，我们可以考虑采用非对称加密算法。 对称加密算法不能实现签名，因此签名只能使用非对称加密算法。 在实际的操作过程中，通常采用的方式是采用非对称加密算法管理对称加密算法的密钥，然后用对称加密算法加密数据，这样我们就集成了两类加密算法的优点，即实现了加密速度快的优点，又实现了安全方便管理密钥的优点。  4 HTTPS连接过程 #   客户端发送Client Hello给服务端，包含以下信息：  支持的TLS版本。 支持的加密方式。 随机数 random_C。 域名。   服务端收到请求后，发送Server Hello，包含以下信息：  确定TLS版本。 随机数 random_S。 确定加密方式。  数字证书：  内容（域名、公钥、有效期等） 签名。     客户端收到请求后，将会做一下事情：  验证证书：  操作系统和浏览器中存有CA机构的公钥，客户端使用CA机构的公钥对签名进行解密，解密成功说明证书由CA机构颁发。 签名成功解密后得到证书摘要，客户端使用Hash签名算法对证书内容进行摘要，然后和签名解密后的摘要比较，相等说明证书没有被修改过，证书内容可信。 客户端验证证书内容的域名和当前网址是否一致，证书是否过期。   生成随机数：  验证通过后，客户端生成随机数 Pre-master，然后用证书中的公钥进行加密，发送给服务端。     服务端：  服务端使用CA机构给的私钥对加密后的随机数进行解密，获取随机数 Pre-master。   客户端/服务端：  服务端和客户端分别用根据 random_C、b和 Pre-master生成密钥，用于加密传输数据。    参考文献 #    彻底搞懂 HTTPS 的加密原理。  https 的认证加密过程。  HTTPS 加密流程理解。  一文读懂对称加密算法、非对称加密算法和 Hash 算法。  对称与非对称加密算法。  对称加密、非对称加密、RSA(总结)。  加密算法原理分析(MD5、SHA-256)。  "},{"id":40,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.9-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81/","title":"2.1.9 异常控制流","section":"2.1 进程和线程","content":"异常控制流 #  1 什么是控制流 #   假设在计算机运行过程中，其指令序列为 $a_1,a_2,\u0026hellip;,a_n$，则每次从 $a_n$ 到 $a_{n+1}$ 的过渡叫做**控制转移**，而 $a_1,a_2,\u0026hellip;,a_n$ 这条**完整的控制转移序列**叫做处理器的**控制流**。 所以，控制流说白了就是CPU 执行指令的序列。  2 什么是控制流的异常 #   程序在加载进内存时指令序列也会跟着一同被加载。 指令序列在内存中是相邻的，程序计数器在通常情况下会依次加载这些相邻的指令交给 CPU 执行。 但是在某些情况下可能会发生突变，从而打破 CPU 本应该顺序执行这些指令序列的预期行为，这种突变就叫做控制流的异常，简称ECF（Exception Control Flow）。  3 理解异常控制流的必要性 #   ECF 是操作系统用来实现 I/O、进程和虚拟内存的基本机制。 陷阱和系统调用都是 ECF 的一种。 操作系统实现并发机制依靠的也是 ECF。 类似于 Java 的 try/catch 异常机制，属于应用层的 ECF。  4 异常处理程序 #  4.1 基本概念 #   异常处理程序就是处理异常的程序。 相关物理含义：  CPU 的状态：  CPU 的状态就是指其内部特定范围的一些数据位和电信号，CPU 状态的变化可以理解为这些数据位或电信号的变化。 CPU 状态的变化可能与当前执行的应用程序的指令有关，比如虚拟内存缺页、算术溢出、除以 0 等，也有可能与当前执行的指令无关，比如I/O 请求完成。   CPU 的事件：  CPU 的状态变化就是 CPU 的事件。   异常表：  在任何情况下，当 CPU 检测到有事件发生时，会通过一张映射表，找到一个被设计用来专门处理这类事件的操作系统的子程序，这个子程序就是异常处理程序。 而记录不同类型的异常处理程序之间的映射关系的映射表就是异常表。 异常表中对于每种可能类型的异常都分配了一个唯一的非负整数的异常号。 其中一些号码是由CPU 的设计者设计的，比如零除异常、缺页异常、内存访问违规异常、算术运算溢出异常，这些属于系统无关的异常。 而有一些异常号码则是由操作系统内核的设计者分配的，比如系统调用和IO 设备的信号，这些属于系统相关的异常。 操作系统在启动时会整合 CPU 异常和内核异常，并且将其记录成一张完整的异常表加载到内存中，然后将其起始地址放在一个叫做异常表基址寄存器的一个特殊的 CPU 寄存器里。      异常处理的基本过程：  假设当前 CPU 正在执行的指令为lcurr，此时CPU 检测到一个异常发生，进而 CPU 转去执行异常处理程序的相关指令。 当异常处理程序执行完之后，根据其处理结果可能有几种动作：  继续执行异常发生时正在执行的应用程序的指令 lcurr。 执行异常发生时正在执行的应用程序的指令 lcurr 的下一条指令 lnext。 该应用程序直接被终止，不再执行其指令。     无论是硬件触发了异常还是软件触发了异常，剩下的工作都是由异常处理程序在软件中完成。 在处理程序处理完事件之后，可以通过执行一条特殊的“从中断返回”的指令可选的返回到被中断的程序。  4.2 异常分类 #  异常可以分为四类，分别为中断、陷阱、故障、终止。\n 同步异常和异步异常的区别？\n 同步异常是指由当前程序本身引起的异常，如系统调用、缺页异常、段错误（访问的内存超过了系统所给这个程序的内存空间）。 异步异常是指由当前程序之外的因素引起的异常，如来自 IO 设备的信号。    4.2.1 中断 #   最常见的中断就是由IO设备引起的，比如我们在进行网络IO时，当网卡收到数据时，需要通知CPU将网卡缓冲区上的数据复制到内核缓冲区。 此时网卡驱动会向CPU芯片的一个引脚发信号，并且将自己的异常号发到系统总线上，这个引脚信号触发了ECF，异常号标识这个ECF是中断。 当CPU执行完当前指令时发现中断引脚的电压变高了，此时会从系统总线中读取异常号，然后从异常表中找到该异常对应的处理程序，即中断处理程序去处理异常。 当中断处理程序结束后，将继续执行中断发生前执行指令的下一条指令，所以对于应用层开发者来说，中断仿佛是不存在的，我们无法感知。 由于指令的执行者是CPU，但中断的发生是随机的，和CPU当前正在执行的指令毫无关系，所以是异步的，而对于陷阱、故障、终止来说，这三种异常是CPU执行当前指令所引起的，所以是同步的，这类指令也叫做故障指令。   4.2.2 陷阱 #   陷阱是有开发人员控制应用程序主动引起的异常，其最主要的用途是给应用程序提供一个调用系统内核服务的接口，也就是我们所熟悉的系统调用。 由于运行程序运行在用户态，是无法直接调用运行在内核态的内核服务，比如读文件read、创建新进程fork、加载新程序execve、终止当前进程exit，但在某些场景之下我们又是极其需要这些内核服务支持的，比如读文件就是一个非常常见的需求。 为了允许应用程序能够受控的访问这些内核服务，处理器提供了一条特殊指令 syscall n，当应用程序想调用编号为 n的内核服务时，就可以执行这条指令，执行 syscall指令会产生一个ECF，即陷阱。 CPU在检测到陷阱时，会在异常表中找到其异常处理程序并执行，执行完之后将控制返还给陷阱发生时正在执行指令的下一条指令。   4.2.3 故障 #   故障是由当前正在执行的指令引起的某种意外情况，这种意外情况可能能够被故障处理程序修复，也有可能修复不了。 假如当前正在执行的指令引起一个故障，处理器会从异常表中找到对应的故障处理程序：  如果能够成功处理故障，则重新执行引起故障的命令。 如果故障处理失败，则终止当前应用程序。   比较典型的故障如缺页故障，当指令引用一个虚拟地址，而该虚拟地址对应的物理页面不在内存中时，就会发生缺页故障，缺页处理程序会先将该页面从磁盘加载到内存中，然后重新运行引起故障的指令。   4.2.4 终止 #   终止可以理解为不可恢复的故障，通常是指硬件故障，比如内存会磁盘损坏。 当发生终止时，终止处理程序从不将控制返回给应用程序，而是将控制返回给 abort进程，该进程会终止引起故障的应用程序。   5 参考文献 #    进程的异常控制流：陷阱、中断、异常和信号。  （一）异常控制流。  "},{"id":41,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.10-%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E6%94%BB%E5%87%BB/","title":"1.2.10 常见网络攻击","section":"1.2 应用层：HTTP和HTTPS","content":"常见网络攻击 #  常见的网络攻击主要包括跨站脚本攻击（XSS）、注入攻击、模糊测试、零日攻击、路径（目录）遍历攻击、分布式拒绝服务攻击（DDos）、中间人攻击、暴力破解攻击、使用未知代码或第三方代码攻击、网络钓鱼攻击。\n1 跨站脚本攻击（XSS） #  1.1 含义 #   跨站脚本攻击，Cross-Site Scripting，简称 XSS，是一种代码注入攻击，攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行，利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。 XSS 的本质是恶意代码未经过滤，与正常的代码混在一起，浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行，而由于直接在用户的终端执行，恶意代码能够直接获取用户的信息，或者利用这些信息冒充用户向网站发起攻击者定义的请求。 在部分情况下，由于输入的限制，注入的恶意脚本比较短，但可以通过引入外部的脚本，并由浏览器执行，来完成比较复杂的攻击策略。 在处理输入时，以下内容都不可信：  来自用户的 UGC 信息。 来自第三方的链接。 URL 参数。 POST 参数。 Refer（可能来自不可信的来源）。 Cookie（可能来自其他子域注入）。    1.2 分类 #  根据攻击的来源，XSS 攻击可分为存储型、反射型和DOM 型三种。\n1.2.1 存储型 XSS #   存储型 XSS 的攻击步骤为：  攻击者将恶意代码提交到目标网站的数据库中。 用户打开目标网站时，网站服务端将恶意代码从数据库取出，拼接在 HTML 中返回给浏览器。 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。   这种攻击常见于带有用户保存数据的网站功能，如论坛发帖、商品评论、用户私信等。  1.2.2 反射型 XSS #   反射型 XSS 的攻击步骤为：  攻击者构造特殊的 URL，其中包含恶意代码。 用户打开带有恶意代码的 URL 时，网站服务端将恶意代码从 URL 中取出，拼接在 HTML 中返回给浏览器。 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。   反射型 XSS 和存储型 XSS 的区别是存储型 XSS 的恶意代码在数据库里，反射型 XSS 的代码在 URL 里。 反射型 XSS 漏洞常见于通过 URL 传递参数的功能，如网站搜索、跳转等，由于需要用户主动打开恶意的 URL 才能生效，攻击者往往会结合多种手段诱导用户点击。 POST 的内容也可以触发反射型 XSS，只不过其触发条件比较苛刻（需要构造表单提交页面，并引导用户点击），所以非常少见。  1.2.3 DOM 型 XSS #   DOM 型 XSS 的攻击步骤为：  攻击者构造特殊的 URL，其中包含恶意代码。 用户打开带有恶意代码的 URL。 用户浏览器接收到响应后解析执行，前端 JavaScript 取出 URL 中的恶意代码并执行。 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。   DOM 型 XSS 跟前两种 XSS 的区别是DOM 型 XSS 攻击中，取出和执行恶意代码由浏览器前端完成，属于前端 JavaScript 自身的安全漏洞，而其他两种 XSS 都属于服务端的安全漏洞。  1.3 预防 #  通过前面的介绍可以得知，XSS 攻击有两大要素：\n 攻击者提交恶意代码。 浏览器执行恶意代码。  1.3.1 输入过滤 #   针对第一个要素，我们是否能够在用户输入的过程，过滤掉用户输入的恶意代码呢：  在用户提交时，由前端过滤输入，然后提交到后端，这样做是否可行呢：  答案是不可行的，因为一旦攻击者绕过前端过滤，直接构造请求，就可以提交恶意代码了。   那么，换一个过滤时机，后端写入数据库前，对输入进行过滤，然后把安全的内容返回给前端，这样是否可行呢：  我们举一个例子，一个正常的用户输入了5 \u0026lt; 7 这个内容，在写入数据库前，被转义，变成了5 \u0026amp;alt; 7。 问题是，在提交阶段我们并不确定内容要输出到哪里：  用户输入的内容可能同时是提供给前端和客户端，而一旦经过了 escapeHTML()，客户端显示的内容就变成了乱码（5 \u0026amp;alt; 7）。 在前端中，不同的位置所需的编码也不同：  当5 \u0026amp;alt; 7作为 HTML 拼接页面时，可以正常显示。 当5 \u0026amp;alt; 7通过 Ajax 返回，然后赋值给 JavaScript 的变量时，前端得到的字符串就是转义后的字符，这个内容不能直接用于 Vue 等模板的展示，也不能直接用于内容长度计算，不能用于标题、等。         所以，输入侧过滤能够在某些条件下解决特定 XSS 问题，但会引入很大的不确定性和乱码问题，在防范 XSS 攻击时应避免此类方法。  1.3.2 防止浏览器执行恶意代码 #  既然输入过滤并非完全可靠，我们就要通过防止浏览器执行某些恶意代码来防范 XSS，这部分分为两类：\n 防止 HTML 出现注入。 防止 JavaScript 执行时，执行恶意代码。  1.3.2.1 防止存储型和反射型 XSS #   存储型和反射型 XSS 都是在服务端取出恶意代码后，插入到响应 HTML 里的，攻击者刻意编写的数据被内嵌到代码中，被浏览器所执行。 预防这两种漏洞，有两种常见做法：  改成纯前端渲染，把代码和数据分隔开。 对 HTML 做充分转义。    1.3.2.1.1 纯前端渲染 #   纯前端渲染的过程为：  浏览器先加载一个静态 HTML，此HTML 中不包含任何跟业务相关的数据。 然后浏览器执行 HTML 中的 JavaScript。 JavaScript 通过 Ajax 加载业务数据，调用 DOM API 更新到页面上。   在前端渲染过程中，我们会明确的告诉浏览器下面要设置的内容是文本（.innerText），还是属性（.setAttribute），还是样式（.style）等等，浏览器不会被轻易的欺骗，执行预期外的代码了。 但纯前端渲染还需注意避免 DOM 型 XSS 漏洞（例如onload 事件和href 中的javascript:xxx 等）。 在很多内部、管理系统中，采用纯前端渲染是非常合适的，但对于性能要求较高，或有 SEO 需求的页面，我们仍然要面对拼接 HTML 的问题。  1.3.2.1.2 转义 HTML #   如果拼接 HTML 是必要的，就需要采用合适的转义库，对 HTML 模板的各处插入点进行充分的转义。 常用的模板引擎，如doT.js、ejs、FreeMarker 等，对于 HTML 转义通常只有一个规则，就是把\u0026amp;、\u0026lt;、\u0026gt;、\u0026quot;、'、/ 这几个字符转义掉，确实能起到一定的 XSS 防护作用，但并不完善。 所以要完善 XSS 防护措施，我们要使用更完善更细致的转义策略，例如 Java 工程里，常用的转义库为org.owasp.encoder。  1.3.2.2 预防 DOM 型 XSS 攻击 #   DOM 型 XSS 攻击，实际上就是网站前端 JavaScript 代码本身不够严谨，把不可信的数据当做代码执行了。 在使用 .innerHTML()、.outerHTML()、document.write()时要特别小心，不要把不可信的数据作为 HTML 插到页面上，而要尽量使用 .textContent()、.setAttribute()等。 DOM 中的内联事件监听器，如location、onclick、onerror、onload、onmouseover等，\u0026lt;a\u0026gt;标签的 href 属性，JavaScript 的 eval()、setTimeout()、setInterval()等，都能把字符串作为代码运行，如果不可信的数据拼接到字符串中传递给这些 API，很容易产生安全隐患，请勿避免。  1.3.3 其他防范措施 #  虽然在渲染页面和执行 JavaScript 时，通过谨慎的转义可以防止 XSS 的发生，但完全依靠开发的谨慎仍然是不够的，以下介绍一些通用的方案，可以降低 XSS 带来的风险和后果。\n1.3.3.1 Content Security Policy #   严格的 CSP 在 XSS 的防范中可以起到以下的作用：  禁止加载外域代码，防止复杂的攻击逻辑。 禁止外域提交，网站被攻击后，用户的数据不会泄露到外域。 禁止内联脚本执行。 禁止未授权的脚本执行。    1.3.3.2 控制输入内容长度 #   对于不受信任的输入，都应该限定一个合理的长度，虽然无法阻止 XSS 的发生，但是可以增加 XSS 攻击的难度。  2 注入攻击 #  注入攻击包括 SQL 注入攻击和代码注入攻击两种，下面主要以 SQL 注入攻击为例进行介绍。\n2.1 SQL 注入攻击 #  2.1.1 含义 #   SQL 注入是发生于应用程序与数据库层的安全漏洞，本质是代码和数据未分离，通过在用户可控参数中注入 SQL 语法，程序未对输入的指令进行合法性判断，注入进去的恶意指令就会被数据库服务器误认为是正常的 SQL 指令而运行，破坏原有 SQL 结构，达到编写程序时意料之外结果的攻击行为。  2.1.2 分类 #   按照参数类型可分为两种，分比为数字型和字符型：  数字型：发生注入点的参数为整数，比如 ID、num、page 等。 字符型：发生注入点的参数为字符串，字符型注入需要引号来闭合。   按照数据库返回的结果可分为回显注入、报错注入、盲注：  回显注入：可以直接在存在注入点的当前页面中获取返回结果。 报错注入：程序将数据库的返回错误信息直接显示在页面上，虽然没有返回数据库的查询结果，但是可以构造一些报错语句从错误信息中获取想要的结果。 盲注：程序后端屏蔽了数据库的错误信息，没有直接显示结果也没有报错信息，只能通过数据库的逻辑和延时函数来判断注入的结果。    2.1.3 危害 #   泄露数据库中的敏感数据给未授权用户，SQL 注入漏洞可以绕过一些认证，导致用户名、密码等用户信息的泄露。 通过一些精心构造的注入手法，可能会获取到管理员的后台密码，甚至获得非法提权和用户系统的权限。 数据结构被黑客探知，得以做进一步攻击。 黑客可能在系统中添加恶意链接、恶意代码等，进行恶意篡改。  2.1.4 预防 #   检查变量数据类型和格式：  如果我们的 SQL 语句是类似where id = {$id} 这种形式，数据库里所有的 id 都是数字，那么就应该在 SQL 被执行前，检查确保变量 id 是 int 类型，如果是接收邮箱，那就应该检查并严格确保变量一定是邮箱的格式，其他的类型比如日期、时间等也是一个道理。 总结起来，也就是说只要有固定格式的变量，在 SQL 语句执行前，应该严格按照固定格式去检查，确保变量是我们想要的格式，这样很大程度上可以避免 SQL 注入攻击。   过滤特殊符号：  对于无法确定固定格式的变量，一定要进行特殊符号过滤或转义处理。 以 PHP 为例，通常是采用 addslashes 函数，他会在指定的预定义字符前添加反斜杠转义，这些预定义的字符是单引号（'）、双引号（\u0026quot;）、反斜杠（\\）、NULL。   绑定变量，使用预编译语句：  一般情况下一条 SQL 语句的执行要经过语义解析、制定执行计划、执行并返回结果。 预编译是指把要执行的 SQL 语句先进行一个解析，解析语法、确定查询范围、返回结果类型，也就是确定了查询的方式，把命令和参数进行了分离，使用预编译的 SQL 语句来进行查询时直接进行执行计划，不会再进行语义解析，也就是数据库不会再进行编译，而是直接执行编译过的 SQL，只需要替换掉参数部分就可以了，因此使用预编译语句能够防止 SQL 注入。 比如SELECT id, title, author, content FROM blog WHERE id = '?'，如果经过了预编译，那么这个时候不管 ? 输入的是什么，都不会调用编译器来解析 SQL 指令了，而是只替换掉参数的位置，然后执行编译好的查询，尽管输入 1 = ' or '1 = 1'--，也只会把这一部分当做参数替换掉 ? 的位置，查找字符串，而不会再解析 or 这个语法了。    3 中间人攻击 #  3.1 含义 #    中间人攻击（Man-in-the-MiddleAttack，简称 MITM 攻击）是一种间接的入侵攻击，这种攻击模式是通过各种技术手段将受入侵者控制的一台计算机虚拟放置在网络连接中的两台通信计算机之间，这台计算机就成为中间人。 中间人攻击的基本步骤为：  某网站有用于非对称加密的公钥 A、私钥 A'。 浏览器向网站服务器请求，服务器把公钥 A 明文传输给浏览器。 中间人劫持到公钥 A，保存下来，把数据包中的公钥 A 替换成自己伪造的公钥 B（他当然也拥有公钥 B 对应的私钥 B）。 浏览器生成一个用于对称加密的密钥 X，用公钥 B（浏览器无法得知公钥被替换了）加密后传输给服务器。 中间人劫持后用私钥 B\u0026rsquo;解密得到密钥 X，再用公钥 A 加密后传输给服务器。 服务器拿到后用私钥 A\u0026rsquo;解密得到密钥 X。   这样在双方都不会发现异常的情况下，中间人通过一套狸猫换太子的操作，掉包了服务器传来的公钥，进而得到了密钥 X，根本原因是浏览器无法确认收到的公钥是不是网站自己的。  3.2 预防 #  预防的基本思想就是解决上面提到的问题如何证明浏览器收到的公钥一定是该网站的公钥，这里就要用到数字证书。\n3.2.1 数字证书 #  3.2.1.1 什么是数字证书 #   网站使用 HTTPS 前，需要向 CA 机构申领一份数字证书，数字证书里含有证书持有者信息、公钥信息等。 服务器把证书传输给浏览器，浏览器从证书里获取公钥就行了，证书就如身份证，证明该公钥对应该网站。  3.2.1.2 如何防止数字证书被篡改 #    我们把证书原本的内容生成一份签名，比对证书内容和签名是否一致就能判别是否被篡改，这就是数字证书的防伪技术，这里的签名就叫数字签名。 数字签名的制作过程为：  CA 机构拥有非对称加密的四要和公钥。 CA 机构对证书明文数据 T 进行 Hash。 对 Hash 后的值用私钥加密，得到数字签名 S。   明文和数字签名共同组成了证书，这样一份数字证书就可以颁发给网站了。 浏览器拿到服务器传来的数字证书后，需要验证他是不是真的（有没有被篡改、掉包），具体的验证过程如下：  拿到证书，得到明文 T，签名 S。 用 CA 机构的公钥对 S 解密（由于是浏览器信任的机构，所以浏览器保有他的公钥，详情见下文），得到 S'。 用证书里指明的 Hash 算法对明文 T 进行 Hash 得到 T'。 通过以上步骤，T\u0026rsquo;应该等于 S'，除非明文或签名被篡改，所以此时通过比较 S\u0026rsquo;是否等于 T\u0026rsquo;来判断证书是否可信。    3.2.1.3 中间人有可能篡改改证书吗 #   假设中间人篡改了证书的原文，由于他没有 CA 机构的私钥，所以无法得到此时加密后的签名，也就无法相应地篡改签名。 浏览器收到该证书后发现原文和签名解密后的值不一致，则说明证书已被篡改，证书不可信，从而终止向服务器传输信息，防止信息泄露给中间人。  3.2.1.4 中间人有可能把证书掉包吗 #   假设有另一个网站 B 也拿到了 CA 机构认证的证书，他想劫持网站 A 的信息，于是他成为中间人拦截到了 A 传输给浏览器的证书，然后替换成自己的证书，传给浏览器，之后浏览器就会错误地拿到 B 的证书里的公钥了，这看起来会导致中间人攻击。 而实际上，这并不会发生，因为证书里包含了网站 A 的信息，包括域名，浏览器把证书里的域名与自己请求的域名比对一下就知道有没有被掉包了。  3.2.1.5 为什么制作签名时要 Hash 一次 #   这里主要是性能问题，因为非对称加密效率较差，证书信息一般较长，比较耗时，而Hash 得到的是固定长度的信息（比如用 MD5 算法 Hash 后可以得到固定 128 位的值），这样加解密就快很多。 当然也有安全上的原因，这部分可参考 Why hash the message before signing it with RSA?  3.2.1.6 怎么证明 CA 机构的公钥是可信的 #    操作系统、浏览器本身会预装一些他们信任的根证书，如果其中会有 CA 结构的根证书，这样就可以拿到他对应的可信公钥了。 实际上证书之间的认证也可以不止一层，可以A 信任 B、B 信任 C，以此类推，我们把他叫做信任链或数字证书链，也就是一连串的数字证书，以根证书为起点，透过层层信任，使终端实体证书的持有者可以获得转授的信任，以证明身份。 我们以前遇到过网站访问不了，提示需要安装证书的情况，这里安装的就是根证书，说明浏览器不认可给这个网站颁发证书的机构，那么我们就得手动下载安装该机构的根证书（风险自己承担），安装后，我们就有了他的公钥，就可以用他验证服务器发来的证书是否可信了。  参考文献 #    10 种常见网站安全攻击手段及防御方法。  前端安全系列（一）：如何防止 XSS 攻击？  SQL 注入攻击原理是什么？如何防护 SQL 注入？  1、sql 注入的原理与分类。  Web 安全之 SQL 注入攻击技巧与防范（转）。  【面经】预编译为什么可以防止 sql 注入，mybatis 是如何预防的。  中间人攻击。  彻底搞懂 HTTPS 的加密原理。  "},{"id":42,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.10-IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/","title":"2.1.10 Io多路复用","section":"2.1 进程和线程","content":"IO多路复用 #  1 什么是文件描述符 #   文件描述符在形式上是一个非负整数。 实际上，他是一个索引值，指向内核为每一个进程所维护该进程打开文件的记录表。 当程序打开一个现有文件或者创建一个新文件时，内核向该进程返回一个文件描述符。  2 水平触发和边缘触发 #  2.1 含义 #   水平触发（LT, Level Trigger）模式下，只要一个文件描述符就绪，就会触发通知，如果用户程序没有一次性把数据读写完，下次还会通知。 边缘触发（ET, Edge Trigger）模式下，当文件描述符从未就绪变为就绪时通知一次，之后不会再通知，直到再次从未就绪变为就绪。  2.2 区别 #   边缘触发效率更高，减少了被重复触发的次数，函数不会返回大量用户程序可能不需要的文件描述符。  2.3 为什么边缘触发一定要用非阻塞 IO #   避免由于一个描述符的阻塞读/写操作让处理其他描述符的任务出现饥饿状态。  3 五种 IO 模型 #  常见的 IO 模型主要有五种，分别是阻塞 IO（Blocking IO, BIO）、非阻塞 IO（NoneBlocking IO, NIO）、IO 多路复用（IO Multiplexing）、信号驱动 IO（Signal Driven IO）、异步 IO（Asynchronous IO, AIO）。\n3.1 阻塞 IO #   阻塞 IO 是一种最传统的 IO 模型，在读写数据过程中会发生阻塞现象。 当用户线程发出 IO 请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，此时用户线程就会处于阻塞状态，同时用户线程会交出 CPU。 当数据就绪之后，内核会将数据拷贝到用户线程，此时用户线程才会解除阻塞状态。 比如 A 同学排队买票，他只能排队买上票才可以离开，这一过程就可以看成是使用了阻塞 IO 模型，因为在没买到票之前，他不能离开队伍做别的事情。 很显然，阻塞 IO 模型是同步的。   3.2 非阻塞 IO #   当用户线程发起一个 IO 请求后，并不需要等待，而是马上就得到一个结果：  如果结果是一个 error 时，他就知道数据还没有准备好，于是他可以再次发送 IO 请求。 一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么他马上就将数据拷贝到了用户线程，然后返回。   在非阻塞 IO 模型中，用户需要不断地询问内核数据是否就绪，也就是说非阻塞 IO 不会交出 CPU，而是一直占用 CPU，这会导致 CPU 占用非常高。 比如 A 同学在买票过程中，采用了取号买票，在没有到他前，他可以不断的返回购票大厅看下是不是到了自己的号，中间的过程可以做其它的事情，这样他就不用向之前一样一刻也不能离开购票大厅了。 对于非阻塞模型来说，他只有检查数据是否到达的时候是非阻塞的，在数据到达时依然需要等待内核复制数据到用户空间，因此他还是同步的。   3.3 IO 多路复用 #  3.3.1 原理 #   所谓 IO 多路复用机制，就是说通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作，这种机制的使用需要 select、poll、epoll 来配合。 在 IO 多路复用模型中，会有一个内核线程不断地去轮询多个 socket 的状态，只有当真正读写事件发送时，才真正调用实际的 IO 读写操作。 在 IO 多路复用模型中，只需要使用一个线程就可以管理多个 socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有真正有读写事件进行时，才会使用 IO 资源，所以他可以大大减少资源占用。 比如 A 同学来买北京到南京的车票，发现有一排售票窗口，售票服务人员告诉他这些窗口目前没有票，等有票再告诉他，于是等啊等（select 调用中），过了一会，售票服务人员告诉他有票了，但不知道是哪个窗口卖北京到南京的票，需要让他自己看，于是 A 同学一个窗口一个窗口的问，直到找到卖北京到南京车票的窗口买上票（recv），epoll 也属于 IO 复用模型，主要区别在于售票服务人员告诉 A 同学哪几个窗口卖北京到南京的车票，不需要一个个去问了。 IO 多路复用本质上也是同步的，因为他们需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。   3.3.2 实现 #  3.3.2.1 select #  3.3.2.1.1 原理 #   将文件描述符放入一个集合中，调用 select 时，将这个集合从用户空间拷贝到内核空间，由内核根据就绪状态修改该集合的内容。 采用水平触发机制，只要一个文件描述符就绪，就会触发通知，如果用户程序没有一次性把数据读写完，下次还会通知。 select 返回后，需要通过遍历这个集合，找到就绪的文件描述符，当文件描述符的数量增加时，效率会线性下降。  3.3.2.1.2 缺点 #   单个进程打开的文件描述符是有限制的，可以通过FD_SETSIZE 设置，默认是 1024。 每次调用 select，都需要把文件描述符集合从用户态拷贝到内核态，当文件描述符很多时这个开销会很大。 select 返回后，不管哪个 socket 是活跃的，都要遍历一下文件描述符的集合，这会浪费很多 CPU 时间，效率较低。  3.3.2.2 poll #  3.3.2.2.1 原理 #   poll 和select 几乎没有区别，主要的区别在于文件描述符的存储方式不同，poll采用链表的方式存储，没有最大存储数量的限制。  3.3.2.3 epoll #   通过内核和用户空间共享内存，避免了不断复制的问题。 支持的同时连接数上限很高（1G 左右的内存支持 10W 左右的连接数）。 文件描述符就绪时，采用回调机制，回调函数将就绪的文件描述符添加到一个链表中，执行 epoll_wait 时，返回这个链表，避免了轮询。 支持水平触发和边缘触发，采用边缘触发机制时，只有活跃的文件描述符才会触发回调函数。  3.3.3 什么时候使用 select/poll，是么时候使用 epoll #   当连接数较多并且有很多的不活跃连接时，epoll的效率比其它两者高很多。 但是当连接数较少并且都十分活跃的情况下，由于epoll需要很多回调，因此性能可能低于其他两者。  3.4 信号驱动 IO #   在信号驱动 IO 模型中，当用户线程发起一个 IO 请求操作，会给对应的 socket 注册一个信号函数，然后用户线程会继续执行。 当内核数据就绪时，会发送一个信号给用户线程，用户线程接收到信号后，便在信号函数中调用 IO 读写操作来进行实际的 IO 请求操作。 这个一般用于 UDP中，对 TCP 套接字几乎没用，原因是该信号产生的过于频繁，并且该信号的出现并没有告诉我们发生了什么请求。 比如 A 同学让售票服务人员等有票的时候通知他，没过多久 A 同学得知有票了，于是跑去买票。 由于读写操作还是需要信号函数来完成，所以信号驱动 IO 也是同步的。   3.5 异步 IO 模型 #   异步 IO 模型是调用 aio_read，让内核数据准备好，并且复制到用户进程空间后执行指定好的函数。 例如 A 同学让售票服务人员帮他买好票后通知他，整个过程 A 同学都可以做别的事情。 前面的四种 IO 模型实际上都属于同步 IO，只有最后一种是真正的异步 IO，因为无论是多路复用模型还是信号驱动模型，IO 操作的第 2 个阶段都会引起用户线程阻塞，也就是内核进行数据拷贝的过程会让用户线程阻塞。   4 参考文献 #    什么是 IO 多路复用？怎么实现？  I/O 多路复用底层原理前篇 - 五种 IO 模型。  彻底理解 IO 多路复用实现机制。  5 中 IO 模型整理总结。  "},{"id":43,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.2-%E5%BA%94%E7%94%A8%E5%B1%82HTTP%E5%92%8CHTTPS/1.2.11-REST/","title":"1.2.11 Rest","section":"1.2 应用层：HTTP和HTTPS","content":"REST #  1 含义 #   REST 这个词，是Roy Thomas Fielding 在他 2000 年的博士论文中提到的。 Fielding 将他对互联网软件的架构原则，定义为 REST（Representational State Transfer），即表现层状态转化，如果一个架构符合 REST 原则，就称它为 RESTful 架构。 要理解 RESTful 架构，最好的方法就是去理解 Representational State Transfer 这个词组到底是什么意思，他的每一个词代表了什么涵义，如果我们把这个名称搞懂了，也就不难体会 REST 是一种什么样的设计了。 REST 架构主要有三个关键词，分别是资源（Resources）、表现层（Representation）、状态转化（State Transfer）：  资源：  REST 的名称表现层状态转化中，省略了主语，表现层其实指的是资源的表现层。 所谓资源，就是网络上的一个实体，或者说是网络上的一个具体信息，他可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的存在。 我们可以用一个 URI（统一资源标识符）指向他，每种资源对应一个特定的 URI，要获取这个资源，访问他的 URI 就可以，因此URI 就成了每一个资源的地址或独一无二的标识符。   表现层：  资源是一种信息实体，他可以有多种外在表现形式，我们把资源具体呈现出来的形式，叫做他的表现层。 比如，文本可以用 TXT 格式表现，也可以用 HTML、XML、JSON 格式表现，甚至可以采用二进制格式。 URI 只代表资源的位置，他的具体表现形式，应该在 HTTP 请求的头信息中用 Accept 和 Content-Type 字段指定，这两个字段才是对表现层的描述。   状态转化：  访问一个网站，就代表了客户端和服务器的一个互动过程，在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP 协议，是一个无状态的协议，这意味着，所有的状态都保存在服务器端，因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生状态转化，而这种转化是建立在表现层之上的，所以就是表现层状态转化。     综合上面的解释，我们总结一下什么是 RESTful 架构：  每一个 URI 代表一种资源。 客户端和服务器之间，传递这种资源的某种表现层。 客户端通过四个 HTTP 动词（GET 获取资源、POST 新建或更新资源、PUT 更新资源、DELETE 删除资源），对服务器端资源进行操作，实现表现层状态转化。    2 特点 #    客户-服务器：\n 在REST风格中，最基本的要求是对于一个程序来说，应当分离用户接口和数据存储，改善用户接口跨平台迁移的可移植性，同时简化服务器组件，改善系统的可伸缩性，客户端负责与用户之间的交互处理，而服务器端则实现数据存储以及相关的业务逻辑。 对于服务器端，完整的系统大部分情况下都会包含多个不同的模块，这些模块之间的调用也应当遵循客户-服务器模式，模块之间通过接口进行互相访问。    无状态：\n 服务端在设计接口时，应当设计为无状态接口，也就是说，服务器端不保存任何与客户端相关的状态上下文信息，客户端在每次调用服务端接口时，需要提供足够的信息，以供服务端完成操作。 在无状态的设计中，服务端减少了保存客户端相关上下文数据，因此，一方面服务端能够更加容易的实现动态扩展，而不至于影响客户端使用，另一方面则减少了服务端从故障中恢复的任务量。    缓存：\n 根据接口的实际情况，应当在接口设计中增加缓存策略，服务端可以决定是否可以缓存当前返回的数据，通过这种方式，可以在一定程度上减少实际到达服务端的请求，从而提高网络访问性能。 但缓存需要谨慎使用，缓存哪些数据、缓存过期时间都是需要根据实际情况进行设计，适当的缓存可以有效地提高系统效率，但是如果设计不当，将有可能导致大量的过期数据，进而影响系统运行。 一般而言，数据字典类数据、修改频率非常低的数据、实时性要求很低的数据等，这些数据可以设计一定的缓存策略，以提高系统运行效率。    系统分层：\n 在设计系统时，尤其是大型系统，通常需要将系统按照不同的功能进行横向和纵向的分层，例如横向分层一般可分为交互层、服务层、数据层等，而纵向分层则通常按照不同的业务功能对系统进行切分。 经过分层后，系统将划分为不同的模块进行独立开发部署运行，不同的模块可以独立进化，实现功能解耦，提高整个系统的可扩展性。    统一接口：\n 统一接口，即不同的系统模块之间的调用接口统一规范，使用统一的调用协议，统一的数据格式等。 统一接口带来的是系统交互功能的规范化，接口调用与业务解耦，各模块独立进化。    按需代码（可选）：\n 按需代码允许我们灵活的发送一些看似特殊的代码给客户端，如JavaScript代码。 按需代码的好处是可以减少一些代码，简化客户端。    3 RESTful API 设计规范 #    协议：\n API 与用户的通信协议，总是使用 HTTPS 协议。    域名：\n  应该尽量将 API 部署在专用域名之下：\nhttps://api.example.com   如果确定 API 很简单，不会有进一步的扩展，可以考虑放在主域名下：\nhttps://example.org/api     版本：\n  应该将 API 的版本号放入 URL：\nhttps://api.example.com/v1   另一种做法是，将版本号放在 HTTP 头信息中，但不如放入 URL 方便直观， Github采用这种做法。\n    路径：\n  路径又称终点（Endpoint），表示 API 的具体网址。\n  在 RESTful 架构中，每个网址代表一种资源，所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库中的表格名对应，一般来说，数据库中的表都是同种记录的集合，所以API 中的名词也应该使用复数。\n  举例来说，有一个 API 提供动物园的信息，还包括各种动物和雇员的信息，则他的路径应该设计成下面这样：\nhttps://api.example.com/v1/zoos https://api.example.com/v1/animals https://api.example.com/v1/employees     HTTP 动词：\n  对于资源的具体操作类型，由 HTTP 动词表示。\n  常见的 HTTP 动词有下面五个：\n GET：从服务器取出资源。 POST：在服务器新建一个资源。 PUT：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH：在服务器更新资源（客户端提供改变的属性） DELETE：从服务器删除资源。    还有两个不常用的 HTTP 动词：\n HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。    具体的示例如下：\nGET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物     过滤信息：\n  如果记录数量很多，服务器不可能都将他们返回给用户，API 应该提供参数，过滤返回结果。\n  下面是一些常见的参数：\n?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2\u0026amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name\u0026amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件   参数的设计允许存在冗余，即允许 API 路径和 URL 参数偶尔有重复，比如 GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。\n    状态码：\n  服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的 HTTP 动词）：\n200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与 401 错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求 JSON 格式，但是只有 XML 格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。   状态码的完整列表参见 10 Status Code Definitions。\n    错误处理：\n  如果状态码是 4xx，就应该向用户返回错误信息，一般来说，返回的信息中将 error 作为键名，出错信息作为键值即可，例如：\n{ error: \u0026#34;Invalid API key\u0026#34; }     返回结果：\n  针对不同操作，服务器向用户返回的结果应该符合以下规范：\nGET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档     Hypermedia API：\n  RESTful API 最好做到 Hypermedia，即返回结果中提供链接，连向其他 API 方法，使得用户不查文档，也知道下一步应该做什么。\n  比如，当用户向 api.example.com 的根目录发出请求，会得到这样一个文档：\n{\u0026#34;link\u0026#34;: { \u0026#34;rel\u0026#34;: \u0026#34;collection https://www.example.com/zoos\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;https://api.example.com/zoos\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;List of zoos\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.yourformat+json\u0026#34; }}   上面代码表示，文档中有一个 link 属性，用户读取这个属性就知道下一步该调用什么 API 了：\n rel：表示这个 API 与当前网址的关系（collection 关系，并给出该collection 的网址）。 href：表示 API 的路径。 title：表示 API 的标题。 type：表示返回类型。    Hypermedia API 的设计被称为 HATEOAS，Github的API就是这种设计，访问 api.github.com会得到一个所有可用API的网址列表：\n{ \u0026#34;current_user_url\u0026#34;: \u0026#34;https://api.github.com/user\u0026#34;, \u0026#34;authorizations_url\u0026#34;: \u0026#34;https://api.github.com/authorizations\u0026#34;, // ... }   从上面可以看到，如果想获取当前用户的信息，应该去访问 api.github.com/user，然后就得到了下面的结果：\n{ \u0026#34;message\u0026#34;: \u0026#34;Requires authentication\u0026#34;, \u0026#34;documentation_url\u0026#34;: \u0026#34;https://developer.github.com/v3\u0026#34; }   上面代码表示，服务器给出了提示信息，以及文档的网址。\n    其他：\n API的身份认证应该使用 OAuth 2.0框架。 服务器返回的数据格式，应该尽量使用JSON，避免使用XML。    参考文献 #    深入理解什么是 RESTful API ？  Restful 应用理解。  怎样用通俗的语言解释REST，以及RESTful？  前端要知道的RESTful API架构风格。  "},{"id":44,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.1-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/2.1.11-%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81/","title":"2.1.11 用户态和内核态","section":"2.1 进程和线程","content":"用户态和内核态 #  1 什么是用户态和内核态 #   为了限制不同程序的访问能力，防止一些程序访问其他程序的内存数据，CPU划分了用户态和内核态两个权限等级：  用户态只能受限地访问内存，且不允许访问外围设备，没有占用CPU的能力，CPU资源可以被其他程序获取。 内核态可以访问内存所有数据以及外围设备，也可以进行程序的切换。   所有用户程序都运行在用户态，但有时需要进行一些内核态的操作，比如从硬盘或者键盘读数据，这时就需要进行系统调用，使用陷阱指令，CPU切换到内核态，执行相应的服务，再切换回用户态并返回系统调用的结果。  2 为什么要区分用户态和内核态 #   主要是为了区别执行特权指令和非特权指令。 在CPU的所有指令中，有一些指令是非常危险的，如果错用，将导致整个系统崩溃，比如清内存、设置时钟等。 所以，CPU将指令分为特权指令和非特权指令，对于那些危险的指令，只允许操作系统及其相关模块使用，普通的应用程序只能使用那些不会造成灾难的指令。  3 用户态和内核态之间如何切换 #  用户态和内核态之间的切换主要有三种方式，分别是系统调用、中断、异常。\n3.1 系统调用 #   这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如fork()实际上就是执行了一个创建新进程的系统调用。 而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现的。  3.2 中断 #   当外围设备完成用户请求操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序。 如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换，比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作。  3.3 异常 #   当CPU在执行运行在用户态下的程序时，发生了某些事不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。  参考文献 #    什么是用户态和内核态？  用户态和核心态的概念以及为什么要区别？以及两者之间的切换？  "},{"id":45,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.2-%E6%AD%BB%E9%94%81/","title":"2.2 死锁","section":"2、操作系统","content":"死锁 #  1 什么是死锁 #   在两个或者多个并发进程中，每个进程持有某种资源而又等待其它进程释放他们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。  2 死锁产生的必要条件 #   互斥条件： 一个资源每次只能被一个进程使用，即在一段时间内某资源仅为一个进程所使用，此时如果有其它进程请求该资源，则请求进程只能等待。 请求与保持条件： 进程中已经保持了至少一个资源，但又提出新的资源请求，而该资源已经被其它进程占有，此时请求进程被阻塞，但对自己已经获得的资源保持不放。 不可剥夺条件： 进程未使用完的资源在未使用完毕之前，不能被其它进程强行夺走，即只能由获得该资源的进程自己来释放。 循环等待条件： 若干进程间形成首尾相接循环等待资源的关系，在发生死锁时必然存在一个进程等待队列 ${P_1,P_2,\u0026hellip;,P_n}$，其中 $P_1$ 等待 $P_2$ 占有的资源，$P_2$ 等待 $P_3$ 占有的资源，\u0026hellip;，$P_n$ 等待 $P_1$ 占有的资源，形成一个进程等待环路，环路中每一个进程所占有的资源同时被另一个申请。   注意：\n 上面的四个条件是死锁的必要条件，只要发生死锁，这些条件必然成立。 但只要上述条件有一条不满足，就不会发生死锁。   3 死锁的处理方法 #  3.1 鸵鸟策略 #   直接忽略死锁，因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任何措施的方案会获得更高的性能。 当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。   鸵鸟策略是指当鸵鸟看到危险的时候，就把头埋在沙子里，装作看不到。\n 3.2 死锁预防 #  死锁预防的基本思想是破坏形成死锁的四个必要条件。\n  破坏互斥条件： 允许某些资源同时被多个进程访问，但是有些资源本身并不具备这种属性，因此这种方案实用性有限。\n  只读数据文件、磁盘等软硬件资源均可采用这种办法管理。 可写文件、键盘等独占性资源只能互斥的占有，不能采用这种办法管理。     破坏请求与保持条件：\n 实现资源预分配策略，当一个进程开始运行之前，必须一次性向系统申请他所需要的全部资源，否则不运行。 这种方式的缺点：  很多时候无法预知一个进程所需的全部资源。 会降低资源利用率，降低系统的并发性，因为在每个进程占有的资源中，有些资源在运行后期使用，有些资源在例外情况下使用，所以可能造成进程占有一些几乎用不到的资源，而使其它想使用这些资源的进程等待。      破坏不可剥夺条件： 剥夺调度能够防止死锁，但是只适用于内存和处理器资源。\n 占有资源的进程若要申请新资源，必须主动释放已占有资源，若需要此资源，应该向系统重新申请。 资源分配管理程序为进程分配新资源时，若有则分配，否则将剥夺此进程已占有的全部资源，并让进程进入等待资源状态，资源充足后再唤醒它重新申请所需的资源。    破坏循环等待条件：\n 给系统的所有资源编号，规定进程请求所需资源的顺序必须按照资源的编号依次进行。  一个进程得到某一层的资源后，只能申请较高一层的资源。 当进程释放某一层的资源时，必须先释放所占有的较高层的资源。 当进程获得某层的一个资源时，如果想申请同层的另一个资源，必须先释放此层中已占有的资源。      3.3 死锁避免 #   动态地检测资源分配状态，以确保系统处于安全状态，只有处于安全状态时才会进行资源的分配。 所谓安全状态是指即使所有进程突然请求需要的所有资源，也能存在某种对进程的资源分配顺序，使得每一个进程运行完毕。  3.4 死锁检测与死锁恢复 #  3.4.1 死锁检测 #   死锁检测是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁发生。  3.4.2 死锁恢复 #   死锁剥夺法： 剥夺陷于死锁的进程所占用的资源，但并不撤销此进程，直至死锁解除。 进程回退法： 根据系统保存的检查点让所有的进程回退，直到足已解除死锁，这种措施要求系统建立保存检查点、回退及重启机制。 进程撤销法：  撤销陷入死锁的所有进程，解除死锁，继续运行。 逐个撤销陷入死锁的进程，回收其资源并重新分配，直至死锁解除。 可选择符合下面条件之一的先撤销：  CPU消耗时间最少者。 产生的输出量最小者。 预计剩余执行时间最长者。 分得的资源数量最少者。 优先级最低者。     系统重启法： 结束所有进程的执行并重新启动操作系统，这种方法很简单，但先前的工作全部作废，损失很大。  4 参考文献 #    什么是死锁？  死锁概念，死锁产生的四个必要条件，如何避免和预防死锁。  计算机操作系统 - 死锁。  死锁的产生、防止、避免、检测和解除。  "},{"id":46,"href":"/school-recruitment/docs/database/2MySQL/2.1-%E7%B4%A2%E5%BC%95/","title":"2.1 索引","section":"2、 My SQL","content":"索引 #  1 含义 #   索引是存储数据库表中的一列或多列的值并对其进行排序的一种数据结构，他通过缩小一张表中需要查询的记录的数目来加快搜索的速度，如果没有索引，数据库将不得不进行全表扫描。 索引相当于图书中的目录，可以根据目录上的页码快速找到所需的内容，提高查询速度。  2 优缺点 #  2.1 优点 #   通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，避免进行全表的数据扫描，大大减少遍历匹配的行数，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。  2.2 缺点 #   创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占用物理空间，数据量越大，占用空间越大。 会降低表的增删改的效率，因为每次增删改索引，都需要进行动态维护。  3 在哪些列建立索引 #   在经常需要搜索的列上，可以加快搜索的速度。 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构。 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度。 在经常需要根据范围进行搜索的列上，因为索引已经排序，其指定的范围是连续的。 在经常需要排序的列上，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。 在经常使用在 where 子句的列上，因为这样可以加快条件的判断速度。 在经常需要统计或分组的列上。  4 不在哪些列创建索引 #   对于那些在查询中很少使用的列不应该创建索引，因为既然这些列很少使用到，因此有索引或者无索引并不能提高查询速度，相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那些只有很少取值的列也不应该创建索引，因为由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大，增加索引并不能明显加快检索速度。 对于那些定义为 text、image、bit 数据类型的列不应该创建索引，因为这些列的数据量要么相当大，要么取值很少。 当修改性能远远大于检索性能时，不应该创建索引，因为修改性能和检索性能是互相矛盾的，当增加索引时，会提高检索性能，但是会降低修改性能，当减少索引时，会提高修改性能，但是会降低检索性能。  5 什么情况下索引会失效 #    在 where 子句中进行 null 值判断：\nSELECT id FROM table WHERE num is null;  在建立数据库的时候尽量为字段设置默认值，如 int 类型可以使用 0，varchar 类型使用''，当我们在指定类型大小如 int(11) 时，其实空间就已经固定了，即使存的是 null 也是这个大小。\n   在 where 子句中使用 !=、\u0026lt;\u0026gt; 这样的符号：\nSELECT id FROM table WHERE num != 0;  可以考虑使用 between，但是只能是连续的数值：\nSELECT id FROM table WHERE num BETWEEN 0 AND 1;    在 where 子句中使用 in（分两种情况，一种走索引，一种不走索引）：\n  走索引：\nSELECT id FROM table WHERE num IN (1);  其实在 in 里面，如果只有一个值的话等价于 num = 1。\n   不走索引：\nSELECT id FROM table WHERE num IN (1,2);     在 where 子句中 = 的左边使用表达式操作或函数操作：\n  表达式：\nSELECT id FROM table WHERE num / 2 = 1;   函数：\nSELECT id FROM table WHERE SUBSTRING(name,1,2) = \u0026#39;wise\u0026#39;;     在 where 子句中使用 like 模糊查询：\nSELECT id FROM table WHERE name LIKE \u0026#39;wise\u0026#39;;   在使用联合索引时要注意最左原则，例如当联合索引是 index(a,b,c) 时，如果 where 子句中有 a 就会用到联合索引，但是如果只用到 b、c 就会失去索引效果。\n  6 索引分类 #  6.1 从存储结构上来划分 #  从存储结构上来划分，索引可分为 哈希索引（Hash Index）、B 树索引（B-Tree Index、B+Tree Index）。\n6.1.1 哈希索引 #  6.1.1.1 含义 #   哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效。 对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，不同键值的行计算出来的哈希码也不一样。 哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 在 MySQL 中，只有 Memory 引擎显式支持哈希索引，这也是 Memory 引擎表的默认索引类型，Memory 引擎同时也支持 B-Tree 索引。 Memory 引擎支持非唯一哈希索引，如果多个列的哈希值相同，索引会以链表的方式存放多个记录指针到同一个哈希条目中。  6.1.1.2 示例 #  假如有如下表：\nCREATE TABLE testhash ( fname VARCHAR(so) NOT NULL, lname VARCHAR(so) NOT NULL, KEYUSING HASH(fname) ) ENGINE=MEMORY; 表中包含如下数据：\n 假设索引使用假想的哈希函数 $f()$，他返回下面的值（都是示例数据，非真实数据）：\n 则哈希索引的数据结构如下：\n  注意每个槽的编号是顺序的，但是数据行不是。\n 现在，来看如下查询：\nmysql\u0026gt; SELECT lname FR 叩 testhash 副 ERE fname=\u0026#39;Peter\u0026#39;;  MySQL 会先计算Peter 的哈希值，并使用该值寻找对应的记录指针。 因为 $f(\u0026lsquo;Peter\u0026rsquo;)=8784$，索引 MySQL 在索引中查找 8784，可以找到指向第 3 行的指针。 最后一步是比较第三行的值是否为Peter，以确保就是要查找的行。  6.1.1.3 优缺点 #  6.1.1.3.1 优点 #   哈希索引把数据的索引以哈希值的形式组织起来，因此检索效率非常高，可以一次定位，不像B-/B+Tree 索引需要进行从根节点到叶节点的多次 I/O 操作。  6.1.1.3.2 缺点 #    哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行，不过，访问内存中的行的速度很快，所以大部分情况下，这一点对性能的影响并不明显。\n  哈希索引数据不是按照索引值顺序存储的，因此无法用于排序，因为数据经过哈希算法后，大小关系就可能发生变化，排序是没有意义的。\n  哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的，例如，在数据列 (A, B) 上建立哈希索引，如果查询只有数据列 A，则无法使用该索引。\n  哈希索引只支持等值比较查询，包括 =、IN()、\u0026lt;=\u0026gt;（注意 \u0026lt;\u0026gt; 和 \u0026lt;=\u0026gt; 是不同的操作），也不支持任何范围查询，例如 WHERE price \u0026gt; 100，因为数据在经过哈希算法后，其大小关系就可能发生变化。\n \u0026lt;=\u0026gt; 是安全比较运算法，用来做 NULL 值的关系运算，和 IS NULL 等价。\n   当哈希冲突较多时，哈希索引的性能会下降，冲突越多，下降的程度越大，这是因为：\n 当出现哈希冲突时，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。 同时，如果哈希冲突很多的话，一些索引维护操作的代价也会很高，因为当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应行的引用。    6.1.2 全文索引 #   MySQL 的 InnoDB 从 1.2.x 版本开始支持全文检索，如无特殊说明，以下的数据库引擎均为 InnoDB。\n 6.1.2.1 倒排索引 #  全文检索使用倒排索引来实现，倒排索引同 B+ 树一样，也是一种数据结构，他在辅助表（Auxiliary Table）中存储了单词与单词自身在一个或多个文档所在未知的映射，这通常利用关联数组来实现，主要拥有两种表现形式：\n Inverted File Index（倒排文件索引）：表现形式为{单词，单词所在文档的 ID}。 Full Inverted Index（详细倒排索引）：表现为{单词，（单词所在文档 ID，文档中的位置）}。  例如，全文检索表为：\n   DocumentID Text 文档内容     1 Souyunku Technical team (搜云库技术团队)   2 Go Technical stack (Go 技术栈)    Inverted File Index 类型的辅助表存储为：\n   Number Text 分词 Documents (单词所在文档 ID)     1 Souyunku 1   2 Technical 1，2   3 team 1   4 Go 2   5 stack 2    Full Inverted Index 类型的辅助表存储为：\n   Number Text 分词 Documents (单词所在文档 ID:文档中的位置)     1 Souyunku 1:1   2 Technical 1:2 ，2:2   3 team 1:3   4 Go 2:1   5 stack 2:3    相比 Inverted File Index，Full Inverted Index 还存储了单词所在的位置信息，因此会占用更多的空间，但是能更好的的定位数据，提供更多的搜索特性。\n6.1.2.2 全文检索索引缓存 #   辅助表是存在于磁盘上的持久化的表，由于磁盘 I/O 比较慢，因此提供 FTS Index Cache（全文检索索引缓存）来提高性能。 FTS Index Cache 是一个红黑树结构，根据（word, list）排序，这意味着插入的数据已经更新了对应的表，但是对全文索引的更新可能在分词操作后还在 FTS Index Cache 中，辅助表可能还没有更新，这是因为InnoDB 存储引擎会批量对辅助表进行更新，而不是每次插入后都更新一次，从而提高了 InnoDB 搜索引擎的性能，并且由于其根据红黑树排序后进行批量插入，因此产生的辅助表相对较小。当对全文检索进行查询时，辅助表首先会将在 FTS Index Cache 中对应的 word 字段合并到辅助表中，然后再进行查询。 当数据库关闭时，在 FTS Index Cache 中的数据库会同步到磁盘上的辅助表中，当数据库宕机时，一些 FTS Index Cache 中的数据库可能未被同步到磁盘上，那么下次重启数据库时，当用户对表进行全文检索（查询或者插入操作）时，InnoDB 存储引擎会自动读取未完成的文档，然后进行分词操作，再将分词的结果放入到 FTS Index Cache 中，可以通过配置参数innodb_ft_cache_size 控制缓存的大小，默认为 32M，当该缓存满时，会将其中的（word, list）分词信息同步到磁盘的辅助表中，提高该值，可以提高全文检索的性能，但在故障时，需要更久的时间恢复。 在 InnoDB 存储引擎中，为了支持全文检索，必须有一个列与 word 进行映射，在 InnoDB 中这个列被命名为FTS_DOC_ID，其类型必须是BIGINT UNSIGNED NOT NULL，并且 InnoDB 存储引擎自动会在该列上加入一个名为FTS_DOC_ID_INDEX的Unique Index，上述操作都由 InnoDB 存储引擎自己完成，用户也可以在建表时自动添加 FTS_DOC_ID，已经相应的 Unique Index，但是必须注意相应的数据类型，否则 MySQL 数据库会报错。 文档中分词的插入操作是在事务提交时完成，然而对于删除操作，其在事务提交时，不删除磁盘辅助表中的记录，而只是删除 FTS Index Cache 中的记录，对于要删除的记录，InnoDB 存储引擎会记录其 FTS Document ID，并将其保存在 DELETED Auxiliary Table 中，在设置参数innodb_ft_aux_table 后，用户可以访问information_schema 架构下的表INNODB_FT_DELETED 来观察删除的 FTS Document ID，由于文档的 DML 操作实际并不删除索引中的数据，相反还会在对应的 DELETED 表中插入记录，因此随着应用程序的运行，索引，索引会变得非常大，此时可以通过optimize table 命令手动删除无效索引记录，如果需要删除的内容非常多，会影响应用程序的可用性，此时可以通过 innodb_ft_num_word_optimize 控制每次删除的分词数量，默认为 2000，可以通过该参数来控制删除幅度。  6.1.2.3 全文索引的查询模式 #  MySQL 数据库支持全文检索（Full-Text Search）的查询，其语法为：\nMATCH (coll, col2,...) AGAINST (expr [search_modifier]) search_modifier: { IN NATURAL LANGUAGE MODE IN NATURAL LANGUAGE MODE WITH QUERY EXPANSION IN BOOLEAN MODE WITH QUERY EXPANSION } 其中\n MATCH 指定了需要被查询的列。 AGAINST 指定了使用何种方法去进行查询。  查询模式主要分为三种，分别是Natural Language、Boolean、Query Expansion。\n6.1.2.3.1 Natural Language #    全文检索通过 MATCH 函数进行查询，默认采用 Natural Language 模式，表示查询带有指定 word 的文档。\nSELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;Porridge\u0026#39; IN NATURAL MODE); -- 由于全文检索默认的查询模式为 NATURAL LANGUAGE，因此上面的查询语句和下面的等效 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;Porridge\u0026#39;);  上述语句的查询计划为：\n-- 在语句后面加上\\G 能够更清晰的查看查询信息 EXPLAIN SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;Porridge\u0026#39;)\\G;  type 这列显示 fulltext，即表示使用全文检索的倒排索引，而 key 这列显示了 idx_fts，表示索引的名字，可见上述查询使用了全文检索技术。\n  在 WHERE 条件中使用了 MATCH 函数，查询返回的结果是根据相关性（Relevance）进行降序排序的，即相关性最高的结果放在第一位，相关性的值是一个非负的浮点数字，0 表示没有任何相关性。\n  相关性的计算依据以下四个条件：\n word是否在文档中出现。 word在文档中出现的次数。 word在索引列中的数量。 多少个文档包含该 word。    对于 InnoDB 存储引擎的全文检索，还需要考虑以下因素：\n 查询的word在 stopword 列中，忽略该字符串的查询。 查询的word 的字符长度不在区间 [innodb_ft_min_token_size, innodb_ft_min_token_size] 内，忽略该字符串的查询，这两个参数用于控制 InnoDB 存储引擎查询字符的长度，在 InnoDB 存储引擎中，innodb_ft_min_token_size 的默认值为 3，innodb_ft_min_token_size 的默认值为 84。    6.1.2.3.2 Boolean #   MySQL 数据库允许使用 IN BOOLEAN MODE 修饰符来进行全文检索，当使用该修饰符时，查询字符串的前后字符会有特殊的含义。 Boolean 全文检索支持以下几种操作符：   + 表示该 word必须存在。\n  - 表示该 word必须被排除。\n-- 查询有字符串 Pease，但没有 hot 的文档 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;+Pease -hot\u0026#39; IN BOOLEAN MODE)\\G;   (no operator) 表示该 word 是可选的，但是如果出现，其相关性会更高。\n-- 查询有字符串 Pease 或有 hot 的文档 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;Pease hot\u0026#39; IN BOOLEAN MODE)\\G;   @distance 表示查询的多个单词之间的距离是否在 distance 之内，distance 的单位是字节，这种全文检索的查询也称为 Proximity Search。\n-- 查询字符串 Pease 和 hot 之间的距离在 30 字节内的文档 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;\u0026#34;Pease hot\u0026#34; @30\u0026#39; IN BOOLEAN MODE)\\G;   \u0026gt; 表示出现该单词时增加相关性。\n  \u0026lt; 表示出现该单词时降低相关性。\n-- 查询有字符串 like、pot 或 some 的文档，如果出现单词 pot，则对应文档的相关性要增加，如果出现单词 some，则对应文档的相关性要减少 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;like \u0026gt;pot \u0026lt;some\u0026#39; IN BOOLEAN MODE)\\G;   ~ 表示允许出现该单词，但是出现时相关性为负（全文检索查询允许负相关性）。\n  * 表示以该单词开头的单词。\n-- 查询包含以 po 开头的单词的文档 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;po*\u0026#39; IN BOOLEAN MODE)\\G;   \u0026quot; 表示短语。\n-- 查询有字符串 like 或有 hot 的文档 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;like hot\u0026#39; IN BOOLEAN MODE)\\G;  -- 查询有字符串 like hot 短语的文档 SELECT * FROM fts_a WHERE MATCH(body) AGAINST(\u0026#39;\u0026#34;like hot\u0026#34;\u0026#39; IN BOOLEAN MODE)\\G;      6.1.2.3.3 Query Expansion #   MySQL 数据库还支持全文检索的扩展查询，这种查询通常在查询的关键词太短，用户需要 implied knowledge（隐含知识）时进行，例如，对于单词database 的查询，用户可能希望的不仅仅是包含database 的文档，可能还指那些包含MySQL、Oracle、DB2、RDBMS 的单词，而这时可以使用 Query Expansion 模式来开启全文检索的 implied knowledge。 可以通过在查询短语中添加 WITH QUERY EXPANSION 或 IN NATURAL LANGUAGE MODE WITH QUERY EXPANSION 来开启 blind query expansion（又称为 automatic relevance feedback）。 Query Expansion 的查询分为两个阶段：  第一阶段：根据搜索的单词进行全文索引查询。 第二阶段：根据第一阶段产生的分词再进行下一次全文检索的查询。   具体实例如下：   Natural Language Mode：\nSELECT * FROM articles WHERE MATCH(title, body) AGAINST(\u0026#39;database\u0026#39; IN NATURAL LANGUAGE MODE);    Query Expansion：\nSELECT * FROM articles WHERE MATCH(title, body) AGAINST(\u0026#39;database\u0026#39; WITH QUERY EXPANSION);      6.1.2.4 全文索引的一些限制 #   每张表只能有一个全文检索的索引。 由多列组合的全文索引的索引列必须使用相同的字符集与排序规则。 不支持没有单词界定符（delimiter）的语言，如中文、日语、韩语等。  6.1.2 B 树索引 #  B 树索引的具体原理可参考 2.2 B 树、B+ 树索引算法原理。\n6.2 从应用层次上来划分 #  从应用层次上来划分，索引可分为普通索引、联合索引、唯一索引、主键索引、全文索引。\n6.2.1 普通索引 #   普通索引是 MySQL 中最基本的索引类型，没有任何限制，允许在定义索引的列中插入重复值和空值。  6.2.2 联合索引 #   联合索引是指对表上的多个列进行索引，其创建方法与单个索引的创建方法一样，不同之处仅在于有多个索引列。 本质上说联合索引也是一棵 B+ 树，不同的是联合索引的键值的数量不是 1，而是大于等于 2。 接下来讨论两个整型列组成的联合索引，假定两个键值的名称分别为 $a$、$b$：   多个键值的 B+ 树的情况和单个键值的 B+ 树的情况大致一样，键值都是排序的，通过叶子节点可以逻辑上顺序地读出所有数据，就下面的例子来说，即（1，1）、（1，2）、（2，1）、（2，4）、（3，1）、（3，2），数据按（a，b）的顺序进行了存放。\n   因此，对于查询 SELECT * FROM TABLE WHERE a=xxx and b=xxx;，是可以使用（a，b）这个联合索引的，同时，对于单个 a 列的查询 SELECT * FROM TABLE WHERE a=xxx;，也可以使用（a，b）这个联合索引，但是对于 b 列的查询 SELECT * FROM TABLE WHERE b=xxx;，则不可以使用（a，b）这个联合索引，我们可以发现叶子节点上的 b 值为 1、2、1、4、1、2，显然不是排序的，因此对于 b 列的查询使用不到（a，b）这个联合索引。\n  联合索引的第二个好处是已经对第二个键值进行了排序处理，例如，在很多情况下应用程序都需要查询某个用户的购物情况，并按照时间进行排序，最后取出最近三次的购买记录，这时使用联合索引可以避免多一次的排序操作，因为索引本身在叶子节点已经排序了，具体示例如下：\n  首先创建测试表 buy_log，里面包含两个字段 userid 和 buy_date，同时含有两个索引，分别是 userid 和 (userid,buy_date)。\n  此时，如果只对 userid 进行查询，如 SELECT * FROM buy_log WHERE userid=2;，则优化器的选择如下图所示。\n   从图中我们可以发现，possible_keys 在这里有两个索引可供使用，分别是单个的 userid 索引和 (userid,buy_date) 的联合索引，但是优化器最终选择的是 userid 索引，因为该索引的叶子节点包含单个键值，所以理论上一个页能存放的记录应该更多。\n  接着假定要取出 userid 为 1 的最近 3 次的购买记录，如 SELECT * FROM buy_log WHERE userid=2 ORDER BY buy_date DESC limit 3;，执行计划如下图所示。\n   同样的，对于上述的 SQL 语句，即可以使用 userid 索引，也可以使用 (userid,buy_date) 索引，但是这次优化器使用了 (userid,buy_date) 的联合索引 userid_2，因为在这个联合索引中，buy_date 已经排好序了，根据该联合索引取出数据，无须再对 buy_date 做一次额外的排序操作。\n  对于上面的查询逻辑，若强制使用 userid 索引，如 select * from buy_log force index(userid) where userid=1 order by buy_date desc limit 3;，执行计划如下图所示。\n   在 Extra 选项中可以看到 Using filesort，即需要额外的一次排序操作才能完成查询，而这次显然需要对列 buy_date 排序，因为索引 userid 中的 buy_date 是未排序的。\n  对于联合索引（a，b，c）来说，下列语句同样可以通过联合索引得到结果：\nSELECT * FROM TABLE WHERE a=xxx ORDER BY b; SELECT * FROM TABLE WHERE a=xxx and b=xxx ORDER BY c; 但是，对于下面的语句，联合索引不能直接得到结果，还需要执行一次 filesort 操作，因为索引（a，c）并未排序。\nSELECT * FROM TABLE WHERE a=xxx ORDER BY c;       6.2.3 唯一索引 #   唯一索引与普通索引类似，不同的是创建唯一性索引的目的不是为了提高访问速度，而是为了避免数据重复。 唯一索引列的值必须唯一，允许有空值，如果是联合索引，则列值的组合必须唯一。 创建唯一索引通常使用 UNIQUE 关键字。  6.2.4 主键索引 #   主键索引就是专门为主键字段创建的索引，也属于索引的一种。 主键索引是一种特殊的唯一索引，不允许值重复或者值为空。 MySQL 创建主键时默认为聚集索引，但主键也可以是非聚集索引。 创建主键索引通常使用 PRIMARY KEY 关键字。  6.3 从表记录的排列顺序和索引的排列顺序是否一致来划分 #  从表记录的排列顺序和索引的排列顺序是否一致来划分，索引可以划分为聚集索引（Clustered Index）、辅助索引（Secondary Index）。\n 需要注意的是：\n 辅助索引有时也称为非聚集索引（Non-Clustered Index）。 不管是聚集索引，还是辅助索引，其内部都是 B+ 树，即高度平衡的，叶子节点存放着所有的数据。 聚集索引与辅助索引不同的是叶子节点存放的是否是一整行的信息。   6.3.1 聚集索引 #  6.3.1.1 含义 #    InnoDB 存储引擎是 索引组织表，即表中的数据按照主键顺序存放。\n  聚集索引就是按照每张表的主键构造一棵 B+ 树，同时叶子节点中存放的即为整张表的行记录数据，因此也将聚集索引的叶子节点称为数据页。\n  数据页上存放的是完整的每行的记录，索引页中存放的仅仅是键值及指向数据页的偏移量。\n   聚集索引的这个特性决定了索引组织表中数据也是索引的一部分，同 B+ 树数据结构一样，每个数据页都通过一个双向链表来进行链接。\n  由于实际的数据页只能按照一棵 B+ 树进行排序，因此每张表只能拥有一个聚集索引。\n  在多数情况下，查询优化器倾向于采用聚集索引：\n 聚集索引能够在 B+ 树索引的叶子节点上直接找到数据。 由于定义了数据的逻辑顺序，聚集索引能够特别快地针对范围值的查询，查询优化器能够快速发现某一段范围的数据页需要扫描。    聚集索引的存储不是物理上连续的，而是逻辑上连续的：\n 数据页通过双向链表链接，页按照主键的顺序排序。 每个数据页中的记录也是通过双向链表维护的，物理存储上可以按照不同主键存储。    聚集索引对于主键的排序查找和范围查找速度非常快：\n  叶子节点的数据就是用户所要查询的数据，如果用户需要在一张注册用户的表中查询最后注册的 10 位用户，由于 B+ 树索引是双向链表的，用户可以快速找到最后一个数据页，并取出 10 条记录，如下图中，虽然使用 ORDER BY 对记录进行排序，但是在实际过程中并没有进行 filesort 操作，这就是聚集索引的特点。\n   另一个是范围查询，如果要查找主键某一范围内的数据，通过叶子节点的上层中间节点就可以得到页的范围，之后直接读取数据即可，下图中的 rows 代表的是返回行数的预估值，不是确切的值，例如下面 SQL 的返回行数的预估值为 14868，实际返回行数为 9946。      6.3.1.2 优缺点 #  6.3.1.2.1 优点 #   可以把相关数据保存在一起，例如实现电子邮件时，可以根据用户 ID 来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件，如果没有使用聚集索引，则每封邮件都可能导致一次磁盘 I/O。 数据访问更快，聚集索引将索引和数据保存在同一个 B+ 树中，因此从聚集索引中获取数据通常比在非聚集索引中获取数据更快。  6.3.1.2.2 缺点 #   插入速度严重依赖于插入顺序，按照主键的顺序插入是插入数据到 InnoDB 表中最快的方式，但如果不是按照主键的顺序来插入数据，那么在插入数据完成之后最好使用 OPTIMIZE TABLE 命令重新组织一下表。 更新聚集索引列的代价很高，因为会强制 InnoDB 将每个被更新的行移动到新的位置。 基于聚集索引的表在插入新行，或者主键被更新导致需要移动行的时候，可能面临页分裂的问题，当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次分裂操作，页分裂会导致表占用更多的存储空间。 聚集索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。  6.3.2 辅助索引 #    辅助索引也称非聚集索引，其叶子节点并不包含行记录的全部数据，而是包含辅助索引列的值和主键的值。\n   辅助索引的存在并不会影响数据在聚集索引中的组织，因此每张表上可以有多个辅助索引，当通过辅助索引来寻找数据时，InnoDB 存储引擎会先遍历辅助索引并通过页级别的指针获得指向主键索引的主键，然后再通过主键索引来找到一个完整的行记录，比如在一棵高度为 3 的辅助索引树中查找数据，那么需要对这可辅助索引树遍历 3 次找到指定主键，如果聚集索引树的高度同样为 3，那么还需要对聚集索引树进行 3 次查找，最终找到一个完整的行数据所在的页，因此一共需要 6 次逻辑 IO 访问以得到最终的一个数据页。\n  6.4 其它索引 #  6.4.1 覆盖索引 #  6.4.1.1 含义 #   覆盖索引是InnoDB中一种特殊的索引，在该索引中包含了查询需要的所有字段，即索引本身已经包含了执行查询所需要的数据，因此不需要进行额外的I/O操作。 在InnoDB中，数据根据主键索引进行聚集存储，物理上数据是根据主键索引以B+树的形式来存储的，因此，辅助索引的叶子节点上存储着主键，从而可以建立辅助索引和实际数据行的连接。 因此，辅助索引中的任何查找都是先从根节点开始，然后经过分支节点，最后到达正确的叶子节点，并获取主键的值，然后又根据主键的值在主键索引中执行一次随机I/O操作（又一次从根节点开始，然后经过分支节点，最后到达正确的叶子节点）去获取真实的数据行。 使用覆盖索引可以避免在主键索引上的随机I/O操作，因为查询需要的所有字段都在覆盖索引中。  6.4.1.2 应用场景 #   根据明确的条件来过滤数据（WHERE）。 对数据进行分组（GROUP BY）。 根据覆盖索引中的字段顺序对数据进行排序（ORDER BY）。 查询数据（SELECT）。  6.4.1.3 具体实例 #    假设我们有如下的表：\nCREATE TABLE big_table( id int primary key auto_increment, field01 int, field02 int, field03 int, field04 decimal, field05 int ) engine=innodb;   假如我们有如下查询：\nmysql\u0026gt; SELECT sum(field04) FROM big_table WHERE field01=1 GROUP BY field03; 当 big_table这张表的数据量非常大时，上面的查询可能会花费很长时间。\n  假如我们想对上面的查询进行优化，我们可以为他创建一个覆盖索引，因此也就不需要去这张表中去查询具体的行了，仅仅需要从索引本身去获取数据字段就可以了，这种方法可以将我们的查询速度提高一个数量级。\n  为了给这个查询创建一个覆盖索引，索引中必须包含 WHERE、GROUP BY、SELECT语句中的所有字段，而且索引中字段的顺序也很重要，通常的规则是先选择 WHERE语句中的字段，然后选择 ORDER BY和 GROUP BY中的字段，最后选择 SELECT中的字段，具体的创建索引的语句如下：\nmysql\u0026gt; ALTER TABLE big_table ADD INDEX (field01, field03, field04);   然后我们可以使用 EXPLAIN语句来查看查询语句的查询计划，具体如下：\nmysql\u0026gt; EXPLAIN SELECT sum(field04) FROM big_table WHERE field01=1 GROUP BY field03 \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: big_table partitions: NULL type: ref possible_keys: field01 key: field01 key_len: 5 ref: const rows: 1 filtered: 100.00 Extra: Using where; Using index 通常查询计划我们可以发现 Extra字段中显示 Using index，这意味着InnoDB在执行查询语句时使用了我们之前创建的索引，因此可以确信我们上面创建的为覆盖索引。\n  需要注意的是，InnoDB中自动包括了主键的值，因此，在一些场景下，他们自动的是覆盖索引，例如下面的查询：\nmysql\u0026gt; SELECT count(id) FROM big_table WHERE field05=10; 下面我们为 field05添加一个索引：\nmysql\u0026gt; ALTER TABLE big_table ADD INDEX (field05); 然后通过 EXPLAIN来查看查询的执行计划：\nmysql\u0026gt; EXPLAIN SELECT count(id) FROM big_table WHERE field05=10 \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: big_table partitions: NULL type: ref possible_keys: field05 key: field05 key_len: 5 ref: const rows: 1 filtered: 100.00 Extra: Using index 1 row in set, 1 warning (0.12 sec) 通过查看执行计划我们可以发现 Extra字段里面显示 Using index，因此该查询也是使用了覆盖索引，因为主键自动包括在辅助索引里面了，因此 id这个字段会自动放在 field05这个索引的右边，即 (field05,id)。\n  另外一种常用的场景是当查询返回一些无用信息时我们可以使用辅助索引，例如下面的查询语句：\nmysql\u0026gt; SELECT * FROM big_table WHERE field03=10; 实际上我们只需要返回 field01这个字段，因此我们可以通过重写这个查询语句来改善查询效果，具体的查询语句如下：\nmysql\u0026gt; SELECT field01 FROM big_table WHERE field03=10; 然后为相应的字段创建覆盖索引：\nmysql\u0026gt; ALTER TABLE big_table ADD INDEX (field03, field01); 这样，整个查询就会走覆盖索引，查询效果就会提升很多。\n  参考文献 #    MySQL 索引是怎么支撑千万级表的快速查找？  深入理解 MySQL 索引。  MySQL 索引详解（优缺点，何时需要/不需要创建索引，索引及 sql 语句的优化）。  Mysql 探索之索引详解。  什么情况下数据库索引会失效? 《高性能 MySQL（第三版）》  mysql 运算符 \u0026lt;=\u0026gt;，:=，@，@@的含义。  MySQL InnoDB 中 B+ 树索引，哈希索引，全文索引 详解。 《MySQL 技术内幕（InnoDB 存储引擎）第 2 版》  老生常谈：Mysql 索引原理的万字总结！  15.6.2.1 Clustered and Secondary Indexes.  聚簇索引的优缺点。  Speed up your queries using the covering index in MySQL.  "},{"id":47,"href":"/school-recruitment/docs/computer-basics/1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1.3-%E8%AE%A1%E7%AE%97%E5%85%B6%E7%BD%91%E7%BB%9C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/","title":"1.3 计算其网络体系结构","section":"1、计算机网络","content":"计算其网络体系结构 #  1 计算机网络体系结构分为哪几层 #   常见的有三种体系结构，分别为原始的 5 层架构、OSI 的 7 层架构、TCP/IP 的 4 层架构。\n 5 层/7 层架构：物理层、数据链路层、网络层、传输层、（会话层、表示层）、应用层。 4 层架构：网络接口层、网际层、传输层、应用层。  2 每一层常见的协议有哪些 #   物理层： CLOCK、IEEE802.2（中继器、集线器）。 数据链路层： PPP、FR、HDLC、VLAN、MAC（网桥、交换机）。 网络层： IP、ARP（地址解析协议）、NAT（网络地址转换协议）、RIP（路由信息协议）。 传输层： TCP、UDP。 应用层：  FTP（21 端口）：文件传输协议。 SSH（22 端口）：远程登录。 TELNET（23 端口）：远程登录。 SMTP（25 端口）：发送邮件。 POP3（110 端口）：接收邮件。 HTTP（80 端口）：超文本传输协议。 DNS（53 端口）：运行在 UDP 上，域名解析服务。    3 路由器、交换机位于哪一层 #   路由器位于网络层，根据IP 地址进行寻址。 交换机位于数据链路层，根据MAC 地址进行寻址。  4 每一层有哪些作用 #   物理层： 通过媒介传输比特，确定机械及电气规范。 数据链路层： 将比特组装成帧和点对点的传递。 网络层： 负责数据包从源到宿的传递和网际互联。 传输层： 提供端到端的可靠报文传递和错误恢复。 会话层： 建立、管理和终止会话。 表示层： 对数据进行翻译、加密和压缩。 应用层： 允许访问OSI环境的手段。  5 参考文献 #    计算机网络体系结构。  OSI，TCP/IP，五层协议的体系结构，以及各层协议。  "},{"id":48,"href":"/school-recruitment/docs/database/2MySQL/2.2-B-%E6%A0%91B+%E6%A0%91%E7%B4%A2%E5%BC%95%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/","title":"2.2 B 树、 B 树索引算法原理","section":"2、 My SQL","content":"1 为什么要用 B 树、B+ 树索引 #    常用的数据结构如二叉查找树（Binary Search Tree, BST）的每个节点只能容纳一个数据，导致树的高度很高，逻辑上挨着的节点的数据可能离得很远，如果在内存中操作数据的话，这样问题并不大，但是由于读写磁盘的速度相比内存慢很多，每次读写磁盘的单位要比读写内存的最小单位大很多。\n  因此，对应的数据结构应该尽量的满足局部性原理，即当一个数据被用到时，其附近的数据也通常会马上被使用，因此，逻辑上相邻的数据在物理上也尽量存储在一起，这样才能减少读写磁盘的数量。\n  因此，对比起一个节点只能存储一个数据的 BST 类数据结构来说，要求这种数据结构在形状上更胖，更加扁平，即每个节点能容纳更多的数据，这样就能降低树的高度，同时让相邻的数据都能尽量的存储在物理上也相邻的硬盘空间上，减少磁盘读写。\n  以下图为例，图中从根节点出发，查找数据 14 的过程中，经过的第二节点中有键值 [3,7,13]，这三个值在逻辑上是相邻的，如果他们在磁盘上的存储也能做到在物理上相邻，那么只需要一次读操作就能把这个节点的数据从磁盘上加载到内存中进行数据比较，这样整个过程就只需要两次磁盘读操作，这种数据结构具有两个特点：\n 高扇出： 临近键值的数据局部性更好。 低高度： 遍历期间的寻道次数更少。     B 树和 B+ 树就是两种利用磁盘局部性原理进行优化的树结构。\n MySQL为什么不使用跳表作为索引的数据结构？\n 数据库的数据量比较大，如果数据库索引使用了跳表，那么：  跳表的层数太高，数据存储不紧凑，产生大量的空间浪费。 查询时会产生大量跨页IO，而且磁盘磁头无法对链表进行预读，会产生大量的随机IO，对磁盘的缓存不友好，查询效率较低。       2 B 树 #  2.1 B 树的定义及性质 #    在 B 树种，分为两种节点：\n 内部节点（Internal Node）： 存储了数据以及指向其子节点的指针。 叶子节点（Leaf Node）： 叶子节点只存储数据，没有子节点。    一个数据，既可能存在内部节点上，也可能存在叶子节点上，这一点是与 B+ 树最大的不同，B+ 树只会将数据存储在叶子节点上。\n  创建 B 树时，需要输入一个 degree 参数（以下简写为 $t$），该参数决定了每个节点上数据量的多少，即节点的胖、瘦程度，而节点的胖瘦程度又会影响整棵树的高度，因为越胖的节点树的高度越矮。\n  为了维护 B 树的平衡性，需要满足以下的属性：\n 每个节点上的键值以递增顺序排列。 一个节点上的键值大于其左子树的所有键值，小于等于其右子树的所有键值。 在内部节点中，指向子节点的指针数量总是存储数据节点的数量 +1。 所有叶子节点的高度一致。 所有节点存储的键值数量在 $[t-1,2t-1]$ 之间，如果数量不满足此条件，需要做重平衡操作：  小于 $ t - 1 $，需要借用或合并数据。 大于 $ 2t - 1 $，需要分裂成两个节点。      如下图所示，该图中的 B 树，$t$ 参数的值为 2（需要特别说明的是，一棵树中每个存储数据的地方，应该既有键值（key），也有数据（value），本文中为了简单起见，存储的数据只有键值）：\n 由于 $t=2$，因此所有节点的键值数量在 $[1,3]$ 之间。 所有叶子节点的高度相同。 以左边的内部节点为例，第一个键值为 3，其左子树的键值为 $[1,2]$，都小于 3，而其右子树的键值为 $[4,5,6]$，都不小于 3。     2.2 B 树算法原理 #  了解 B 树的性质之后，下面讨论 B 树中的两个核心操作：插入、删除，这两个操作的核心，都是在操作如果破坏 B 树的平衡性之后进行重新平衡以满足 B 树的性质。\n2.2.1 插入数据 #    B 树中插入数据的时候，首先要查找插入新关键字的叶节点的位置，但是，不能简单的创建一个新的叶节点，然后将其插入，因为这样得到的树将不再是合法的 B 树，相反，我们是将新的关键字插入一个已经存在的叶节点上。\n  由于不能将关键字插入一个满的叶节点，因此引入一个操作，将一个满的节点 $y$（有 $ 2t-1 $ 个关键字）按其中间关键字分裂为两个各含 $t-1$ 个关键字的节点，中间关键字被提升到 $y$ 的父节点，以标识两棵新树的划分点，但是如果 $y$ 的父节点也是满的，就必须在插入新的关键字之前将其分裂，最终满节点的分裂会沿着树向上传播。\n    我们并不是等到找出插入过程中实际要分裂的满节点时才做分裂，而是当沿着树往下查找新的关键字所属位置时，就分裂沿途遇到的每个满节点（包括叶节点本身），因此，每当要分裂一个满节点 $y$ 时，就能确保他的父节点不是满的。\n   2.2.1 删除数据 #  2.2.1.1 所有删除的都是叶子节点 #    首先，我们需要清楚第一个要点，无论我们当前删除的元素是什么，最终都会落实到叶子节点上，也就是说所有的情况都可以转化成删除叶子节点的问题。\n  例如：\n 在下面这张图中，假如我们要删除元素 11，而 11 在根节点上，显然我们要删除的位置并不在根节点上。  为了避免删除非叶子节点的元素，我们可以先找到 11 的后继节点，这里的后继节点指的是在这棵树上比当前元素大的最小的节点，在这个图当中，11 的后继节点是 12，我们将 12 赋值给 11，递归往下调用，转变成删除 12。  当然，我们选出来的后继节点仍然可能并不是叶子节点，这没有关系，我们只需要重复执行以上操作即可，因为我们可以保证后继节点出现的位置在树上的深度只会比当前元素更大，不会更小，而树深是有限的，也就是说最多经过有限次转化，我们就可以把删除操作转嫁到叶子节点身上。    2.2.1.2 直接删除叶节点 #   假设叶子节点当中元素数量很多，我们删除一个仍然可以保证他是合法的，这种情况下直接删除即可。 例如：  在下图当中 $t=2$，因此非叶节点的取值范围为 $[1,3]$，最多允许一个节点出现 4 个分支。 假如我们要删除的元素是 19，由于节点 3 当中元素众多，即使删除掉一个元素，依然符合节点的要求，那么就不做任何操作。 假如我们要删除的元素是 10，由于节点 10 只有一个元素，如果删除了，那么就会破坏节点的最小元素数量的限制，在这种情况下，只有一个办法，就是先删除，再和其他节点借。     2.2.1.3 和兄弟节点借 #    我们首先考虑和节点的兄弟节点借一个元素，以为兄弟节点和当前节点的父亲节点相同，可以很方便的转移节点并保证树的性质。\n  对于一个叶子节点来说，他的兄弟节点最多只有两个，也就是他左侧的兄弟和右侧的兄弟，由于 B 树上的元素存在从左往右的递增性，我们认为右侧的是哥哥节点，左侧的是弟弟节点，借的顺序虽然会影响树的形状，但是并不会影响树的合法性，所以我们先和哥哥借，再和弟弟借，或者反过来都行 ，本文采取的是先哥哥后弟弟的顺序。\n  例如：\n 假设我们要删除节点 10，节点 10 只有一个元素，删除了必然破坏合法性。 这个时候我们先看看哥哥的情况，哥哥节点当中有 3 个元素，即使借走一个仍然可以满足要求，那么我们就和哥哥借。  借的方法很简单，由于哥哥节点当中所有的元素都大于当前节点，为了保证元素的顺序我们会借第一个元素，也就是 13，但是 13 大于父节点中的 12，所以我们不能直接把 13 塞到原来 10 的位置，而是需要先将父节点的 12 挪下来，放到 10 的位置，再将 13 填到 12 的位置上去。 最后，达到的平衡态的样子如下：  同理，如果我们删除的是 23，由于他没有哥哥节点，只有弟弟节点，并且弟弟节点满足条件，那么我们就和弟弟节点借一个元素，逻辑和上面的一样，先从父节点要一个元素下来，再从弟弟节点接一个元素放回父节点，得到的结果如下：     2.2.1.4 和父亲节点借 #   如果兄弟节点自身也是勉强达到条件，显然是借不了的，这种时候没办法，只能还是和父亲节点要，如果父亲节点稍稍富裕，给出了一个元素之后还是能满足条件，那么就从父亲节点借出。 但是需要注意的是，父亲节点给出一个元素，那么他的子树数量也应该随之减少，不然也会不满足 B 树的特性，为了达成这一点，可以通过合并两个子树来实现。 例如：   在下图中，我们需要删除 10 节点。   删除之后，得到：   这个图最大的问题是他的根节点有两个元素，但是却有四棵子树，这违反了 B 树的性质，这是因为原本属于根节点的元素 9 被子树借走了。\n  为了解决这个问题，我们需要将邻近的两棵子树合并，也就是将 $[6,7]$ 和 $[9]$ 子树合并，得到：\n   如果我们跟父节点借合并子树导致父节点中的元素减少而不满足条件时，不可以跟子节点借，因为我们即使能找到富裕的子节点，也没办法让子树的数量随着也增加 1，我们应该做的是递归借节点的操作，让父节点去和他的兄弟以及父节点借元素就好了，在极端的情况下，这有可能导致树的高度发生变化，例如：\n  在下面的图中 $t=3$，如果我们删除 9，根据刚才的惯例，我们会跟父节点借元素 6，并且和 $[1,3]$ 子树合并，得到：\n   但是这一借会导致父节点破坏了 B 树的最低要求，所以我们需要递归维护父节点，也就是让父节点重复借元素的步骤，我们可以发现对于节点 $[10]$ 来说，他没有富裕的兄弟节点，只能继续和父节点借，这一借会再次导致合并的发生，最终我们得到的结果如下：\n   通过观察上面树结构的变化，我们发现只要是和父亲节点借元素，必然伴随着和兄弟节点合并的情况，而和兄弟节点合并，除了需要维护两个节点当中的元素之外，还需要维护各自的子树，尤其是如果我们在每个节点当中记录父亲节点以及在父亲节点当中的位置的话，这些都需要维护。\n      3 B+ 树 #  3.1 B+ 树的定义及性质 #   各种资料上 B+ 树的定义各有不同，一种定义方式是关键字个数和孩子节点个数相同（百度百科），一种是关键字个数比孩子结点个数小 1（维基百科），这种方式是和 B 树基本等价的，本文采取的是维基百科上所定义的方式。 B+ 树包含两种类型的节点，分别是内部节点（也称索引节点）和叶子节点，根节点本身既可以是内部节点，也可以是叶子节点，根节点的关键字个数最少可以只有 1 个。 B+ 树与 B 树最大的不同是内部节点不保存数据，只用于索引，所有数据（或者说记录）都保存在叶子节点中。 $m$ 阶 B+ 树除了根节点外每个节点包含关键字的个数的取值范围为 $[m/2,m-1]$。 对于所有内部节点，子指针的数目总是比关键字的数目多一个。 内部节点中的 $key$ 都按照从小到大的顺序排列，对于内部节点中的一个 $key$，左子树中的所有 $key$ 都小于他，右子树中的 $key$ 都大于他。 所有叶子节点都在相同高度上，叶子节点中的记录也按照 $key$ 从小到大排列。 每个叶子节点都存有相邻叶子节点的指针，叶子节点本身按照关键字大小从小到大链接。   2.2 B+ 树算法原理 #  2.2.1 插入数据 #  下面的插入操作是以一棵 5 阶 B+ 树为例，其节点（除根节点外）的取值范围为 $[2,4]$。\n 若为空树，创建一个叶子节点，然后将记录插入其中，此时这个叶子节点也是根节点，插入操作结束。  针对叶子类型的节点：   根据 $key$ 值找到叶子节点，向这个叶子节点插入记录。\n  插入后，若当前节点 $key$ 的个数小于等于 $m-1$，则插入结束。\n  否则，将这个叶子节点分裂成左右两个叶子节点，左叶子节点包含前 $m/2$ 个记录，右节点包含剩下的记录，将第 $m/2+1$ 个记录的 $key$ 进位到父节点中（父节点一定是索引类型节点），进位到父节点的 $key$ 的左指针指向左子树，右指针指向右子树，将当前节点的指针指向父节点，然后执行下一步。\n    针对索引类型节点：   若当前节点 $key$ 的个数小于等于 $m-1$，则插入结束。\n  否则，将这个索引类型节点分裂成两个索引节点，将第 $m/2$ 个 $key$ 进位到父节点中，该节点左边的节点放到左索引节点中，右边的节点放到右索引节点中，然后将进位到父节点的 $key$ 的左指针指向左子树，右指针指向右子树，然后将当前节点的指针指向父节点，然后重复第 3 步。\n       2.2.2 删除数据 #  下面的插入操作是以一棵 5 阶 B+ 树为例，其节点（除根节点外）的取值范围为 $[2,4]$。\n  如果叶子节点中没有相应的 $key$，则删除失败，否则，执行下一步\n  删除叶子节点对应的 $key$，如果删除后节点的 $key$ 大于等于 $m/2$，删除操作结束，否则，执行下一步。\n    如果兄弟节点的 $key$ 有富余（借走一个后仍满足 B+ 树的相关要求），则从兄弟节点借一个 $key$，同时用借到的 $key$ 替换父节点（指当前节点和兄弟节点共同的父节点）中的 $key$，删除结束，否则，执行下一步。\n   若兄弟节点没有富余的 $key$，则将当前节点和兄弟节点合并成一个新的叶子节点，并删除父节点中的 $key$（父节点中的这个 $key$ 两边的孩子指针就变成了一个指针，正好指向这个新的叶子节点），将当前节点指向父节点（比为索引节点）：\n 若索引节点的 $key$ 的个数大于等于 $m/2$，则删除操作结束，否则，执行下一步。 若兄弟节点有富余，则父节点 $key$ 下移，兄弟节点 $key$ 上移，删除结束，否则，执行下一步。 将父节点的 $key$ 下移，和当前节点及其兄弟节点合并成一个新的节点，然后将当前节点指向父节点，重复第 4 步。      需要注意的是，通过 B+ 树的删除操作后，索引节点存在的 $key$，不一定在叶子节点中存在相应记录。\n  4 B+ 树的优点 #   InnoDB 的索引使用的是 B+ 树实现的。\n   B+ 树索引节点上只有索引而没有数据，因此能存储比 B 树更多的索引，这样树的高度就会更矮，磁盘寻道的次数就会越少。\n  因为数据都集中在叶子节点，而所有叶子节点的高度相同，那么可以在叶子节点中增加前后指针，指向同一个父亲节点的相邻兄弟节点，给范围查询提供便利，比如：\n 对于 SQL 语句select * from tbl where t \u0026gt; 10，如果使用 B+ 树存储数据的话，可以首先定位到数据为 10 的节点，再沿着他的 $next$ 指针一路找到所有在该叶子节点右边的叶子节点数据返回。 而如果使用 B 树结构，由于数据即可以存储在内部节点，也可以存储在叶子节点，因此范围查询会比较繁琐。    参考文献 #    B 树、B+ 树索引算法原理（上）。 《算法导论（第三版）》  万字长文——详细阐述 B-树中所有细节。  B 树、B+ 树索引算法原理（下）。  B 树和 B+ 树的插入、删除图文详解。  B+ 树。  为什么MySQL的索引结构，采用了B+树，没有使用跳跃表呢？  "},{"id":49,"href":"/school-recruitment/docs/database/3Redis/3.2-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","title":"3.2 线程模型","section":"3、 Redis","content":"线程模型 #  1 什么是 Redis 事件 #    Redis 服务器是一个事件驱动程序，服务器需要处理以下两类事件：\n  文件事件：\n Redis 服务器通过套接字与客户端（或者其他 Redis 服务器）进行连接，而文件事件就是服务器对套接字操作的抽象。 服务器与客户端（或者其他服务器）的通信会产生相应的文件事件，而服务器就是通过监听并处理这些事件来完成一系列网络通信操作。    时间事件：\n Redis 服务器中的一些操作（比如 serverCron 函数）需要在给定的时间点运行，而时间事件就是服务器对这类定时操作的抽象。    2 事件分类 #  Redis 中的事件可以分为两类，分别是文件事件和时间事件。\n2.1 文件事件 #  2.1.1 含义 #   Redis基于 Reactor 模式开发了自己的网络事件处理器，这个处理器被称为文件时间处理器。 文件事件处理器使用 I/O 多路复用程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答、读取、写入、关闭等操作时，与操作相对应的文件事件就会产生，这时文件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字，文件处理器既实现了高性能的网络通信模型，又很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。  2.1.2 文件事件处理器 #  文件事件处理器主要由四个部分组成，分别是套接字、I/O 多路复用程序、文件事件分派器、事件处理器。\n  套接字：  文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件，因为一个服务器通常会连接多个套接字，所以多个文件事件可能会并发地出现。   I/O 多路复用程序：   I/O 多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。\n  尽管多个文件事件可能会并发地出现，但I/O 多路复用程序总是会将所有产生事件的套接字放到一个队列里面，然后通过这个队列，以有序、同步、每次一个套接字的方式向文件事件分派器传送套接字，当上一个套接字产生的事件被处理完毕之后，I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字。\n    文件事件分派器：  文件事件分派器接收 I/O 多路复用程序传来的套接字，并根据套接字产生的事件的类型，调用相应的事件处理器。   事件处理器：  服务器会为执行不同任务的套接字关联不同的事件处理器，这些处理器是一个个函数，他们定义了某个事件发生时，服务器应该执行怎样的动作。 Redis 为文件事件编写了多个处理器，这些事件处理器分别用于实现不同的网络通信需求：  为了对连接服务器的各个客户端进行应答，服务器要监听套接字关联连接应答处理器。 为了接收客户端传来的命令请求，服务器要为客户端套接字关联命令请求处理器。 为了向客户端返回命令的执行结果，服务器要为客户端套接字关联命令回复处理器。 当主服务器和从服务器进行复制操作时，主从服务器都需要关联特别为复制功能编写的复制处理器。   这些事件处理器里面，服务器最常用的有连接应答处理器、命令请求处理器和命令回复处理器。     套接字产生的事件类型有哪些？\n 套接字产生的事件类型有两种，分别是 AS_READABLE 事件和 AS_WRITABLE 事件：  当套接字变得可读时（客户端对套接字执行 write 操作，或者执行 close 操作），或者有新的可应答套接字出现时（客户端对服务器的监听套接字执行 connect 操作），套接字产生 AE_READABLE 事件。 当套接字变得可写时（客户端对套接字执行 read 操作），套接字产生 AE_WRITABLE 事件。   I/O 多路复用程序允许同时监听套接字的 AE_READABLE 事件和 AE_WRITABLE 事件，如果一个套接字同时产生了这两种事件，那么文件事件分派器会优先处理 AE_READABLE 事件，等到 AE_READABLE 事件处理完之后，才处理 AE_WRITABLE 事件。   2.1.3 一次完整的客户端与服务器连接事件示例 #    假设一个 Redis 服务器正在运作，那么这个服务器的监听套接字的 AE_READABLE 事件应该正处于监听状态之下，而该事件所对应的处理器为连接应答处理器。\n  如果这时有一个 Redis 客户端向服务器发起连接，那么监听套接字将产生 AE_READABLE 事件，触发连接应答处理器执行，处理器会对客户端的连接请求进行应答，然后创建客户端套接字，以及客户端状态，并将客户端套接字的 AS_READABLE 事件与命令请求处理器进行关联，使得客户端可以向主服务器发送命令请求。\n  之后，假设客户端向主服务器发送一个命令请求，那么客户端套接字将产生 AE_READABLE 事件，引发命令请求处理器执行，处理器读取客户端的命令内容，然后传给相关程序去执行。\n  执行程序将产生相应的命令回复，为了将这些命令回复传送回客户端，服务器将客户端套接字的 AE_WRITABLE事件与命令回复处理器进行关联，当客户端尝试读取命令回复的时候，客户端套接字将产生 AE_WRITABLE 事件，触发命令回复处理器执行，当命令回复处理器将命令回复全部写入到套接字之后，服务器就会解除客户端套接字的 AE_WRITABLE 事件与命令回复处理器之间的关联。\n   参考文献 #    【面试题】技术面试题汇总 🔥。 redis 设计与实现（第二版）。  Redis 系列（六）：你说要看 Redis 线程模型？安排。  "},{"id":50,"href":"/school-recruitment/docs/computer-basics/2%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.4-%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6/","title":"2.4 磁盘调度","section":"2、操作系统","content":"磁盘调度 #  1 磁盘结构 #  1.1 盘片 #   一个磁盘由多个盘片叠加而成。 盘片的表面涂有磁性物质，这些磁性物质用来记录二进制数据，因为正反两面都可涂上磁性物质，所以一个盘片可能会有两个盘面。 每个盘面对应一个磁头，所有的磁头都是连在同一个磁臂上的，因此所有磁头只能共进退。   1.2 磁道、扇区 #   每个盘片被划分为一个个磁道，每个磁道又划分为一个个扇区，每个扇区就是一个磁盘块，各个扇区存放的数据量相同。 最内侧磁道上的扇区面积最小，因此其数据密度最大。   1.3 柱面 #   所有盘面相对位置相同的磁道组成柱面。   2 相关时间 #   寻道时间：将磁头移动到指定磁道所花费的时间，主要包括两部分：  启动磁头臂消耗的时间。 移动磁头消耗的时间。   旋转时间：通过旋转磁盘，使磁头定位到目标扇区所需要的时间。 传输时间：从磁盘读出或向磁盘写入数据所经历的时间。 由于旋转时间和传输时间都是与磁盘转速有关，而转速又是磁盘的固有属性，因此无法通过操作系统优化旋转时间和传输时间，只能优化寻道时间。  3 磁盘调度算法 #   磁盘调度算法都是用来减少寻道时间的。\n 现在常用的磁盘调度算法主要包括先来先服务算法（FCFS）、最短寻找时间优先算法（SSTF）、扫描算法（SCAN）、循环扫描算法（C_SCAN）。\n3.1 先来先服务算法 #  3.1.1 原理 #   先来先服务算法的基本思想是根据进程请求访问磁盘的先后顺序进行调度。  3.1.2 优缺点 #  3.1.2.1 优点 #   公平。 如果请求访问的磁道比较集中的话，算法性能还算可以。  3.1.2.2 缺点 #   如果大量进程竞争使用磁盘，请求访问的磁盘很分散，则FCFS在性能上很差，寻道时间长。  3.1.2 示例 #   假设磁头的初始位置是100号磁道，有多个进程陆续地请求访问55、58、39、18、90、160、150、38、184号磁道。 按照先来先服务算法规则，按照请求到达的顺序，磁头需要一次移动到55、58、39、18、90、160、150、38、184号磁道。 磁头共移动了45 + 3 + 19 + 21 + 72 + 70 + 10 + 112 + 146 = 498个磁道，响应一个请求平均需要移动498 / 9 = 55.3个磁道（平均寻找长度）。   3.2 最短寻找时间优先 #  3.2.1 原理 #   最短寻找时间优先算法的基本思想是优先处理的磁道是与当前磁头最近的磁道，可以保证每次寻道时间最短，但是不能保证总的寻道时间最短（其实是贪心算法的思想，只是选择眼前最优，但是总体未必最优）。  3.2.2 优缺点 #  3.2.2.1 缺点 #   可能产生饥饿现象，即磁头在一小块区域移动，导致其他区域的访问请求无法得到响应。  3.2.3 示例 #   假设磁头的初始位置是100号磁道，有多个进程先后陆续地请求访问55、58、39、18、90、160、150、38、184号磁道。 磁头总共访问了（100 -18）+ （184 -18） = 248个磁道，响应一个请求平均需要移动248 / 9 = 27.5个磁道（平均寻找长度）。 如果在处理18号磁道的访问请求时又来了一个38号磁道的访问请求，处理38号磁道的访问请求又来了一个18号磁道的访问请求，如果有源源不断的18号、38号磁道访问请求，那么150、160、184号磁道的访问请求就永远得不到满足，从而产生饥饿现象。   3.3 扫描算法 #  3.3.1 原理 #   SSTF算法会产生饥饿的原因在于磁头有可能在一个小区域内来回移动。 为了防止这个问题，可以规定磁头只有移动到请求最外侧磁道或最内侧磁道才可以反向移动，如果在磁头移动的方向上已经没有请求，就可以立即改变磁头移动，不必移动到最内/外侧的磁道。 由于磁头移动的方式很像电梯，因此也叫电梯算法。  3.3.2 优缺点 #  3.3.2.1 优点 #   性能较好，寻到时间较短，不会产生饥饿现象。  3.3.2.2 缺点 #   对各个位置磁道的响应频率不平均。  3.3.3 示例 #   假设某磁盘的磁道为0~200号，磁头的初始位置是100号磁道，且此时磁头正在往磁道号增大的方向移动，有多个进程先后陆续地访问55、58、39、18、90、160、150、38、184号磁道。 磁头共移动了（184 - 100）+ （184 -18） = 250个磁道，响应一个请求平均需要移动250 / 9 = 27.5个磁道（平均寻找长度）。 但是该算法对各个位置磁道的响应频率不平均，假设此时磁头正在往右移动，且刚处理过90号磁道，那么下次处理90号磁道的请求就需要等待磁头移动很长一段距离，而响应了184号磁道的请求之后，很快又可以再次响应184号磁道的请求了。   3.4 循环扫描算法 #  3.4.1 原理 #   SCAN算法对各个位置磁道的响应频率不平均，而C-SCAN算法就是为了解决这个问题的。 规定只有磁头朝某个特定方向上移动时才处理磁道访问请求，而返回时直接快速移动至最靠边缘的并且需要访问的磁道上而不处理任何请求。  3.4.2 优缺点 #  3.4.2.1 优点 #   相比于SCAN算法，对于各个位置磁道响应频率很平均。  3.4.2.2 缺点 #   相比于SCAN算法，平均寻道时间更长。  3.4.3 示例 #   假设某磁盘的磁道为0~200号，磁头的初始位置是100号磁道，且此时磁头正在往磁道号增大的方向移动，有多个进程先后陆续地访问55、58、39、18、90、160、150、38、184号磁道。 磁头共移动了（184 -100）+ （184 - 18）+（90 - 18）=322个磁道，响应一个请求平均需要移动322 / 9 = 35.8个磁道（平均寻找长度）。  4 参考文献 #    5 分钟图解 磁盘的结构（盘片、磁道、扇区、柱面）。  磁盘调度算法。  "},{"id":51,"href":"/school-recruitment/docs/database/2MySQL/2.3-%E7%B4%A2%E5%BC%95%E7%BB%84%E7%BB%87%E8%A1%A8/","title":"2.3 索引组织表","section":"2、 My SQL","content":"索引组织表 #   在InnoDB存储引擎中，表都是按照主键顺序组织存放的，这种存储方式的表称为索引组织表（Index Organzied Table）。 在InnoDB存储引擎中，每张表都有个主键（Primary Key），如果在创建表时没有显式地定义主键，则InnoDB存储引擎会按如下方式选择或创建主键：  首先判断表中是否有非空的唯一索引（Unique NOT NULL），如果有，该列即为主键，当表中有多个非空唯一索引时，InnoDB存储引擎将选择建表时第一个定义的非空唯一索引为主键，这里需要注意的是，主键的选择根据的是定义索引的顺序，而不是建表时列的顺序，例如下面的例子：   建表及插入数据语句为：\ncreate table z( a int not null, b int null, c int not null, d int not null, unique key(b), unique key(d), unique key(c) ) engine=innodb; insert into z select 1,2,3,4; insert into z select 5,6,7,8; insert into z select 9,10,11,12;   上面示例中创建了一张表 z，有 a、b、c、d四列，b、c、d三列都有唯一索引，不同的是 b列允许NULL值，由于没有显式地定义主键，因此会选择非空的唯一索引，虽然 c、d列都是非空唯一索引，都可以作为主键的候选，但是在定义的过程中，由于 d列首先定义为唯一索引，因此InnoDB存储引擎将 d列视为主键。\n  可以通过下面的SQL语句判断表的主键值，其中 _rowid可以显示表的主键，但是只能用于查看单个列作为主键的情况，不能用于多列组成主键的情况。\n       参考文献 #    《MySQL 技术内幕（InnoDB 存储引擎）第 2 版》  "},{"id":52,"href":"/school-recruitment/docs/database/2MySQL/2.4-InnoDB%E5%92%8CMyISAM%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"2.4 Inno Db和 My Isam的区别","section":"2、 My SQL","content":"InnoDB和MyISAM的区别 #  存储引擎主要用于从数据库中读取数据，MySQL 支持很多 存储引擎，但是 MyISAM 和 InnoDB 是使用最广泛的两个存储引擎，他们每一个都有各自的优缺点，因此选取一个适合我们应用的存储引擎来说非常重要。\n MySQL 5.5.5 之前的版本的默认存储引擎是 MyISAM，5.5.5 及其之后的版本的默认存储引擎是 InnoDB。\n MyISAM 和 InnoDB 的主要区别包括参照完整性（Referential Integrity）、事务和原子性（Transactions \u0026amp; Atomicity）、表锁定与行锁定（Table-locking vs Row-locking）、可靠性（Reliability）、全文索引（FULLTEXT Indexing）、缓存（Caching）、ACID 属性（ACID property）七个方面。\n1 参照完整性 #   参照完整性确保了表与表之间的关系的一致性，比如一张表拥有指向另外一张表的外键，当被指向的表发生更改时，这些更改也会级联到链接表。 InnoDB 是一个关系型数据库管理系统（RDBMS），因此InnoDB 支持外键和参照完整性，包括级联删除和更新，但是MyISAM 不支持外键。  2 事务和原子性 #   MyISAM 不支持事务，但是InnoDB 支持。 因此，当一张表使用 MyISAM 引擎并且操作在执行的过程中被终止了，已经被更改的行将会被永久更改，即使操作还没有完成，但是当一张表是使用 InnoDB 引擎并且操作在执行的过程中被终止了，因为使用了事务，因此当我们提交之前，所有的更改将不会生效。 当我们使用 MyISAM 引擎时，所有的更改不能回滚，但是当我们使用 InnoDB 引擎时，更改是可以被回滚的。  3 表锁定和行锁定 #   当在MyISAM 引擎的表中执行一个查询时，整张表都会被锁定，这意味着后续查询只能等到当前查询结束之后才会被执行，当我们正在读一张大表时，而且同时这张表会有其他频繁的读写操作，这可能导致大量的查询被积压。 当在InnoDB 引擎的表中执行一个查询时，只有相关的行才会被锁定，表中的其他行可以继续进行其他操作，这意味着当查询不使用同一行时，可以在一个表上同时运行。  4 可靠性 #   MyISAM 引擎不提供数据完整性，比如硬件损坏（Hardware Failures）、突然关机（Unclean Shutdowns）或者操作取消（Canceled Operations）都会导致数据损坏，这就需要对表进行完全修复或者重建索引和表。 InnoDB 引擎使用事务日志（Transactional Log）、双写缓冲区（Double-Write Buffer）以及自动校验和验证（Automatic Checksum and Validation）来防止数据损坏，当 InnoDB对数据进行更改之前，他会在事务之前把数据记录到一个名为 ibdata1 的系统表空间文件（System Tablespace File），如果 MySQL 服务器发生崩溃，InnoDB 将会根据这些日志来自动恢复数据。  5 全文索引 #   在 MySQL 5.6.4 版本之前 InnoDB 不支持全文索引，从 MySQL 5.6.4 版本 InnoDB 开始支持全文索引。 使用全文索引的 MyISAM 表不能转换为 InnoDB 表。  6 缓存 #   InnoDB 会将数据和索引都缓存在内存中，所有的更改会先写入到日志缓冲区（Log Buffer），然后再根据设置的策略（根据 innodb_flush_log_at_trx_commit 来控制）刷到日志文件中，这种将数据保存在内存中的方式对性能来说是一个巨大的提升，但同时需要的内存和存储也会更高。 MyISAM 只把索引缓存进内存，因此相对于 InnoDB 来说性能会稍微差一些，但是使用的内存和存储可能会更低一些。  7 ACID 属性 #  InnoDB 支持 ACID（Atomicity, Consistency, Isolation, and Durability）属性，即原子性、一致性、隔离性和持久性，但是MyISAM 不支持这些属性。\n参考文献 #    Difference Between InnoDB and MyISAM.  "},{"id":53,"href":"/school-recruitment/docs/database/2MySQL/2.5-Checkpoint%E6%8A%80%E6%9C%AF/","title":"2.5 Checkpoint技术","section":"2、 My SQL","content":"Checkpoint技术 #  1 前言 #   缓冲池设计的目的是为了协调 CPU 速度与磁盘速度的鸿沟，因此页的操作首先都是在缓冲池中完成的。 如果一条DML 语句，如UPDATE 或DELETE改变了页中的记录，那么此时页是脏的，即缓冲池中的页的版本要比磁盘的新，数据库需要将新版本的页从缓冲池刷新到磁盘。 倘若每次一个页发生变化，就将新页的版本刷新到磁盘，那么这个开销是非常大的，若热点数据集中在某几个页中，那么数据库的性能将变得非常差，同时，如果在从缓冲池将页的新版本刷新到磁盘时发生了宕机，那么数据就不能恢复了。 为了避免发生数据丢失的问题，当前事务数据库系统普遍都采用了WAL（Write Ahead Log）策略，即当事务提交时，先写 重做日志（Redo Log），再修改页，当由于发生宕机而导致数据丢失时，通过重做日志来完成数据的恢复，这也是 事务 ACID中 D（Durability，持久性）的要求。  2 为什么需要 Checkpoint 技术 #   思考下面的场景，如果重做日志可以无限的做大，同时缓冲池也足够大，能够缓冲所有数据库的数据，那么是不需要将缓冲池页的新版本刷新回磁盘，因为当发生宕机时，完全可以通过重做日志来恢复整个数据库系统中的数据到宕机发生的时刻，但是这需要两个前提条件：  缓冲池可以缓存数据库中所有的数据。  有经验的用户都知道，当数据库开始建的时候，表中没有任何数据，缓冲池的确可以缓存所有的数据库文件。 然而随着市场的推广，用户的增加，产品越来越受到关注，使用量也越来越大，这时负责后台存储的数据库的容量注定会不断增大。 当前3TB的MySQL数据库已并不少见，但是3TB的内存却非常少见，因此这一假设对于生产环境应用中的数据库是很难得到保证的。   重做日志可以无限增大。  这个也许是可以的，但是对成本要求太高，同时不便于运维，因为我们不知道什么时候重做日志是否已接近于磁盘可使用空间的阈值，并且要让存储设备支持可动态扩展也是需要一定的技巧和设备支持的。     即使上面两个条件都满足，那么还有一个条件需要考虑，那就是宕机后数据库的恢复时间，当数据库运行了几个月甚至几年时，这时发生宕机，重新应用重做日志的时间会非常久，此时恢复的代价也会非常大，此时，就需要使用Checkpoint（检查点）技术了。  3 解决的主要问题 #  Checkpoint技术的目的是解决以下几个问题：\n 缩短数据库的恢复时间。  当数据库发生宕机时，数据库不需要重做所有的日志，因为Checkpoint之前的页都已经刷新回磁盘，故数据库只需对Checkpoint后的重做日志进行恢复，这样就大大缩短了恢复的时间。   缓冲池不够用时，将脏页刷新到磁盘。  当缓冲池不够用时，根据LRU算法会溢出最近最少使用的页，若此页为脏页，那么需要强制执行Checkpoint，将脏页也就是页的新版本刷回磁盘。   重做日志不可用时，刷新脏页。  重做日志不可用的情况是因为当前事务数据库系统对重做日志的设计都是循环使用的，并不是让其无限增大的，这从成本及管理上都是比较困难的。 重做日志可以被重用的部分是指这些重做日志已经不再需要，即当数据库发生宕机时，数据库恢复操作不需要这部分的重做日志，因此这部分就可以被覆盖重用。 若此时重做日志还需要使用，那么必须强制产生Checkpoint，将缓冲池中的页至少刷新到当前重做日志的位置。    4 分类 #  在InnoDB存储引擎中，Checkpoint发生的时间、条件及脏页的选择等都非常复杂，而Checkpoint所做的事情无外乎是将缓冲池中的脏页刷回到磁盘，不同之处在于每次刷新多少页到磁盘，每次从哪里取脏页，以及什么时间触发Checkpoint，在InnoDB引擎内部，有两种Checkpoint，分别为Sharp Checkpoint、Fuzzy Checkpoint。\n4.1 Sharp Checkpoint #  Sharp Checkpoint发生在数据库关闭时将所有的脏页都刷新回磁盘，这是默认的工作方式，即参数 innodb_fast_shutdown = 1。\n4.2 Fuzzy Checkpoint #  如果数据库在运行时也使用Sharp Checkpoint，那么数据库的可用性就会受到很大的影响，因此在InnoDB存储引擎内部使用Fuzzy Checkpoint进行页的刷新，即只刷新一部分脏页，而不是刷新所有的脏页回磁盘，在InnoDB存储引擎内部可能发生如下几种情况的Fuzzy Checkpoint：\n Master Thread Checkpoint. FLUSH_LRU_LIST Checkpoint. Async/Sync Flush Checkpoint. Dirty Page too much Checkpoint.  4.2.1 Master Thread Checkpoint #   对于Master Thread中发生的Checkpoint，差不多以每秒或每十秒的速度从缓冲池的脏页列表中刷新一定比例的页回磁盘，这个过程是异步的，即此时InnoDB存储引擎可以进行其他的操作，用户查询线程不会阻塞。  4.2.2 FLUSH_LRU_LIST Checkpoint #    FLUSH_LRU_LIST Checkpoint是因为InnoDB存储引擎需要保证LRU列表中需要有差不多100个空闲页可供使用。\n  在InnoDB 1.1.x版本之前，需要检查LRU列表中是否有足够的空间，这个操作发生在用户查询线程中，显然会阻塞用户的查询操作，倘若没有100个可用空闲页，那么InnoDB存储引擎会将LRU列表尾端的页移除，如果这些页中有脏页，那么需要进行Checkpoint，而这些页是来自LRU列表的，因此也称为FLUSH_LRU_LIST Checkpoint。\n  而从MySQL 5.6版本，也就是InnoDB 1.2.x版本开始，这个检查被放在了一个单独的Page Cleaner线程中进行，并且用户可以通过参数 innodb_lru_scan_depth控制LRU列表中可用页的数量，该值默认为1024。\nmysql\u0026gt; show variables like \u0026#39;innodb_lru_scan_depth%\u0026#39;; +-----------------------+-------+ | Variable_name | Value | +-----------------------+-------+ | innodb_lru_scan_depth | 1024 | +-----------------------+-------+   4.2.3 Async/Sync Flush Checkpoint #    Async/Sync Flush Checkpoint指的是 重做日志文件不可用的情况，这时需要强制将一些页刷新回磁盘，而此时脏页是从脏页列表中选取的。\n  若将已经写入到重做日志的LSN记为$redo_lsn$，将已经刷回磁盘最新页的LSN记为$checkpoint_lsn$，则可定义：\n$$ checkpoint_age = redo_lsn - checkpoint_lsn $$\n再定义以下的变量：\n$$ async_watermark = 75% * total_redo_log_file_size $$\n$$ sync_watermark = 75% * total_redo_log_file_size $$\n  若每个重做日志文件的大小为1GB，并且定义了两个重做日志文件，则重做日志文件的总大小为2GB，那么$async_watermark = 1.5GB$，$sync_watermark = 1.8GB$，则：\n 当$checkpoint_age \u0026lt; async_watermark$时，不需要刷新任何脏页到磁盘。 当$async_watermark \u0026lt; checkpoint_age \u0026lt; sync_watermark$时触发Async Flush，从Flush列表中刷新足够的脏页回磁盘，使得刷新后满足$checkpoint_age \u0026lt; async_watermark$。 $checkpoint_age \u0026gt; sync_watermark$这种情况一般很少发生，除非设置的重做日志文件太小，并且在进行类似LOAD DATA的BULK INSERT操作，此时触发Sync Flush操作，从Flush列表中刷新足够的脏页回磁盘，使得刷新后满足 $checkpoint_age \u0026lt; async_watermark$。    可见，Async/Sync Flush Checkpoint是为了保证重做日志的循环使用的可用性。\n  在InnoDB 1.2.x版本之前，Async Flush Checkpoint会阻塞发现问题的用户查询线程，而Sync Flush Checkpoint会阻塞所有的用户查询线程，并且等待脏页刷新完成。\n  从InnoDB 1.2.x，也就是MySQL 5.6版本开始，这部分的刷新操作同样放入到了单独的Page Cleaner Thread中，因此不会阻塞用户查询线程。\n  4.2.4 Dirty Page too much Checkpoint #    这种情况是因为脏页的数量太多，导致InnoDB存储引擎强制进行Checkpoint，其目的总的来说还是为了保证缓冲池中有足够可用的页，其可由参数 innodb_max_dirty_pages_pct控制，具体如下：\nmysql\u0026gt; show variables like 'innodb_max_dirty_pages_pct%'; +--------------------------------+-----------+ | Variable_name | Value | +--------------------------------+-----------+ | innodb_max_dirty_pages_pct | 90.000000 | | innodb_max_dirty_pages_pct_lwm | 0.000000 | +--------------------------------+-----------+ innodb_max_dirty_pages_pct的值为90表示当缓冲池中脏页的数量占据90%时，强制进行Checkpoint，刷新一部分的脏页到磁盘。\n  参考文献 #   《MySQL 技术内幕（InnoDB 存储引擎）第 2 版》  "},{"id":54,"href":"/school-recruitment/docs/database/2MySQL/2.6-%E5%AE%95%E6%9C%BA%E6%81%A2%E5%A4%8D%E5%8E%9F%E7%90%86/","title":"2.6 宕机恢复原理","section":"2、 My SQL","content":"宕机恢复原理 #  1 前言 #  MySQL 保证数据不丢失的能力主要体现在两个方面：\n 能够恢复到任何时间点的状态。  对于这一点，只要保留足够的 Binlog，就可以通过重跑 Binlog 来实现。   能够保证 MySQL 在任何时间段突然崩溃，重启之后之前提交的记录都不会丢失。  对于这一点，也就是本文所说的宕机恢复，即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL 突然崩溃，重启后都能保证事务的完整性，已提交的事务不会丢失，未提交完整的数据会自动进行回滚，这个能力依赖的就是 Redo Log和 Undo Log两个日志。 因为宕机恢复主要体现在事务执行过程中突然崩溃，重启后能保证事务的完整性，所以在讲解具体的原理前，先了解一下 MySQL 事务执行有哪些关键阶段，后面才能依据这几个阶段来进行解析，下面以一条更新语句的执行流程为例来进行说明：  从内存中找出这条数据记录，对其进行更新。 将旧数据记录到 Undo Log 中。 将对数据页的更改记录到 Redo Log Buffer 中，状态为prepare 状态。 将逻辑操作记录到 Binlog Cache 中。 将 Redo Log 中的更改记录设置为commit 状态。  对于内存中的数据和日志，都是由后台线程来进行处理，当触发到落盘规则后再异步进行刷盘。\n       2 WAL 技术 #   MySQL更改数据的时候，不是直接写磁盘文件中的数据，因为直接写磁盘文件是随机写，开销大性能低，没办法满足 MySQL 的性能要求，因此会设计成先在内存中对数据进行更改，再异步落盘。 但是内存总是不可靠，万一断电重启，还没来得及落盘的内存数据就会丢失，所以还需要加上写日志这个步骤，万一断电重启，还能通过日志中的记录进行恢复。 写日志虽然也是写磁盘，但是他是顺序写，相比随机写开销更小，能提升语句的执行性能。 这个技术就是大多数存储系统都会用的WAL（Write Ahead Log）技术，也称为日志先行的技术，指的是对数据文件进行修改前，必须将修改先记录日志，这样可以保证数据的一致性和持久性，也能提升语句执行性能。  3 核心日志模块 #  更新 SQL 执行过程中，总共涉及 MySQL 日志模块其中的三个核心日志，分别是 Redo Log（重做日志）、Undo Log（回滚日志）、Binlog（归档日志），下面将会对的三个日志进行一个概要介绍，详细的内容可以参考 1.4 事务日志。\n3.1 Redo Log #   Redo Log 也称为重做日志，由InnoDB 存储引擎层产生，记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页（恢复数据页只能恢复到最后一次提交的位置，因为后面的修改恢复改之前的）。 前面提到的[WAL 技术](#2-WAL 技术)，Redo Log 就是 WAL 的典型应用，MySQL 在有事务提交对数据进行更改时，只会在内存中修改对应的数据页和 Redo Log 日志，完成后即表示事务提交成功，至于磁盘数据文件的更新，则由后台线程异步处理。 由于 Redo Log 的加入，保证了 MySQL 数据一致性和持久性（即使数据刷盘之前 MySQL 崩溃了，重启后仍然能通过 Redo Log 里的更改记录进行重放，重新刷盘），此外，还能提升语句的执行性能（写 Redo Log 是顺序写，相比于更新数据文件的随机写，日志的写入开销更小，能显著提升语句的执行性能，提高并发量），由此可见 Redo Log 是必不可少的。 Redo Log 是固定大小的，所以只能循环写，从头开始写，写到末尾就又回到开头，相当于一个环形，当日志写满了，就需要对旧的记录进行擦除，但在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘了，在 Redo Log 满了到擦除旧记录腾出新空间这段时间，是不能再接收新的更新请求，所以有可能会导致 MySQL 卡顿，所以针对并发量大的系统，适当设置 Redo Log 的文件大小非常重要。  3.2 Undo Log #   Undo Log 主要提供了回滚 和** 多行版本控制**（MVCC，保证事务的原子性）两个作用。 在数据修改的流程中，会记录一条与当前操作相反的逻辑日志到 Undo Log 中（可以认为当delete 一条记录时，Undo Log 中会记录一条对应的insert 记录，反之亦然，当update 一条记录时，他记录一条对应相反的update 记录），如果因为某些原因导致事务异常失败了，可以借助该 Undo Log 进行回滚，保证事务的完整性，所以 Undo Log 也必不可少。  3.3 Binlog #   Binlog在 MySQL 的 Server 层产生，不属于任何引擎，主要记录用户对数据库操作的 SQL 语句（除了查询语句）。 之所以将 Binlog 称为归档日志，是因为Binlog 不会像 Redo Log 一样擦掉之前的记录循环写，而是一直记录，等到超过有效期才会被清理，如果超过单日志的最大值（默认 1G，可以通过变量mac_binlog_size 设置），则会新起一个文件继续记录，但由于日志可能是基于事务来记录的（如 InnoDB 表类型），而事务是绝不可能也不应该跨文件记录的，如果正好 Binlog 日志文件达到了最大值但事务还没有提交，则不会切换新的文件记录，而是继续增大日志，所以max_binlog_size指定的值和实际的 Binlog 日志大小不一定相等。 正是由于Binlog 有归档的作用，所以 Binlog主要用于主从同步和数据库基于时间点的还原。 Binlog 是否可以简化掉，需要分场景来看：  如果是主从模式，Binlog 是必须的，因为从库的数据同步依赖的就是 Binlog。 如果是单机模式，并且不考虑数据库基于时间点的还原，Binlog 就不是必须的，因为有 Redo Log 就可以保证宕机恢复的能力了，但是万一需要回滚到某个时间点的状态，这个时候就无能为力了，所以建议 Binlog 还是一直开启。    4 两阶段提交 #   从上面可以看出，因为Redo Log 影响主库的数据，Binlog 影响从库的数据，所以Redo Log 和 Binlog 必须保持一致才能保证主从数据一致，这是前提。 这里的 Redo Log 和 Binlog 其实就是很典型的分布式事务场景，因为两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理，而将 Redo Log 分成了两步，其实就是使用了两阶段提交协议（Two-phase Commit，2PC）。 下面对更新语句的执行流程进行简化，看一下 MySQL 的两阶段提交是如何实现的：  从图中可以看出，事务的提交有两个阶段，就是将 Redo Log 的写入拆成了两个步骤：prepare和commit，中间再穿插写入 Binlog。 有时候我们也很疑惑，为什么一定要用两阶段提交呢，如果不用两阶段提交会出现什么情况，比如先写 Redo Log，再写 Binlog，或者先写 Binlog，再写 Redo Log 不行吗，下面我们用反证法来进行论证，我们继续用update T set c = c + 1 where id = 2 这个例子，假设id = 2 这一条数据的c 初始值为 0：   假如在Redo Log 写完，Binlog 还没有写完的时候，MySQL 进程异常重启，由于 Redo Log 已经写完了，系统重启会通过 Redo Log 将数据恢复回来，所以恢复后这一行 c 的值是 1，但是由于 Binlog 没写完就 crash 了，这时候Binlog 里面就没有记录这个语句，因此，不管是现在的从库还是之后通过这份 Binlog 还原临时库都没有这一次更新，c的值还是 0，与原库不同，这就造成了主从不一致。\n  同理，如果先写 Binlog，再写 Redo Log，中途系统 crash 了，也会导致主从不一致，这里就不再详述了。\n  所以将 Redo Log 分成两步写，即两阶段提交，才能保证 Redo Log 和 Binlog 内容一致，从而保证主从数据一致。\n  两阶段提交虽然能保证但事务两个日志的内容一致，但在多事务的情况下，却不能保证两者的提交顺序一致，比如下面这个例子，假设现在有 3 个事务同时提交：\nT1 (--prepare--binlog---------------------commit) T2 (-----prepare-----binlog----commit) T3 (--------prepare-------binlog------commit) 此时各个阶段写入的顺序如下：\n Redo Log Prepare 的顺序： T1 \u0026ndash;》T2 \u0026ndash;》T3。 Binlog 的写入顺序： T1 \u0026ndash;》T2 \u0026ndash;》T3。 Redo Log Commit 的顺序： T2 \u0026ndash;》T3 \u0026ndash;\u0026gt; T1。  由于 Binlog 写入的顺序和 Redo Log 提交结束的顺序不一致，导致 Binlog 和 Redo Log 所记录的事务提交结束的顺序不一样，最终导致的结果就是主从数据不一致。\n  因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致：\n 在早期的 MySQL 版本中，通过使用prepare_commit_mutex 锁来保证事务提交的顺序。 在一个事务获取到锁时才能进入 prepare，一直到 commit 结束才能释放锁，下个事务才可以继续进行 prepare 操作。 加锁虽然完美地解决了顺序一致性的问题，但是又会导致另外两个新的问题：  在并发量较大的时候，会导致对锁的争用，性能不佳。 每个事务提交都会进行两次 fsync（写磁盘），一次是 Redo Log 落盘，另一次是 Binlog 落盘，而写磁盘是很昂贵的操作，对于普通磁盘，每秒的 QPS 大概也就是几百。          5 组提交 #   针对通过在两阶段提交中加锁控制事务提交顺序这种实现方式遇到的性能瓶颈问题可以通过组提交的方式来解决。 在 MySQL 5.6 就引入了Binlog 组提交，即BLGC（Binary Log Group Commit）。 Binlog 组提交的基本思想是引入队列机制保证 InnoDB 事务提交顺序与 Binlog 落盘顺序一致，并将事务分组，组内的 Binlog 刷盘动作交给一个事务进行，实现组提交目的，具体如下图所示：  第一阶段（prepare阶段）：  持有prepare_commit_mutex，然后Redo Log 到磁盘，设置为 prepare状态 ，完成后就释放prepare_commit_mutex，Binlog 不作任何操作 。   第二阶段（commit阶段），这里拆分成了三步，每一步的任务分配给一个专门的线程处理：  Flush Stage（写入 Binlog 缓存）：  持有lock_log_mutex（leader 持有，follower 等待）。 获取队列中的一组Binlog（队列中的所有事务）。 写入Binlog缓存。   Sync Stage（将Binlog落盘）：  释放 lock_log_mutex，持有 lock_sync_mutex（leader持有，follower等待）。 将一组Binlog落盘（fsync动作，最耗时，假设sync_binlog为1）。   Commit Stage（InnoDB Commit，清除Undo信息）：  释放 lock_sync_mutex，持有 lock_commit_mutex（leader持有，follower等待）。 遍历队列中的事务，逐一进行InnoDB Commit。 释放 lock_commit_mutex。       每个Stage都有自己的队列，队列中的第一个事务称为leader，其他事务称为follower，leader控制着follower的行为。 每个队列各自有 mutex保护，队列之间是顺序的，只有 flush完成后，才能进入到 sync阶段的队列中，sync完成后，才能进入到 commit阶段的队列中，但是这三个阶段的作业是可以同时并发执行的，即当一组事务在进行 commit阶段时，其他新事务可以进行 flush阶段，实现真正意义上的组提交，大幅度降低磁盘的IOPS消耗。 组提交虽然在每个队列中仍然保留了 prepare_commit_mutex锁，但是锁的粒度变小了，变成了原来两阶段提交的$\\frac14$，所以锁的争用性也会大大降低，另外，组提交是批量刷盘，相比之前的单条记录刷盘，大幅度降低了磁盘的IO消耗，因此组提交比两阶段提交加锁性能更好。  6 数据恢复流程 #  6.1 整体流程 #  MySQL重启后，恢复数据的流程如下图所示：\n  首先会检查Redo Log中是完整并且处于 prepare状态的事务。 然后根据XID（事务ID）从Binlog中找到对应事务：  如果找不到，则根据Undo Log进行回滚。 如果找到并且事务完整，则重新设置Redo Log的 commit标识，完成事务的提交。    6.2 各阶段MySQL崩溃的恢复策略 #   时刻A（刚在内存中更改完数据页，还没有开始写Redo Log的时候崩溃）：  因为内存中的脏页还没刷盘，也没有写Redo Log和Binlog，即这个事务还没有开始提交，所以崩溃恢复跟该事务没有关系。   时刻B（正在写Redo Log，或者已经写完Redo Log并且落盘后，处于 prepare状态，还没有开始写Binlog的时候崩溃）：  恢复后判断Redo Log的事务是不是完整的：  如果不是，则根据Undo Log回滚。 如果是完整的，并且是 prepare状态，则进一步判断对应事务的Binlog是不是完整的：  如果不是，则根据Undo Log回滚。 如果是完整的，则重新设置Redo Log的 commit标识，完成事务的提交。       时刻C（正在写Binlog，或者已经写完Binlog并且落盘了，还没有开始 commitRedo Log的时候崩溃）：  恢复后跟时刻B一样，按照时刻B的处理方式进行处理即可。   时刻D（正在 commitRedo Log或者事务已经提交完的时候，还没有反馈成功给客户端的时候崩溃）：  恢复后跟时刻C基本一样，都会对照Redo Log和Binlog的事务完整性，来确认是回滚还是重新提交。    参考文献 #    MySQL 的 crash-safe 原理解析。  "},{"id":55,"href":"/school-recruitment/docs/database/2MySQL/2.7-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96/","title":"2.7 数据库优化","section":"2、 My SQL","content":"1 优化方法 #  1.1 SQL 语句的优化 #   优化 SQL 语句时可以按照以下的步骤来进行：\n 首先分析慢查询日志，这里面记录了响应时间超过阈值 long_query_time 的 SQL 语句，通过日志找出 IO 大的 SQL 以及发现未命中索引的 SQL。 然后使用 EXPLAIN 对慢查询 SQL 进行分析，通过EXPLAIN 命令可以得到表的读取顺序、数据读取操作的操作类型、哪些索引可以使用、哪些索引被实际使用、表之间的引用以及被扫描的行数等问题。     应尽量避免在 WHERE 子句中使用 !=、\u0026lt;、\u0026gt;操作符或对字段进行 null 值判断，否则数据库引擎将放弃使用索引而进行全表扫描。\n  只返回必要的列，最好不要用 SELECT * 语句。\n  只返回必要的行，使用 LIMIT 语句来限制返回的数据。\n  高并发高性能的应用中尽量对关联查询分解为单表查询，然后将结果在应用程序中进行关联，例如下面这个查询：\nselect * from tag join tag_post on tag_post.tag_id=tag.id join post on tag_post.post_id=post.id where tag.tag=\u0026#39;mysql\u0026#39;; 可以分解成下面这些查询来代替：\nselect * from tag where tag=\u0026#39;mysql\u0026#39;; select * from tag_post where tag_id=1234; select * from post where id in(123,456,567,9989,8909); 这种用分解关联查询的方式重构查询具有如下优势：\n 让缓存的效率更高：  许多应用程序可以方便地缓存单表查询对应的结果对象。 对于 MySQL 的查询缓存来说，如果关联中的某个表发生了变化，那么就无法使用查询缓存了，而拆分后，如果某个表很少改变，那么基于该表的查询就可以重复利用查询缓存结果了。   将查询分解后，执行单个查询可以减少锁的竞争。 在应用层做关联，可以更容易对数据库进行拆分，更容易做到高性能和可扩展。 分解成多个单表查询，这些单表查询的缓存结果更可能被其他查询使用到，从而减少冗余的查询。    1.2 索引的优化 #   注意会引起索引失效的情况，以及在适合的地方建立索引，具体可参考 2.1 索引。  1.3 数据库表结构的优化 #   设计表时遵循 三范式。 选择合适的数据类型：  简单的原则：  通常最小的是最好的：  因为这样可以用更少的磁盘、内容、CPU 缓存、大大减少 IO 开销。   简单就好：  简单的数据类型操作通常需要更少的 CPU 周期，例如：  整型比字符操作代价更小，因为字符集和校对规则（排序规则）使他比整型更复杂。 应该使用 MySQL 内置的类型而不是使用字符型来存储日期和时间。     尽量避免使用 NULL：  NULL 是列默认的属性，通常我们要指定为 NOT NULL。 有 NULL 的列值会使得索引、索引统计和值比较更加复杂。 可为 NULL 的列会使用更多的存储空间，在 MySQL 中也需要对他特殊处理，当可为 NULL 的列做索引时，每个索引需要一个额外的字节，在 MyISAM 更有可能导致固定大小的索引变成可变大小的索引，在 InnoDB 中使用单独的位（bit）存储 NULL 值。     相关数据类型的使用方法如下：   整数类型：\n  几种整数类型：TINYINT（1 字节）、SMALLINT（2 字节）、MEDIUMINT（3 字节）、INT（4 字节）、BIGINT（8 字节），他们的范围是-2 的 $(n-1)$ 次方到 2 的 $(n-1)$ 次方-1，如果选择了 UNSIGNED，表示非负，他可以使整数最大值提高一倍，有符号和无符号使用相同的存储空间，具有相同的性能。\n 注：上面的 $n$ 指的是整数类型的位数，其中 1 字节等于 8 位。\n   为整型指定宽度，如 INT（11），对于存储来说，INT（11）和 INT（20）是相同的，他不会限制值的合法范围，只是规定了 MySQL 与客户端的交互应该显示多少位，而且这个值和 ZEROFILL 结合着用才有效，当该字段开启了 ZEROFILL 时，如果存储的位数小于定义的位数，则会在前面补 0，如果大于定义的位数，则直接显示，例如：\n  定义一张表 test_data_type，包含两个字段，分别为 a int(4)、b int(4) unsigned zerofill：\nDROP TABLE IF EXISTS `test_data_type`; CREATE TABLE `test_data_type` ( `a` int(4) NOT NULL, `b` int(4) unsigned zerofill DEFAULT NULL );   向这张表中插入数据：\nmysql\u0026gt; insert into test_data_type (a,b) values (12,12); mysql\u0026gt; select * from test_data_type; +----+------+ | a | b | +----+------+ | 12 | 0012 | +----+------+ mysql\u0026gt; insert into test_data_type (a,b) values (123456,123456); mysql\u0026gt; select * from test_data_type; +--------+--------+ | a | b | +--------+--------+ | 12 | 0012 | | 123456 | 123456 | +--------+--------+   从上面的结果中可以看出：\n a 字段没有开启zerofill，因此即使存储的数据位数小于该字段定义时的位数，也不会补 0，而是直接显示相应数据。 b 字段开启了zerofill，因此当存储的数据位数小于该字段定义时的位数时，会自动在前面补 0。 存储的数据位数大于该字段定义时的位数时，两个字段都是直接显示相应数据。        实数类型：\n MySQL 中有 3 种类型可以表示实数，分别是float、double、decimal，这三种类型的详细信息如下：  float：4 字节，单精度，浮点数值。 double，8 字节，双精度，浮点数值。 decimal：对于decimal(m,d)，如果m\u0026gt;d，则为m+2，否则为d+2，小数值。   选择方式：  如果我们要表示的浮点型数据转成二进制之后能被 32 位 float 存储，或者可以容忍截断，则使用 float，这个范围大概为要精确保存 6 位数字左右。 如果我们要表示的浮点型数据转成二进制之后能被 64 位 double 存储，或者可以容忍截断，则使用 double，这个范围大概为要精确保存 13 为数字左右。 相比double，已经能满足我们大部分浮点型数据存储精度要求，如果还要精益求精，可以使用 decimal 存储一些科学数据或者精度要求很高的金钱。 因为decimal需要额外的空间和计算开销，应尽量在只对小数进行精确计算的时候才使用 decimal，在有金额交易的过程中，更倾向于使用 bigint 代替 decimal，将金额单位扩大相应倍数，如金额单位为元，可以在存储的时候扩大 100 倍，使用分为单位，这样可以避免使用浮点计算不精确的问题和使用 decimal 计算代价高的问题。      字符串类型：\n varchar 和 char：  字符串类型的表示：  char 和varchar 表示字符串的格式为char(N) 和varchar(N)。 其中 $N$ 在 MySQL 5.0 之前表示最大存储的字节数，在 MySQL 5.0 之后表示最大存储的字符数，字符数超过 $N$ 会被截断，超过 $N$ 的部分将会被丢弃。   存储上的不同：  对于char 来说，最多存放 255 个字符，和编码无关。 对于varchar 来说，可以表示 65535 个字节，但是最多存放 65532 个字节，因为需要 2 个字节来存放字符串的长度，以及结尾还要用 1 个字节表示结束，所以有效长度就是 65535-1-2 =65532。   定长和变长：  char(N)：char 是定长的，插入数据不足规定长度 $N$，右边使用空格补全，字符数超过 $N$ 会被截断，超过 $N$ 的部分将会被丢弃。 varchar(N)：varchar 是不定长的，当 $N \\le 255$ 时，需要用 1 个字节来存放字符串的长度，当 $N \\gt 255$ 时，需要用 2 个字节来存放字符串的长度，另外外加1 个代表结束的字节，其他的就是所需表示字符。   查找效率：  char大于varchar。 因为char 在存放数据的时候中间没有间隔，数据长度是固定的，而且数据段之间没有间隔，因此在 MySQL查询的时候只需要按部就班寻找就行了，不需要在中途计算这个数据段的长度。 varchar 类型的存储方式就不同了，在每个数据段开头，都要有一段空间（1~2 个字节）存放数据段的长度，在数据段的结尾还有一段空间（1 个字节）标记此字段的字节数，因此 MySQL 在读取一个数据段的时候，首先要读开头，比如读到了 3，说明数据段的长度是 3，之后就不多不少，只读 3 个字节，又因为数据段被隔开了，所以 MySQL在遍历 varchar 类型数据的时候，磁针要比 char 类型的列多读很多次磁盘来获取字段的真实长度，这也就是为什么 varchar 比 char 查询效率低的原因了。   末尾空格处理：  char(N) 会去掉结尾的空格，varchar(N)不会去掉结尾的空格。 例如：   创建如下所示的表 test_data_type：\nDROP TABLE IF EXISTS `test_data_type`; CREATE TABLE `test_data_type` ( `e` varchar(10) COLLATE utf8_bin DEFAULT NULL, `f` char(10) COLLATE utf8_bin DEFAULT NULL );   向 test_data_type 中插入数据：\nmysql\u0026gt; insert into test_data_type (e, f) values (\u0026#34;abcde \u0026#34;, \u0026#34;abcde \u0026#34;); mysql\u0026gt; select e, char_length(e) as e_length, f, char_length(f) as f_length from test_data_type; +--------+----------+-------+----------+ | e | e_length | f | f_length | +--------+----------+-------+----------+ | abcde | 6 | abcde | 5 | +--------+----------+-------+----------+   从上面的结果中可以看出 e 插入 abcde  后占用 6 个字符，说明没有去除结尾的空格，而 f 插入 abcde  后占用 5 个字符，说明去除了结尾的空格。\n     使用建议：  在设置 MySQL 字符串字段属性的时候要结合业务场景进行选择，不要脱离实际业务盲目选择。 存储定长字符串，尽量用 char，索引极快（例如手机号、身份证号）。 长度 255 以上字符串，只能用 varchar 和 text，能用 varchar 尽量不用 text（这点会在下面进行说明）。     BLOB 和 TEXT：  主要差别：  TEXT 和 BLOB 的主要差别就是BLOB 保存二进制数据，TEXT 保存字符数据。   类型区别：  BLOB 有四种类型：TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB，他们只是可容纳值的最大长度不同。 TEXT 也有四种类型：TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT，这些类型同 BLOB 类型一样，有相同的最大长度和存储需求。   字符集：  BLOB 列没有字符集，并且排序和比较基于列值字节的数据。 TEXT 列有一个字符集，并且根据字符集的校对规则对值进行排序和比较。   大小写：  在 TEXT 或 BLOB 列的存储或检索过程中，不存在大小写转换，都一样。   严格模式：  运行在非严格模式下，如果我们为 BLOB 或 TEXT 列分配一个超过该列类型的最大长度的值，那么超过长度的部分将会被截断，如果被截掉的字符不是空格，那么将会产生一条警告。 如果使用严格模式，那么将会直接报错。   其他：  当保存或检索 BLOB 和 TEXT 列的值时不删除尾部空格。 对于 BLOB 和 TEXT 列的索引，必须指定索引前缀的长度。 BLOB 和 TEXT 列不能有默认值。 当排序时只能使用该列的前 max_sort_length 个字节，其默认值是 1024。 BLOB 和 TEXT 对象的最大大小由其类型决定，但在客户端和服务器之间实际可以传递的最大值由可用内存数量和通信缓冲区大小决定，我们可以通过更改 max_allowed_packet 变量的值更改消息缓冲区的大小，但必须同时修改服务器和客户端程序。        时间和日期类型：\n DATETIME：  这个类型能保存 1001 到 9999 年，精度为秒，与时区无关。 使用 8 个字节存储，存储格式封装为 YYYYMMDDHHMMSS 的整数，因此是一种可排序的类型。 显示时以 ANSI 标准定义的日期和时间表示方法显示。   TIMESTAMP：  他保存了从 1970 年 1 月 1 日午夜以来的秒数，也就是常说的时间戳。 使用 4 个字节存储，依赖于时区。 除特殊情况外，通常我们应该尽量使用 TIMESTAMP 存储时间，因为他比 DATETIME 更省空间。       进行 分库分表。    1.4 系统配置的优化 #   操作系统：   增加 TCP 支持的队列数（网络方面的配置，要修改 /etc/sysctl.conf）：\n#增加 tcp 支持的队列数 net.ipv4.tcp_max_syn_backlog = 65535 #减少断开连接时，资源回收 net.ipv4.tcp_max_tw_buckets = 8000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_fin_timeout = 10    MySQL 配置文件优化：  innodb_buffer_pool_size：配置缓冲池大小，如果数据库中只有 innodb 表，则推荐配置量为总内存的 75%。 innodb_buffer_pool_instances：配置缓冲池的个数，默认只有一个缓冲池。 innodb_log_buffer_size：配置日志缓冲大小，由于日志最长每秒钟就会刷新一次，所以一般不用太大。 innodb_flush_log_at_trx_commit：配置数据多久将变更刷新到磁盘，具体可参考 2.2 含义。 innodb_read_io_threads：配置读的 IO 进程数，默认为 4。 innodb_write_io_threads：配置写的 IO 进程数，默认为 4。 innodb_file_per_table：配置每一个表是否使用独立的表空间，具体可参考 2.5 Log Group 和 Redo Log File。    1.5 硬件的优化 #   使用固态硬盘。 使用多核且高频的 CPU。 增大内存。  2 场景示例 #  2.1 MySQL 处理千万级数据分页查询的优化方案 #    使用分页查询时我们一般使用 limit 关键词，对于小的偏移量，直接使用 limit 来查询没有什么问题，但随着数据量的增大，越往后分页，limit语句的偏移量就会越大，速度也会明显变慢，这是因为每次都要取出大量的数据，然后把大部分的数据抛弃，只留下后面一下部分的数据，性能较差。\n  假如有一张表，表的相关信息如下：\n 表名：order，订单表。 字段情况：该表一共 37 个字段，不包含text 等大型数据，最大为varchar(500)，id 字段为索引，且为递增。 数据量：5709294。    假设现在有如下查询：\nselect * from order where user_id = 3 order by id limit 100000, 100; 该查询大约耗时 14 秒多，耗时较长。\n  可以按照如下方式进行优化：\n 使用 覆盖索引：   假设我们只需要查询 order_type 和 order_amt 两个字段，可以建立联合索引（order_type、order_amt），这样可以让查询走 联合索引，加快性能：\nselect order_type, order_amt from order limit 100000, 100;   但是我们这个表有 37 个字段，这么优化明显不合适，而且这种加快性能的效果并不明显。\n   将offset计算出来后将主键索引作为where条件：   假如数据表的 id是连续递增的，可以直接将对应的 offset计算出来，作为 where条件，这样可以利用主键索引，性能提升非常明显：\n-- 方式一 select order_type, order_amt from order where id \u0026gt;= 100000 limit 100; -- 方式二 select order_type, order_amt from order where id between 100000 and 100100 limit 100;   但是这种方案存在严重问题，可能存在部分数据被删过，导致 id不连续，所以上面查出来的数据并不是我们想要的。\n   每次分页查询记录上一次分页最后一条 id：   我们可以在每次分页查询的时候记录上一次分页最后一条 id，然后在查询的时候直接根据上一次分页最后一条 id来进行查询即可，此时就算 id不连续也没问题，但是必须自增，由于数据库主键我们一般设置成连续自增，所以这种方式可以大幅度提升性能：\nselect * order where id \u0026gt;= 上一页最大id limit 100;    使用子查询：   上面的一种方法每次都需要记录上一次分页的最大 id，比较麻烦，我们可以使用子查询代替：\n-- 方式一 select * from order where id in (select id from order limit 100000, 100); -- 方式二 select * from order where id \u0026gt;= (select id from order limit 100000, 1) limit 100; -- 方式三 select * from order t1 join (select id from order limit 100000, 1) t2 on t1.id \u0026gt;= t2.id limit 100;   这种方式之所以能够大幅度优化性能，主要在于：\n 直接分页，不会走索引，全表扫描，其实是遍历主键索引树，但是每次都需要把对应行的数据取出来，要取100100条数据，然后丢弃前100000条，太耗性能，可以通过explain执行计划查看对应索引情况。 使用子查询，会走主键索引，虽然也是遍历主键索引树，但是只取 id，不需要取整行数据，最后外层查询拿到对应的100条 id，查询对应数据即可，通过数据索引字段定位后，大大减少了查询的数据量，效率自然大大提升。 上述使用 in、where条件和 join，差别不大。     业务优化：  一般来说翻页不会超过20页，可以通过限制翻页的数量来解决这个问题，像百度分页最多只展示76页，还有一种方式，就是使用滚动，和微博一样，没有翻页，只能不断下拉，就是使用之前记录上一页最大 offset那个方法就可以做到。      参考文献 #    如何优化数据库？  MySQL 数据库优化，看这篇就够了。  数据库优化和查询优化方案。  MySQL 数据库优化的八种方式(经典必看)。  单表查询和多表连接查询哪个效率更快。  MySQL 之选择字段数据类型。  What does “size” in int(size) of MySQL mean?  MySQL 如何选择 float, double, decimal。  MySQL 实数类型使用注意事项。  【Mysql】：搞清楚字符串类型 char、varchar、text。  MySQL 中 TEXT 与 BLOB 字段类型的区别。  MySql 系统配置优化。  MySQL 大数据量分页 limit 优化_LJJZJ 的博客-程序员宅基地。  "},{"id":56,"href":"/school-recruitment/docs/database/2MySQL/2.8-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/","title":"2.8 分库分表","section":"2、 My SQL","content":"分库分表 #  1 为什么要进行分库分表 #    在业务数据量比较少的时代，我们使用单机数据库就能满足业务使用，随着业务请求量越来越多，数据库中的数据量快速增加，这时单机数据库已经不能满足业务的性能要求，数据库主从架构随之应运而生。\n  主从复制是将数据库写操作和读操作进行分离，使用多个只读实例（Slaver Replication）负责处理读请求，主实例（Master）负责处理写请求，只读实例通过复制主实例的数据来保持与主实例的数据一致性，由于只读实例可以水平扩展，所以更多的读请求不成问题。\n  随着云计算、大数据时代的到来，事情并没有完美的得以解决，当写请求越来越多，主实例的写请求变成主要的性能瓶颈。\n  如果仅仅通过增加一个主实例来分担写请求，写操作如何在两个主实例之间同步来保证数据一致性，如何避免双写，此时问题会变得更加复杂，这时就需要用到分库分表（Sharding），对写操作进行切分来解决，如下图所示：\n  上图中的 DDM 为华为云的中间件产品，全称是 Distributed Database Middleware，作为 RDS（Relational Database Service）的前置分布式数据库访问服务，彻底解决了数据库的扩展性问题，对应用透明地实现海量数据的高并发访问，实现了读写分离和分库分表。\n   2 分库分表的切分方式 #   数据的切分（Sharding）根据其切分规则的类型，可以分为两种切分模式：  一种是按照不同的表（或者 Schema）来切分到不同的数据库（主机）上，这种切分方式可以称之为数据的垂直（纵向）切分。 另外一种则是根据表中的数据的逻辑关系，将同一个表中的数据按照某种条件拆分到多台数据库（主机）上面，这种切分称之为数据的水平（横向）切分。   垂直切分最大的特点就是规则简单，实施也更为方便，尤其适合各业务之间的耦合度非常低，相互影响很小，业务逻辑非常清晰的系统，在这种系统中，可以很容易做到将不同业务模块所使用的的表拆分到不同的数据库中，根据不同的表来进行拆分，对应用程序的影响也更小，拆分规则也会比较简单清晰。 水平切分相对于垂直切分来说，稍微复杂一些，因为要将同一个表中的不同数据拆分到不同的数据库中，对于应用程序来说，拆分规则本身比根据表名来拆分更为复杂，后期的数据维护也会更为复杂一些。 具体而言，如果单个库太大，这是我们要看是因为表多而导致数据多，还是因为单张表里面的数据多：  如果是因为表多而数据多，则使用垂直切分，根据业务切分成不同的库。 如果是因为单张表的数据量太大，这时要用水平切分，即把表的数据按照某种规则切分成多张表，甚至多个库上的多张表。   分库分表的顺序应该是先垂直分，后水平分，因为垂直分更简单，更符合我们处理现实世界问题的方式。  3 垂直切分 #  3.1 含义 #    垂直切分常见有垂直分库和垂直分表两种。\n  垂直分库就是根据业务耦合性，将关联度低的表存储在不同的数据库，做法与大系统拆分为多个小系统类似，按业务分类进行独立划分：\n 这种方法业务和数据结构最清晰。 但是模块间不能相互关联查询，如果有就必须通过数据冗余或应用层二次加工来解决。     垂直分表是基于数据库中的列进行，如果某个表字段较多，可以新建一张扩展表，将不经常用或字段长度较大的字段拆分出去到扩展表中：\n 这样更便于开发和维护。 同时也能避免跨页问题，因为 MySQL底层是通过数据页存储的，一条记录占用空间过大会导致跨页，造成额外的性能开销， 另外数据库以行为单位将数据加载到内存中，这样表中字段长度较短且访问频率较高，内存能加载更多的数据，命中率更高，减少了磁盘 IO，从而提升了数据库性能。     3.2 优缺点 #  3.2.1 优点 #   解决业务系统层面的耦合，业务清晰。 与微服务的治理类似，能对不同业务的数据进行分级管理、维护、监控、扩展等。 高并发场景下，垂直切分能在一定程度上提升 IO、数据库连接数、单机硬件资源的瓶颈等。  3.2.2 缺点 #   部分表无法 join，只能通过接口聚合方式解决，提升了开发的复杂度。 分布式事务处理复杂。 依然存在单表数据量过大的问题。  4 水平切分 #  4.1 含义 #   当一个应用难以再细粒度的垂直切分，或切分后数据量行数巨大，存在单库读写、存储性能瓶颈，这时候就需要进行水平切分了。 水平切分分为库内分表和分库分表两种方式，是根据表内数据内在的逻辑关系，将同一个表按不同的条件分散到多个数据库或多个表中，每个表中只包含一部分数据，从而使得单个表的数据量变小，达到分布式的效果。  库内分表只解决了单一表数据量过大的问题，但没有将表分布到不同机器的库上，因此对于减轻 MySQL 数据库的压力来说，帮助不是很大，大家还是竞争同一个物理机的 CPU、内存、网络 IO，因此最好通过分库分表来解决。  4.2 优缺点 #  4.2.1 优点 #   不存在单库数据量过大、高并发的性能瓶颈，提升系统稳定性和负载能力。 应用端改造较小，不需要拆分业务模块。  4.2.2 缺点 #   跨分片的事务一致性难以保证。 跨库的 join 关联查询性能较差。 数据多次扩展难度和维护量极大。  4.3 切分规则 #  4.3.1 根据数值范围 #  4.3.1 含义 #   根据时间区间或ID 区间来切分，例如：  按日期将不同月甚至是日的数据分散到不同的库中。 将 userid 为 1~9999 的记录分到第一个库，10000~20000 分到第二个库，以此类推。   某种意义上，某些系统中使用的冷热数据分离，将一些使用较少的历史数据迁移到其他库中，业务功能上只提供热点数据的查询，也是类似的实践。  4.3.2 优缺点 #  4.3.2.1 优点 #   单表大小可控。 天然便于水平扩展，后期如果想对整个分片集群扩容时，只需要添加节点即可，无需对其他分片的数据进行迁移。 使用分片字段进行范围查找时，连续分片可可快速定位分片进行快速查询，有效避免跨分片查询的问题。  4.3.2.2 缺点 #   热点数据成为性能瓶颈，连续分片可能存在数据热点，例如按时间字段分片，有些分片存储最近时间段的数据，可能会被频繁的读写，而有些分片存储的历史数据，则很少被查询。   4.3.2 根据数值取模 #   一般采用hash取模的切分方式，例如，将Customer 表根据cusno 字段切分到 4 个库中，余数为 0 的放到第一个库，余数为 1 的放到第二个库，以此类推，这样同一个用户的数据会分散到同一个库中，如果查询条件带有 cusno 字段，则可明确定位到相应库去查询。  4.3.2.1 优点 #   数据分片相对比较均匀，不容易出现热点和并发访问的瓶颈。  4.3.2.2 缺点 #   后期分片集群扩容时，需要迁移旧的数据（使用 一致性哈希算法能较好的避免这个问题）。 容易面临跨分片查询的复杂问题，比如上例中，如果频繁用到的查询条件中不带 cusno 时，将会导致无法定位数据库，从而需要同时向 4 个库发起查询，再在内存中合并数据，取最小集返回给应用，分库反而成为拖累。   5 分库分表带来的问题 #  5.1 事务一致性问题 #  5.1.1 分布式事务 #   当更新内容同时分布在不同库中，不可避免会带来跨库事务问题，跨分片事务也是分布式事务，没有简单的方案，一般可使用XA 协议和 两阶段提交来处理。 分布式事务能最大限度保证数据库操作的原子性，但在提交事务时需要协调多个节点，推后了提交事务的时间点，延长了事务的执行时间，导致事务在访问共享资源时发生冲突或死锁的概率增高。 随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平扩展的枷锁。  5.1.2 最终一致性 #    对于那些性能要求很高，但对一致性要求不高的系统，往往不苛求系统的实时一致性，只要在允许的时间段内达到最终一致性即可，可采用事务补偿的方式。\n  与事务在执行过程中发生错误后立即回滚的方式不同，事务补偿是一种事后检查补救的措施，一些常见的实现方法有：\n 对数据进行对账检查。 基于日志进行对比。 定期同标准数据来源进行同步。  事务补偿还要结合业务系统来考虑。\n  5.2 跨节点关联查询 join 问题 #  切分之前，系统中很多列表和详情页所需的数据可以通过 sql join 来完成，而切分之后，数据可能分布在不同的节点上，此时 join 带来的问题就比较麻烦了，考虑到性能，应该尽量避免使用 join 查询。\n解决这个问题可以采用以下方法：\n5.2.1 全局表 #   全局表，也可以看做是数据字典表，就是系统中所有模块都可能依赖的一些表。 字典表一般具有以下特性：  变动不频繁。 数据量总体变化不大。 数据规模不大，很少有超过数十万条记录。     比如 MyCat 定义的全局表具有以下特性：\n 全局表的插入、更新操作会实时在所有节点上执行，保持各个分片的数据一致性。 全局表的查询操作，只从一个节点获取。 全局表可以跟任何一个表进行 join 操作。   5.2.2 字段冗余 #   这是一种典型的反范式设计，利用空间换时间，为了性能而避免 join 查询。 例如，订单表保存userId 的时候，也将userName 冗余保存一份，这样查询订单详情时就不需要再去查询买家user 表了。 但这种方法适用场景也有限，比较适用于依赖字段比较少的情况，而且冗余字段的一致性也比较难保证，就像上面订单表的例子，买家修改了userName 后，是否需要在历史订单中同步更新呢，这也需要结合实际业务场景进行考虑。  5.2.3 数据组装 #   在系统层面，分两次查询，第一次查询的结果集中找出关联数据 id，然后根据 id 发起第二次请求得到关联数据，最后将得到的数据进行字段拼装。  5.2.4 ER 分片 #   关系型数据库中，如果可以先确定表之间的关联关系，并将那些存在关联关系的表记录存放在同一分片上，那么就能较好的避免跨分片join 问题，在1:1 或1:n 的情况下，通常按照主表的 ID 主键切分，如下图所示：  这样一来，$Data \\space Node1$ 上面的 order 订单表与orderdetail 订单详情表就可以通过 orderId 进行局部的关联查询了，$Data \\space Node2$ 上也一样。  5.3 跨节点分页、排序、函数问题 #   跨节点多库进行查询时，会出现 limit 分页、order by 排序等问题。 分页需要按照指定字段进行排序：  当排序字段就是分片字段时，通过分片规则就比较容易定位到指定的分片。 当排序字段为非分片字段时，就变得比较复杂了，需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户，如下图所示：  上图中只是取第一页的数据，对性能影响还不是很大，但是如果取的页数很大，情况则变得复杂很多，因为各分片节点中的数据可能是随机的，为了排序的准确性，需要将所有节点的前 $N$ 页数据都排序做好合并，最后再进行整体的排序。 这样的操作很耗费 CPU 和内存资源的，所以页数越大，系统的性能也会越差。     在使用max、min、sum、count之类的函数进行计算的时候，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终将结果返回，如下图所示：   5.4 全局主键避重问题 #  在分库分表环境中，由于表中数据同时存在不同数据库中，主键值平时使用的自增长将无用武之地，某个分区数据库自生成的 ID 无法保证全局唯一，因此需要单独设置全局主键，以避免跨库主键重复问题，常见的主键生成策略如下：\n5.4.1 UUID #   UUID 标准形式包含32 个 16 进制数字，分为 5 段，形式为 8-4-4-4-12 的 36 个字符，例如550e8400-e29b-41d4-a716-446655440000。 优点：  UUID 是主键生成的最简单的方案，本地生成、性能高、没有网络耗时。   缺点：  由于UUID 非常长，会占用大量的内存空间。 作为主键建立索引和基于索引进行查询时都会存在性能问题，在 InnoDB 下，UUID 的无序性会引起数据位置频繁变动，导致分页。    5.4.2 结合数据库维护主键 ID 表 #    在数据库中建立 sequence 表：\nCREATE TABLE sequence ( id bigint(20) unsigned NOT NULL auto_increment, stub char(1) NOT NULL default \u0026#39;\u0026#39;, PRIMARY KEY (id), UNIQUE KEY stub (stub) )ENGINE=MyISAM;   studb 字段设置为唯一索引，stub 值在 sequence 表中只有一条记录，可以同时为多张表生成全局 ID，sequence 表的内容，如下所示：\n+--------------------+-------+ | id | stub | +--------------------+-------+ | 72157623227190423 | a | +--------------------+-------+   使用 MyISAM 存储引擎而不是 InnoDB，以获取更高的性能，因为 MyISAM 使用的是表级别的锁，对表的读写是串行的，所以不用担心在并发时两次读取同一个 ID 值。\n  优点：\n 方案简单。    缺点：\n 存在单点问题，强依赖数据库，当数据库异常时，整个系统都不可用。 配置主从可以增加可用性，但当主库挂了，主从切换时，数据一致性在特殊情况下难以保证。 性能瓶颈限制在单台 MySQL 的读写性能。    5.4.3 Flickr 团队使用的主键生成策略 #   Flickr 团队使用的一种主键生成策略，与上面的sequence 表方案类似，但更好的解决了单点和性能瓶颈问题。 这一方案的整体思想是建立 2 个以上的全局 ID 生成的服务器，每个服务器上只部署一个数据库，每个库有一张 sequence 表用于记录当前全局 ID，表中 ID 增长的步长是库的数量，起始值依次错开，这样能将 ID 的生成散列到各个数据库上，如下图所示：  由两个数据库服务器生成 ID，设置不同的auto_increment 值，第一台的sequence 的起始值为 1，每次步长增长 2，另一台的sequence 起始值为 2，每次步长增长也是 2，结果第一台生成的 ID 都是奇数 $(1,3,5,7,\u0026hellip;)$，第二台生成的 ID 都是偶数 $(2,4,6,8,\u0026hellip;)$。 优点：  这种方案将生成 ID 的压力均匀分布在两台机器上。 同时提供了系统容错，第一台出现了错误，可以自动切换到第二台机器上获取 ID。   缺点：  系统添加机器水平扩展时较为复杂。 每次获取 ID 都要读写一次数据库，数据库的压力还是很大，只能靠堆机器来提升性能。    5.4.4 优化后的 Flickr 方案 #   可以基于 Flickr 的方案继续优化，使用批量的方式降低数据库的写压力，每次获取一段区间的 ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力，如下图所示：  还是使用两台数据库服务器保证可用性，数据库中只存储当前的最大 ID，ID 生成服务器每次批量拉取 6 个 ID，先将max_id 修改为 5，当应用访问 ID 生成服务时，就不需要访问数据库，而是从号段缓存中依次派发 0 ~ 5 的 ID，当这些ID 发完后，再将 max_id 修改为 11，下次就能派发 6 ~ 11的ID，于是数据库的压力降低为原来的$\\frac16$。  5.4.5 Snowflake分布式自增ID算法 #   Twitter的Snowflake算法解决了分布式系统生成全局ID的需求，生成64位的Long型数字，组成部分如下图所示：  第一位未使用。 接下来是41位的毫秒级时间，41位的长度可以表示69年的时间。 5位Data Center ID，5位Worker ID，10位的长度最多支持部署1024个节点。 最后12位是毫秒内的计数，12位的计数顺序号支持每个节点每毫秒产生4096个ID序列。   优点：  毫秒数在高位，生成的ID整体上按时间趋势递增。 不依赖第三方系统，稳定性和效率较高，理论上QPS约为$ 409.6w/s \\space (1000 * 2^{12})$。 整个分布式系统内不会产生ID碰撞。 可根据自身业务灵活分配 bit位。   缺点：  强依赖机器时钟，如果时钟回拨，则可能导致生成ID重复。    5.5 数据迁移、扩容问题 #   当业务高速发展，面临性能和存储的瓶颈时，才会考虑分片设计，此时就不可避免的需要考虑历史数据迁移的问题，一般做法是先读出历史数据，然后按指定的分片规则再将数据写入到各个分片节点中。 此外，还需要根据当前的数据量和QPS，以及业务发展的速度，进行容量规划，推算出大概需要多少分片，一般建议单个分片上的单表数据量不超过1000W。 如果采用数值范围分片，只需要添加节点就可以进行扩容了，不需要对分片数据进行迁移，如果采用的是数值取模分片，则考虑后期扩容的问题就相对比较麻烦。  参考文献 #    干货丨数据库分库分表基础和实践。  数据库的垂直切分与水平切分。  MySQL 分库分表方案，总结的非常好！  数据库分库分表如何避免“过度设计”和“过早优化”。  mycat 读写分离 + 分库分表 + 全局表。  "},{"id":57,"href":"/school-recruitment/docs/database/2MySQL/2.9-%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/","title":"2.9 一致性哈希算法","section":"2、 My SQL","content":"1 传统哈希算法 #  1.1 含义 #   简单地说，哈希就是一个键值对存储，在给定键的情况下，可以非常高效地找到所关联的值，假如我们要根据邮政编码查找城市中的街道名称，一种最简单的实现方式是将此信息以哈希字典的形式进行存储\u0026lt;ZipCode, StreetName\u0026gt;。 当数据太大而无法存储在一个节点或机器上时，系统就会需要多个这样的节点或机器来存储他，比如，使用多个 Web 缓存中间件的系统，对于如何确定哪个key 存储在哪个节点上，最简单的解决方案是使用哈希取模来决定：   给定一个 key，先对 key 进行哈希运算，将其除以系统中的节点数，然后将 key 放入该节点。\n  在获取 key 时，先对 key 进行哈希运算，将其除以系统中的节点数，然后转到该节点并获取值。\n  上述过程对应的哈希算法定义如下：\n# 下面的 N 为节点数 node_number = hash(key) % N   下图描绘了多节点系统中的传统的哈希取模算法，基于该算法可以实现简单的负载均衡。\n     1.2 局限性 #  假设初始时有如下对应关系：\n 1.2.1 节点减少的场景 #   在分布式多节点系统中，出现故障很常见，任何节点都可能在没有任何事先通知的情况下挂掉，针对这种情况，我们希望系统只是出现性能降低，正常的功能不会受到影响。 对于原始示例，假设其中 1 个节点出现了故障，这时节点数发生了变化，节点个数从 3 减少为 2，此时表格中的状态发生了变化：  很明显节点的减少会导致键与节点的映射关系发生变化，这个变化对于新的键来说并不会产生任何影响，但对于已有的键来说，将会导致节点映射错误，以semlinker 为例，变化前系统有 3 个节点，该键对应的节点编号为 1，当出现故障时，节点数减少为 2 个，此时该键对应的节点编号为 0。  1.2.2 节点增加的场景 #   在分布式多节点系统中，对于某些场景比如节日大促，就需要对服务节点进行扩容，以应对突发的流量。 对于原始示例，假设进行扩容临时增加了 1 个节点，这时节点数发生了变化，节点个数从 3 增加到 4 个，此时表格的状态发生了变化：  很明显节点的增加也会导致键与节点的映射关系发生变化，这个变化对于新的键来说并不会产生任何影响，但对于已有的键来说，将导致节点映射错误，同样以semlinker 为例，变化前系统有 3 个节点，该键对应的节点编号为 1，当增加节点时，节点数增加为 4 个，此时该键对应的节点编号为 2。  1.3 影响 #   当集群中节点的数量发生变化时，之前的映射规则就可能发生变化，如果集群中每个机器提供的服务没有差别，这不会有什么影响，但对于分布式缓存这种的系统而言，映射规则失效就意味着之前缓存的失效，若同一时刻出现大量的缓存失效，则可能会出现缓存雪崩，这将会造成灾难性的后果。 要解决此问题，我们必须在其余节点上重新分配现在所有键，这可能是非常昂贵的操作，并且可能对正在运行的系统产生不利的影响。 当然除了重新分配所有现有键的方案之外，还有另一种更好的方案，即使用一致性哈希算法。  2 一致性哈希算法 #  2.1 含义 #   一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的哈希算法，在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。 一致性哈希算法解决了简单哈希算法在分布式哈希表（Distributed Hash Table, DHT）中存在的动态伸缩等问题。  2.2 原理 #    一致性哈希算法通过一个叫做一致性哈希环的数据结构实现，这个环的起点是 0，终点是 $ 2^{32}-1 $，并且起点与终点连接，故这个环的整数分布范围是 $[0, 2^{32}-1]$，如下图所示：\n   假设我们有 semlinker、kakuqo、lolo、fer 四个对象，分别简写为 $o_1$、$o_2$、$o_3$、$o_4$，然后使用哈希函数计算这个对象的哈希值，值的范围是 $[0,2^{32}-1]$。\nhash(o1) = k1; hash(o2) = k2; hash(o3) = k3; hash(o4) = k4;    接着使用同样的哈希函数，我们将服务器也放置到哈希环上，可以选择服务器的 IP 或主机名作为键进行哈希，这样每台服务器就能确定其在哈希环上的位置，假设我们有 3 台缓存服务器，分别为 $cs_1$、$cs_2$、$cs_3$：\n# Cache Server hash(cs1) = t1; hash(cs2) = t2; hash(cs3) = t3;    将对象和服务器都放置到同一个哈希环后，在哈希环上顺时针查找距离这个对象的哈希值最近的机器，即是这个对象所属的机器，以 $o_2$ 为例，顺时针找到最近的机器是 $cs_2$，故服务器 $cs_2$ 会缓存 $o_2$ 对象，而服务器 $cs_1$ 会缓存 $o_1$、$o_3$ 对象，服务器 $cs_3$ 则缓存 $cs_4$ 对象。\n   假设由于业务需要，我们需要增加一台服务器 $cs_4$，经过同样的哈希运算，该服务器最终落于 $t_1$ 和 $t_2$ 服务器之间，具体如下图所示：\n   对于上述的情况，只有 $t_1$ 和 $t_2$ 服务器之间的对象需要重新分配，在以上示例中只有 $o_3$ 对象需要重新分配，即他被重新分配到 $cs_4$ 服务器，在前面我们分析过，如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为只有少部分对象需要重新分配。\n  假如 $cs_3$ 服务器出现故障导致服务下线，这时原本存储于 $cs_3$ 服务器的对象 $o_4$，需要重新分配至 $cs_2$ 服务器，其他对象仍存储在原有的机器上。\n   2.3 优缺点 #  2.3.1 优点 #   一致性哈希算法是对普通哈希算法的改进，有效的解决了稳定性的问题。 当服务器节点加入或退出时，只影响该节点在哈希环上顺时针相邻的后继节点：  当加入一个服务器节点时，原来分配到后继节点的一部分请求，重新分配给新加入的服务器节点。 当退出一个服务器节点时，原来分配到该节点的请求，全部重新分配到后继节点。    2.3.2 缺点 #   一致性哈希算法解决了稳定性问题，但是又产生了负载不均衡问题，或者热点问题，当某个服务器节点或者几个服务器节点存在热点资源，这几个服务器节点就会处理大量的用户请求，其他服务器只处理很少的用户请求，这就产生了负载不均衡问题。 另外，当某个服务器节点崩溃退出，就会使该节点的后继节点负载增大，如果后继节点承受不住崩溃，就会传递给后继节点的后继节点，产生雪崩效应。  2.4 解决方案 #  针对已执行哈希算法存在的问题，有以下两种解决方案：\n 带有限负载的一致性哈希。 带虚拟节点的一致性哈希。  2.4.1 带有限负载的一致性哈希 #   根据当前负载情况对每个服务器节点设置一个最大请求负载值，在一致性哈希环中进行查找时将跳过达到最大负载限制的服务器节点，以此类推，这样就把过载的请求转移到其他服务器节点上来解决热点和不均衡问题。 如下图所示，服务器节点Server3 正在处理两个用户请求，已经达到了最大负载限制数，所以第四个用户请求到来时，直接跳过Server3，分配Server4 来处理请求。   2.4.2 带有虚拟节点的一致性哈希 #   对每个物理服务器节点计算多个哈希值，每个计算结果位置都放置在对应节点上，这些节点称为虚拟节点，然后再将这些虚拟节点映射到哈希环上，由于这些虚拟节点分散到哈希环上，因此很大程度上解决了负载不均衡的问题。  在查找时，如果要确定对象的服务器，需要先确定对象的虚拟服务器，再由虚拟服务器确定物理服务器。   2.5 算法实现 #  /** * @author peng.wei * @version 1.0 * @date 2021/8/29 15:36 * @Description 一致性哈希算法 */ public class ConsistentHash { /** * 虚拟节点的数量 */ private static final Integer V_NODE_SIZE = 10; /** * 虚拟节点的前缀 */ private static final String V_NODE_SUFFIX = \u0026#34;V_NODE_SUFFIX\u0026#34;; /** * 构造哈希环 * * @param servers 服务器映射信息 * @return 哈希环 */ public TreeMap\u0026lt;Integer, Node\u0026gt; buildConsistentHashRing(Map\u0026lt;String, Node\u0026gt; servers) { TreeMap\u0026lt;Integer, Node\u0026gt; hashRing = new TreeMap\u0026lt;\u0026gt;(); for (Node node : servers.values()) { hashRing.put(getHashCode(node.getNodeAddress()), node); } return hashRing; } /** * 构造带虚拟节点的哈希环 * * @param servers 服务器映射信息 * @return 哈希环 */ public TreeMap\u0026lt;Integer, Node\u0026gt; buildConsistentHashRingWithVirtualNode(Map\u0026lt;String, Node\u0026gt; servers) { TreeMap\u0026lt;Integer, Node\u0026gt; hashRing = new TreeMap\u0026lt;\u0026gt;(); for (Node node : servers.values()) { for (Integer i = 0; i \u0026lt; V_NODE_SIZE; i++) { hashRing.put(getHashCode(String.format(\u0026#34;%s%s%s\u0026#34;, node.getNodeAddress(), V_NODE_SUFFIX, i)), node); } } return hashRing; } /** * 根据请求 key 的哈希值在哈希环上寻找节点 * @param servers 服务器映射信息 * @param keyHashCode 节点哈希值 * @return 目标节点 */ public Node locate(Map\u0026lt;String, Node\u0026gt; servers, int keyHashCode) { TreeMap\u0026lt;Integer, Node\u0026gt; hashRing = buildConsistentHashRingWithVirtualNode(servers); // 向右找到第一个 key  Map.Entry\u0026lt;Integer, Node\u0026gt; locateEntry = hashRing.ceilingEntry(keyHashCode); if (locateEntry == null) { locateEntry = hashRing.firstEntry(); } return locateEntry.getValue(); } /** * 根据服务器节点地址获取哈希值 * @param nodeAddress 服务器节点地址 * @return 服务器节点地址对应的哈希值 */ public Integer getHashCode(String nodeAddress) { return System.identityHashCode(nodeAddress); } } 参考文献 #    图解一致性哈希算法。  分布式高可用-负载均衡-3 一致性哈希(有限负载、虚拟节点)。  5 分钟理解一致性哈希算法。  一致性哈希 （Consistent Hashing）的前世今生。  "},{"id":58,"href":"/school-recruitment/docs/database/2MySQL/2.10-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","title":"2.10 主从复制","section":"2、 My SQL","content":"主从复制 #  1 什么是主从复制 #   MySQL 主从复制是指数据可以从一个 MySQL 数据库服务器主节点复制到一个或多个从节点。 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。  2 为什么要主从复制 #   读写分离：  在开发工作中，有时候会遇见某个 SQL 语句需要锁表，导致暂时不能使用读的服务，这样就会影响现有业务。 使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。   数据实时备份：  当系统中某个节点发生故障时，可以方便的故障切换。   高可用 HA。 架构扩展：  随着系统中业务访问量的增大，如果是单机部署数据库，就会导致I/O 访问频率过高。 有了主从复制，增加多个数据存储节点，将负载分布在多个从节点上，降低单机磁盘 I/O 访问的频率，提高单个机器的 I/O 性能。    3 常见架构模型 #  3.1 一主一从/一主多从 #   一主一从和一主多从是最常见的主从架构模式，一般实现主从配置或者读写分离都可以采用这种架构。 如果是一主多从模式，当b增加到一定数量时，Slave 对 Master 的负载以及网络带宽都会成为一个严重的问题。   3.2 多主一从 #   MySQL 5.7 开始支持多主一从的模式，将多个库的数据备份到一个库中存储。   3.3 双主复制 #   理论上跟主从一样，但是两个 MySQL 服务器互做对方的从，主从相互授权连接，读取对方 Binlog 日志并更新到本地数据库，只要对方数据改变，自己就跟着改变。 双主适用于写压力比较大的场景，或者DBA 做维护需要主从切换的场景，通过双主架构避免了重复搭建从库的麻烦。  3.4 级联复制 #   级联模式下因为涉及到的Slave 节点很多，所以如果都连在Master 上对主服务器的压力肯定是不小的，所以部分Slave 节点连接到他上一级的从节点上，这样就缓解了主服务器的压力。 级联复制解决了一主多从场景下多个从库复制对方从库的压力，带来的弊端就是数据同步延迟比较大。   4 主从复制原理 #  主从复制涉及到三个线程：\n 一个在主节点的线程：  Log Dump Thread，主要用来给从库 I/O Thread 传 Binlog 数据。   两个在从库的线程：  一个I/O Thread，主要用于请求主库的 Binlog，并将得到的 Binlog 写到本地的 Relay Log（中继日志）文件中。 一个SQL Thread，主要用于读取 Relay Log 文件中的日志，并解析成 SQL 语句逐一执行。      Log Dump Thread：\n 当从节点连接主节点时，主节点会为其创建一个 Log Dump Thread，用于发送和读取 Binlog 的内容。 在读取 Binlog 的操作中，Log Dump Thread 会对主节点上的 Binlog 加锁，当读取完成发送给从节点之前，锁会被释放。 主节点会为自己的每一个从节点创建一个 Log Dump Thread。    I/O Thread：\n 当从节点上执行start slave 命令之后，从节点会创建一个 I/O Thread 用来连接主节点，请求主库中更新的 Binlog。 I/O Thread 接收到主节点的 Log Dump Thread 进程发来的更新之后，保存在 Relay Log 中。    SQL Thread：\n SQL Thread 负责读取 Relay Log 中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。    Relay Log：\n MySQL 进行主主复制或主从复制的时候会在要复制的服务器下面产生相应的 Relay Log。 从服务器 I/O Thread 将主服务器的 Binlog 日志读取过来，解析到各类 Events 之后记录到从服务器本地文件，这个文件就被称为 Relay Log。 然后SQL Thread会读取 Relay Log 日志的内容并应用到从服务器，从而使从服务器和主服务器的数据保持一致。 Relay Log充当缓冲区，这样Master 就不必等到Slave 执行完成才发送下一个事件。 Relay Log 相关参数：   查询方式：\nmysql\u0026gt; show variables like \u0026#39;%relay%\u0026#39;; +---------------------------+--------------------------------------------+ | Variable_name | Value | +---------------------------+--------------------------------------------+ | max_relay_log_size | 0 | | relay_log | | | relay_log_basename | /www/server/data/ecs-q9gui-relay-bin | | relay_log_index | /www/server/data/ecs-q9gui-relay-bin.index | | relay_log_info_file | relay-log.info | | relay_log_info_repository | FILE | | relay_log_purge | ON | | relay_log_recovery | OFF | | relay_log_space_limit | 0 | | sync_relay_log | 10000 | | sync_relay_log_info | 10000 | +---------------------------+--------------------------------------------+   max_relay_log_size：\n 标记 Relay Log 允许的最大值。 如果该值为 0，则默认值为max_binlog_size(1G)。 如果不为 0，则max_relay_log_size 为最大的 Relay Log 文件大小。    relay_log_purge：\n 是否自动清空不再需要的中继日志，默认为 1（启用）。    relay_log_recovery：\n 当**Slave 从库宕机后**，**假如 Relay Log 损坏了**，**导致一部分中继日志没有处理**，则**自动放弃所有未执行的 Relay Log**，并且**重新从 Master 上获取日志**，这样就**保证了 Relay Log 的完整性**。 默认情况下该功能是关闭的，将relay_log_recovery 的值设置为 1 时，可在Slave 从库上开启该功能，建议开启。    relay_log_space_limit：\n 防止中继日志写满磁盘，这里设置中继日志最大限额。 此设置存在主库崩溃，从库中继日志不全的情况，不到万不得已，不推荐使用。    sync_relay_log：\n 这个参数和 Binlog 中的 sync_binlog 作用相同。 当设置为 1 时，Slave的 I/O Thread 每次接收到 Master 发送过来的 Binlog 日志都要写入系统缓冲区，然后刷入 Relay Log 中继日志里，这样是最安全的，因为在崩溃的时候，我们最多丢失一个事务，但会造成磁盘的大量 I/O。 当设置为 0 时，并不是马上就刷入中继日志里，而是由操作系统决定何时来写入，虽然安全性降低了，但减少了大量的磁盘 I/O 操作。 这个值默认是 0，可动态修改，建议采取默认值。    sync_relay_log_info：\n 这个参数和 Binlog 中的 sync_binlog 作用相同。 当设置为 1 时，Slave的 I/O Thread 每次接收到 Master 发送过来的 Binlog 日志都要写入系统缓冲区，然后刷入 relay-log.info 里，这样是最安全的，因为在崩溃的时候，我们最多丢失一个事务，但会造成磁盘的大量 I/O。 当设置为 0 时，并不是马上就刷入 relay-log.info 里，而是由操作系统决定何时来写入，虽然安全性降低了，但减少了大量的磁盘 I/O 操作。 这个值默认是 0，可动态修改，建议采取默认值。        对于每一个主从连接，都需要这三个进程来完成，当主节点有多个从节点时，主节点会为每一个当前连接的从节点创建一个 Log Dump Thread，而每个从节点都有自己的 I/O Thread、SQL Thread。\n  从节点用两个线程将从主库拉取更新和执行分成独立的任务，这样在执行同步数据任务的时候，不会降低读操作的性能，比如：\n 如果从节点没有运行，此时I/O Thread 可以很快从主节点获取更新，尽管SQL Thread 还没有执行。 如果在SQL Thread 执行之前从节点服务停止，至少I/O Thread 已经从主节点拉取到了最新的变更并且保存在本地 Relay Log 中，当服务再次起来之后就可以完成数据的同步。    5 复制的基本过程 #   要实施复制，首先必须打开 Master 端的 Binlog 功能，否则无法实现，因为整个复制过程实际上就是 Slave 从 Master 端获取该日志，然后再在自己身上完全顺序的执行日志中所记录的各种操作。\n   在从节点执行start slave 命令开启主从复制开关，开始进行主从复制，从节点上的 I/O Thread 连接主节点，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。 主节点接收到来自从节点的 I/O 请求后，通过负责复制的 Log Dump Thread 根据请求信息读取指定日志指定位置之后的日志信息，返回给从节点，返回信息中除了日志所包含的信息之外，还包括本次返回的信息的 Binlog File 以及 Binlog Position（Binlog 下一个数据读取位置）。 从节点的 I/O Thread 接收到主节点发送过来的日志内容、日志文件及位置点后，将接收到的日志内容更新到本机的 Relay Log 文件的最末端，并将读取到的 Binlog 文件名和位置保存到 master-info 文件中，以便在下一次读取的时候能够清楚的告诉 Master“我需要从哪个 Binlog 的哪个位置开始往后的日志内容，请发送给我”。 Slave 的 SQL Thread 检测到 Relay Log 中新增加了内容后，会将 Relay Log 的内容解析成能够执行的 SQL 语句，然后在本数据库中按照解析出来的顺序执行，并在 relay-log.info 中记录当前应用中继日志的文件名和位置点。  6 复制的模式 #   MySQL 主从复制默认是异步的模式。\n 6.1 异步模式（Async Mode） #    这种模式下，主节点不会主动推送数据到从节点，主库在执行完客户端提交的事务后会立即将结果返回给客户端，并不关心从库是否已经接收并处理。 这样就会有一个问题，主节点如果崩溃掉了，此时主节点上已经提交的事务可能并没有传到从节点上，如果此时强行将从提升为主，可能导致新主节点上的数据不完整。  6.2 半同步模式（Semi Sync Mode） #    介于异步复制和全同步复制之间，主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个从库接收到并写到 Relay Log 中才返回成功信息给客户端（只能保证主库的 Binlog 至少传输到了一个从节点上），否则需要等待直到超时时间然后切换成异步模式再提交。 相对于异步复制，半同步复制提高了数据的安全性，一定程度的保证了数据能成功备份到从库。 同时他也造成了一定程度的延迟，但是比全同步模式延迟要低，这个延迟至少是一个 TCP/IP 往返的时间，所以，半同步复制最好在低延时的网络中使用。 半同步模式不是 MySQL 内置的，从 MySQL 5.5 开始集成，需要 Master 和 Slave 安装插件开启半同步模式。  6.3 全同步模式 #   指当主库执行完一个事务，然后所有的从库都复制了该事务并成功执行完才返回成功信息给客户端。 因为需要等待所有从库执行完该事务才能返回成功信息，所以全同步复制的性能必然会受到严重的影响。  7 主从复制可能会出现的问题 #  7.1 Slave 同步延迟 #   因为 Slave 端是通过 I/O Thread 单线程来实现数据解析入库，而Master 端写 Binlog 是顺序写效率很高，因此当主库的 TPS 很高的时候，必然Master 端的写效率要高过 Slave 端的读效率，这时候就有同步延迟的问题。 I/O Thread 的同步是基于库的，即同步几个库就会开启几个I/O Thread，可以通过show slave status 命令查看Seconds_Behind_Master 的值来看是否出现同步延迟，这个值代表主从同步延迟的时间，值为0 表示正常情况，正值表示已经出现延迟，值越大说明延迟越严重，从库落后主库也就越多。  参考文献 #    什么是主从复制？实现原理是什么？  深度探索 MySQL 主从复制原理。  MySQL 主从复制原理不再难。  深入了解 MySQL 主从复制的原理。  "},{"id":59,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.1-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/","title":"1.1.1 斐波那契数列","section":"1.1 动态规划","content":"斐波那契数列 #  1 题目 #  写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项（即 F(N)）。斐波那契数列的定义如下：\nF(0) = 0, F(1) = 1 F(N) = F(N - 1) + F(N - 2), 其中 N \u0026gt; 1. 斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。\n答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。\n示例 1：\n输入：n = 2 输出：1 示例 2：\n输入：n = 5 输出：5 提示：\n 0 \u0026lt;= n \u0026lt;= 100     遇到求总数的问题时一般考虑用动态规划来求。\n  这类问题的基本思路就是先寻找状态之间的关系，确定状态转移方程，然后使用暴力递归的方法求解，接着使用带有备忘录的递归、dp 数组的迭代解法进行优化。\n  类似的题目还有：\n  62. 不同路径。  64. 最小路径和。     2 解题思路 #  2.1 暴力递归 #   代码  int fib(int N) { if (N == 1) || (N == 2) return 1; return fib(N - 1) + fib(N - 2); }  递归树   2.2 带备忘录的递归解法 #   代码  int fib(int N) { if (N \u0026lt; 1) return 0; // 备忘录全初始化为 0  vector\u0026lt;int\u0026gt; memo(N + 1, 0); // 初始化最简情况  return helper(memo, N); } int helper(vector\u0026lt;int\u0026gt;\u0026amp; memo, int n) { // base case  if (n == 1) || (n == 2) return 1; // 已经计算过  if (memo[n] != 0) return memo[n]; memo[n] = memo[n - 1] + memo[n - 2]; return memo[n] }  递归树    此时本算法不存在冗余计算，子问题就是 f(1)、f(2)\u0026hellip;f(20)，所以子问题个数为 o(n)，解决一个子问题的时间为 o(1)，因此本算法的时间复杂度为 o(n)。\n2.3 dp 数组的迭代解法 #  int fib(int N) { vector\u0026lt;int\u0026gt; dp(N + 1, 0); // base case  dp[1] = dp[2] == 1; for (int i = 1; i \u0026lt;= N; i++) dp[i] = dp[i - 1] + dp[i - 2]; return dp[N]; }  2.4 细节优化 #  斐波那契数列的状态转移方程如下：\n 根据斐波那契数列的状态转移方程可知，当前状态只和之前的两个状态有关，其实并不需要那么长的一个 DP table 来存储所有的状态，只要想办法存储之前的两个状态就行。所以，可以进一步优化，把空间复杂度降为 o(1)。\nint fib(int n) { if (n == 1 || n == 2) return 1; int prev = 1, curr = 1; for (int i = 3; i \u0026lt;= N; i++) { int sum = prev + next; prev = curr; curr = sum; } return curr; } 3 参考文献 #    剑指 Offer 10- I. 斐波那契数列。  62. 不同路径。  64. 最小路径和。  动态规划解题核心框架。  "},{"id":60,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.1-%E4%BA%8B%E5%8A%A1%E7%9A%84%E6%A6%82%E5%BF%B5%E5%92%8C%E7%89%B9%E6%80%A7/","title":"1.1 事务的概念和特性","section":"1、数据库基础","content":"事务的概念和特性 #  1 事务的概念 #   数据库中的事务是一个操作序列，包含了一组数据库操作命令。 事务把这一组命令作为一个整体一起向系统提交或撤销操作请求，即这一组命令要么都执行，要么都不执行，因此事务是一个不可分割的工作逻辑单元。  2 事务的特性 #   注：如果没有特别说明，下面的事物的特性的原理指的都是MySQL的InnoDB引擎。\n 事务具有 4 个特性，即原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability），这 4 个特性通常简称为ACID。\n2.1 原子性 #  2.1.1 含义 #   事务开始后的所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。 事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。  2.1.2 实现原理 #   InnoDB 引擎使用Undo Log（回滚日志）来保证原子性操作，我们对数据库中的每一条数据的改动（Insert、Delete、Update）都会被记录到 Undo Log 中，比如以下这些操作：  插入一条记录时，至少要把这条记录的主键记下来，之后回滚的时候只需把这个主键对应的记录删掉就好了。 删除一条记录时，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。 修改一条记录时，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。   当事务执行失败或者调用了 rollback 方法时，就会触发回滚事件，利用 Undo Log 中的记录将数据回滚到修改之前的样子。  2.2 一致性 #  2.2.1 含义 #   事务开始前和结束后，数据库的完整性约束没有被破坏。 比如A 向 B 转账，不可能 A 扣了钱，B 却没收到。  2.2.2 如何保证一致性 #  这个主要包括两个方面，一个是数据库层面，一个是应用层面。\n 数据库层面：  数据库通过原子性、隔离性、持久性来保证一致性。 也就是说四大特性中，一致性是目的，其他三种特性是手段。 数据库必须要实现其它三大特性，才能实现一致性，例如，原子性无法保证，显然一致性也无法保证。   应用层面：  通过代码判断数据库数据是否有效，然后决定回滚还是提交数据。 如果我们故意在事务里面写出违反约束的代码，一致性还是无法保证的，例如，我们在转账中，故意不给 B 账户加钱，那一致性还是无法保证，因此，还必须从应用层考虑。    2.3 隔离性 #  2.3.1 含义 #   不同的事务并发操作相同的数据的时候，每个事务都有各自完整的数据空间，即一个事务内部的操作及使用的数据对其他并发事务是隔离的，并发执行各个事务之间不能相互干扰。  2.3.2 为什么要进行事务隔离 #  因为数据库中有多个事务并发执行的时候，可能会导致脏读（Dirty Read）、不可重复读（Non-repeatable Read）、幻读（Phantom Read）。\n2.3.2.1 脏读 #  2.3.2.1.1 含义 #   脏读是指事务 B 读取事务 A 还未提交的数据，然后事务 A 进行了回滚，导致事务 B 再次读取数据时和第一次读到的数据不一致了。  2.3.2.1.2 示例 #    事务 A 先写数据，把一行数据的值从null 改成了test，同时事务 A 还没有提交。 然后事务 B 过来读取该数据，此时如果事务之间没有有效隔离，那么事务 B 读到的数据就为test。 接着事务 A回滚了，回滚之后该数据对应的值从test 变为null。 然后事务 B 再去读取该数据时读到的就是null 了，和第一次读取到的数据不一致，这就是脏读。  2.3.2.2 不可重复读 #  2.3.2.2.1 含义 #   事务 A多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A多次读取同一数据时结果不一致。 不可重复读和脏读的区别在于脏读是由于别的事务回滚导致，而不可重复读读到的其实是已经提交的数据。  2.3.2.2.2 示例 #    事务 A 先去读取一行数据，读到的值是null。 事务 B 去修改数据，改成了test，这里和前面不一样的地方就在于事务 B 还提交了，不回滚了。 事务 A 第二次去读，读到的是test，和第一次读到的null 不一样。  2.3.2.3 幻读 #  2.3.2.3.1 含义 #   事务 B 在事务 A多次读取同一数据的过程中，添加了部分数据，导致事务 A多次读取到的数据不一致，仿佛出现了幻觉一样。  2.3.2.3.2 示例 #    事务 A 里有一个条件查询的语句select name from t where id \u0026gt; 10，他进行了一次范围查询，查到了 10 行数据。 然后事务 B 往数据库里面插入了一条数据。 事务 A 再用刚才的查询条件查询时，发现查到了 15 条数据，其中 5 条是之前没见过的，这时事务 A 以为自己出现了幻觉，这就是幻读。  2.3.3 事务的隔离级别有哪些 #   为了解决上面的那些问题，提出了隔离级别的概念。\n MySQL 中主要包括四种隔离级别，分别是读未提交（Read Uncommitted）、读提交（Read Committed）、可重复读（Repetable Read）、串行化（Serializable）。\n2.3.3.1 读未提交 #  2.3.3.1.1 含义 #   读未提交其实就是可以读到其它事务未提交的数据，但没有办法保证我们读到的数据最终一定是提交后的数据，如果中间发生回滚，那就会出现脏数据问题。 读未提交没办法解决脏数据问题，更不能解决可重复读和幻读的问题。 MySQL 事务隔离其实是依靠锁来实现的，加锁自然会带来性能的损失，而读未提交隔离级别是不加锁的，所以他的性能是最好的，没有加锁、解锁带来的性能开销。  2.3.3.1.2 示例 #    设置全局隔离级别为读未提交。\nset global transaction isolation level read uncommitted;   启动两个事务，分别是事务 A 和事务 B，在事务 A 中使用 update 语句，修改 age 的值为 10，初始时是 1。\n  在执行完 update 语句之后，在事务 B 中查询 user 表，会看到 age 的值已经是 10 了，这时候事务 A 还没有提交，而此时事务 B 有可能拿着已经修改过的 age=10 去进行其他操作了。\n  在事务 B 进行其他操作的过程中，很有可能事务 A 由于某些原因，进行了事务回滚操作，那其实 B 得到的数据就是脏数据了，拿着脏数据去进行其它计算，那结果肯定也是有问题的。\n   2.3.3.2 读提交 #  2.3.3.2.1 含义 #   读提交就是一个事务只能读到其它事务已经提交过的数据，因此可以解决脏读的问题，但是不能解决可重复读和幻读的问题。 读提交是大多数流行数据库的默认事务隔离级别，比如Oracle，但不是 MySQL 的默认隔离级别。  2.3.3.2.2 示例 #    把事务隔离级别改为读提交。\nset global transaction isolation level read committed;   同时开启事务 A 和事务 B 两个事务，在事务 A 中使用 update 语句将 id=1 的记录行 age 字段更改为 10。\n  此时，在事务 B 中使用 select 语句进行查询，我们发现在事务 A 提交之前，事务 B 中查询到的记录 age 一直为 1，直到事务 A 提交，此时在事务 B 中使用 select 查询时，发现 age 的值已经是 10 了。\n  这就出现了一个问题，在同一事务中（本例中的事务 B），事务的不同时刻，同样的查询条件，查询出来的记录内容是不一样的，事务 A 的提交影响了事务 B 的查询结果，这就是不可重复读，因此，虽然读提交隔离级别解决了脏读问题，但是不能解决不可重复读和幻读问题。\n   2.3.3.3 可重复读 #  2.3.3.3.1 含义 #   可重复读是指事务不会读到其它事务对已有数据的修改，即使其它事务已经提交。 也就是说，事务开始读到的已有数据是什么，在本事务提交前的任意时刻，这些数据的值都是一样的。 但是，对于其它事务新插入的数据是可以读到的，这就引发了幻读的问题。 MySQL 的默认隔离级别就是可重复读。   注：MySQL 的可重复读隔离级别其实解决了幻读问题。\n 2.3.3.3.2 示例 #   修改全局隔离级别为可重复读。 同时开启两个事务：  测试解决不可重复读问题：  事务 A 先修改了数据，并且在事务 B 之前提交，事务 B 在事务开始后和事务 A 提交之后读取的数据相同，说明可重复读隔离级别解决了不可重复读问题。   测试产生幻读问题：  事务 A 开始后，执行update 操作，将age=1 的记录的name 改为“风筝 2 号”。 事务 B 开始后，在事务执行完update 后，执行insert 操作，插入记录age=1, name=古时的风筝 ，这和事务 A 修改的那条记录值相同，然后提交。 事务 B 提交后，事务 A 执行select，查询page=1 的数据，这时会发现多了一行，并且发现还有一条age=1, name=古时的风筝  的记录，这就是事务 B 刚刚插入的，这就是幻读，说明可重复读隔离级别解决不了幻读的问题。        2.3.3.4 串行化 #   串行化是 4 种隔离级别中隔离效果最好的，解决了脏读、不可重复读、幻读的问题。 但是串行化是这 4 种隔离级别中性能最差的，因为与其它 3 中隔离级别相比，它就相当于单线程，后一个事务的执行必须等待前一个事务结束。  2.4 持久性 #  2.4.1 含义 #   事务一旦提交，它对数据库的改变就应该是永久性的，接下来的操作或故障不应该对其有任何影响。  2.4.2 实现原理 #   InnoDB 引擎使用Redo Log（归档日志）来实现持久性。 当有一条记录需要更新的时候，InnoDB 引擎会先把记录写到 Redo Log 日志里面，并更新内存，这个时候更新就算完成了。 当数据库宕机重启的时候，会将 Redo Log 中的内容恢复到数据库中，再根据 Undo Log 和 Binlog 内容决定回滚数据还是提交数据。 相比直接刷磁盘，Redo Log 有以下两个优势：  Redo Log 体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。 Redo Log 是一直往末尾进行追加，属于顺序 IO，效率比随机 IO 要高。    3 参考文献 #    事务的概念和特性？  数据库事务的概念和特性。  你可能知道事务的四大特性，但是你不一定知道事务的实现原理。  小胖问我：MySQL 事务与 MVCC 原理？  【原创】Mysql 中事务 ACID 实现原理。  MySQL 事务隔离性与隔离级别原理。  图解脏写、脏读、不可重复读、幻读。  数据库常用的事务隔离级别都有哪些？都是什么原理？  MySQL 事务隔离级别和实现原理（看这一篇文章就够了！）。  "},{"id":61,"href":"/school-recruitment/docs/database/3Redis/3.1-%E6%A6%82%E8%BF%B0/3.1.1-Redis%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/","title":"3.1.1 Redis为什么这么快","section":"3.1 概述","content":"Redis为什么这么快 #    完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。\n  数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的。\n Redis 的具体数据类型及数据结构可参考 3.1.2 Redis 数据类型。\n   采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗。\n Redis 为什么采用单线程，单线程有什么优点？\n 因为Redis 的操作都是基于内存的，绝大部分请求是纯粹的内存操作，非常迅速。 而CPU 的速度是远大于内存的速度的，因此CPU 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存或者网络带宽。 而使用单线程可以省去多线程时 CPU 上下文切换的时间，也不用去考虑各种锁的问题，不存在加锁释放锁的操作，没有死锁问题导致的性能消耗。 对于内存系统来说，多次读写都是在一个 CPU 上，没有上下文切换效率就是最高的。 既然单线程容易实现，而且CPU 不会成为瓶颈，那么就顺理成章的采用单线程的方案了。     使用多路 I/O 复用模型，非阻塞 IO。\n  使用底层模型不同，他们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制，因为一般的系统调用函数的话，会浪费一定的时间去移动和请求。\n  参考文献 #    几率大的 Redis 面试题（含答案）。  Redis 面试题（总结最全面的面试题）。  面试官：Redis 为什么这么快？除了基于内存操作还有其他原因吗？  Redis 是单线程的，但 Redis 为什么这么快？  Redis 为什么是单线程？为什么有如此高的性能？  "},{"id":62,"href":"/school-recruitment/docs/database/3Redis/3.3-%E5%88%86%E5%B8%83%E5%BC%8F%E9%97%AE%E9%A2%98/3.3.1-Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"3.3.1 Redis实现分布式锁","section":"3.3 分布式问题","content":"Redis实现分布式锁 #  1 背景 #   我们日常在电商网站购物时经常会遇到一些高并发的场景，例如电商 APP 上经常出现的秒杀活动、限量优惠券抢购，还有去哪儿网的火车抢票系统等，这些场景有一个共同特点就是访问量激增，虽然在系统设计时会通过限流、异步、排队等方式优化，但整体的并发还是平时的数倍以上，为了避免并发问题，防止库存超卖，给用户提供一个良好的购物体验，这些系统中都会用到锁的机制。 对于单进程的并发场景，可以使用编程语言及相应的类库提供的锁，如 Java 中的Synchronized 语法以及ReentrantLock 类等，避免并发问题。 如果在分布式场景中，实现不同客户端的线程对代码和资源的同步访问，保证在多线程下处理共享数据的安全性，就需要用到分布式锁技术。   2 含义 #   分布式锁是控制分布式系统或不同系统之间共同访问共享资源的一种锁实现，如果不同的系统或同一个系统的不同主机之间共享了某个资源时，往往需要互斥来防止彼此干扰保证一致性。 一个相对安全的分布式锁，一般需要具备以下特征：  互斥性：互斥是锁的基本特征，同一时刻锁只能被一个线程持有，执行临界区操作。 超时释放：通过超时释放，可以避免死锁，防止不必要的线程等待和资源浪费。 可重入性：一个线程在持有锁的情况可以对其再次请求加锁，防止锁在线程执行完临界区操作之前释放。 高性能和高可用：加锁和释放锁的过程性能开销要尽可能的低，同时也要保证高可用，防止分布式锁意外失效。    3 实现方式 #   使用 Redis 作为分布式锁，本质上要实现的目标就是一个进程在 Redis 里面占据了一个仅有的“茅坑”，当别的进程也想来“占坑”时，发现已经有人蹲在那里了，就只好放弃或者等待稍后再试。 目前基于 Redis 实现分布式锁主要有两大类，一类是基于单机，另一类是基于多机，不管是哪种实现方式，均需实现加锁、解锁、锁超时这三个分布式锁的核心要素。  3.1 基于 Redis 单机实现的分布式 #  3.1.1 使用 SETNX 指令 #    最简单的加锁方式就是直接使用 Redis 的 SETNX 指令，该指令只有在 key 不存在的情况下，将 key 的值设置为 true，若 key 已经存在，则 SETNX 命令不做任何动作。\n  key 是锁的唯一标识，可以按照业务需要锁定的资源来命名：\n 比如，在某商城的秒杀活动中对某一商品加锁，那么key 可以设置为lock_resource_id，value 可以设置为任意值，在资源使用完成后，使用 DEL 删除该key 对锁进行释放，整个过程如下：  很显然，这种获取锁的方式很简单，但也存在一个问题，就是我们上面提到的分布式锁三个核心要素之一的锁超时问题，即如果获得锁的进程在业务逻辑处理过程中出现了异常，可能会导致 DEL 指令一直无法执行，导致锁无法释放，该资源将会永远被锁住。  所以，在使用 SETNX 拿到锁以后，必须给 key 设置一个过期时间，以保证即使没有被显式释放，在获取锁达到一定时间后也要自动释放，防止资源被长时间独占，由于 SETNX 不支持设置过期时间，所以需要额外的 EXPIRE 指令，整个过程如下：     这样实现的分布式锁存在一个严重问题，由于SETNX 和 EXPIRE 这两个操作是非原子性的，如果进程在执行 SETNX 和 EXPIRE 之间发生异常，SETNX 执行成功，但 EXPIRE 没有执行，导致这把锁变得长生不老，这种情况可能出现锁超时问题，其它进程无法正常获取锁。   3.1.2 使用 SET 扩展指令 #    为了解决 SETNX 和 EXPIRE 两个操作非原子性的问题，可以使用 Redis 的 SET 指令的扩展参数，使得 SETNX 和 EXPIRE 这两个操作可以原子执行，整个过程如下：   NX：表示只有当 lock_resource_id 对应的 key 值不存在的时候才能 SET 成功，保证了只有第一个请求的客户端才能获得锁，而其他客户端在锁被释放之前都无法获得锁。 EX：表示这个锁 10 秒钟后会自动过期，业务可以根据实际情况设置这个时间大小。     但是这种方式仍然不能彻底解决分布式锁超时问题：\n 锁被提前释放：假如线程 A 在加锁和释放锁之间的逻辑执行的时间很长（或者线程 A 执行过程中被阻塞），以至于超出了锁的过期时间后进行了释放，但线程 A 在临界区的逻辑还没有执行完，那么这时候线程 B 就可以提前重新获取这把锁，导致临界区代码不能严格的串行执行。 锁被误删：假如以上情形中的线程 A 执行完后，他并不知道此时的锁持有者是线程 B，线程 A 会继续执行 DEL 指令来释放锁，如果线程 B 在临界区的逻辑还没有执行完，线程 A 实际上释放了线程 B 的锁。    为了避免以上情况，建议不要在执行时间过长的场景中使用 Redis 分布式锁，同时一个比较安全的做法是在执行 DEL 释放锁之前对锁进行判断，验证当前锁的持有者是否是自己，具体的实现如下：\n  在加锁时将 value 设置为一个唯一的随机数（或者线程 ID），释放锁时先判断随机数是否一致，然后再执行释放操作，确保不会错误地释放其他线程持有的锁，除非是锁过期了被服务器自动释放，整个过程如下：\n   但判断 value 和删除 key 是两个独立的操作，并不是原子性的，所以这个地方需要使用 Lua 脚本进行处理，因为 Lua 脚本可以保证连续多个指令的原子性执行：\n      基于 Redis 单节点的分布式锁基本完成了，但是这并不是一个完美的方案，只是相对完全一点，因为他并没有完全解决当前线程执行超时锁被提前释放后，其他线程趁虚而入的问题。\n  3.1.3 使用 Redisson 的分布式锁 #  3.1.3.1 什么是 Redisson #   Redisson 是架设在 Redis 基础上的一个 Java 驻内存数据网络（In-Memory Data Grid）。 他充分利用了 Redis 键值数据库提供的一系列优势，基于 Java 实用工具包中常用接口，为使用者提供了一系列具有分布式特性的常用工具类，使得原本作为协调单机多线程并发程序的工具包获得了协调分布式多机多线程并发系统的能力，大大降低了设计和研发大规模分布式系统的难度。 同时结合各富特色的分布式服务，更进一步简化了分布式环境中程序相互之间的协作。  3.1.3.2 Redisson 实现 Redis 分布式锁的底层原理 #   3.1.3.2.1 加锁机制 #    假如现在某个客户端要加锁，而且该客户端面对的是一个 Redis Cluster 集群，那么他首先会根据 hash 节点选择一台机器。\n  然后，就会发送一段 Lua 脚本到 Redis 上，这段 Lua 脚本如下所示：   KEYS[1]：表示我们加锁的那个 key，比如说 RLock lock = redisson.getLock(\u0026quot;myLock\u0026quot;);，这里我们自己设置了加锁的那个锁 key 就是 mylock。\n  ARGV[1]：表示锁 key 的默认生存时间，默认是 30 秒。\n  ARGV[2]：表示加锁的客户端 ID，类似于 8743c9c0-0795-4907-87fd-6c719a6b4586:1。\n  为什么要用 Lua 脚本？ 因为一大坨复杂的业务逻辑，可以通过封装在 Lua 脚本发送给 Redis，保证这段复杂业务逻辑执行的原子性。       上面脚本中，第一段 if 判断语句，就是用 exists myLock 命令判断一下，如果我们要加锁的那个 key 不存在的话，我们就进行加锁。\n  加锁的方法为：\nhset myLock 8743c9c0-0795-4907-87fd-6c719a6b4586:1 1 通过这个命令，设置一个 hash 数据结构，这行命令执行后，会出现一个类似于下面的数据结构：\n 上述就代表 8743c9c0-0795-4907-87fd-6c719a6b4586:1 这个客户端对 myLock 这个锁 key 完成了加锁。\n  接着会执行 pexpire myLock 30000，设置 myLock 这个锁 key 的生存时间是 30 秒。\n  到此为止，加锁就完成了。\n  3.1.3.2.2 锁互斥机制 #   在这个时候，如果客户端 2 来尝试加锁，执行了同样一段的 Lua 脚本，在第一个 if 判断里会执行 exists myLock，发现 myLock 这个锁 key 已经存在了。 接着第二个判断里，会判断 myLock 锁 key 的 hash 数据结构中是否包含客户端 2 的 ID，这时候明显不是的，因为那里包含的是客户端 1 的 ID。 所以，客户端 2 会获取到 pttl myLock 返回的一个数字，这个数字代表了 myLock 这个锁 key 的剩余生存时间，比如还剩15000 毫秒的生存时间。 此时客户端 2 会进入一个 while 循环，不停的尝试加锁。  3.1.3.2.3 Watch Dog 自动延期机制 #   只要客户端 1 一旦加锁成功，就会启动一个 Watch Dog 看门狗，他是一个后台线程，会每隔 10 秒检查一下，如果客户端 1 还持有 key，那么就会不断的延长锁 key 的生存时间。  3.1.3.2.4 可重入加锁机制 #   如果客户端 1 已经持有了这把锁，结果可重入的加锁，会通过一下机制来实现：  假如有如下代码：  然后我们来分析一下上面那段 Lua 脚本：  第一个 if 判断肯定不成立，exists myLock会显示锁 key 已经存在了。 第二个 if 判断会成立，因为 myLock 的 hash 数据结构中包含的那个 ID，就是客户端 1 的那个 ID，也就是 8743c9c0-0795-4907-87fd-6c719a6b4586:1，此时就会执行可重入加锁的逻辑：   执行以下命令，对客户端 1 的加锁次数累加 1：\nhincrby myLock 8743c9c0-0795-4907-87fd-6c71a6b4586:1 1   此时 myLock 数据结构变为下面这样：\n   可以看出，myLock的 hash 数据结构中的那个客户端 ID 就对应着加锁的次数。\n        3.1.3.2.5 释放锁机制 #   如果执行 lock.unlock()，就可以释放分布式锁，此时会对 myLock 数据结构中的那个加锁次数减 1。 如果发现加锁次数是 0 了，说明这个客户端已经不再持有锁了，此时就会用 del myLock 命令，从 Redis 里删除这个 key，然后客户端 2 就可以尝试完成加锁了。  3.1.3.3 优缺点 #  3.1.3.3.1 缺点 #   上面那种方案的最大问题是如果我们对某个 Redis Master 实例写入了 myLock 这种锁 key 的 value，此时会异步复制给对应的 Master Slave 实例，但是这个过程中一旦发生 Redis Master 宕机，主备切换，Redis Slave 变成了 Redis Master，接着就会导致客户端 2 来尝试加锁的时候，在新的 Redis Master 完成了加锁，而客户端 1 也以为自己完成了加锁，此时就会导致多个客户端对一个分布式锁完成了加锁，这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以上面就是Redis Cluster 或者 Redis Master-Slave 架构的主从异步复制导致的 Redis 分布式锁的最大缺陷，即在 Redis Master 实例宕机的时候，可能导致多个客户端同时完成加锁。  3.2 基于 Redis 多机实现的分布式锁 RedLock #  3.2.1 背景 #   以上几种基于 Redis 单机实现的分布式锁其实都存在一个问题，就是加锁时只作用在一个 Redis 节点上，即使 Redis 通过 Sentinel 保证了高可用，但由于 Redis 的复制是异步的，Master 节点获取到锁后在未完成数据同步的情况下发生故障转移，此时其他客户端上的线程依然可以获取到锁，因此会丧失锁的安全性。 整个过程如下：  客户端 A 从 Master 节点获取锁。 Master 节点出现故障，主从复制过程中，锁对应的 key 没有同步到 Slave 节点。 Slave 升级为 Master 节点，但此时 Master 中没有锁数据。 客户端 B 请求新的 Master 节点，并获取到了对应同一个资源的锁。 出现多个客户端同时持有同一个资源的锁，不满足锁的互斥性。   正因为如此，在 Redis 的分布式环境中，Redis 的作者 Antirez 提供了 RedLock 的算法实现一个分布式锁。  3.2.2 算法原理 #   在 Redis 的分布式环境中，我们假设有 $N$ 个 Redis Master，这些节点完全相互独立，不存在主从复制或者其他集群协调机制，我们确保将在 $N$ 个实例上使用与 Redis 单实例下相同方法获取和释放锁。 现在我们假设有 5 个 Redis Master 节点，同时我们需要在 5 台服务器上面运行这些实例，这样保证他们不会同时宕掉。 为了获取到锁，客户端应该执行以下动作：   获取当前 Unix 时间，以毫秒为单位。\n  依次尝试从 5 个实例，使用相同的 key 和具有唯一性的 value（例如 UUID）获取锁，当向 Redis 请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个时间应该小于锁的失效时间（例如，我们的锁的自动失效时间为 10 秒，则超时时间应该在 5~50 毫秒之间）这样可以避免服务器端 Redis 已经挂掉的情况下，客户端还在死死地等待响应结果，如果服务器没有在规定时间内响应，客户端应该尽快尝试去另外一个 Redis 实例请求获取锁。\n  客户端使用当前时间减去开始获取锁时间，就可以得到获取锁使用的时间，当且仅当从大多数（$\\frac N 2 + 1$）的Redis节点都取到锁，并且使用的时间小于锁失效的时间时，锁才算获取成功。\n  如果成功获取锁，则锁的真正有效时间是 TTL减去第3步的时间差 的时间，比如，TTL是5s，获取锁用了3s，则锁真正有效时间为3s（其实应该再减去时钟漂移）。\n  什么是TTL？ TTL，全称为Time to Live，即Redis的 key的过期时间或有效生存时间。 什么是始终漂移？ 时钟漂移是指在时间流速基本相同的情况下，两个电脑（或两个进程间）时间的差值，如果电脑距离过远会造成时钟漂移值过大。     如果因为某些原因获取锁失败（没有在至少$\\frac N 2 + 1$个Redis实例取到锁，或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁。\n 为什么在获取锁失败的时候加锁失败的节点也需要执行释放锁的操作？\n 因为可能存在某个节点加锁成功后返回客户端的响应包丢失了，这种情况在异步通信模型中是有可能发生的。  客户端向服务器通信是正常的，但反方向却是有问题的，虽然对于客户端而言，由于响应超时导致加锁失败，但是对于Redis节点而言，SET指令执行成功，意味着加锁成功。   因此，释放锁的时候，客户端也应该对那些获取锁失败的Redis节点同样发起解锁请求。        参考文献 #    浅析 Redis 分布式锁解决方案。  拜托，面试请不要再问我 Redis 分布式锁的实现原理【石杉的架构笔记】。  Redisson。  Redlock（redis 分布式锁）原理分析。  Redlock：Redis分布式锁最牛逼的实现。  "},{"id":63,"href":"/school-recruitment/docs/database/3Redis/3.4-%E7%BC%93%E5%AD%98%E5%BC%82%E5%B8%B8/3.4.1-%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/","title":"3.4.1 缓存穿透、缓存击穿、缓存雪崩","section":"3.4 缓存异常","content":"缓存穿透、缓存击穿、缓存雪崩 #  1 缓存穿透 #  1.1 含义 #   缓存穿透是指数据库中没有符合条件的数据，缓存服务器中也就没有缓存数据，导致业务系统每次都绕过缓存服务器查询下游的数据库，缓存服务器完全失去了其应有的作用。 如果黑客试图发起针对该 key 的大量访问攻击，数据库将不堪重负，最终可能导致崩溃宕机。 从下图可以看出查询时是直接穿过缓存到达下游数据库，大致业务流程如下图所示：   1.2 解决方法 #  1.2.1 接口层增加校验 #   在接口层增加校验，不合法的参数直接返回。 不相信任务调用方，根据自己提供的接口规范来，作为被调用方，要考虑可能任何的参数传值。  1.2.2 存储空值或默认值 #   虽然数据库中没有符合条件的数据，可以考虑缓存空值或者适合业务的默认值，来缓解这种情况。 为了降低数据的不一致，需要注意两点：  缓存的过期时间需要设置的比较短。 当数据库数据更新时也需要及时更新缓存中对应的数据。    1.2.3 为 IP 设置访问阈值 #   正常用户不会这样暴力攻击，只有是恶意者才会这样做，可以在网关 Nginx 作一个配置项，为每一个 IP 设置访问阈值。  1.2.4 使用布隆过滤器 #  1.2.4.1 背景 #   假如我们需要过滤某些不安全的网页，现在有 100 亿个黑名单页面，每个网页的 URL 最多占用 64 字节，现要设计一种网页过滤系统，可以根据网页的 URL 判断该网页是否在黑名单上，要求该系统允许有万分之一以下的判断错误率，并且使用的额外空间不要超过 30G。 可以采用以下几种方案：  将访问过的 URL 保存到数据库：每次需要过滤网页就需要启用一个数据库 select 查询，且当数据量变得非常庞大后，关系型数据库查询的效率会变得很低。 用 HashSet 将访问过的 URL 保存起来：这样只需要接近 $O(1)$ 的代价就可以查到一个 URL 是否被访问过了，但是内存消耗太大（存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，一旦我们的值很多，例如上亿的时候，那么 HashSet 占据的内存大小就变得很可观了）。 URL 经过 MD5 或者 SHA-1 等单向哈希后再保存到 HashSet 数据库：字符串经过 MD5 散列处理后的信息摘要长度只有 128bit，SHA-1 处理后也只有 160bit，因此方法 3 比方法 2 节省了好几倍的内存。 BitMap 的方法：建立一个 BitSet，将每个 URL 经过哈希函数映射到某一位，这样消耗内存是比较少的，但缺点是单一哈希函数发生冲突的概率太高。    1.2.4.2 含义 #   布隆过滤器（Bloom Filter）实际上是一个很长的二进制向量和一系列随机映射函数，主要用于检索一个元素是否在一个集合中。 他的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。  1.2.4.3 原理 #    布隆过滤器是一个 bit向量或者说 bit数组，如下图所示：   如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并为每个生成的哈希值指向的 bit 位置 1，例如，针对值 baidu 和三个不同的哈希函数分别生成了哈希值 1、4、7，则上图转变为：   假设我们现在再存一个值 tencent，如果哈希函数返回 3、4、8 的话，图继续变为：   值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit 位，因此他被覆盖了。\n  假设现在我们想查询 dianping 这个值是否存在，哈希函数返回了 1、5、8 三个值，结果我们发现5 这个 bit 位上的值为 0，说明没有任何一个值映射到这个 bit 位上，因此我们可以很确定地说 dianping 这个值不存在。\n  假设我们现在想查询 baidu 这个值是否存在的话，那么哈希函数必然会返回 1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，此时我们不能说 baidu 一定存在，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值 taobao 即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置为了 1，那么程序还是会判断 taobao 这个值存在。\n 需要注意的地方：\n 在布隆过滤器中，字符串加入了就不能被删除，因为删除会影响其他字符串。 布隆过滤器使用了 $k$ 个哈希函数，每个字符串跟 $k$ 个 bit 位对应，从而降低了冲突的概率。     1.2.4.4 如何选择哈希函数个数和布隆过滤器长度 #    很显然，过小的布隆过滤器很快所有的 bit 位均为 1，那么查询任何值都会返回可能存在，起不到过滤的目的了，布隆过滤器的长度会直接影响误报率，布隆过滤器越长，误报率越小。\n  另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置为 1 的速度越快，布隆过滤器的效率越低，但是如果太少的话，我们的误报率会变高。\n  哈希函数个数和布隆过滤器长度对布隆过滤器误报率的影响如下图所示，其中 $k$为哈希函数的个数，$m$ 为布隆过滤器的长度，$n$ 为插入的元素个数，$p$ 为误报率： 当 $k$、$m$、$n$ 满足如下关系时，误差率 $p$ 最小：\n$$ m = -\\frac{nlnp}{\\left( ln2 \\right)^2} $$\n$$ k = \\frac{m}{n}ln2 $$\n  1.2.4.5 适用场景 #   大数据判断是否存在：  这就可以实现出上述的去重功能，如果我们的服务器内存足够的话，那么使用 HashMap可能是一个不错的解决方案，理论上时间复杂度可以达到 $O(1)$ 的级别，但是当数据量较大时，还是只能考虑布隆过滤器。   缓解缓存穿透的问题：  利用布隆过滤器我们可以预先把数据查询的主键（比如用户 ID 或者文章 ID）缓存到过滤器中，当根据 ID 进行数据查询的时候，我们先判断该 ID 是否存在，若存在的话，则进行下一步处理，若不存在的话，直接返回，这样就不会触发后续的数据库的查询。 需要注意的是缓存穿透不能完全解决，只能将其控制在一个可以容忍的范围内。   爬虫/邮箱等系统的过滤：  平时我们一些正常的邮件也会被放进垃圾邮件目录中，这就是使用布隆过滤器误判导致的。   识别恶意 URL：  Google Chrome 使用布隆过滤器识别恶意 URL。    2 缓存击穿 #  2.1 含义 #   缓存击穿是指一个 key 非常热点，在不停地扛着大并发，大并发集中对这一个点进行访问，当这个 key 在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，导致数据库处于负载状态。  2.2 解决方法 #  2.2.1 使用互斥锁 #    这是比较常用的方法，就是在缓存失效的时候（判断拿出来的值为空），不是立即去查询数据库，而是先使用缓存工具的某些带成功操作返回值的操作（比如 Redis 的 SETNX 或者 Memcache 的 ADD）去 set 一个 mutex key，当操作返回成功时，再进行查询数据库的操作并回设缓存，否则，就重试整个 get 缓存的方法。\n  具体的互斥代码如下：\nfunction get($key){ $value = $redis-\u0026gt;get($key); if($value == null){ //不存在，设置 3min 的超时，防止 del 操作失败的时候，下次缓存过期一直不能查询数据库 if ($redis-\u0026gt;setnx(\u0026quot;key_mutex\u0026quot;, 1, 3 * 60) == 1){ $value = \u0026quot;\u0026quot;;//这是查询数据库文件 $redis-\u0026gt;set(key, value, expire_secs); $redis-\u0026gt;del(key_mutex); }else{ //这个时候代表同时候的其他线程已经查询数据库并回设到缓存了，这时候重试获取缓存值即可 sleep(50); get(key); //重试 } }else{ //存在则直接返回 return $value; } }   2.2.2 异步更新 #   异步更新是指把这个热点 key 设置为永不过期，然后异步定时更新缓存，比如后台有个值守线程专门定时频繁地去检测缓存，一旦发现被踢掉，就需要立刻更新缓存。  3 缓存雪崩 #  3.1 含义 #   缓存雪崩是指在同一个时间，缓存大批量的失效，然后所有的请求都打到 DB 数据库，导致 DB 数据库直接扛不住崩了。 比如，电商首页缓存，如果首页的key 全部在某一时刻失效，刚好在那一刻有秒杀活动，那这样的话就所有的请求都打到了 DB，并发大的情况下 DB 必然扛不住，没有其他降级之类的方案的话，DBA 也只能重启 DB，但是这样又回被新的流量搞挂。  3.2 解决方法 #   采取不同分类的数据，使用不同周期的失效时间。 同一分类的数据，在失效时间上加上随机因子，如果是特别热门的数据，也可以设置永不过期，有更新操作再更新缓存就可以。  参考文献 #    缓存穿透、缓存击穿、缓存雪崩，看这篇就够了。  Redis 缓存击穿、穿透、雪崩的原因以及解决方案。  布隆过滤器原理及应用。  详解布隆过滤器的原理，使用场景和注意事项。  Redis 布隆过滤器原理与实践。  聊聊布隆过滤器。  布隆过滤器。  深入了解 Redis(7)-缓存穿透,雪崩,击穿。  "},{"id":64,"href":"/school-recruitment/docs/java/3JVM/3.1-%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/","title":"3.1 运行时数据区域","section":"3、 Jvm","content":"运行时数据区域 #  JVM 运行时数据区域主要有 程序计数器、 Java 虚拟机栈 、本地方法栈、堆、方法区 、运行时常量池 六个区域。\n 1 程序计数器（Program Counter Register） #   由于 JVM 同时可以处理多个线程，所以就涉及到一些线程调度，当 CPU 暂停运行线程把时间片让给另一个线程的时候，我们需要保存这个线程被暂停执行前的一些现场状态，需要记录当前执行到哪一行字节码了，所以具备保存现场的功能。 每条线程都有自己的PC 寄存器，在任意时刻虚拟机只会执行一个方法，如果执行的方法不是 Native 方法，PC 寄存器则保存指向当前执行字节码的指令地址；如果执行的是 Native 方法，则 PC 寄存器为空。   If that method is not native, the pc register contains the address of the Java Virtual Machine instruction currently being executed. If the method currently being executed by the thread is native, the value of the Java Virtual Machine\u0026rsquo;s pc register is undefined.\n  2 Java 虚拟机栈（Java Virtual Machine Stacks） #   虚拟机栈是一个线程执行的区域，保存着一个线程中方法的调用状态，所以虚拟机栈是线程私有的，随着线程的创建而创建。 每一个被线程执行的方法，为该栈中的栈帧，即每一个方法对应一个栈帧。 调用一个方法，就会向栈中压入一个栈帧；一个方法调用完成，就会把该栈帧从栈中弹出。   3 本地方法栈（Native Method Stacks） #   如果当前线程执行的方法是Native类型的，这些方法就会在本地方法栈中执行。   4 堆（Heap） #   在 Java 虚拟机中堆是所有线程都可以共享的内存区域，是存放所有类实例和数组对象的地方。 在虚拟机启动时就根据相关堆参数创建堆，他也是垃圾收集器工作的主要区域。 堆内存的对象不会被显式的回收，而是由垃圾收集器回收。   5 方法区（Method Area） #   方法区是各个线程共享的内存区域，在虚拟机启动时创建。 方法区主要用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 方法区可以是固定大小的，也可以根据计算的需求进行扩展。   6 运行时常量池（Run-Time Constant Pool） #   运行时常量池是类文件 constant_pool 表的每个类或每个接口的运行时表示。 它包含几种类型的常量，从编译时已知的数值常量到必须在运行时解析的方法和字段引用。 每个运行时常量池都是从 Java 虚拟机的方法区中分配，在创建类或接口时，如果构建运行时常量池所需的内存超过了 Java 虚拟机的方法区所能提供的内存，则 Java 虚拟机将抛出 OutOfMemoryError。   7 参考文献 #    2.5. Run-Time Data Areas。  JVM 运行时数据区详解，写得非常好！。  JVM 运行时数据区(Run-TimeDataAreas)及内存结构。  "},{"id":65,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.2-%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/","title":"1.1.2 背包问题","section":"1.1 动态规划","content":"1 含义 #   给定一个背包容量 $target$，再给定一个数组$nums$（物品），能否按一定方式选取 $nums$ 中的元素得到 $target$。 需要注意的是：  背包容量 $target$ 和物品 $nums$ 的类型可能是数，也可能是字符串。 $target$可能题目已经给出（显式），也可能是需要我们从题目的信息中挖掘出来（非显式）（常见的非显式 $target$ 比如 $sum / 2$ 等）。 选取的方式有常见的以下几种：  每个元素选一次。 每个元素选多次。 选元素进行排列组合。      2 分类及解题模板 #   常见的背包类型有以下几种：  0/1 背包问题：  每个元素最多选取一次。 外循环 $nums$，内循环（倒序）$target$，且 $target \\ge num$。   完全背包问题：  每个元素可以重复选择。 外循环 $nums$，内循环（正序）$target$，且 $target \\ge num$。   组合背包问题：  背包中的物品要考虑顺序。 外循环（正序）$target$，内循环 $nums$，且 $target \\ge num$。   分组背包问题：  不止一个背包，需要遍历每一个背包。 这个比较特殊，需要三重循环，外循环背包 $bags$，内部两层循环根据题目的要求转化为上面三种背包类型的模板。     而每个背包问题要求的也是不同的，按照所求问题分类，又可以分为以下几种：  最值问题：  要求最大值或最小值。 $dp[i] = max/min(dp[i], dp[i - num] + 1)$ 或 $dp[i] = max/min(dp[i], dp[i - num] + num)$。   存在问题：  是否存在\u0026hellip;，满足\u0026hellip;。 $dp[i] = dp[i] || dp[i - num]$。   组合问题：  求所有满足\u0026hellip;的排列组合。 $dp[i] += dp[i - num]$。      3 题目示例 #  3.1 完全背包最值问题 #   题目来源 322. 零钱兑换。\n 3.1.1 题目 #  给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。\n计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。\n你可以认为每种硬币的数量是无限的。\n示例 1：\n输入：coins = [1, 2, 5], amount = 11 输出：3 解释：11 = 5 + 5 + 1 示例 2：\n输入：coins = [2], amount = 3 输出：-1 示例 3：\n输入：coins = [1], amount = 0 输出：0 示例 4：\n输入：coins = [1], amount = 1 输出：1 示例 5：\n输入：coins = [1], amount = 2 输出：2 提示：\n 1 \u0026lt;= coins.length \u0026lt;= 12 1 \u0026lt;= coins[i] \u0026lt;= 231 - 1 0 \u0026lt;= amount \u0026lt;= 104  3.1.2 问题分析 #   该题目属于完全背包最值问题，直接套用相应的解题模板即可。  3.1.3 参考代码 #  /** * 322. 零钱兑换 * @param coins 不同面额的硬币数组 * @param amount 总金额 * @return 可以凑成总金额所需的最少的硬币个数 */ public int coinChange(int[] coins, int amount) { int m = coins.length; // dp 数组，其中 dp[i] 表示凑成金额 i 所需的最少的硬币个数  int[] dp = new int[amount + 1]; Arrays.fill(dp, amount + 1); dp[0] = 0; for (int coin: coins) { for (int i = coin; i \u0026lt;= amount; i++) { dp[i] = Math.min(dp[i], dp[i - coin] + 1); } } return dp[amount] == amount + 1 ? -1 : dp[amount]; } 3.1.3 扩展题目 #  3.1.3.1 完全平方数 #  3.1.3.1.1 题目 #  给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, \u0026hellip;）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。\n给你一个整数 n ，返回和为 n 的完全平方数的 最少数量 。\n完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。\n示例 1：\n输入：n = 12 输出：3 解释：12 = 4 + 4 + 4 示例 2：\n输入：n = 13 输出：2 解释：13 = 4 + 9 提示：\n 1 \u0026lt;= n \u0026lt;= 104  3.1.3.1.2 问题分析 #   完全平方数最小为 1，最大为 $sqrt(n)$，故题目转换为在 $nums = [1,2,\u0026hellip;,sqrt(n)]$ 中选任意数平方和为 $target = n$。 该题目属于完全背包最值问题，直接套用相应的解题模板即可。  3.1.3.1.3 参考代码 #  /** * 279. 完全平方数 * @param n 一个正整数 * @return 和为 n 的完全平方数的 最少数量 */ public int numSquares(int n) { // dp 数组，其中 dp[i] 表示和为 i 的完全平方数的最少数量  int[] dp = new int[n + 1]; Arrays.fill(dp, Integer.MAX_VALUE); dp[0] = 0; for (int num = 1; num \u0026lt;= Math.sqrt(n); num++) { for (int i = num * num; i \u0026lt;= n; i++) { dp[i] = Math.min(dp[i], dp[i - num * num] + 1); } } return dp[n]; } 3.2 0/1 背包存在性问题 #   题目来源 416. 分割等和子集。\n 3.2.1 题目 #  给你一个 只包含正整数 的 非空 数组 nums 。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。\n示例 1：\n输入：nums = [1,5,11,5] 输出：true 解释：数组可以分割成 [1, 5, 5] 和 [11] 。 示例 2：\n输入：nums = [1,2,3,5] 输出：false 解释：数组不能分割成两个元素和相等的子集。 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 200 1 \u0026lt;= nums[i] \u0026lt;= 100  3.2.2 问题分析 #   该题目等价于是否存在一个子集，其和为 $target = sum / 2$。 该题目属于0/1 背包存在性问题，直接套用相应的解题模板即可。  3.2.3 参考代码 #  /** * 416. 分割等和子集 * @param nums 只包含正整数的非空数组 * @return 是否可以将这个数组分割成两个子集，使得两个子集的元素和相等 */ public boolean canPartition(int[] nums) { int sum = Arrays.stream(nums).sum(), target = sum / 2; int m = nums.length + 1; // dp 数组，其中 dp[i] 表示是否可以将原数组分成两个和为 i 的子集  boolean[] dp = new boolean[target + 1]; // base case  // 如果和为奇数，显然无法分成两个等和子集  if (sum % 2 != 0) {return false;} dp[0] = true; for (int num: nums) { for (int i = target; i \u0026gt;= num; i--) { dp[i] = dp[i] || dp[i - num]; } } return dp[target]; } 3.3 0/1 背包组合问题 #   题目来源 494. 目标和。\n 3.3.1 题目 #  给你一个整数数组 nums 和一个整数 target 。\n向数组中的每个整数前添加 \u0026lsquo;+\u0026rsquo; 或 \u0026lsquo;-\u0026rsquo; ，然后串联起所有整数，可以构造一个 表达式 ：\n例如，nums = [2, 1] ，可以在 2 之前添加 \u0026lsquo;+\u0026rsquo; ，在 1 之前添加 \u0026lsquo;-\u0026rsquo; ，然后串联起来得到表达式 \u0026ldquo;+2-1\u0026rdquo; 。 返回可以通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目。\n示例 1：\n输入：nums = [1,1,1,1,1], target = 3 输出：5 解释：一共有 5 种方法让最终目标和为 3 。 -1 + 1 + 1 + 1 + 1 = 3 +1 - 1 + 1 + 1 + 1 = 3 +1 + 1 - 1 + 1 + 1 = 3 +1 + 1 + 1 - 1 + 1 = 3 +1 + 1 + 1 + 1 - 1 = 3 示例 2：\n输入：nums = [1], target = 1 输出：1 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 20 0 \u0026lt;= nums[i] \u0026lt;= 1000 0 \u0026lt;= sum(nums[i]) \u0026lt;= 1000 -1000 \u0026lt;= target \u0026lt;= 1000  3.3.2 问题分析 #    假设数组和为 $sum$，目标和为 $s$，正数和为 $x$，负数和为 $y$，则：\n$$ x + y = sum, x - y = s $$\n可得：\n$$ x = \\frac{s + sum}{2} $$\n  所以该题目可以转换为从数组$nums$中无放回的选取几个数，其和等于$x$的组合的个数。\n  该题目属于0/1 背包组合问题，直接套用相应的解题模板即可。\n  3.3.3 参考代码 #  /** * 494. 目标和 * @param nums 整数数组 * @param target 目标整数 * @return 通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目 */ public int findTargetSumWays(int[] nums, int target) { int m = nums.length; int sum = Arrays.stream(nums).sum(); // dp 数组，其中 dp[i] 表示 从数组 nums 中无放回选取元素，其和等于 i 的组合的个数  int[] dp = null; if ((sum + target) % 2 != 0 || sum \u0026lt; Math.abs(target)) {return 0;} target = (sum + target) / 2; dp = new int[target + 1]; dp[0] = 1; for (int num: nums) { for (int i = target; i \u0026gt;= num; i--) { dp[i] += dp[i - num]; } } return dp[target]; } 3.4 组合背包组合问题 #   题目来源 377. 组合总和 Ⅳ。\n 3.4.1 题目 #  给你一个由 不同 整数组成的数组 nums ，和一个目标整数 target 。请你从 nums 中找出并返回总和为 target 的元素组合的个数。\n题目数据保证答案符合 32 位整数范围。\n示例 1：\n输入：nums = [1,2,3], target = 4 输出：7 解释： 所有可能的组合为： (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) 请注意，顺序不同的序列被视作不同的组合。 示例 2：\n输入：nums = [9], target = 3 输出：0 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 200 1 \u0026lt;= nums[i] \u0026lt;= 1000 nums 中的所有元素 互不相同 1 \u0026lt;= target \u0026lt;= 1000  进阶：如果给定的数组中含有负数会发生什么？问题会产生何种变化？如果允许负数出现，需要向题目中添加哪些限制条件？\n3.4.2 问题分析 #   该题目中顺序不同的序列被视作不同的组合，即背包中的物品需要考虑顺序，所以该题目属于组合背包组合问题，直接套用相应的解题模板即可。  3.4.3 参考代码 #  /** * 377. 组合总和 Ⅳ * @param nums 不同整数组成的数组 * @param target 目标整数 * @return 从 nums 中可以找到的总和为 target 的元素组合的个数 */ public int combinationSum4(int[] nums, int target) { int m = nums.length; // dp 数组，其中 dp[i] 表示从 nums 中可以找到的总和为 i 的元素组合的个数  int[] dp = new int[target + 1]; dp[0] = 1; for (int i = 1; i \u0026lt;= target; i++) { for (int num: nums) { if (i \u0026gt;= num) { dp[i] += dp[i - num]; } } } return dp[target]; } 3.5 完全背包组合问题 #   题目来源 518. 零钱兑换 II。\n 3.5.1 题目 #  给你一个整数数组 coins 表示不同面额的硬币，另给一个整数 amount 表示总金额。\n请你计算并返回可以凑成总金额的硬币组合数。如果任何硬币组合都无法凑出总金额，返回 0 。\n假设每一种面额的硬币有无限个。\n题目数据保证结果符合 32 位带符号整数。\n示例 1：\n输入：amount = 5, coins = [1, 2, 5] 输出：4 解释：有四种方式可以凑成总金额： 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 示例 2：\n输入：amount = 3, coins = [2] 输出：0 解释：只用面额 2 的硬币不能凑成总金额 3 。 示例 3：\n输入：amount = 10, coins = [10] 输出：1 提示：\n 1 \u0026lt;= coins.length \u0026lt;= 300 1 \u0026lt;= coins[i] \u0026lt;= 5000 coins 中的所有值 互不相同 0 \u0026lt;= amount \u0026lt;= 5000  3.5.2 问题分析 #   该题目属于完全背包组合问题，直接套用相应的解题模板即可。  3.5.3 参考代码 #  /** * 518. 零钱兑换 II * @param amount 总金额 * @param coins 不同面额的硬币数组 * @return 可以凑成总金额的硬币组合数 */ public int change(int amount, int[] coins) { int m = coins.length; // dp 数组，其中 dp[i] 表示可以凑成总金额为 i 的硬币组合数  int[] dp = new int[amount + 1]; dp[0] = 1; for (int coin: coins) { for (int i = coin; i \u0026lt;= amount; i++) { dp[i] += dp[i - coin]; } } return dp[amount]; } 参考文献 #    一篇文章吃透背包问题！（细致引入 + 解题模板 + 例题分析 + 代码呈现）。  322. 零钱兑换。  416. 分割等和子集。  494. 目标和。  279. 完全平方数。  377. 组合总和 Ⅳ。  518. 零钱兑换 II。  "},{"id":66,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.2-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","title":"2.2 二分查找","section":"2、高频面试题","content":"二分查找 #  "},{"id":67,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.2-%E9%94%81/","title":"1.2 锁","section":"1、数据库基础","content":"锁 #  首先，对 MySQL 锁进行划分：\n 按照锁的粒度划分：表锁（Table Level Lock）、行锁（Row Level Lock）。 按照锁的使用方式划分：共享锁（Share Lock）、排他锁（悲观锁的一种实现）（eXclusive Lock）。 按照锁的思想划分：悲观锁（Pessimistic Concurrency Control, PCC）、乐观锁（Optimistic Concurrency Control, PCC）。 InnoDB的几种意向锁：意向共享锁（Intensive Shared Lock, IS）、意向排他锁（Intensive Exclusive Lock）。 InnoDB的几种行级锁：记录锁（Recoord Lock）、间隙锁（Gap Lock）、临键锁（Next-key Lock）。  1 表锁和行锁 #  1.1 表锁 #  1.1.1 含义 #   表级锁是 MySQL 中粒度最大的一种锁，表示当前的操作对整张表加锁。 表锁响应的是非索引字段，即全表扫描，全表扫描时锁定整张表。 表级锁有两种模式：表共享锁（Table Read Lock）、表独占写锁（Table Write Lock）：  表共享锁： 不会阻塞其他用户对同一张表的读请求，但会阻塞对同一张表的写请求。 表独占写锁：会阻塞其他用户对同一张表的读写请求，当一个线程获得对一个表的写锁之后，只有持有锁的线程可以对表进行更新操作，其他线程的读、写操作都会等待，直到锁被释放为止。    1.1.2 如何加表锁 #    给表显示加锁，一般是为了一定程度模拟事务操作，实现对某一时间点多个表的一致性读取。\n  例如，有一个订单表 orders，其中记录有订单的总金额 total，同时还有一个订单明细表 order_detail，其中记录有订单每一产品的金额小计 subtotal，假设我们需要检查这两个表的金额合计是否相等，可能就需要执行如下两条 SQL：\nselect sum(total) from orders; select sum(subtotal) from order_detail;   这时，如果不给这两个表加锁，就可能产生错误的结果，因为第一条语句在执行的过程中，order_detail 表可能已经发生了改变，因此，正确的方法应该是：\nlock tables order read local, order_detail read local; select sum(total) from orders; select sum(subtotal) from order_detail; unlock tables;   针对上面的叙述，有以下需要说明的地方：\n 上面的例子在lock tables 时加了local 选项，其作用就是在满足表并发插入的条件下，允许其他用户在表尾插入记录（InnoDB 中read local 和read 等价）。 在用lock tables 给表显式加表锁时，必须同时取得所有涉及表的锁，也就是说，再执行lock tables 后，只能访问显式加锁的这些表，不能访问未加锁的表，这也正是表锁不会出现死锁的原因。同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。    1.1.3 优缺点 #  1.1.3.1 优点 #   开销小，加锁快。 不会出现死锁。  1.1.3.2 缺点 #   锁定力度大，发生冲突的概率最高，并发度最低。  1.2 行锁 #  1.2.1 含义 #   读取操作行级锁是 MySQL 中锁定粒度最细的一种锁，表示只对当前操作的行进行加锁。 MySQL 的行锁是基于索引加载的，所以行锁是要加在索引响应的行上，即命中索引。 行级锁按照使用方式分为共享锁（Share Lock）和排他锁（sXclusive Lock），具体可参照 共享锁和排他锁。  1.2.2 加锁原理 #  1.2.2.1 单行数据的加锁原理 #    假如有如下两条 SQL：\nupdate user set age = 10 where id = 49; update user set age = 10 where name = \u0026#39;Tom\u0026#39;;   第一条 SQL使用主键索引来查询，则只需要在 id=49 这个主键索引上加上写锁。\n  第二条 SQL 则使用辅助索引来查询，首先在 name=Tom 这个索引上加写锁，然后由于使用InnoDB 的辅助索引，还需要根据主键索引进行查询，所以还需要在 id=49 这个主键索引上加写锁。\n  也就是说主键索引需要加一把锁，使用辅助索引需要在辅助索引和主键索引上各加一把锁。\n  1.2.3 优缺点 #  1.2.3.1 优点 #   能大大减少数据库操作的冲突。  1.2.3.2 缺点 #   加锁粒度最小，但是加锁的开销也最大，有可能出现死锁的情况。  2 共享锁和排他锁 #  2.1 共享锁 #  2.1.1 含义 #   共享锁又称读锁，是读取操作创建的锁。 其他用户可以并发读取数据，但任何事务都不能对数据进行修改，直到已释放所有共享锁。 如果事务 T 对数据 A 加上共享锁后，则其他事务只能对 A 再加共享锁，不能加排他锁。 获取共享锁的事务只能读数据，不能修改数据。  2.1.2 用法 #   可以通过在查询语句后面加上 LOCK IN SHARE MODE 来使用共享锁。 此时 MySQL 会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。 其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。  2.2 排他锁 #  2.2.1 含义 #   排他锁又称写锁。 如果事务 T 对数据 A 加上排他锁后，则其他事务不能再对 A 加任何类型的锁。 获取排他锁的事务既能读数据，又能修改数据。  2.2.2 用法 #   可以通过在查询语句后面添加 FOR UPDATE 来使用排他锁，此时 MySQL 会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。  3 乐观锁和悲观锁 #  3.1 悲观锁 #  3.1.1 含义 #   悲观锁指的是对数据被外界（包括本系统当前的其它事务，以及来自外部系统的事务处理）修改持保守态度，认为数据随时可能会修改，因此，每次去取数据的时候都会给他上锁，防止其他事务读取或修改数据。 悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库底层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。  3.1.2 实现方式 #   在对任意记录修改前，先尝试为该记录加上排他锁。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常，具体的响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录作修改，事务成功后就可以解锁了。 期间如果有其它对该记录做修改或加排他锁的操作，都会等待我们解锁或者抛出异常。  3.1.3 优缺点 #  3.1.3.1 优点 #   悲观锁采取的是保守策略，先加锁，成功了才访问数据，这保证了数据获取和修改都是有序进行的，因此适合在写多读少的环境中使用，可以有效地保证数据的安全性。  3.1.3.2 缺点 #   由于需要加锁，可能会面临锁冲突甚至死锁的问题。 加锁和释放锁会增加系统的额外开销，降低系统的效率，同时也会降低系统的并行性。  3.1.4 示例 #    下面是以 MySQL 的 InnoDB 引擎为例。 要使用悲观锁，我们必须关闭 MySQL 数据库的自动提交属性，因为 MySQL 默认使用autocommit 模式，也就是说，当我们执行一个更新操作后，MySQL 会立即将结果进行提交，可以使用下面的命令设置 MySQL 为非autocommit 模式。 set autocommit = 0;    商品 goods 表中有一个字段 status，status 为 1 代表商品未被下单，status 为 2 代表商品已被下单，那么对某个商品下单时必须确保该商品 status 为 1，假设该商品的 id 为 1。\n  不使用锁时：  第一步操作中，查询出来的商品status 为 1，但是当我们执行第三步update 操作的时候，在高并发情况下有可能出现其他人先一步对商品下单，把goods 中id 为 1 的status 修改为 2 了，但是我们并不知道数据已经被修改了，这样就可能造成同一个商品被下单 2 次。   使用悲观锁时：  与普通查询不一样，这里使用了select ... for update 的方式，这样就通过数据库实现了悲观锁。 此时在goods 表中，id为 1 的那条数据就被锁定了，直到我们修改完毕后再解锁，在这个过程中，因为goods 被锁定了，就不会出现有第三者来对其进行修改了。 此时如果有其他的事务针对这条数据进行select ... for update，那么这个事务就会处于阻塞状态，等待第一个事务的提交，只有当第一个事务提交数据之后，第二个事务才能正常执行。 但是在这个过程中，正常的 select 是不会受影响的。 执行select ... for update 时，锁有三种级别，分别是无锁、行级锁、表级锁：  无锁： 明确指定主键/索引，但查无数据。 行级锁： 明确指定主键/索引，并且有此数据。 表级锁：  未指定主键/索引。 主键/索引不明确。         3.2 乐观锁 #  3.2.1 含义 #   相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回错误的信息，让用户决定如何去做。 相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制，一般的实现方式是记录数据的版本。   数据版本是什么？\n 数据版本就是为数据增加的一个版本标识。 当读取数据的时候，将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新。 当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本号与第一次取出来的版本标识值相等，则予以更新，否则认为是过期数据。   3.2.2 优缺点 #  3.2.2.1 优点 #   乐观并发控制相信事务之间的数据竞争的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。 乐观并发控制实际上没有加锁，所以没有额外开销，适合读多写少的场景，能极大提高数据库的性能。  3.2.2.2 缺点 #   如果两个事务都读取了数据库的某一行，经过同时修改以后写回数据库，这时就遇到了问题，因为两个事务都会修改数据库的数据，这时数据就会发生错乱。  3.2.3 示例 #    t_goods 表中的数据如下：\n   首先用户 A 和用户 B 同时将臭豆腐（id=2）的数据查出来，此时二者读到的臭豆腐对应的 id 为 2， num 为 1， version 为 0：\nselect * from t_goods where name = \u0026#39;臭豆腐\u0026#39;;   然后用户 A 先买，将 id=1 和 version=0 作为条件进行更新（将臭豆腐的数量减 1，并且将版本加 1）:\nupdate t_goods set num = {num} - 1, version = {version} + 1 where id = {id} and version = {version};   接着用户 B 开始购买，按照相同的条件进行更新。\n  B 更新完成后，发现更新的数据行数为 0，说明已经有人改动过数据，此时就应该提示用户 B 重新查看最新数据来进行购买。\n  4 意向锁 #  4.1 为什么要引入意向锁 #   由于表锁和行锁虽然锁定范围不同，但是会相互冲突，所以，当我们要加表锁时，势必要先遍历该表的所有记录，判断是否加有排他锁。 这种遍历检查的方式显然是一种低效的方式，因此 MySQL引入了意向锁，来检测表锁和行锁的冲突。  4.2 特点 #   意向锁是表级锁，可以分为意向共享锁（IS 锁）和意向排他锁（IX 锁）。 当事务要在记录上加共享锁或者排他锁时，要首先在表上加上意向锁，这样判断表中是否有记录加锁就很简单了，只要看一下表上是否有意向锁就行了。 意向锁之间是不会产生冲突的，他只会阻塞表级共享锁或表级排他锁。 同时，意向锁也不会和行锁冲突，行锁只会和行锁冲突。 意向锁是InnoDB 自动加的，不需要用户干预。  4.3 意向锁的兼容互斥性 #   意向锁之间是互相兼容的。      意向共享锁（IS） 意向排他锁（IX)     意向共享锁（IS） 兼容 兼容   意向排他锁（IX) 兼容 兼容    意向锁会和表级共享锁/排他锁互斥。      意向共享锁（IS） 意向排他锁（IX）     表级共享锁 兼容 互斥   表级排他锁 互斥 互斥    4.4 示例 #    假设有一张 users 表，事务 A 获取了其中某一行的排他锁，尚未提交：\nSELECT * FROM users WHERE id = 6 FOR UPDATE;   此时 users 表存在两把锁，分别是 users 表上的意向排他锁和 id 为 6 的数据行上的排他锁。\n  事务 B 想要获取 users表的共享锁：\nLOCK TABLES users READ;   此时事务 B 检测事务 A 持有 users 表的意向排他锁，就可以得知事务 A 必然持有该表中某些数据行的排他锁，那么事务 B 对 users 表的加锁请求就会被排斥（阻塞），而无需去检测表中的每一行数据是否存在排他锁。\n  最后事务 C 也想获取 users 表中某一行的排他锁：\nSELECT * FROM users WHERE id = 5 FOR UPDATE;   此时事务 C 会申请 users 表的意向排他锁，虽然事务 C检测到了事务 A 持有 users 表的意向排他锁，但是因为意向锁之间不互斥，所以事务 C获取到了 users 表的意向排他锁，因为 id 为 5 的数据行上不存在任何排他锁，最终事务 C成功获取到了该数据行的排他锁。\n  5 行级锁 #  5.1 记录锁 #   记录锁是最简单的行锁。 当SQL语句无法使用索引时，会进行全表扫描，这个时候MySQL会给整张表的所有数据行加记录锁，再右MySQL Server层进行过滤，如果发现不满足 WHERE条件，会释放对应记录的锁，这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。 所以，更新操作必须要根据索引进行操作，没有索引时，不仅会消耗大量的锁资源，增加数据库的开销，还会极大的降低了数据库的并发性能。  5.2 间隙锁 #  5.2.1 含义 #   当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。 对于键值在条件范围内但并不存在的记录，InnoDB也会对这个间隙加锁，这种锁机制就是所谓的间隙锁。 间隙锁是索引记录中的间隔，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围。 间隙锁在InnoDB的唯一作用就是防止其他事务的插入操作，以此来达到防止幻读的发生。  5.2.2 如何禁止间隙锁 #   要禁止间隙锁，主要有两种方法：   把隔离级别降为读已提交。\n  开启参数 innodb_locks_unsafe_for_binlog，这个值默认值为 OFF，即启用间隙锁，因为此参数是只读模式，如果想要禁用间隙锁，需要修改 my.cnf（Windows是 my.ini），然后重新启动才行。\n # 在 my.cnf 里面的[mysqld]添加 [mysqld] innodb_locks_unsafe_for_binlog = 1     5.2.3 间隙锁定的区域 #   根据检索条件向左寻找最靠近检索条件的记录值$A$，作为左区间，向右寻找最靠近检索条件的记录值$B$，作为右区间，即锁定的间隙为$(A,B)$。  5.2.4 间隙锁的分类 #  下面场景的测试环境为MySQL 5.7，存储引擎为InnoDB，默认的隔离级别为可重复读（Repeatable Read）。\n5.2.4.1 唯一索引的间隙锁 #  5.2.4.1.1 总结 #   对于指定查询某一条记录的加锁语句，如果记录存在，则会产生记录锁，如果该记录不存在，则会产生间隙锁。 对于查找某一范围内的查询语句，会产生间隙锁。  5.2.4.1.2 示例 #    假如有如下示例表：\nCREATE TABLE `my_gap` ( `id` int(1) NOT NULL AUTO_INCREMENT, `name` varchar(8) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `my_gap` VALUES (\u0026#39;1\u0026#39;, \u0026#39;张三\u0026#39;); INSERT INTO `my_gap` VALUES (\u0026#39;5\u0026#39;, \u0026#39;李四\u0026#39;); INSERT INTO `my_gap` VALUES (\u0026#39;7\u0026#39;, \u0026#39;王五\u0026#39;); INSERT INTO `my_gap` VALUES (\u0026#39;11\u0026#39;, \u0026#39;赵六\u0026#39;);   该表会产生的间隙如下：\n1. (-infinity, 1] 2. (1, 5] 3. (5, 7] 4. (7, 11] 5. (11, +infinity]   只是用记录锁，不会产生间隙锁：\n  /* 开启事务1 */ BEGIN; /* 查询 id = 5 的数据并加记录锁 */ SELECT * FROM `my_gap` WHERE `id` = 5 FOR UPDATE; /* 延迟30秒执行，防止锁释放 */ SELECT SLEEP(30); # 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句 /* 事务2插入一条 name = \u0026#39;杰伦\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (4, \u0026#39;杰伦\u0026#39;); # 正常执行 /* 事务3插入一条 name = \u0026#39;学友\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (8, \u0026#39;学友\u0026#39;); # 正常执行 /* 提交事务1，释放事务1的锁 */ COMMIT;  上面的案例中，由于主键是唯一索引，而且只使用一个索引查询，并且只锁定了一条记录，所以只会对 id=5的数据加上记录锁，而不会产生间隙锁。    产生间隙锁：\n  恢复初始化的4条记录，继续在 id唯一索引列上做如下测试：\n   然后执行以下SQL：\n/* 开启事务1 */ BEGIN; /* 查询 id 在 7 - 11 范围的数据并加记录锁 */ SELECT * FROM `my_gap` WHERE `id` BETWEEN 5 AND 7 FOR UPDATE; /* 延迟30秒执行，防止锁释放 */ SELECT SLEEP(30); # 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句 /* 事务2插入一条 id = 3，name = \u0026#39;思聪3\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (3, \u0026#39;思聪3\u0026#39;); # 正常执行 /* 事务3插入一条 id = 4，name = \u0026#39;思聪4\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (4, \u0026#39;思聪4\u0026#39;); # 正常执行 /* 事务4插入一条 id = 6，name = \u0026#39;思聪6\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (6, \u0026#39;思聪6\u0026#39;); # 阻塞 /* 事务5插入一条 id = 8， name = \u0026#39;思聪8\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (8, \u0026#39;思聪8\u0026#39;); # 阻塞 /* 事务6插入一条 id = 9， name = \u0026#39;思聪9\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (9, \u0026#39;思聪9\u0026#39;); # 阻塞 /* 事务7插入一条 id = 11， name = \u0026#39;思聪11\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (11, \u0026#39;思聪11\u0026#39;); # 阻塞 /* 事务8插入一条 id = 12， name = \u0026#39;思聪12\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (12, \u0026#39;思聪12\u0026#39;); # 正常执行 /* 提交事务1，释放事务1的锁 */ COMMIT;   从上面可以看到，$(5,7]$、$(7,11]$这两个区间都不可插入数据，其他区间都可以正常插入数据，所以可以得出结论：当我们给$(5,7]$这两个区间加锁的时候，会锁住$(5,7]$、$(7,11]$这两个区间。\n  如果上面的查询SQL改为：\nSELECT * FROM `my_gap` WHERE `id` \u0026gt; 5 AND id \u0026lt; 7 FOR UPDATE; 此时，产生的间隙锁会锁住$(5,7)$这个区间。\n  恢复初始化的4条记录，我们再来测试如果锁住不存在的数据时，会如何。\n  然后执行以下SQL：\n/* 开启事务1 */ BEGIN; /* 查询 id = 3 这一条不存在的数据并加记录锁 */ SELECT * FROM `my_gap` WHERE `id` = 3 FOR UPDATE; /* 延迟30秒执行，防止锁释放 */ SELECT SLEEP(30); # 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句 /* 事务2插入一条 id = 3，name = \u0026#39;小张\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (2, \u0026#39;小张\u0026#39;); # 阻塞 /* 事务3插入一条 id = 4，name = \u0026#39;小白\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (4, \u0026#39;小白\u0026#39;); # 阻塞 /* 事务4插入一条 id = 6，name = \u0026#39;小东\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (6, \u0026#39;小东\u0026#39;); # 正常执行 /* 事务5插入一条 id = 8， name = \u0026#39;大罗\u0026#39; 的数据 */ INSERT INTO `my_gap` (`id`, `name`) VALUES (8, \u0026#39;大罗\u0026#39;); # 正常执行 /* 提交事务1，释放事务1的锁 */ COMMIT;   从上面可以看出，指定查询某一条记录时，如果这条记录不存在，会产生间隙锁。\n    5.2.4.2 普通索引的间隙锁 #  5.2.4.2.1 总结 #   在普通索引列上，不管是何种查询，只要加锁，都会产生间隙锁，这跟唯一索引不一样。 在普通索引跟唯一索引中，数据间隙的分析，数据行是优先根据普通索引排序，然后再根据唯一索引排序。  5.2.4.2.2 示例 #    假如有如下示例表，其中 id是主键，同时在 number上，建立了一个普通索引：\n# 注意：number 不是唯一值 CREATE TABLE `my_gap1` ( `id` int(1) NOT NULL AUTO_INCREMENT, `number` int(1) NOT NULL COMMENT \u0026#39;数字\u0026#39;, PRIMARY KEY (`id`), KEY `number` (`number`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; INSERT INTO `my_gap1` VALUES (1, 1); INSERT INTO `my_gap1` VALUES (5, 3); INSERT INTO `my_gap1` VALUES (7, 8); INSERT INTO `my_gap1` VALUES (11, 12);   my_gap1表中 number索引存在如下隐藏间隙：\n1. (-infinity, 1] 2. (1, 3] 3. (3, 8] 4. (8, 12] 5. (12, +infinity]   测试1：\n  执行以下SQL：\n/* 开启事务1 */ BEGIN; /* 查询 number = 3 的数据并加记录锁 */ SELECT * FROM `my_gap1` WHERE `number` = 3 FOR UPDATE; /* 延迟30秒执行，防止锁释放 */ SELECT SLEEP(30); # 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句 /* 事务2插入一条 number = 0 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (0); # 正常执行 /* 事务3插入一条 number = 1 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (1); # 被阻塞 /* 事务4插入一条 number = 2 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (2); # 被阻塞 /* 事务5插入一条 number = 4 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (4); # 被阻塞 /* 事务6插入一条 number = 8 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (8); # 正常执行 /* 事务7插入一条 number = 9 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (9); # 正常执行 /* 事务8插入一条 number = 10 的数据 */ INSERT INTO `my_gap1` (`number`) VALUES (10); # 正常执行 /* 提交事务1 */ COMMIT;   因为查询的 number的值为3，而且 number为普通索引，因此会产生间隙锁，间隙锁的区间为$(1,8)$，因此 number在这个区间的插入SQL都会执行成功，不在这个区间的插入SQL都会执行失败。\n    测试2：\n  首先将数据还原为初始化的形式，然后执行如下SQL：\n/* 开启事务1 */ BEGIN; /* 查询 number = 3 的数据并加记录锁 */ SELECT * FROM `my_gap1` WHERE `number` = 3 FOR UPDATE; /* 延迟30秒执行，防止锁释放 */ SELECT SLEEP(30); /* 事务1插入一条 id = 2， number = 1 的数据 */ INSERT INTO `my_gap1` (`id`, `number`) VALUES (2, 1); # 阻塞 /* 事务2插入一条 id = 3， number = 2 的数据 */ INSERT INTO `my_gap1` (`id`, `number`) VALUES (3, 2); # 阻塞 /* 事务3插入一条 id = 6， number = 8 的数据 */ INSERT INTO `my_gap1` (`id`, `number`) VALUES (6, 8); # 阻塞 /* 事务4插入一条 id = 8， number = 8 的数据 */ INSERT INTO `my_gap1` (`id`, `number`) VALUES (8, 8); # 正常执行 /* 事务5插入一条 id = 9， number = 9 的数据 */ INSERT INTO `my_gap1` (`id`, `number`) VALUES (9, 9); # 正常执行 /* 事务6插入一条 id = 10， number = 12 的数据 */ INSERT INTO `my_gap1` (`id`, `number`) VALUES (10, 12); # 正常执行 /* 事务7修改 id = 11， number = 12 的数据 */ UPDATE `my_gap1` SET `number` = 5 WHERE `id` = 11 AND `number` = 12; # 阻塞 /* 提交事务1 */ COMMIT;   查看表中的数据：\n   这里有一个奇怪的现象：\n 事务3 添加 id = 6，number = 8 的数据，阻塞了。 事务4 添加 id = 8，number = 8 的数据，正常执行了。 事务7 将 id = 11，number = 12 的数据修改为 id = 11， number = 5 的操作，给阻塞了。  首先我们可以看一下下面这张图：\n 从图中可以看出，当 number相同时，会根据主键 id来排序：\n 事务 3 添加的 id = 6，number = 8，这条数据是在 (3,8) 的区间里边，所以会阻塞。 事务 4 添加的 id = 8，number = 8，这条数据实在 (8,12) 区间里边，所以不会阻塞。 事务 7 的修改语句相当于 在 (3,8) 的区间里边插入一条数据，所以也被阻塞了。      5.3 临键锁 #   临键锁是记录锁和间隙锁的组合，他的锁范围既包含索引记录，又包含索引区间。 InnoDB默认使用可重复读（Repeatable Read）隔离级别，同时在索引查询时使用临键锁来避免幻读的产生。  参考文献 #    深入理解 MySQL 锁类型和加锁原理。  mysql 悲观锁详解。  一文读懂数据库中的乐观锁和悲观锁和 MVCC。  合理的使用 MySQL 乐观锁与悲观锁。  mysql 悲观锁和乐观优缺点_乐观锁、悲观锁和 MVCC 各是什么？各自优缺点是什么？\u0026hellip;  悲观锁与乐观锁的实现(详情图解)。  深入理解数据库行锁与表锁。  MySQL 当中的各种锁（中级篇）。  MySQL 中的锁（表锁、行锁）。  13.3.6 LOCK TABLES and UNLOCK TABLES Statements。  详解 MySql InnoDB 中意向锁的作用。  mysql-行锁 + 间隙锁（next-key lock）。  mysql-行锁+间隙锁（next-key lock）。  In-depth understanding of mysql\u0026ndash;gap locks, Next-Key Locks.  14.7.4 Phantom Rows.  "},{"id":68,"href":"/school-recruitment/docs/database/3Redis/3.1-%E6%A6%82%E8%BF%B0/3.1.2-Redis%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","title":"3.1.2 Redis数据类型","section":"3.1 概述","content":" 如无特殊说明，本文内容针对的 Redis 版本为 3.0.0，源码下载地址为 redis-3.0.0。\n 1 前言 #    Redis 的 key 是顶层模型，他的 value 是扁平化的，所有的 value 都是一个 object，它的结构如下：\ntypedef struct redisObject { unsigned [type] 4; unsigned [encoding] 4; unsigned [lru] REDIS_LRU_BITS; int refcount; void *ptr; } robj;   上面的几个字段的意义如下：\n type：数据类型，就是我们熟悉的string、hash、list 等。 encoding：内部编码，其实就是数据结构，指的是当前这个 value 底层是用的什么数据结构，因为同一个数据类型底层也有多种数据结构的实现，所以这里需要指定数据结构。 REDIS_LRU_BITS：当前对象可以保留的时长。 refcount：对象引用计数，用于 GC。 ptr：指针，指向以 encoding 的方式实现这个对象的实际地址。    整个 Redis 的数据结构组织如下图所示：\n   2 数据类型 #  Redis 中主要有 5 种数据类型，分别是字符串（String）、列表（List）、集合（Set）、有序集合（ZSet）、哈希（Hash）。\n2.1 字符串 #  2.1.1 存储数据 #   可以用于存储字符串、整数、浮点数、JSON、XML、二进制等，最大不能超过 512M，是可变的动态字符串。  2.1.2 使用场景 #   计数器：可以利用 Redis 的incr、decr 等命令实现计数。 共享 Session：Redis 的key 可以方便地设置过期时间，用于实现session key 的自动过期，验证时先根据uid 路由到对应的 Redis，如取不到session key，说明session key 已过期，需要重新登录，如取到session key，则校验通过，升级此session key 的过期时间即可。 限流限速。 实现分布式锁。  2.1.3 数据结构 #   在 Redis 内部，String 类型有两种底层存储结构，Redis 会根据存储的数据和用户的操作指令自动选择合适的结构：  int：存放整数类型。 SDS：Simple Dynamic String，简单动态字符串，存放浮点、字符串、字节类型。    2.1.4 SDS #  2.1.4.1 前言 #    Redis没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串），而是自己构建了一种名为简单动态字符串（Simple Dynamic String, SDS）的抽象类型，并将 SDS 作为 Redis 的默认字符串表示。\n  在 Redis 里，C 字符串只会作为字符串字面量用在一些无需对字符串进行修改的地方，比如打印日志：\nredisLog(REDIS_WARNING, \u0026#34;Redis is now ready to exit, bye bye...\u0026#34;);   当 Redis 需要的不仅仅是一个字符串字面量，而是一个可以被修改的字符串值时，Redis 就会使用 SDS 来表示字符串值，比如在 Redis 的数据库里面，包含字符串值的键值对在底层都是由 SDS 实现的，例如：\nredis\u0026gt; SET msg \u0026#34;hello world\u0026#34; OK 那么 Redis 将在数据库中创建一个新的键值对，其中：\n 键值对的键是一个字符串对象，对象的底层实现是一个保存着字符串 msg 的 SDS。 键值对的值也是一个字符串对象，对象的底层实现是一个保存着字符串 hello world 的 SDS。    除了用来保存数据库中的字符串值之外，SDS 还被用作缓冲区，其中AOF 模块中的 AOF 缓冲区，以及客户端状态中的输入缓冲区，都是 SDS 实现的。\n  2.1.4.2 定义 #    SDS 的定义在源码中的 src/sds.h/sdshdr 中，其结构为：\nstruct sdshdr { unsigned int len; /*记录 buf 数组中已使用字节的数量，等于 SDS 所保存字符串的长度*/ unsigned int free; /*记录 buf 数组中未使用字节的数量*/ char buf[]; /*字符数组，用于保存字符串*/ };   结合下面的实例对上面定义中每个参数的具体含义说明如下：\n  len：记录buf 数组中已使用字节的数量，等于 SDS 所保存字符串的长度，实例中len 的值为 5，表示这个 SDS 保存了一个 5 字节长的字符串。 free：记录buf 数组中未使用字节的数量，实例中free 的值为 0，表示这个 SDS 没有分配任何未使用的空间。 buf：字符数组，用于保存字符串，实例中数组的前 5 个字节分别保存了R、e、d、i、s，最后一个字节保存了空字符\\0。    SDS遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间，不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作，都是由 SDS 函数自动完成，所以这个空字符对于 SDS 的使用者来说是完全透明的，遵循空字符结尾这一惯例的好处是SDS 可以直接重用一部分 C 字符串函数库里面的函数。\n  2.1.4.3 SDS 与 C 字符串的区别 #  根据传统，C 语言使用长度为 $N+1$ 的字符数组来表示长度为 $N$ 的字符串，并且字符数组的最后一个元素总是字符串 \\0，这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率以及功能方面的要求，下面将详细对比 C 字符串和 SDS 之间的区别，并说明 SDS 比 C 字符串更适用于 Redis 的原因。\n2.1.4.3.1 常数复杂度获取字符串长度 #   因为C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 $O(N)$。 和 C 字符串不同，因为SDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 $O(1)$。 设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 时无须进行任何手动修改长度的工作。 通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 $O(N)$ 降低到了 $O(1)$，这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。  2.1.4.3.2 杜绝缓冲区溢出 #    除了获取字符串长度的复杂度高外，C 字符串不记录自身长度带来的另外一个问题是容易造成缓冲区溢出，例如 \u0026lt;string.h\u0026gt;/strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾：\nchar *strcat(char *dest, const char *src);   因为 C 字符串不记录自身的长度，所以 strcat假定用户在执行这个函数时，已经为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。\n  与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：\n 当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现缓冲区溢出问题。    2.1.4.3.3 减少修改字符串时带来的内存重分配次数 #   因为 C 字符串并不记录自身的长度，因此每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作。 因为内存重分配设计复杂的算法，并且可能需要执行系统调用，所以他通常是一个比较耗时的操作。 在一般的程序中，如果每次修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的，但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁发生的话，可能还会对性能造成影响。 为了避免 C 字符串的这种缺陷，SDS 通过未使用空间实现了空间预分配和惰性空间释放两种优化策略：  空间预分配：  空间预分配用于优化 SDS 的字符串增长操作，当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必要的空间，还会为 SDS 分配额外的未使用空间，分配的未使用空间数量由以下公式决定：  如果对 SDS 进行修改之后，SDS 的长度（也即是len 属性的值）小于 1MB，那么程序分配和 len 属性同样大小的未使用空间，这时 SDS 的len属性的值将和 free 属性的值相同，例如，如果进行修改之后，SDS 的len 将变成 13 字节，那么程序也会分配 13 字节的未使用空间，SDS 的buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的 1 字节用于保存空字符）。 如果对 SDS 进行修改之后，SDS 的长度大于 1MB，那么程序会分配 1MB 的未使用空间，例如，如果进行修改之后，SDS 的len 将变成 30MB，那么程序会分配 1MB 的未使用空间，SDS 的buf 数组的实际长度将为 30MB + 1MB + 1byte。   在扩展 SDS 之前，SDS API会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无需执行内存重分配。 通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。   惰性空间释放：  惰性空间释放用于优化 SDS 的字符串缩短操作，当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来使用。 通过惰性空间释放策略，SDS避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。 同时，SDS 也提供了相应的 API，让我们可以在有需要时，真正地释放 SDS 的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。 例如：   sdstrim 函数接受一个 SDS 和一个 C 字符串作为参数，移除 SDS 中所有在 C 字符串中出现过的字符。\n  假如有一个 SDS 值 s 为 XYXaYYbcXYX，如下图所示：\n   当执行完 sdstrim(s, \u0026quot;XY\u0026quot;)（移除 SDS 字符串中的所有 X 和 Y）命令后，SDS 被修改为 abc，如下图所示：\n   此时 SDS 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。\n  例如，现在对 s 执行 sdscat(s, \u0026quot; Redis\u0026quot;)，执行完成之后，s 被修改为 abc Redis，如下图所示：\n   完成这次 sdscat 操作将不需要执行内存重分配，因为 SDS 里面预留的 8 字节空间已经足以拼接 6 个字节长的  Redis。\n        2.1.4.3.4 二进制安全 #   C 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的，所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据，程序不会对其中的数据做任何限制、过滤、假设，数据在写入时是什么样的，他被读取时就是什么样的，这也是我们将 SDS 的buf 属性称为字节数组的原因，因为Redis 不是用这个数组来保存字符，而是用来保存一系列二进制数据。  2.1.3.4.5 兼容部分 C 字符串函数 #   虽然 SDS 的 API 都是二进制安全的，但他们一样遵循 C 字符串以空字符结尾的惯例，这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 buf 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 \u0026lt;string.h\u0026gt; 库定义的函数。  2.2 列表 #  2.2.1 存储数据 #   列表可以有序地存储多个字符串，并且列表里的元素是可以重复的，可以对列表的两端进行插入或者弹出元素操作。  2.2.2 使用场景 #   消息队列。  2.2.3 数据结构 #   在 Redis 中，List 底层有两种数据结构，分别为链表（LinkedList）和压缩列表（ZipList）：  当List元素个数少且元素内容长度不大时，使用ZipList实现。 否则，使用LinkedList实现。    2.2.4 链表 #  2.2.4.1 前言 #   链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。 作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为 Redis 使用的 C 语言并没有内置这种数据结构，所以Redis 构建了自己的链表实现。 链表在 Redis 中的应用非常广泛：  列表键的底层实现之一就是链表，当一个链表键包含了数量比较多的元素，或者列表中包含的元素都是比较长的字符串时，Redis 就会使用链表作为列表键的底层实现。 同时，发布与订阅、慢查询、监视器等功能也用到了链表。 Redis 服务器本身该使用链表保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区。    2.2.4.2 实现 #  2.2.4.2.1 链表节点 #    每个链表节点使用一个 src/adlist.h/listNode 结构来表示，具体如下：\ntypedef struct listNode { struct listNode *prev; /*前置节点*/ struct listNode *next; /*后置节点*/ void *value; /*节点的值*/ } listNode;   多个 listNode 可以通过 prev 和 next 指针组成双链表，如下图所示：\n   2.2.4.2.2 链表 #    虽然使用多个 listNode 结构就可以组成链表，但使用 src/adlist.h/list 来持有链表的话，操作起来会更方便，具体结构如下：\ntypedef struct list { listNode *head; /*表头结点*/ listNode *tail; /*表尾节点*/ void *(*dup)(void *ptr); /*节点值复制函数*/ void (*free)(void *ptr); /*节点值释放函数*/ int (*match)(void *ptr, void *key); /*节点值对比函数*/ unsigned long len; /*链表所包含的节点数量*/ } list;   list 结构为链表提供了表头指针 head、表尾指针 tail、以及链表长度计数器 len，还提供了 dup、free、和 match 成员用于实现多态链表所需的类型特定函数，具体如下：\n dup：节点值复制函数。 free：节点值释放函数。 match：节点值对比函数。    list 可以和 listNode 结合组成链表，如下图所示：\n   Redis 的链表具有如下特性：\n 双端：链表节点都带有 prev 和 next 指针，获取某个前置节点和后置节点的复杂度都是 $O(1)$。 无环：表头结点的 prev 指针和表尾节点的 next 指针都指向 NULL，对链表的访问以 NULL 为终点。 带表头指针和表尾指针：通过list 的head 指针和tail 指针，程序获取链表的头结点和尾节点的复杂度为 $O(1)$。 带链表长度计算器：程序使用list 结构的len 属性来对list 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 $O(1)$。 多态：链表节点使用 void* 指针来保存节点值，并且可以通过 list 结构的 dup、free、match三个属性为节点值设置类型特定函数，所以链表可以用于保存不同类型的值。    2.2.5 压缩列表 #  2.2.5.1 前言 #   压缩列表是列表键和哈希键的底层实现之一。 当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做列表键的底层实现。 当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做哈希键的底层实现。  2.2.5.2 实现 #    压缩列表是 Redis为了节约内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构。\n  一个压缩列表可以包含任意多个节点，每个节点可以保存一个字节数组或者一个整数值。\n  压缩列表的结构如下图所示：\n  zlbytes：uint32_t 类型，大小为 4 字节，主要用于记录整个压缩列表占用的内存字节数，在对压缩列表进行内存重分配或者计算 zlend 的位置时使用。 zltail：uint32_t 类型，大小为 4 字节，主要用于记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过这个偏移量，程序无须遍历整个压缩列表就可以确定表尾节点的地址。 zllen：uint16_t 类型，大小为 2 字节，主要用于记录压缩列表包含的节点数量，当这个属性值小于 UINT_MAX（65535）时，这个属性的值就是压缩列表包含节点的数量，当这个值等于 UINT16_MAX 时，节点的真实数量需要遍历整个列表才能计算得出。 entryX：列表节点，长度不定，指压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend：uint_8t，大小为 1 字节，使用特殊值0xFF（十进制 255），用于标记压缩列表的末端。    压缩列表的示例如下：\n  列表zlbytes 的值为0x50（十进制 80），表示压缩列表的总长为 80 字节。 列表zltail 的值为0x3c（十进制 60），表示如果我们有一个指向压缩列表起始地址的指针p，那么只要用指针p 加上偏移量 60，就可以计算出表尾节点entry3 的地址。 列表zllen 属性的值为0x3（十进制 3），表示压缩列表包含三个节点。    2.3 哈希 #  2.3.1 存储数据 #   Hash 存的是字符串和字符串值之间的映射。 Hash 将对象的各个属性存入Map 里，可以只读取或更新对象的某些属性。  2.3.2 使用场景 #   存放结构化数据，比如用户信息，key 是用户ID，value 是一个Map，这个Map 的key 是成员的属性名，value 是属性值，这样对数据的修改和存取都可以直接通过其内部的key 来实现，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。 建索引，比如User 对象，除了id 有时还要按name 来查询，可以建一个key 为user:name:id 的Hash，在插入User 对象（set user:101{\u0026quot;id\u0026quot;:101,\u0026quot;name\u0026quot;:\u0026quot;calvin\u0026quot;}）时，顺便往这个Hash 插入一条hset user:name:id calvin 101，这时calvin 作为Hash 里的一个key，值为 101，按name 查询的时候，用hgetuser:name:id calvin 就能从名为calvin 的key 里取出id。  2.3.3 数据结构 #   Hash 底层有两种实现，分别是压缩列表和字典：  当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做哈希键的底层实现。 但一个哈希键包含的键值对比较多，或者键值对中的元素都是比较长的字符串时，Redis 就会使用字典作为哈希键的底层实现。   压缩链表的内容上面已经叙述过，具体可参考 2.2.5 压缩列表，下面将主要叙述字典相关的内容。  2.3.4 字典 #  2.3.4.1 前言 #   字典是一种用于保存键值对的抽象数据结构。 字典经常作为一种数据结构内置在很多高级编程语言里面，但 Redis 所使用的的 C 语言并没有内置这种数据结构，因此 Redis构建了自己的字典实现。 字典在 Redis 中的应用非常广泛：  Redis 的数据库就是使用字典来作为底层实现的，对数据库的增、删、改、查操作都是构建在对字典的操作之上的：   例如，当我们执行如下命令：\nredis\u0026gt; SET msg \u0026#34;hello world\u0026#34; OK   此时，就会在数据库中创建一个键为 msg，值为 hello world 的键值对，这个键值对就是保存在代表数据库的字典里面的。\n   字典也是哈希键的底层实现之一，当一个哈希键包含的键值对比较多，或者键值对中的元素都是比较长的字符串时，Redis 就会使用字典作为哈希键的底层实现：   例如，wensite 是一个包含 10086 个键值对的哈希键，这个哈希键的键都是一些数据库的名字，而键的值就是数据库的主页网址：\nredis\u0026gt; HLEN website (integer) 10086 redis\u0026gt; HGETALL website 1)\u0026quot;Redis\u0026quot; 2)\u0026quot;Redis.io\u0026quot; 3)\u0026quot;MariaDB\u0026quot; 4)\u0026quot;MariaDB.org\u0026quot; 5)\u0026quot;MongoDB\u0026quot; 6)\u0026quot;MongoDB.org\u0026quot; # ...   website 的底层实现就是一个字典，字典中包含了 10086 个键值对，例如：\n 键值对的键为Redis，值为Redis.io。 键值对的键为MariaDB，值为MariaDB.org。 键值对的键为MongoDB，值为MongoDB.org。        2.3.4.2 实现 #   Redis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。\n 2.3.4.2.1 哈希表 #    Redis 字典所使用的的哈希表由 src/dict.h/dictht 定义：\ntypedef struct dictht { dictEntry **table; /*哈希表数组*/ unsigned long size; /*哈希表大小*/ unsigned long sizemask; /*哈希表大小掩码，用于计算索引值，总是等于 size - 1*/ unsigned long used; /*该哈希表已有节点的数量*/ } dictht;  table：表示哈希表数组，数组中的每个元素都是一个指向 src/dict.h/dictEntry 结构的指针，每个 dictEntry 结构保存着一个键值对。 size：表示哈希表的大小，也即是哈希表数组的大小。 used：表示哈希表已有节点的数量。 sizemask：表示哈希表大小掩码，总是等于 size - 1，这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。    哈希表的示例如下图所示：\n   2.3.4.2.2 哈希表节点 #    哈希表节点使用 dictEntry 结构表示，每个 dictEntry 结构都保存着一个键值对：\ntypedef struct dictEntry { void *key; /*键*/ union { /*值*/ void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; /*指向下个哈希表节点，形成链表*/ } dictEntry;  key：表示键值对中的键。 v：表示键值对中的值，其中键值对的值可以是一个指针、uint64_t的整数、int64_t的整数、双精度类型的浮点数。 next：表示指向下个哈希节点的指针，这个指针可以将多个哈希值相同的键值对连接在一起，以此来解决键冲突的问题：   如下图所示就是将两个索引值相同的键 k1 和 k0 连接在一起：\n       2.3.4.2.3 字典 #    Redis 中的字典由 src/dict.h/dict 结构表示：\ntypedef struct dict { dictType *type; /*类型特定函数*/ void *privdata; /*私有数据*/ dictht ht[2]; /*哈希表*/ long rehashidx; /* rehash 索引，当 rehash 不再进行时，值为 1 rehashing not in progress if rehashidx == -1 */ int iterators; /* 当前正在运行的迭代器数量 number of iterators currently running */ } dict;   type：指向 dictType 结构的指针，每个 dictType 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。\n  privdata：保存了需要传给那些类型特定函数的可选参数：\ntypedef struct dictType { unsigned int (*hashFunction)(const void *key); /*计算哈希值的函数*/ void *(*keyDup)(void *privdata, const void *key); /*复制键的函数*/ void *(*valDup)(void *privdata, const void *obj); /*复制值的函数*/ int (*keyCompare)(void *privdata, const void *key1, const void *key2); /*对比键的函数*/ void (*keyDestructor)(void *privdata, void *key); /*销毁键的函数*/ void (*valDestructor)(void *privdata, void *obj); /*销毁值的函数*/ } dictType;  type 属性和 privdata 属性是针对不同类型的键值对，为创建多态字典而设置的。\n   ht：是一个包含两个项的数组，数组中的每个项都是一个 dictht 哈希表，一般情况下，字典只使用 ht[0] 哈希表，ht[1]哈希表只会在对 ht[0] 进行 rehash 时使用。\n  rehashidx：记录了 rehash 目前的进度，如果目前没有在进行 hash，那么他的值为-1。\n    下图展示了一个普通状态下（没有进行 rehash）的字典：\n   2.3.4.2.4 哈希算法 #    当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键值计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希数组指定的索引上面。\n  哈希值和索引值的计算方法如下：\n// 使用字典设置的哈希函数，计算键 key 的哈希值 hash = dict-\u0026gt;type-\u0026gt;hashFunction(key); // 使用哈希表的 sizemask 属性和哈希值，计算出索引值 // 根据情况不同，ht[x] 可以是 ht[0] 或是 ht[1] index = hash \u0026amp; dict-\u0026gt;ht[x].sizemask;   当字典被用作数据库的底层实现，或者哈希键的底层实现，Redis使用 MurmurHash2 算法来计算键值的哈希值，该算法的优点是即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。\n  2.3.4.2.5 解决键值冲突 #    Redis 的哈希表使用链地址法来解决键值冲突问题，每个哈希表节点都有一个 next 指针，多个哈希表节点可以构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来，这就解决了键值冲突问题。\n  因为 dictEntry节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置，排在其它已有节点的前面。\n 常见的解决键值冲突的方法主要有以下几种：\n 开放定址法：   也叫再散列法，当关键字 $key$ 的哈希地址 $p = H(key)$ 出现冲突时，以 $p$ 为基础，产生另一个哈希地址 $p_1$，如果 $p_1$ 仍然冲突，再以 $p_1$ 为基础产生另一个哈希地址 $p_2$，\u0026hellip;，直到找出一个不冲突的哈希地址 $p_i$，将相应程序存入其中。\n  通常都是用以下公式计算：\n$$ H(i) = (H(key) + d_i) % m \\space i = 1, 2,\u0026hellip;, n $$\n 其中 $H(key)$ 为哈希函数，$m$ 为表长，$d_i$ 为增量序列，增量序列的取值方式不同，相应的再散列方式也不同，主要有三种：  线性探测再散列：冲突发生时，顺序查看表中下一单元，直到找出一个空单元或查遍全表。 二次探测再散列：冲突发生时，在表的右边进行跳跃式探测，直到找到空单元。 伪随机探测再散列。       链地址法：  这种方法的基本思想是将所有哈希地址为 $i$ 的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第 $i$ 个单元中，因而查找、插入和删除主要在同义词链中进行。 链地址法适用于经常进行插入和删除的情况，例如HashMap 就是利用这种方法解决 Hash 冲突的。   再哈希法：  多写几个哈希函数，算出来一个 hashcode 重复的就用另一个哈希函数算，直到算出来不一样。   建立一个公共溢出区域，就是把冲突的都放在另一个地方，不在表里面。     2.3.4.2.6 rehash #   负载因子 = 哈希表已经保存节点数量 / 哈希表大小\n   随着操作的不断执行，哈希表保存的键值对会逐渐地增多或减少，为了让哈希表的负载因子维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或收缩。\n  扩展或收缩哈希表的工作可以通过 rehash（重新散列）操作来完成，具体步骤如下：\n 为字典 ht[1] 哈希表分配空间，这个哈希表空间的大小取决于要执行的操作，以及 ht[0] 当前包含的键值对的数量（也即是ht[0].used 属性的值）：  如果执行的是扩展操作，那么ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 $ 2^n $。 如果执行的是收缩操作，那么ht[1] 的大小为第一个大于等于 ht[0].used 的 $ 2^n $。   将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面，rehash 指的是重新计算键的哈希值和索引值，然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对迁移到 ht[1] 之后（ht[0] 变为空表），释放 ht[0]，将 ht[1] 设置为 ht[0]，并在 ht[1] 新建一个空白哈希表，为下一次 rehash 做准备。    具体的 rehash 例子如下：\n      2.3.4.2.7 渐进式 rehash #    扩展和收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面，但是，这个 rehash动作不是一次性、集中式地完成的，而是分多次、渐进式地完成的。\n  因为 Redis 是单进程的，如果哈希表中保存的键值对数量过多的话，当一次性将所有键值对全部 rehash 到 ht[1] 的话，庞大的计算量可能会导致服务器在一段时间内停止服务。\n  哈希表渐进式 rehash 的详细步骤如下：\n 为 ht[1] 分配空间，让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量，并将他的值设置为 0，表示 rehash 工作正式开始。 在rehash进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作外，还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1]，当rehash工作完成之后，程序将 rehashidx 增 1。 随着字典操作的不断执行，最终在某个时间点上，ht[0]的所有键值对都会被 rehash 至 ht[1]，这时程序将 rehashidx 属性的值设为-1，表示rehash操作已完成。    渐进式 rehash 的好处在于他采取分而治之的方式，将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 而带来的庞大的计算量。\n  渐进式 rehash 执行期间的哈希表操作：\n 字典的添加、删除、查找或者更新操作会在两个哈希表上进行：  在字典里面查找一个键的话，程序会先在 ht[0] 里面进行查找，如果没找到的话，就会继续到 ht[1] 里面进行查找。   新添加到字典里面的键值对一律会被保存到 ht[1] 里面，而ht[0]则不进行任何添加操作，这一措施保证了 ht[0] 包含的键值对数量会只减不增，并随着 rehash 操作的执行而最终变成空表。    具体的渐进式 rehash 例子如下：\n        2.4 集合 #  2.4.1 存储数据 #   是一种无序的集合，集合中的元素没有先后顺序。  2.4.2 使用场景 #   某些需要去重的列表，并且Set 里面提供了判断某个成员是否在 Set 集合内的重要接口，这个也是List 所不能提供的。 存储一些集合性的数据，比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在另一个集合中，因为 Redis 为集合提供了求交集、并集、差集等操作，可以非常方便的实现共同关注、共同喜好、二度好友等功能。  2.4.3 数据结构 #   Set 底层有两种实现，分别是整数集合和字典：  当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集合键的底层实现。 当一个集合不止包含整数值元素，还包含其它类型元素时，或者这个集合的元素数量很大时，Redis 就会使用字典作为集合键的底层实现。   字典的内容上面已经叙述过，具体可参考 2.3.4 字典。  2.5 有序集合 #  2.5.1 存储数据 #   元素放入集合时还要提供该元素的分数，可根据分数自动排序。  2.5.2 使用场景 #   存放一个有序且不重复的集合列表，比如Twitter 的public timeline 可以以发表时间作为score 来存储，这样获取时就是自动按时间排好序的。 可以做带权重的队列，比如普通消息的score 为 1，重要消息的score 为 2，然后工作线程可以按score 的倒序来获取工作任务，让重要的任务优先执行。 排行榜。  2.5.3 数据结构 #   ZSet 底层有两种实现，分别是压缩列表和跳表：  当一个有序集合包含的元素数量不多，并且有序集合中元素的成员的长度不大时，Redis 就会使用压缩列表作为有序集合键的底层实现：  每个集合元素都使用两个紧挨在一起的压缩列表结点来保存，第一个节点保存元素的成员（member），第二个元素保存元素的分值（score）。 压缩列表内的集合按分值从小到大进行排序，分值较小的元素被放置在靠近表头的方向，分值较大的元素放置在靠近表尾的方向。 例如：   执行以下命令：\nredis\u0026gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry   元素在压缩列表中的保存方式如下图所示：\n      当一个有序集合中元素数量比较多，或者有序集合中元素的成员是比较长的字符串时，Redis 就会使用跳跃链表作为有序集合键的底层实现。   压缩列表的内容上面已经叙述过，具体可参考 2.2.5 压缩列表，下面将主要叙述跳表相关的内容。  2.5.4 跳表 #  2.5.4.1 前言 #   跳表是一种有序数据结构，他通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。 跳表支持平均 $O(logN)$、最坏 $O(N)$ 复杂度的节点查找，还可以通过顺序性操作来批处理节点。 Redis使用跳表作为有序集合键的底层实现之一，如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员是比较长的字符串时，Redis 就会使用跳表来作为有序集合键的底层实现。  2.5.4.2 实现 #    Redis 的跳表由 src/redis.h/zskiplistNode 和 src/redis.h/zskiplist 两个结构定义，其中 zskiplistNode结构用于表示跳表节点，而 zskiplist结构则用于保存跳表节点的相关信息，比如节点的数量、指向表头结点和表尾节点的指针等。\n  跳表的结构如下图所示：\n   位于图片最左边的是 zskiplist 结构，该结构包含以下属性：\n header：指向跳表的表头结点。 tail：指向跳表的表尾节点。 level：记录目前跳表内，层数最大的那个节点的层数（表头结点的层数不计算在内）。 length：记录跳表的长度，也即是，跳表目前包含节点的数量（表头结点不计算在内）。    位于 zskiplist 结构右方的是四个 zskiplistNode 结构，该结构包含以下属性：\n level：节点中用 $L1$、$L2$、$L3$等字样标记节点的各个层，$L1$代表第一层，$L2$代表第二层，以此类推，每个层都带有两个属性，分别是前进指针和跨度，前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离，在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度，当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 backward：节点中用 BW 字样标记节点的后退指针，他指向位于当前节点的前一个节点，后退指针在程序从表尾向表头遍历时使用。 score：各个节点中的 1.0、2.0 和 3.0 是节点所保存的分值，在跳表中，节点按各自保存的分值从小到大排列。 obj：各个节点中的 $o1$、$o2$、$o3$是节点所保存的成员对象。   需要注意的是表头结点和其他节点的构造是一样的，也有后退指针、分值和成员对象，不过表头结点的这些属性都不会被用到，所以图中省略了这些部分，只显示了表头结点的各个层。\n     2.5.4.2.1 跳表节点 #    跳表节点的实现由 src/redis.h/zskiplistNode 结构定义：\ntypedef struct zskiplistNode { robj *obj; /*成员对象*/ double score; /*分值*/ struct zskiplistNode *backward; /*后退指针*/ struct zskiplistLevel { /*层*/ struct zskiplistNode *forward; /*前进指针*/ unsigned int span; /*跨度*/ } level[]; } zskiplistNode;   层：\n  跳表节点的 level数组可以包含多个元素，每一个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。\n  每次创建一个新跳表节点的时候，程序都根据幂次定律（越大的数出现的概率越小）随机生成一个介于 1 和 32 之间的值作为 level 数组的大小，这个大小就是层的高度。\n  下图分别展示了三个高度为 1 层、3 层和 5 层的节点：\n     前进指针：\n 每个层都有一个指向表尾方向的前进指针，用于从表头向表尾方向访问节点，下图用虚线表示出了程序从表头向表尾方向，遍历跳表中所有节点的路径：  迭代程序首先访问跳表的第一个节点（表头），然后从第四层的前进指针移动到表中的第二个节点。 在第二个节点时，程序沿着第二层的前进指针移动到表中的第三个节点。 在第三个节点时，程序同样沿着第二层的前进指针移动到表中的第四个节点。 当程序再次沿着第四个节点的前进指针移动时，他碰到了一个 NULL，程序知道这时已经到达了跳表的表尾，于是结束了这次遍历。      跨度：\n 层的跨度用于记录两个节点之间的距离：  两个节点之间的跨度越大，他们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0，因为他们没有连向任何节点。   跨度主要用来计算排位（Rank）的，在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位，例如：   下图用虚线标记了在跳跃表中查找分值为 3.0、成员对象为 $o3$ 的节点时沿途经历的层，并且层的跨度为 3，所以目标节点在跳跃表中的排位为 3。\n   下图用虚线标记了在跳跃表中查找分值为 2.0、成员对象为 $o2$ 的节点时沿途经历的层，在查找节点的过程中，程序经过了两个跨度为 1 的节点，因此可以计算出，目标节点在跳跃表中的排位为 2。\n       后退指针：\n  节点的后退指针用于从表尾向表头方向访问节点，与可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。\n  下图用虚线展示了如何从表尾向表头遍历跳表中的所有节点，程序首先通过跳表的 tail 指针访问表尾节点，然后通过后退指针访问倒数第二个节点，之后再沿着后退指针访问倒数第三个节点，再之后遇到指向 NULL 的后退指针，于是访问结束。\n     分值和成员：\n  节点的分值是一个 double 类型的浮点数，跳表中的所有节点都按分值从小到大来排序。\n  节点的成员是一个指针，他指向一个字符串对象，而字符串对象则保存着一个 SDS 值。\n  在同一个跳表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却是可以相同的，分值相同的节点将按照成员对象在字典中的大小来进行排序，成员对象较小的节点会排在前面（靠近表头的方向），而成员对象较大的节点则会排在后面（靠近表尾的方向）。\n  如下图所示，三个跳表节点都保存了相同的分值 10086.0，保存成员对象 $o1$ 的节点排在保存成员对象 $o2$ 和 $o3$ 节点之前，而保存成员对象 $o2$ 的节点又排在保存成员对象 $o3$ 的节点之前，由此可见，$o1$、$o2$、$o3$ 三个成员对象在字典中的排序为 $o1 \\lt o2 \\lt o3$。\n     2.5.4.2.2 跳表 #    仅靠多个跳表节点就可以组成一个跳表，如下图所示：\n   但通过使用一个 zskiplist 结构来持有这些节点，程序可以更方便地对整个跳表进行处理，比如快速访问跳表的表头结点和表尾节点，或者快速地获取跳表节点的数量（即跳表的长度）等信息，如下图所示：\n   zskiplist 的结构定义如下：\ntypedef struct zskiplist { struct zskiplistNode *header, *tail; /*表头结点和表尾节点*/ unsigned long length; /*表中节点的数量*/ int level; /*表中层数最大的节点的层数*/ } zskiplist;   2.5.4.3 跳表与平衡树、哈希表的比较 #   跳表和各种平衡树（如 AVL、红黑树）的元素是有序排列的，而哈希表不是有序的，因此，在哈希表上只能做单个 key 的查找，不适宜做范围查找，所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比跳表操作要复杂，在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其他不超过大值的节点，如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现，而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第一层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻接点的指针，操作简单又快速。 从内存占用上来说，跳表比平衡树更灵活一些，一般来说，平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 $\\frac{1}{1-p}$，具体取决于参数 $p$，如果向 Redis 里的实现一样，取 $p = \\frac{1}{4}$，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。 查找单个 key，跳表和平衡树的时间复杂度都为 $O(logn)$，大体相当，而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近 $O(1)$，性能更高一些，所以我们平常使用的各种Map 或dictionary 结构，大都是基于哈希表实现的。 从算法实现难度上来比较，跳表比平衡树要简单的多。  2.5.4.4 Redis 为什么使用跳表而不用平衡树作为集合的实现 #  可以参考 2.5.4.3 跳表与平衡树、哈希表的比较，然后从内存占用和算法复杂度两个角度来分析。\n参考文献 #    几率大的 Redis 面试题（含答案）。  Redis 面试题（总结最全面的面试题）。  Redis 基本类型及其数据结构。  Redis 数据类型及使用场景。 redis 设计与实现（第二版）。  Redis 为什么用跳表而不用平衡树？  Hash 冲突的几种解决方法。  "},{"id":69,"href":"/school-recruitment/docs/java/3JVM/3.2-%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E5%AD%98%E6%B4%BB%E7%9A%84%E6%96%B9%E6%B3%95/","title":"3.2 判断对象是否存活的方法","section":"3、 Jvm","content":"判断对象是否存活的方法有两种，分别是 引用计数法、可达性分析算法。\n1. 引用计数法 #    引用计数法是指给对象添加一个引用计数器，每当有一个地方 引用它时，计数器值就加 1，当引用失效时，计数器值就减 1，任何时刻计数器为 0 的对象就是不可能再被使用的。\n  缺点是它很难解决对象之间相互循环引用的问题。举个简单的例子，对象 objA 和 obB 都有字段 Instance,赋值令 obja instance=objB 及 obiB. Instance=ojA，除此之外，这两个对象再无任何引用,实际上这两个对象已经不可能再被访问,但是它们因为互相引用着对方，导致它们的引用计数都不为 0，于是引用计数算法无法通知 GC 收集器回收它们，具体代码如下：\n/** * @author peng.wei * @version 1.0 * @date 2021/9/3 16:17 * @Description 引用计数法 GC 测试 */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 这个成员属性的唯一意义就是占点内存，以便能在 GC 日志中看清楚是否有回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void main(String[] args) { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; // 假设在这行发生 GC，看一下 objA 和 objB 是否能被回收  System.gc(); } } 运行结果：\n[GC (System.gc()) [PSYoungGen: 7449K-\u0026gt;679K(38400K)] 7449K-\u0026gt;687K(125952K), 0.0019256 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 679K-\u0026gt;0K(38400K)] [ParOldGen: 8K-\u0026gt;577K(87552K)] 687K-\u0026gt;577K(125952K), [Metaspace: 3178K-\u0026gt;3178K(1056768K)], 0.0082725 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] Heap PSYoungGen total 38400K, used 998K [0x0000000795580000, 0x0000000798000000, 0x00000007c0000000) eden space 33280K, 3% used [0x0000000795580000,0x0000000795679b20,0x0000000797600000) from space 5120K, 0% used [0x0000000797600000,0x0000000797600000,0x0000000797b00000) to space 5120K, 0% used [0x0000000797b00000,0x0000000797b00000,0x0000000798000000) ParOldGen total 87552K, used 577K [0x0000000740000000, 0x0000000745580000, 0x0000000795580000) object space 87552K, 0% used [0x0000000740000000,0x0000000740090418,0x0000000745580000) Metaspace used 3185K, capacity 4556K, committed 4864K, reserved 1056768K class space used 339K, capacity 392K, committed 512K, reserved 1048576K 从运行结果中可以清除看到内存回收日志中包含「7449K -\u0026gt; 687K」，意味着虚拟机并没有因为这两个对象相互引用就放弃回收他们，这也从侧面说明了Java虚拟机并不是通过引用计数算法来判断对象是否存活的。\n  2. 可达性分析算法 #    主要通过一系列的称为“GC Roots”的对象作为起点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连（即从 GC Roots 到这个对象不可达）时，则证明对象是不可用的。\n  如下图所示，对象 object5、object6、object7 虽然互有关联，但是他们到 GC Roots 是不可达的，因此他们会被判定为可回收的对象。\n   在 Java 技术体系里面，固定可作为 GC Roots 的对下那个包括以下几种：\n 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程中被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在方法区中类静态属性引用的对象，譬如Java 类的引用类型静态变量。 在方法区中类静态属性引用的对象，譬如Java 类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中 JNI（通常所说的 Native 方法）引用的对象。 Java 虚拟机内部的引用，如基本数据类型对应的 Class 对象，一些常驻的异常对象（比如 NullPointerException、OutOfMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized 关键字）持有的对象。 反映 Java 虚拟机内部情况的 JMXBean、JVMT1 中注册的回调、本地代码缓存等。 除了这些固定的 GC Roots 集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象临时性地加入，共同构成完整 GC Roots 集合，比如分代收集和局部回收（Partial GC），如果只针对 Java 堆中某一块区域发起垃圾收集时（如最典型的只针对新生代的垃圾收集），必须考虑到内存区域是虚拟机自己的实现细节（在用户视角里任何内存区域都是不可见的），更不是孤立封闭的，所以某个区域里的对象完全有可能被位于堆中其他区域的对象所引用，这时候就需要将这些关联区域的对象也一并加入 GC Roots 集合中去，才能保证可达性分析的正确性。    即使在可达性分析算法中判定为不可达对象，也不是非死不可的，这时候他们还暂时还处于缓刑阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：\n  如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那他将会被第一次标记。\n  随后进行一次筛选，筛选的条件是此对象是否有必要执行 finalize() 方法，假如对象没有覆盖 finalize() 方法，或者 finalize() 已经被虚拟机调用过，那么虚拟机将这两种情况都视为「没有必要执行」。\n  如果这个对象被判定为「有必要执行」finalize() 方法，那么该对象会被放置在一个名为 F-Queue 的队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的 Finalizer 线程去执行他们的 finalize() 方法，这里所说的执行是指虚拟机会触发这个方法开始运行，但并不承诺一定会等待他运行结束，这样做的原因是，如果某个对象的 finalize() 方法执行缓慢，或者更极端地发生了死循环，将很可能导致 F-Queue 队列中的其他对象永久处于等待，甚至导致整个内存回收子系统的崩溃。\n  finalize()方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对 F-Queue 中的对象进行第二次小规模的标记，如果对象要在 finalize() 中成功拯救自己（只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this 关键字）赋值给某个类变量或者对象的成员变量），那么第二次标记时他将被移出「即将回收」的集合，如果对象这时候还没有逃脱，那基本上他就真的要被回收了，具体的示例代码如下所示：\n/** * @author peng.wei * @version 1.0 * @date 2021/9/3 15:56 * @Description GC 回收对象在 finalize() 方法中逃脱测试类 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() { System.out.println(\u0026#34;Yes, i am still alive.\u0026#34;); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\u0026#34;finalize method executed.\u0026#34;); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次成功拯救自己  SAVE_HOOK = null; System.gc(); // 因为 Finalizer 线程的优先级很低，所以暂停 0.5 秒，等待他一下  Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\u0026#34;No, i am dead.\u0026#34;); } // 第二次拯救自己，却失败了  SAVE_HOOK = null; System.gc(); // 因为 Finalizer 线程的优先级很低，所以暂停 0.5 秒，等待他一下  Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\u0026#34;No, i am dead.\u0026#34;); } } } 运行结果：\nfinalize method executed. Yes, i am still alive. No, i am dead. 从运行结果可以看出：\n SAVE_HOOK对象的 finaliz() 方法确实被垃圾收集器触发过，并且在被收集前成功逃脱了。 另外一个值得注意的地方就是，代码中有两段完全一样的代码片段，执行结果却是一次逃脱成功，一次失败了，这是因为任何一个对象的 finalize() 方法都只会被系统自动调用一次，如果对象面临下一次回收，他的 finalize() 方法不会再被执行，因此第二段代码的自救行动失败了。      参考文献 #   《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第 3 版） - 周志明》   "},{"id":70,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.3-%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","title":"1.1.3 贪心算法","section":"1.1 动态规划","content":"贪心算法 #  1 定义 #  贪心算法可以认为是动态规划算法的一个特例，相比动态规划，使用贪心算法需要满足更多的条件（贪心选择性质），但是效率比动态规划要高。\n贪心选择性质： 简单地说就是每一步做出一个局部最优的选择，最终的结果就是全局最优。需要注意的是，这是一种特殊性质，只有一部分问题拥有这个性质。\n 比如面前放着 100 张人民币，我们只能拿 10 张，怎么才能拿到最多的面额？显然每次选择剩下钞票中面值最大的一张，最后我们的选择一定是最优的。 但是大部分问题明显不具有贪心选择性质，比如斗地主，对手出对儿三，按照贪心策略，我们应该出尽可能小的牌刚好压制住对方，但现实情况我们甚至可能会出王炸。这种情况就不能用贪心算法，而得使用动态规划解决。  2 示例 #  2.1 区间调度问题 #  2.1.1 无重叠区间 #  2.1.1.1 题目 #  给定一个区间的集合，找到需要移除区间的最小数量，使剩余区间互不重叠。\n注意:\n1. 可以认为区间的终点总是大于它的起点。 2. 区间 [1,2] 和 [2,3] 的边界相互“接触”，但没有相互重叠。 示例 1:\n输入: [ [1,2], [2,3], [3,4], [1,3] ] 输出: 1 解释: 移除 [1,3] 后，剩下的区间没有重叠。 示例 2:\n输入: [ [1,2], [1,2], [1,2] ] 输出: 2 解释: 你需要移除两个 [1,2] 来使剩下的区间没有重叠。 示例 3:\n输入: [ [1,2], [2,3] ] 输出: 0 解释: 你不需要移除任何区间，因为它们已经是无重叠的了。 2.1.1.2 问题分析 #  这个问题的实质是需要我们设计一个算法，算出这些区间中最多有几个互不相交的区间。\n这个问题在生活中的应用广泛，比如我们今天有好几个活动，每个活动都可以用区间 [start, end] 表示开始和结束时间，请问我们今天最多能参加几个活动呢？显然我们一个人不能同时参加两个活动，所以说这个问题就是求这些时间区间的最大不相交子集。\n正确的思路其实很简单，可以分为以下三步：\n 从区间集合intvs 中选择一个区间x，这个x 是在当前所有区间中结束最早的（end 最小）。 把所有与x 相交的区间从区间集合intvs 中删除。 重复步骤 1 和 2，直到intvs 为空为止，之前选出的那些x 就是最大不相交子集。  把这个思路实现成算法的话，可以按每个区间的 end 数值升序排序，因为这样处理之后实现步骤 1 和步骤 2 都方便很多。\n 现在来实现算法，对于步骤 1，由于我们预先按照 end 排了序，不难发现所有与 x 相交的区间必然会与 x 的 end 相交，他的 start 必然要大于或等于 x 的 end：\n 2.1.1.3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.Arrays; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/23 21:19 * @Description 无重叠区间 */ public class L435 { /** * 435.无重叠区间 * 给定一个区间的集合，找到需要移除区间的最小数量，使剩余区间互不重叠。 * @param intervals 区间集合 * @return 需要移除区间的最小数量 */ public static int eraseOverlapIntervals(int[][] intervals) { if (intervals.length == 0) {return 0;} Arrays.sort(intervals, (int[] a, int[] b) -\u0026gt; a[1] - b[1]); int count = 1; int x_end = intervals[0][1]; for (int i = 1; i \u0026lt; intervals.length; i++) { if (intervals[i][0] \u0026gt;= x_end) { count++; x_end = intervals[i][1]; } } return intervals.length - count; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[][] intervals = {{1,2}, {2,3}, {3,4}, {1,3}}; int res = eraseOverlapIntervals(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); intervals = new int[][]{{1,2}, {1,2}, {1,2}}; res = eraseOverlapIntervals(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); intervals = new int[][]{{1,2}, {2,3}}; res = eraseOverlapIntervals(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 3 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } 2.1.2 用最少数量的箭引爆气球 #  2.1.2.1 题目 #  在二维空间中有许多球形的气球。对于每个气球，提供的输入是水平方向上，气球直径的开始和结束坐标。由于它是水平的，所以纵坐标并不重要，因此只要知道开始和结束的横坐标就足够了。开始坐标总是小于结束坐标。\n一支弓箭可以沿着 x 轴从不同点完全垂直地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 xstart，xend， 且满足 xstart ≤ x ≤ xend，则该气球会被引爆。可以射出的弓箭的数量没有限制。 弓箭一旦被射出之后，可以无限地前进。我们想找到使得所有气球全部被引爆，所需的弓箭的最小数量。\n给你一个数组 points ，其中 points [i] = [xstart,xend] ，返回引爆所有气球所必须射出的最小弓箭数。 示例 1：\n输入：points = [[10,16],[2,8],[1,6],[7,12]] 输出：2 解释：对于该样例，x = 6 可以射爆 [2,8],[1,6] 两个气球，以及 x = 11 射爆另外两个气球 示例 2：\n输入：points = [[1,2],[3,4],[5,6],[7,8]] 输出：4 示例 3：\n输入：points = [[1,2],[2,3],[3,4],[4,5]] 输出：2 示例 4：\n输入：points = [[1,2]] 输出：1 示例 5：\n输入：points = [[2,3],[2,3]] 输出：1 2.1.2.2 问题分析 #  这个问题和区间调度的算法一样，如果最多有 n 个不重叠的区间，那么就至少需要 n 个箭头穿透所有区间。 只是有点不一样，在 无重叠区间 算法中，如果两个区间的边界触碰，不算重叠，而按照这道题目的描述，箭头如果碰到气球的边界，气球也会爆炸，所以说相当于区间的边界触碰也算重叠。\n 所以只要将之前的算法稍作修改，就是这道题目的答案。\n2.1.2.3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.Arrays; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/24 14:05 * @Description 用最少数量的箭引爆气球 */ public class L452 { /** * 452.用最少数量的箭引爆气球 * 在二维空间中有许多球形的气球。对于每个气球，提供的输入是水平方向上，气球直径的开始和结束坐标。由于它是水平的，所以纵坐标并不重要，因此只要知道开始和结束的横坐标就足够了。开始坐标总是小于结束坐标。 * 一支弓箭可以沿着 x 轴从不同点完全垂直地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 xstart，xend， 且满足 xstart ≤ x ≤ xend，则该气球会被引爆。可以射出的弓箭的数量没有限制。 弓箭一旦被射出之后，可以无限地前进。我们想找到使得所有气球全部被引爆，所需的弓箭的最小数量。 * 给你一个数组 points ，其中 points [i] = [xstart,xend] ，返回引爆所有气球所必须射出的最小弓箭数。 * @param points 气球位置 * @return 引爆气球所需要的箭的最少数量 */ public static int findMinArrowShots(int[][] points) { // 如果没有气球，则返回 0  if (points.length == 0) {return 0;} // 对气球坐标按照 xend 正序排序  Arrays.sort(points, (int[] a, int [] b) -\u0026gt; { // 防止 a[1] - b[1] 越界导致排序不正确  // [[-2147483646,-2147483645],[2147483646,2147483647]]  if (a[1] \u0026gt; b[1]) {return 1;} else {return -1;} }); // 至少需要一个箭  int count = 1; // 遍历计算引爆气球所需的最少数量的箭  int x_end = points[0][1]; for (int i = 1; i \u0026lt; points.length; i++) { int start = points[i][0]; if (start \u0026gt; x_end) { // 找到下一个不相邻的起球了  count++; x_end = points[i][1]; } } return count; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[][] intervals = {{10,16}, {2,8}, {1,6}, {7,12}}; int res = findMinArrowShots(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); intervals = new int[][]{{1,2}, {3,4}, {5,6}, {7,8}}; res = findMinArrowShots(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); intervals = new int[][]{{1,2}, {2,3}, {3,4}, {4,5}}; res = findMinArrowShots(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 3 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); // [[-2147483646,-2147483645],[2147483646,2147483647]]  stopWatch.reset(); stopWatch.start(); intervals = new int[][]{{-2147483646,-2147483645}, {2147483646,2147483647}}; res = findMinArrowShots(intervals); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 3 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } 2.2 跳跃游戏 #  2.2.1 跳跃游戏 1 #  2.2.1.1 题目 #  给定一个非负整数数组 nums ，你最初位于数组的 第一个下标 。\n数组中的每个元素代表你在该位置可以跳跃的最大长度。\n判断你是否能够到达最后一个下标。\n示例 1：\n输入：nums = [2,3,1,1,4] 输出：true 解释：可以先跳 1 步，从下标 0 到达下标 1, 然后再从下标 1 跳 3 步到达最后一个下标。 示例 2：\n输入：nums = [3,2,1,0,4] 输出：false 解释：无论怎样，总会到达下标为 3 的位置。但该下标的最大跳跃长度是 0 ， 所以永远不可能到达最后一个下标。 2.2.1.2 问题分析 #  这题让求的是能否到达最后一个位置，我们先遍历数组的数字，然后保存下来他所能跳到的最大距离，如果能到达最后一个位置，直接返回 true，如果不能到达就继续遍历，如果最大距离连下一步都到不了，就直接返回 false。\n2.2.1.3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/23 17:00 * @Description 跳跃游戏 */ public class L55 { /** * 55.跳跃游戏 * 给定一个非负整数数组 nums ，你最初位于数组的第一个下标。 * 数组中的每个元素代表你在该位置可以跳跃的最大长度。 * 判断你是否能够到达最后一个下标。 * @param nums 数组 * @return 是否能够到达最后一个下标 */ public boolean canJump(int[] nums) { int m = nums.length; int farthest = 0; if (nums[0] == 0 \u0026amp;\u0026amp; m \u0026gt; 1) {return false;} for (int i = 0; i \u0026lt; m - 1; i++) { int item = nums[i]; farthest = Math.max(farthest, i + item); if (farthest \u0026lt;= i) {return false;} } return farthest \u0026gt;= m - 1; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[] nums = {2, 3, 1, 1, 4}; boolean res = canJump(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); nums = new int[]{3,2,1,0,4}; res = canJump(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } 2.2.2 跳跃游戏 2 #  2.2.2.1 题目 #  给定一个非负整数数组，你最初位于数组的第一个位置。\n数组中的每个元素代表你在该位置可以跳跃的最大长度。\n你的目标是使用最少的跳跃次数到达数组的最后一个位置。\n示例:\n输入: [2,3,1,1,4] 输出: 2 解释: 跳到最后一个位置的最小跳跃数是 2。 从下标为 0 跳到下标为 1 的位置，跳 1 步，然后跳 3 步到达数组的最后一个位置。 说明:\n假设你总是可以到达数组的最后一个位置。\n2.2.2.2 问题分析 #   当我们站在索引 0 的位置上时，可以向前跳 1、2 或 3 步，但是我们应该跳 2 步到索引 2，因为 nums[2] 的可跳跃区域涵盖了索引区间 [3..6]，比其他都大，如果我们想求最少的跳跃次数，那么往索引 2 跳必然是最优的选择，这就是贪心选择性质，我们不需要【递归地】计算出所有选择的具体结果然后比较求最值，而只需要做出那个最有【潜力】，看起来最优的选择即可。\n如果某一个作为起跳点的格子可以跳跃的距离是 3，那么表示后面 3 个格子都可以作为起跳点。可以对每一个能作为起跳点的格子都尝试跳一次，把能跳到最远的距离不断更新。\n 2.2.2.3 参考代码 #   下面的代码中 i 和 end 标记了可以选择的跳跃步数，farthest 标记了所有选择 [i..end] 中能够跳到的最远距离，jumps 记录了跳跃次数。\npackage com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/23 20:21 * @Description 跳跃游戏 II */ public class L45 { /** * 45.跳跃游戏 II * 给定一个非负整数数组，你最初位于数组的第一个位置。 * 数组中的每个元素代表你在该位置可以跳跃的最大长度。 * 你的目标是使用最少的跳跃次数到达数组的最后一个位置。 * @param nums * @return */ public static int jump(int[] nums) { int n = nums.length; int end = 0, farthest = 0; int jumps = 0; for (int i = 0; i \u0026lt; nums.length - 1; i++) { farthest = Math.max(i + nums[i], farthest); if (end == i) { jumps++; end = farthest; } } return jumps; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[] nums = {2,3,1,1,4}; int res = jump(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } 3 参考文献 #    435. 无重叠区间。  452. 用最少数量的箭引爆气球。  55. 跳跃游戏。  45. 跳跃游戏 II。  贪心算法之区间调度问题。  "},{"id":71,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.3-%E9%94%81%E5%8D%8F%E8%AE%AE/","title":"1.3 锁协议","section":"1、数据库基础","content":"锁协议 #  1 三级封锁协议 #   三级封锁协议就是三个不同级别的封锁协议，他们是以何时加锁，何时解锁来区分的。\n 1.1 一级封锁协议 #  1.1.1 含义 #   事务 $T$ 在修改数据 $R$ 之前必须先对其加 $X$ 锁，直到事务结束才释放，事务结束包括正常结束（Commit）和非正常结束（Rollback）。 在一级封锁协议中，如果仅仅是读数据而不对其进行修改，是不需要加锁的。 一级封锁协议可以解决更新丢失问题，但是他不能保证可重复读和不读脏数据。  1.1.2 示例 #  1.1.2.1 不使用锁导致的更新丢失问题 #   1.1.2.2 使用一级锁解决更新丢失问题 #    1.1.2.3 为什么一级锁不能解决脏读问题 #  脏读例子：\n 用一级锁的情况：\n 1.2 二级封锁协议 #  1.2.1 含义 #   在一级封锁协议之上，事务 $T$ 在读取数据 $R$ 之前必须先对其加 $S$ 锁，读完后方可释放 $S$ 锁。 二级封锁协议除了可以解决丢失更新问题，还可以进一步解决脏读问题。 但在二级封锁协议中，由于读完数据后即可释放 $S$ 锁，所以他不能保证可重复读。  1.2.2 示例 #  1.2.2.1 使用二级锁解决脏读问题 #   1.2.2.2 为什么二级锁不能解决不可重复读的问题 #  不可重复读例子：\n 使用二级锁的情况：\n 1.3 三级封锁协议 #  1.3.1 含义 #   在一级封锁协议之上，事务 $T$ 在读取数据 $R$ 之前必须先对其加 $S$ 锁，直到事务结束才释放。 三级封锁协议除了解决丢失修改问题和脏读问题，还进一步解决了不可重复读问题。 因为三级锁协议相当于完全串行化，即一个事务开始了，其他事务都完全不能开始运行，无论是读还是写，那么就必定不会出现不可重复读的问题了。 三级锁协议的完全串行化，也就是为了解决并行化带来的问题，直接不使用并行，改为串行执行。  2 两段锁协议 #  2.1 含义 #   两段锁协议规定所有事务都必须遵循以下规则：  在对数据进行读、写之前，首先要申请并获得对该数据的锁。 在释放一个锁之后，事务不能再申请和获得其它任何锁。   这些规则将对数据的加锁和解锁操作分为两个阶段：  第一阶段是扩展阶段：  即获得锁的阶段，这一阶段的事务可以获得任何数据项上的任何类型的锁，但是不能释放。 其实也就是该阶段可以进行加锁操作，在对任何数据进行读操作之前要申请获得$S$锁，在进行写操作之前要申请并获得$X$锁，如果加锁不成功，则事务进行等待状态，直到加锁成功才可以继续执行，就是加锁后不能再解锁了。   第二阶段是收缩阶段：  即释放锁的阶段，这一阶段事务可以释放任何数据项上的任何类型的锁，但是不能申请。 当事务释放一个锁之后，事务进入收缩阶段，在该阶段只能进行解锁而不能再进行加锁操作。     如果所有事务都遵守两段锁协议，那么这些事务的交叉调度都是可串行化的。   如果一个并行调度的结果等价于一个串行调度的结果，那么称这个并行调度为可串行化的。\n 2.2 示例 #   2.3 优缺点 #  2.3.1 缺点 #   遵循两段锁协议的事务有可能发生死锁，例如两个事务都坚持请求对对方已经占有的数据加锁，这样就会导致死锁。 为此又有了一次封锁法，他要求事务必须一次性将所有要使用的数据全部加锁，否则就不能继续执行。 因此，一次封锁法遵守两段锁协议，但两段锁协议并不要求事务必须一次性将所有要使用的数据全部加锁，这一点是与一次封锁法不同的地方。  参考文献 #    mysql: 三级封锁协议。  数据库三级封锁协议。  两阶段加锁协议。  "},{"id":72,"href":"/school-recruitment/docs/database/3Redis/3.1-%E6%A6%82%E8%BF%B0/3.1.3-%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/","title":"3.1.3 持久化机制","section":"3.1 概述","content":"持久化机制 #  1 什么是持久化 #   持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。  2 Redis 中持久化机制有哪些 #  Redis 中持久化机制主要有两种，分别是RDB（默认）和AOF。\n2.1 RDB #  2.1.1 含义 #   RDB 文件主要用于保存和还原 Redis 服务器所有数据库中的所有键值对数据。 创建方式主要有两种，分别是 SAVE 和 BGSAVE：  SAVE 命令由服务器进程直接执行保存操作，所以该命令会阻塞服务器。 BGSAVE 命令由子进程执行保存操作，所以该命令不会阻塞服务器。   因为 BGSAVE 命令可以在不阻塞服务器进程的情况下执行，所以可以通过设置服务器配置的 save 选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令，用户可以通过 save 选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行 BGSAVE 命令。  2.1.2 优缺点 #  2.1.2.1 优点 #   整个 Redis 数据库只包含一个文件，这对于文件备份来说非常方便，比如，我们可能打算每个小时归档一次最近 24 小时的数据，同时还要每天归档一次最近 30 天的数据，通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难性恢复而言，RDB 也是一个不错的选择，因为我们可以非常轻松的将一个单独的文件压缩后再转移到其他存储介质上。 性能最大化，对于 Redis 的服务进程而言，在开始持久化时，他唯一需要做的只是 fork 出子进程，之后再由子进程完成这些持久化的工作，这样可以极大地避免服务进程执行 IO 操作。 相比于 AOF 机制，如果数据集很大，RDB 的启动效率会更高。  2.1.2.2 缺点 #   数据安全性低，因为RDB 是间隔一段时间进行持久化，如果持久化之前 Redis 发生故障，会发生数据丢失，所以这种方式更适合数据要求不严谨的时候。  2.2 AOF #  2.2.1 含义 #   AOF 文件通过保存所有修改数据库的写命令请求来记录服务器的数据库状态。 命令请求会先保存到 AOF 缓冲区里面，之后再定期写入并同步到 AOF 文件，然后服务器只要载入并重新执行保存在 AOF 文件中的命令，就可以还原数据库本来的状态。  2.2.2 持久化的效率和安全性 #    appendfsync选项的不同值对 AOF 持久化功能的安全性以及 Redis 服务器的性能有很大的影响：\n  当 appendfsync 的值为 always 时：\n 服务器在每个事件循环都要将 aof_buf 缓冲区中的所有内容写入到 AOF 文件，并且同步 AOF 文件。 所以 always 的效率是最低的，但却是最安全的，因为即使出现故障停机，AOF 持久化也只会丢失一个事件循环中所产生的的命令数据。    当 appendfsync 的值为 everysec 时：\n 服务器在每个事件循环都要将 aof_buf 缓冲区的所有内容写入到 AOF 文件，并且每隔一秒就要在子线程中对 AOF 文件进行一次同步。 从效率上来讲，everysec 模式足够快，并且就算出现故障停机，数据库也只会丢失一秒钟的命令数据。    当 appendfsync 的值为 no 时：\n 服务器在每个事件循环都要将 aof_buf 缓冲区的所有内容写入到 AOF 文件，至于何时对 AOF 文件进行同步，则由操作系统控制。 因为该模式下 flushAppendOnlyFile 调用无须执行同步操作，所以该模式下的AOF 文件写入速度是最快的，不过因为该模式下会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长是最长的，当出现故障停机时，将会丢失上次同步 AOF 文件之后的所有鞋命令数据。        文件写入和文件同步的区别？  文件写入是指写入到内存缓冲区中，文件同步是指将内存缓冲区中的数据刷新到磁盘中。   事件循环是什么？  Redis服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责像 serverCron 函数这样需要定时运行的函数。     2.2.3 AOF 重写 #  2.2.3.1 前言 #   因为 AOF 持久化是通过保存被执行的写命令来记录数据库状态的，所以随着服务器运行时间的流逝，AOF 文件中的内容会越来越多，文件的体积也会越来越大。 如果不加以控制，体积过大的 AOF 文件可能会对 Redis 服务器，甚至整个宿主计算机造成影响，同时使用 AOF 文件来进行数据还原所需要的时间也会越来越多，为了解决 AOF 体积膨胀的问题，Redis 提供了文件重写的功能。 通过文件重写，Redis 服务器可以创建一个新的 AOF 文件来替代现有的 AOF 文件，新旧两个 AOF 文件所保存的数据库状态相同，但新 AOF 文件不会包含任何浪费空间的冗余命令，所以新 AOF 文件的体积通常会比旧 AOF 文件的体积要小得多。  2.2.3.2 原理 #   AOF 重写功能的实现原理是从数据库中读取键现在的值，然后用一条命令去记录键值对，代替之前记录这个键值对的多条命令。 这样通过 AOF 重写生成的新 AOF 文件只包含当前数据库状态所必须的命令，因此不会浪费任何硬盘空间。 在实际中，为了避免在执行命令时造成客户端输入缓冲区溢出，重写程序在处理列表、哈希表、集合、有序集合这四种可能会带有多个元素的键时，会先检查所包含的元素数量，如果元素数量超过了src/redic.h/REDIS_AOF_REWRITE_ITEMS_PER_CMD常量的值（当前版本为 64），那么重写程序将使用多条命令来记录键的值，而不单单使用一条命令。 具体的示例如下：   假如服务器对 animals 键执行了以下命令：\n   为了记录 animals 键的状态，AOF 文件必须保存上面列出的四条命令。\n  如果服务器想减少保存 animals 键所需命令的数量，那么服务器可以通过读取 animals 键的值，然后用一条 SADD animals \u0026quot;Dog\u0026quot; \u0026quot;Panda\u0026quot; \u0026quot;Tiger\u0026quot; \u0026quot;Lion\u0026quot; \u0026quot;Cat\u0026quot; 命令来代替上面的四条命令，这样就将保存 animals 键所需的命令从 4 条减为一条了。\n  除了上面列举的列表键和集合键之外，其他所有类型的键都可以用同样的方法去减少 AOF 文件中的命令数量。\n    2.2.4 AOF 后台重写 #  2.2.4.1 前言 #   上面介绍的 AOF 重写程序 aof_rewrite() 函数可以很好地完成创建一个新的 AOF 文件的任务，但是，因为这个函数会进行大量的写入操作，所以调用这个函数的线程将被长时间阻塞，因为Redis 服务器使用单线程来处理命令请求，所以如果这个服务器直接调用 aof_rewrite() 函数的话，那么在重写 AOF 文件期间，服务器将无法处理客户端发来的命令请求。 因此 Redis 决定将 AOF 重写程序放到子线程里执行，这样做可以同时达到两个目的：  子进程进行 AOF 重写期间，服务器进程（父进程）可以继续处理命令请求。 子进程带有服务器进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下，保证数据的安全性。    2.2.4.2 原理 #    虽然使用子进程有诸多好处，但是使用子进程也有一个问题需要解决，因为子进程在进行 AOF 重写期间，服务器进程还需要继续处理命令请求，而新的命令可能会对现有的数据库状态进行修改，从而使得服务器当前的数据库状态和重写后的 AOF 文件所保存的数据库状态不一致，例如：\n 假如当子进程开始进行文件重写时，数据库中只有 k1 一个键，但是当子进程完成 AOF 文件重写之后，服务器进程的数据库中已经新设置了 k2、k3、k4 三个键，因此，重写后的 AOF 文件和服务器当前的数据库状态并不一致，新的 AOF 文件只保存了 k1 一个键的数据，而服务器数据库现在却有 k1、k2、k3、k4 四个键。 上面的执行过程如下所示：     为了解决这种数据不一致问题，Redis 服务器设置了一个AOF 重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用，当 Redis 服务器执行完一个写命令之后，他会同时将这个写命令发送给 AOF 缓冲区和 AOF 重写缓冲区。\n   这样一来可以保证：\n AOF 缓冲区的内容会定期被写入和同步到 AOF 文件，对现有 AOF 文件的处理工作会如常进行。 从创建子进程开始，服务期执行的所有写命令都会被记录到 AOF 重写缓冲区里面。    当子进程完成 AOF 重写工作之后，他会向父进程发送一个信号，父进程在接收到该信号后，会调用一个信号处理函数，并执行以下工作：\n 将 AOF 重新缓冲区中的所有内容写入到新 AOF 文件，这时新 AOF 文件所保存的数据库状态将和服务器当前的数据库状态一致。 对新的 AOF 文件进行改名，原子地覆盖现有的 AOF 文件，完成新旧两个 AOF 文件的替换。    这个信号处理函数执行完毕之后，父进程就可以像往常一样接受命令请求了。\n  在整个 AOF 后台重写过程中，只有信号处理函数执行时会对服务器进程（父进程）造成阻塞，在其他时候，AOF 后台重写都不会阻塞父进程，这将 AOF 重写对服务器性能造成的影响降到了最低。\n  具体示例如下：\n   2.2.5 优缺点 #  2.2.5.1 优点 #   该机制可以带来 更高的数据安全性。 如果日志过大，Redis 可以自动启用 重写机制。 AOF包含一个格式清新、易于理解的日志文件用于记录所有的修改操作。  2.2.5.2 缺点 #   对于相同数量的数据集而言，AOF 文件通常要大于 RDB 文件，而且因为 AOF 文件记录的是执行命令，在恢复的时候需要重新执行相应的命令，RDB 存储是数据，恢复时直接恢复即可，因此RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 根据同步策略的不同，AOF 在运行效率上往往会慢于 RDB。  3 如何选择何时的持久化方式 #   一般来说，如果想达到足以达到 PostgreSQL 的数据安全性，我们应该同时使用两种持久化功能，在这种情况下，当Redis 重启的时候，会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 保存的数据集要完整。 如果我们可以承受数据分钟以内的丢失，那么可以只使用 RDB 持久化。 一般不推荐只使用 AOF 持久化，因为定时生成的 RDB 快照非常便于数据库备份，并且RDB 恢复数据集的速度要比 AOF 快。  参考文献 #    几率大的 Redis 面试题（含答案）。  Redis 面试题（总结最全面的面试题）。  redis 持久存储 RDB 和 AOF 的区别及优缺点。 redis 设计与实现（第二版）。  "},{"id":73,"href":"/school-recruitment/docs/java/3JVM/3.3-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/","title":"3.3 垃圾收集算法","section":"3、 Jvm","content":"垃圾收集算法 #  3.3.1 标记-清除算法 #  最基础的收集算法是“标记-清除”算法，算法分为“标记”和“清除”两个阶段：首先需要标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它的主要不足有两个：一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。\n3.3.2 复制算法 #  为了解决效率问题，一种称为“复制”的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效，只是这种算法的代价是将内存缩小为了原来的一半。\n复制算法一般用来回收新生代，由于新生代中的对象是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性复制到另一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。当Survivor空间不够用时，需要依赖其他内存进行分配担保。\n3.3.3 标记-整理算法 #  复制收集算法在对象存活率较高时就要进行较多的赋值操作，效率就会变低，更关键的是，如果不想浪费50%的空间，就需要额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。\n根据老年代的特点，有人提出了“标记-整理”算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。\n3.3.4 分代收集算法 #  当代商业虚拟机的垃圾收集都采用“分代收集”算法，这种算法根据对象存活周期的不同将内存划分为几块。一般是把Java堆划分为新生代和老年代，这样就可以根据各个年代的特点选用最适当的收集算法：\n 在新生代中，每次垃圾回收都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。 在老年代中，因为对象存活率高，没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。  "},{"id":74,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.4-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/","title":"1.1.4 最长递增子序列","section":"1.1 动态规划","content":"最长递增子序列 #  1 题目 #  给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。\n子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。\n示例 1：\n输入：nums = [10,9,2,5,3,7,101,18] 输出：4 解释：最长递增子序列是 [2,3,7,101]，因此长度为 4 。 示例 2：\n输入：nums = [0,1,0,3,2,3] 输出：4 示例 3：\n输入：nums = [7,7,7,7,7,7,7] 输出：1 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 2500 -104 \u0026lt;= nums[i] \u0026lt;= 104  进阶：\n 你可以设计时间复杂度为 O(n2) 的解决方案吗？ 你能将算法的时间复杂度降低到 O(n log(n)) 吗?  2 解题思路 #  2.1 动态规划法 #  2.1.1 问题分析 #  动态规划的核心设计思想是数学归纳法。\n比如我们想证明一个数学结论，那么我们先假设这个结论在 $k \u0026lt; n$ 时成立，然后想办法证明 $k = n$ 的时候此结论也成立。如果能够证明出来，那么就说明这个结论对于 $k$ 等于任何数都成立。\n类似的，我们在设计动态规划算法时，需要一个 $dp$ 数组，我们可以假设 $dp[0\u0026hellip;i-1]$ 都已经被算出来了，然后需要通过这些结果算出 $dp[i]$。\n该题目的具体解题思路如下：\n 定义 $dp$ 数组：$dp[i]$ 表示以 $nums[i]$ 这个数结尾的最长递增子序列的长度。 根据这个定义，我们的最终结果（子序列的最大长度）应该是 $dp$ 数组中的最大值。    刚在这个过程中每个 $dp[i]$ 的结果是我们肉眼看出来的，我们应该怎么设计算法逻辑来正确计算每个 $dp[i]$ 呢？这就是动态规划中关键的部分了，要思考如何进行状态转移，这里就可以使用数学归纳的思想了。    假设此时我们已经知道了 $dp[0\u0026hellip;4]$ 的所有结果，现在需要求 $dp[5]$ 值，也就是相求以 $nums[5]$ 结尾的最长递增子序列。 $nums[5]=3$，既然是递增子序列，我们只要找到前面那些结尾比 3 小的子序列，然后把 3 接到最后，就可以形成一个新的递增子序列，而且这个新的子序列长度加 1。 当然，可能形成很多新的子序列，但是我们只要最长的，把最长子序列的长度作为 $dp[5]$ 即可。  for (int j = 0; j \u0026lt; i; j++) { // 找出 nums[i] 之前的元素中小于 nums[i] 的元素，将 nums[i] 接在其后面，然后把 dp[i] + 1 即可  if (nums[j] \u0026lt; nums[i]) { dp[i] = Math.max(dp[i], dp[j] + 1); } }  这段代码的逻辑可以算出 $dp[5]$，类似数学归纳法，我们可以算出 $dp[5]$，其他的就可以都算出来了。  // 遍历 nums for (int i = 0; i \u0026lt; nums.length; i++) { for (int j = 0; j \u0026lt; i; j++) { // 找出 nums[i] 之前的元素中小于 nums[i] 的元素，将 nums[i] 接在其后面，然后把 dp[i] + 1 即可  if (nums[j] \u0026lt; nums[i]) { dp[i] = Math.max(dp[i], dp[j] + 1); } } } 还有一个细节问题，$dp$ 数组应该全部初始化为 1，因为子序列最少也要包含自己，所以长度最少为 1。  至此，这道题就解决了，时间复杂度为 $O(N^2)$，最后总结一下动态规划的设计流程：\n 首先明确 $dp$ 数组所存数据的含义。 这步很重要，如果不得当或者不够清晰，会阻碍之后的步骤。 然后根据 $dp$ 数组的定义，运用数学归纳法的思想，假设 $dp[0\u0026hellip;i-1]$ 都已知，想办法求出 $dp[i]$，一旦这一步完成，整个题目就解决了。但如果无法完成这一步，可能是以下原因：  $dp$ 数组的定义不够恰当，需要重新定义 $dp$ 数组的含义。 $dp$ 数组存储的信息不够，不足以推出下一步的答案，需要把 $dp$ 数组扩大成二维数组甚至三维数组。   最后想一想问题的 $base\\space case$ 是什么，以此来初始化 $dp$ 数组，以保证算法正确运行。  2.1.2 参考代码 #  /** * 300. 最长递增子序列（版本 1：动态规划） * 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 * 子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 * * @param nums 整数数组 * @return 最长严格递增子序列的长度 */ public int lengthOfLISV1(int[] nums) { // 定义 dp 数组，其中第 i 个元素表示以 nums[i]这个数结尾的最长递增子序列的长度  int[] dp = new int[nums.length]; // 将数组中的元素初始化为 1  Arrays.fill(dp, 1); // 遍历 nums  for (int i = 0; i \u0026lt; nums.length; i++) { for (int j = 0; j \u0026lt; i; j++) { // 找出 nums[i] 之前的元素中小于 nums[i] 的元素，将 nums[i] 接在其后面，然后把 dp[i] + 1 即可  if (nums[j] \u0026lt; nums[i]) { dp[i] = Math.max(dp[i], dp[j] + 1); } } } // dp 数组中最大的元素即为最长递增子序列的长度  int res = 0; for (int i = 0; i \u0026lt; dp.length; i++) { res = Math.max(res, dp[i]); } // 返回结果  return res; } 2.2 二分查找法 #  2.2.1 问题分析 #  最长递增子序列和一种叫做Patience Game的纸牌游戏有关，甚至有一种排序方法就叫做Patience Sorting（耐心排序）。该纸牌游戏的玩法如下：\n 首先，给我们一副扑克牌，我们想遍历数组那样从左到右一张一张处理这些扑克牌，最终要把这些牌分成若干堆。   处理这些扑克牌要遵循以下规则：  只能把点数小的牌压到点数比他大的牌上。 如果当前牌点数较大没有可以放置的堆，则新建一个堆，把这张牌放进去。 如果当前牌有多个堆可供选择，则选择最左边的堆放置（保证牌堆顶的牌有序）。   比如说上述的扑克牌最终会被分成这样5堆（我们认为$A$的值最大，而不是1）。   按照上述规则执行，可以算出最长递增子序列，牌的堆数就是最长递增子序列的长度。   我们只要把处理扑克牌的过程编程写出来即可。每次处理一张扑克牌不是要找到一个合适的牌堆顶来放吗，牌堆顶的牌不是有序吗，这就能用到二分查找了：用寻找左侧边界的二分查找法来搜索当前牌应放置的位置。  2.2.2 参考代码 #  /** * 300. 最长递增子序列（版本2：二分数组） * 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 * 子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 * * @param nums 整数数组 * @return 最长严格递增子序列的长度 */ public int lengthOfLISV2(int[] nums) { // 牌堆顶部的牌  int[] top = new int[nums.length]; // 牌堆数  int piles = 0; // 遍历 nums，将牌进行分堆  for (int i = 0; i \u0026lt; nums.length; i++) { int poker = nums[i]; // 采用寻找左侧边界的二分查找法，寻找牌应放置的堆的位置  int left = 0, right = piles - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (top[mid] \u0026gt; poker) { right = mid - 1; } else if (top[mid] \u0026lt; poker) { left = mid + 1; } else if (top[mid] == poker) { right = mid - 1; } } // 没找到放牌的位置，则新建一堆  if (left \u0026gt;= piles) {piles++;}; // 将牌放到该堆的位置  top[left] = poker; } // 牌堆数即为最长递增子序列的长度，将其直接返回即可  return piles; } 3 参考文献 #    300. 最长递增子序列。  "},{"id":75,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.4-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","title":"1.4 排序算法","section":"1、算法框架","content":"排序算法 #  1 简单选择排序 #  1.1 算法原理 #   简单排序算法的基本思想为每一趟从待排序的数据元素中选择最小（最大）的一个元素作为首元素，直到所有元素排完为止。  1.2 参考代码 #   在算法实现时，每一趟确定最小元素的时候会通过不断地比较交换来使得首位置为当前最小。 交换是个比较耗时的操作，其实我们很容易发现，在还未完全确定当前最小元素之前，这些交换都是无意义的。 我们可以通过设置一个变量minInd，每一次比较仅存储较小元素的数组下标，当这一轮循环结束之后，那这个变量存储的就是当前最小元素的下标，此时再执行交换操作即可。  /** * @author peng.wei * @version 1.0 * @date 2021/5/3 20:52 * @Description 简单选择排序算法 */ public class SimpleSelectionSort { public static void sort(int[] arr) { int m = arr.length; for (int i = 0; i \u0026lt; m - 1; i++) { int minInd = i; for (int j = minInd + 1; j \u0026lt; m; j++) { // 只记录最小元素的位置，而不是每一次比较都交换，减少交换的次数  if (arr[minInd] \u0026gt; arr[j]) {minInd = j;} } if (minInd != i) { // 如果当前位置 i 不是这一次的最小元素，再交换元素的位置  CommonUtils.swap(arr, i, minInd); } } } } 1.3 算法分析 #   简单排序算法无论数组原始排列如何，比较次数是不变的；对于交换操作，在最好情况下也就是数组完全有序的时候，无需任何交换移动，在最差情况下，也就是数组倒序的时候，交换次数为n-1 次，综合下来，时间复杂度为 $O(n^2)$。 简单排序算法是不稳定的排序算法。  1.4 适用场景 #   选择排序实现也比较简单，并且由于在各种情况下复杂度波动较小，因此一般是优于冒泡排序的。 在所有的完全交换排序中，选择排序也是比较不错的一种算法，但是由于固有的 $O(n^2)$ 复杂度，选择排序在海量数据面前显得力不从心，因此，它适用于简单数据排序。  2 冒泡排序 #  2.1 算法原理 #   冒泡排序的基本思想是对相邻的元素进行两两比较，顺序相反则进行交换，这样，每一趟会将最小（最大）的元素浮到顶端，最终达到完全有序。   2.2 参考代码 #   在冒泡排序过程中，如果某一趟执行完毕，没有做任何一次交换操作，这就说明剩下的序列已经是有序的，排序操作也就可以完成了。  /** * @author peng.wei * @version 1.0 * @date 2021/5/3 21:10 * @Description 冒泡排序算法 */ public class BubbleSort { public static void sort(int[] arr) { int m = arr.length; for (int i = 0; i \u0026lt; m - 1; i++) { // 判断是否需要交换  boolean exchange = false; for (int j = 0; j \u0026lt; m - i - 1; j++) { // 如果前面一个元素比后面一个元素大，则交换两个元素的位置，同时将 exchange 置为 true  if (arr[j] \u0026gt; arr[j+1]) { CommonUtils.swap(arr, j, j+1); exchange = true; } } if (!exchange) { // 如果这一次冒泡没有发生交换，则说明前面的元素都已经有序了，没有必要再进行下一次冒泡了  break; } } } } 2.3 算法分析 #   对于冒泡排序算法，若原数组本身就是有序的，仅需 $n-1$ 次比较即可完成；若是倒序，比较次数为 $(n-1) + (n-2) + \u0026hellip; + 1 = n(n-1)/2$，交换次数和比较次数等值，所以，时间复杂度依然为 $O(n^2)$。 综合来看，冒泡排序性能还是稍差于上面的简单选择排序的。 在相邻元素相等时，他们不会交换位置，所以，冒泡排序是稳定排序。  2.4 适用场景 #   冒泡排序思路简单，代码也简单，特别适合小数据的排序，但是，由于算法复杂度较高，在数据量大的时候不适用。  3 直接插入排序 #  3.1 算法原理 #   直接插入排序算法的基本思想是每一步将一个待排序的记录，插入到前面已经排好序的有序序列中去，直到插完所有元素为止。   3.2 参考代码 #  /** * @author peng.wei * @version 1.0 * @date 2021/5/3 21:28 * @Description 直接插入排序算法 */ public class DirectInsertionSort { public static void sort(int[] arr) { int m = arr.length; for (int i = 1; i \u0026lt; m; i++) { int j = i; while (j \u0026gt; 0 \u0026amp;\u0026amp; arr[j - 1] \u0026gt; arr[j]) { CommonUtils.swap(arr, j - 1, j); j--; } } } } 3.3 算法分析 #   简单插入排序在最好情况下需要比较 $n-1$ 次，无需交换元素，时间复杂度为 $O(n)$；在最坏情况下，时间复杂度依然为 $O(n^2)$。 但是，在数组元素随机排列的情况下，插入排序还是要优于上面两种排序的。 由于只需要找到不大于当前数的位置而并不需要交换，因此，直接插入排序是稳定排序。  3.4 适用场景 #   简单插入排序由于 $O(n^2)$ 的复杂度，在数组较大时不适用，但是，当数据比较少的时候，是一个不错的选择，一般作为快速排序的扩充。 例如，在JDK 7 中的java.util.Arrays 所用的sort 方法的实现中，当待排序数组长度小于 47 时，会使用插入排序。  4 希尔排序 #  4.1 算法原理 #   希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为递减增量排序算法。 希尔排序的基本思想是把记录按下标的一定增量分组，对每组使用直接插入算法排序，随着增量逐渐减少，每组包含的关键词越来越多，当增量减至 1 时，整个文件恰被分成一组，算法便终止。 简单插入排序很循规蹈矩，不管数组分布是怎样的，依然一步一步对元素进行比较、移动、插入，比如 $[5,4,3,2,1]$ 这种倒序序列，数组末端的 0 要回到首位是很费劲的，比较和移动元素均需 $n-1$ 次。 而希尔排序在数组中采用跳跃式分组的策略，通过某个增量将数组元素划分为若干组，然后分组进行插入排序，随后逐步缩小增量，继续按组进行插入排序操作，直至增量为 1。 希尔排序通过这种策略使得整个数组在初始阶段达到从宏观上看基本有序，小的基本在前，大的基本在后，然后缩小增量，到增量为 1 时，大多数情况下只需微调即可，不会涉及过多的数据移动。 希尔排序的具体排序过程如下：   假如有这样一组数 $[13,14,94,33,82,25,59,94,65,23,45,27,73,25,39,10]$，如果我们以步长为 5 开始进行排序，我们可以通过将这列表放在有 5 列的表中来更好的描述算法：\n13 14 94 33 82 25 59 94 65 23 45 27 73 25 39 10   然后我们对每列进行直接插入排序：\n10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45   然后再以 3 为步长进行排序：\n10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45   排序之后的结果为：\n10 14 13 25 23 33 27 25 59 39 65 73 45 94 82 94   最后再以 1 为步长进行排序，此时就是简单排序了。\n    4.2 参考代码 #  /** * @author peng.wei * @version 1.0 * @date 2021/5/4 19:34 * @Description 希尔排序 */ public class ShellSort { public static void sort(int[] arr) { int len = arr.length; // 增量 gap，并不断缩小增量  for (int gap = len / 2; gap \u0026gt;= 1; gap = gap / 2) { // 从第 gap 个元素开始，对每个组使用直接插入排序  for (int i = gap; i \u0026lt; len; i++) { int j = i; int temp = arr[i]; while (j - gap \u0026gt;= 0 \u0026amp;\u0026amp; temp \u0026lt; arr[j - gap]) { // 将元素向后移动 gap 位  arr[j] = arr[j - gap]; j -= gap; } arr[j] = temp; } } } } 4.3 算法分析 #   希尔排序算法中对增量序列的选择十分重要，直接影响到希尔排序的性能，当选择增量序列为 $n/2^i$ 时，其最坏时间复杂度依然为 $O(n^2)$。 希尔排序是不稳定排序算法。  4.4 适用场景 #   希尔排序虽然快，但是毕竟是插入排序，其数量级并没有快速排序快，在大量数据面前，希尔排序不是一个好的算法，但是中小型规模的数据完全可以使用它。  5 快速排序 #  5.1 算法原理 #   快速排序是对冒泡排序的改进，冒泡排序每次只能交换相邻的两个元素，而快速排序是跳跃式的交换，交换的距离很大，因此总的比较和交换次数少了很多，速度也快了不少。 快速排序的基本思想是：  在待排序的元素任取一个元素作为基准（通常选第一个元素），称为基准元素。 将待排序的元素进行分区，比基准元素大的元素放在他的右边，比其小的放在他的左边。 对左右两个分区重复以上的步骤直到所有元素都是有序的。   快速排序算法的具体过程如下图：   5.2 参考代码 #  /** * @author peng.wei * @version 1.0 * @date 2021/5/4 20:54 * @Description 快速排序算法 */ public class QuickSort { public static void sort(int[] arr) { quickSort(arr, 0, arr.length - 1); } /** * 快速排序算法 * @param arr 数组 * @param _left 左边界 * @param _right 右边界 */ public static void quickSort(int[] arr, int _left, int _right) { int left = _left; int right = _right; int temp = 0; if (left \u0026lt;= right) { // 待排序的第一个元素作为基准元素 temp = arr[left]; // 从左到有交替扫描，直到 left = right while (left != right) { // 从右往左扫描，找到第一个比基准元素小的元素 while (right \u0026gt; left \u0026amp;\u0026amp; arr[right] \u0026gt;= temp) { right--; } // 找到这种元素 arr[right] 后与 arr[left] 交换 arr[left] = arr[right]; // 从左往右扫描，找到第一个比基准元素大的元素 while (left \u0026lt; right \u0026amp;\u0026amp; arr[left] \u0026lt;= temp) { left++; } // 找到这种元素 arr[left] 后与 arr[right] 交换 arr[right] = arr[left]; } // 基准元素归位 arr[right] = temp; // 对基准元素左边的元素进行递归排序 quickSort(arr, _left, left - 1); // 对基准元素右边的元素进行递归排序 quickSort(arr, right + 1, _right); } } } 5.3 算法分析 #   当分区选取的基准元素为待排元素中的最大或最小值时，为最坏的情况，时间复杂度和直接插入排序的一样，移动次数达到最大值 $C_{max}=1+2+\u0026hellip;+(n-1)=n*(n-1)/2=O(n^2)$。 当分区选取的基准元素为待排序中的中值，为最好的情况，时间复杂度为 $O(nlog_2n)$。 快速排序的空间复杂度为 $O(log_2n)$。 当待排元素类似 $[6,1,3,7,3]$ 且基准元素为 6 时，经过分区，形成 $[1,3,3,6,7]$，两个 3 的相对位置发生了改变，所以快速排序是一种不稳定排序算法。  5.4 适用场景 #   快速排序在大多数情况下都是适用的，尤其在数据量大的时候性能优越更加明显，但在必要的时候，需要考虑下优化以提高其在最坏情况下的性能。  6 堆排序 #  6.1 算法原理 #    堆是具有以下性质的完全二叉树：\n 每个节点的值都大于或等于其左右孩子节点的值，称为大顶堆。 每个节点的值都小于或等于其左右孩子节点的值，称为小顶堆。    堆排序的基本思想为：\n 将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。 将堆顶的根节点与末尾元素进行交换，此时末尾元素就是最大值。 然后将剩余 $n-1$ 个元素重新构造成一个堆，这样会得到 $n$ 个元素的次小值。 如此反复执行，便能得到一个有序序列了。     第一步：创建堆，从下至上，从右往左。\n 第二步：堆排序，进行 $(n-1)$ 次循环，一直到最后一个元素。\n 6.2 参考代码 #  /** * @author peng.wei * @version 1.0 * @date 2021/5/5 19:49 * @Description 堆排序 */ public class HeapSort { public static void sort(int[] arr) { // 1. 构建大顶堆：从第一个非叶子节点开始从下至上，从右至左调整结构  int len = arr.length; // 第一个非叶子节点  int beginIndex = (len \u0026gt;\u0026gt; 1) - 1; for (int i = beginIndex; i \u0026gt;= 0; i--) { adjustHeap(arr, i, len - 1); } // 2.调整堆结构:  // 2.1 每次都是移出最顶层的根节点，与最尾部节点位置调换，同时遍历长度-1。  // 2.2 然后重新整理被换到根节点的末尾元素，使其符合堆的特性。  // 2.3 直到未排序的堆长度为 0  for (int i = len - 1; i \u0026gt; 0; i--) { CommonUtils.swap(arr, 0, i); adjustHeap(arr, 0, i - 1); } } /** * 调整大顶推（仅是调整过程，建立在大顶堆已经构建的基础上） * @param arr 数组 * @param index 需要堆化处理的数据的索引 * @param length 未排序的数组的长度 */ public static void adjustHeap(int[] arr, int index, int length) { // 左子节点索引  int left = (index \u0026lt;\u0026lt; 1) + 1; // 右子节点索引  int right = left + 1; // 子节点的最大索引，默认是左子节点  int max = left; // 如果左子节点索引超出范围，则直接返回  if (left \u0026gt; length) {return;} // 判断左右子节点哪个最大  if (right \u0026lt;= length \u0026amp;\u0026amp; arr[right] \u0026gt; arr[left]) {max = right;} // 判断是否需要交换子节点和父节点：  // 如果需要的话，则交换相应的子节点和父节点，然后调整换下父节点后的堆使其符合堆的特性  if (arr[max] \u0026gt; arr[index]) { CommonUtils.swap(arr, index, max); adjustHeap(arr, max, length); } } } 6.3 算法分析 #   堆排序算法在最好和最坏情况下的时间复杂度都为 $O(nlog_2n)$，空间复杂度为 $O(1)$。 堆排序算法是不稳定排序算法。  6.4 适用场景 #   堆排序在建立堆和调整堆的过程中会产生比较大的开销，在元素少的时候并不适用，但是在元素比较多的时候还是一个不错的选择。 在解决诸如 前 n 的数 一类问题时，几乎是首选算法。  7 归并排序 #  7.1 算法原理 #   归并排序是创建在归并操作上的一种有效的排序算法。 该算法是采用分治法（Divide and Conquer）的一种非常典型的应用，且各层递归可以同时进行。 归并算法的具体过程如下（分而治之）：   合并相邻有序子序列的方法：    7.2 参考代码 #  /** * @author peng.wei * @version 1.0 * @date 2021/5/5 21:28 * @Description 归并排序 */ public class MergeSort { /** * 归并排序 * @param arr 数组 */ public static void sort(int[] arr) { int length = arr.length; int[] temp = new int[length]; sort(arr, 0, length - 1, temp); } /** * 归并排序（递归） * @param arr 数组 * @param left 左边界 * @param right 右边界 * @param temp 临时数组 */ public static void sort(int[] arr, int left, int right, int[] temp) { if (left \u0026lt; right) { int mid = left + (right - left) / 2; // 左边归并排序，使得左子序列有序  sort(arr, left, mid, temp); // 右边归并排序，使得右子序列有序  sort(arr, mid + 1, right, temp); // 将两个有序子数组合并  merge(arr, left, mid, right, temp); } } /** * 合并两个序列 * @param arr 数组 * @param left 左边界 * @param mid 中间元素 * @param right 右边界 * @param temp 临时数组 */ public static void merge(int[] arr, int left, int mid, int right, int[] temp) { // 左序列指针  int i = left; // 右序列指针  int j = mid + 1; // 临时数组指针  int t = 0; // 开始遍历左右两个序列  while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= right) { // 如果左边的元素小一些，则将左边的元素移动到 temp 数组中，同时左边的指针加 1  if (arr[i] \u0026lt; arr[j]) {temp[t++] = arr[i++];} // 如果右边的元素小一些，则将右边的元素移动到 temp 数组中，同时右边的指针加 1  else if (arr[i] \u0026gt; arr[j]) {temp[t++] = arr[j++];} } // 将左边的剩余元素移动到 temp 中  while (i \u0026lt;= mid) {temp[t++] = arr[i++];} // 将右边的剩余元素移动到 temp 中  while (j \u0026lt;= right) {temp[t++] = arr[j++];} // 将 temp 中的元素全部拷贝到原数组中  t = 0; while (left \u0026lt;= right) {arr[left++] = temp[t++];} } } 7.3 算法分析 #   归并排序算法在最好情况下和最坏情况下的时间复杂度均为 $O(nlog_2n)$，空间复杂度为 $O(n)$。 归并排序算法是一种稳定排序算法，同时也是一种十分高效的排序算法，其速度仅次于快速排序。  7.4 适用场景 #   归并排序在数据量比较大的时候在效率上也有较为出色的表现。 但是，其空间复杂度$O(n)$ 使得在数据量特别大的时候（例如 1000 万条数据）几乎不可接受，而且，考虑到有的机器内存本身就比较小，因此，采用归并排序时一定要注意。  8 总结 #  8.1 算法分类 #  十种常见排序算法可以分为两大类：\n 比较类排序： 通过比较来决定元素间的相对次序，由于其时间复杂度不能突破 $O(nlogn)$，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，他可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。   8.2 算法复杂度 #   稳定： 如果a 原本在b 前面，且a=b，排序之后a 仍然在b 前面。 不稳定： 如果a 原本在b 前面，且a=b，排序之后a 可能会在b 的后面。 时间复杂度： 对排序数据的总的操作次数，反映当n 变化时呈现什么规律。 空间复杂度： 指算法在计算机内执行时所需存储空间的度量，他也是数据规模n 的函数。   8.2.1 稳定性 #  稳定的算法有：插（如排序）、冒（泡排序）、归（并排序）、计（数排序）、桶（排序）、基（数排序）。\n不稳定的算法有：其他的 4 种都为不稳定的排序算法。\n8.2.2 时间复杂度 #  平均时间复杂度为 $O(nlog_2n)$ 的有：堆（排序）、快（速排序）、归（并排序）。\n平均时间复杂度为 $O(n^{1.3})$ 的有：希（尔排序）。\n平均时间复杂度为 $O(n^2)$ 的有：插（入排序）、选（择排序）、冒（泡排序）。\n8.2.3 空间复杂度 #  空间复杂度为 $O(1)$ 的有：插（入排序）、希（尔排序）、选（择排序）、堆（排序）、冒（泡排序）。\n空间复杂度为 $O(n)$ 的有：归（并排序）。\n空间复杂度为 $O(nlog_2n)$ 的有：快（速排序）。\n参考文献 #    图解排序算法(一)之 3 种简单排序(选择，冒泡，直接插入)。  希尔排序。  图解快速排序。  快速排序算法详解（原理、实现和时间复杂度）。  图解排序算法(三)之堆排序。  堆排序。  图解排序算法(四)之归并排序。  【算法】排序算法之归并排序。  [算法总结] 十大排序算法。  十大经典排序算法（动图演示）。  "},{"id":76,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.4-%E4%BA%8B%E5%8A%A1%E6%97%A5%E5%BF%97/","title":"1.4 事务日志","section":"1、数据库基础","content":"事务日志 #  数据库的事务日志主要分三类，分别是Binlog、Redo Log、Undo Log。\n1 Binlog #  1.1 含义 #   Binlog 记录了对 MySQL 数据库执行更改的所有操作，但是不包括 SELECT 和 SHOW 这类操作，但是若操作本身并没有导致数据库发生变化，那么该操作可能也会写入 Binlog，如update t set a = 1 where a = 2;，但是表t 中并不存在a = 2 的巨鹿，此时并不会对数据库进行更改，但是这个操作也会写入 Binlog。 Binlog 主要有以下几种作用：  恢复： 某些数据的恢复需要 Binlog，例如，在一个数据库全备文件恢复后，用户可以通过 Binlog 进行 point-in-time 的恢复（将数据恢复到一个给定的时间点，通常是在进行全量备份之后进行）。 复制： 其原理与恢复类似，通过复制和执行 Binlog 使一台远程的 MySQL 数据库（一般称为 slave 或 standby）与一台 MySQL 数据库（一般称为 master 或 primary）进行实时同步。 审计： 用户可以通过 Binlog 中的信息进行审计，判断是否有对数据库进行注入的攻击。    1.2 常用参数 #  1.2.1 max_binlog_size #   max_binlog_size 指定了单个 binlog 文件的最大值，如果超过该值，将产生一个新的 binlog，后缀名加 1，并记录到 .index 文件。 从 MySQL 5.0 开始的默认值为1073741824，代表1G，在之前版本中max_binlog_size 默认大小为 1.1G。  1.2.2 binlog_cache_size #   当使用事务的表存储引擎（如 InnoDB 存储引擎）时，所有未提交的 binlog 会被记录到一个缓存中，等到改该事务提交时直接将缓冲中的 binlog 写入 binlog 文件，而该缓冲的大小由 binlog_cache_size 决定，默认大小为 32K。 binlog_cache_size是基于会话的，也就是说，当一个线程开始一个事务时，MySQL会自动分配一个大小为 binlog_cache_size 的缓存，因此，该值的设置需要相当小心，不能设置过大，当一个事务的记录大于设定的 binlog_cache_size 时，MySQL会把缓冲中的日志写入一个临时文件中，因此该值又不能设得太小。  1.2.3 sync_binlog #   在默认情况下，binlog 并不是在每次写的时候同步到磁盘，因此，当数据库所在操作系统发生宕机时，可能会有最后一部分数据没有写入 binlog 文件中，这会给恢复和复制带来问题。 参数 sync_binlog = [N] 表示每写缓冲多少次就同步到磁盘，默认值为 0，如果 N 设为 1，表示采用同步写磁盘的方式来写 binlog，这时写操作不使用缓冲来写 binlog。 但是，即使将 sync_binlog 设为 1，还是会有一种情况导致问题的发生，当使用 InnoDB 存储引擎时，在一个事务发出 COMMIT 动作之前，由于sync_binlog 为 1，因此会将 binlog 立即写入磁盘，如果这时已经写入了 binlog，但是提交还没有发生，并且此时发生了宕机，那么在 MySQL 数据库下次启动时，由于**COMMIT 操作并没有发生**，这个**事务会被回滚**掉，但是**二进制日志已经记录了该事务信息**，**不能被回滚**，**这个问题可以通过将参数 innodb_support_xa 设为 1 来解决**，虽然innodb_support_xa 与XA 事务有关，但他同时也**确保了 binlog 和 InnoDB 存储引擎数据文件的同步**。  1.2.4 binlog-do-db 和 binlog-ignore-db #   binlog-do-db 和binlog-ignore-db 表示需要写入或忽略写入哪些库的日志，默认为空，表示需要同步所有库的日志到 binlog。  1.2.5 log-slave-update #   如果当前数据库是复制中的 slave 角色，则他不会将从 master 取得并执行的 binlog 写入到自己的 binlog 文件中去，如果需要写入，要设置 log-slave-update。 如果需要搭建 master =\u0026gt; slave =\u0026gt; slave 架构的复制，则必须设置该参数。  1.2.6 binlog_format #    binlog_format 参数十分重要，他影响了记录 binlog 的格式。\n  在MySQL 5.1 版本之前，没有这个参数，所有 binlog 的格式都是基于 SQL 语句级别的，但是如果在主服务器运行 rand、uuid 等函数，又或者使用触发器等操作，这些都可能会导致主从服务器上表中数据的不一致。\n  MySQL 5.1 开始引入了 binlog_format 参数，该参数可设的值有 STATEMENT、ROW、MIXED，每个参数的具体含义如下：\n STATEMENT 格式和之前的 MySQL 版本一样，binlog 记录的是日志的逻辑 SQL 语句。 在ROW 格式下，binlog 记录的不再是简单的 SQL 语句了，而是记录表的行更改情况。 在MIXED 格式下，MySQL默认采用 STATEMENT 格式进行二进制日志文件的记录，但是在一些情况下会使用 ROW 格式，例如：  表的存储引擎为 NDB，这时对表的 DML 操作都会以 ROW 格式记录。 使用了 UUID()、USER()、CURRENT_USER()、FOUND_ROWS()、ROW_COUNT() 等不确定函数。 使用了 INSERT DELAY 语句。 使用了用户定义函数。 使用了临时表。      binlog_format 是动态参数，因此可以在数据库运行环境下更改，例如：\nSET @session.binlog.format = \u0026#39;ROW\u0026#39;;   在通常情况下，我们将参数 binlog_format 设置为 ROW，这样可以为数据库的恢复和复制带来更好的可靠性，但是会带来 binlog 文件大小的增加，而且由于复制是采用传输 binlog 的方式实现的，因此复制的网络开销也会有所增加。\n  2 Redo Log #  2.1 Redo Log 与 Binlog 的区别 #   Binlog 是在存储引擎的上层产生的，不管是什么存储引擎，对数据库的修改都会产生 Binlog，而Redo Log 是 InnoDB 层产生的，只记录该存储引擎中表的修改，并且Binlog 先于 Redo Log 被记录。 Binlog 记录操作的方法是逻辑性的语句，即便他是基于 ROW 格式的记录方式，其本质也还是逻辑的 SQL 设置，而Redo Log 是物理格式的日志，它记录的是数据库中每个页的修改。 Binlog 只在每次事务提交的时候一次性写入缓存中的日志文件，而Redo Log 在数据准备修改前要先写入缓存中的 Redo Log 中，然后才对缓存中的数据执行修改操作，而且保证在发出事务提交指令时，先向缓存中的 Redo Log 写入日志，写入完成后才执行提交动作。 因为 Binlog 只在提交的时候一次性写入，所以Binlog 中的记录方式和提交顺序有关，且一次提交对应一次记录，而Redo Log 中记录的是物理页的修改，Redo Log 文件中同一事务可能多次记录，最后一个提交的事务记录会覆盖所有未提交的事务记录，例如事务 $T_1$，可能在 Redo Log 中记录了 $T_{11}$、$T_{12}$、$T_{13}$、$T_{1*}$ 共 4 个操作，其中 $T_{1*}$ 表示最后提交时的日志记录，所以对应的数据页最终状态是 $T_{1*}$ 对应的操作结果，而且**Redo Log 是并发写入的**，**不同事务之间的不同版本的记录会穿插写入到 Redo Log 文件中**，例如可能 Redo Log 的记录方式为 $T_{11}$、$T_{12}$、$T_{21}$、$T_{22}$、$T_{2*}$、$T_{13}$、$T_{1*}$。 Redo Log 记录的是物理页的情况，它具有幂等性，因此记录日志的方式极其简练，幂等性的意思是多次操作前后状态是一样的，例如新插入一行又删除该行，前后状态没有变化，而Binlog 记录的是所有影响数据的操作，记录的内容较多，例如插入一行记录一次，删除该行又记录一次。  2.2 含义 #    Redo Log 包括两部分，一部分是内存中的日志缓冲（Redo Log Buffer），这部分日志是易失的，另一部分是磁盘上的重做日志文件（Redo Log File），这部分日志是持久的。\n  InnoDB 通过 Force Log at Commit 机制实现事务的持久性，即在事务提交的时候，必须先将该事务的所有事务日志写入到磁盘上的 Redo Log File 和 Undo Log File 中进行持久化。\n  为了确保每次日志都能写入到日志文件中，在每次将 Log Buffer 中的日志写入到日志文件的过程中都会调用一次操作系统的 fsync 操作，即 fsync() 系统调用，因为MySQL是工作在用户空间的，其 Log Buffer 处于用户空间的内存中，要写入到磁盘上的 Log File 中，中间还要经过操作系统内核空间的 OS Buffer，调用 fsync() 就是将 OS Buffer 中的日志刷到磁盘上的 Log File 中。\n   在此处需要注意的一点是，一般所说的 Log File 并不是磁盘上的物理日志文件，而是操作系统缓存中的 Log File，但这本身不太容易理解，既然都称为 file 了，应该已经属于物理文件了，所以在本文后续内容中都以 OS Buffer 或 File System Buffer 来表示官方所说的 file，然后Log File 则表示磁盘上的物理日志文件，即 Log File on Disk。 之所以要经过一层 OS Buffer，是因为打开日志文件的时候，没有使用 O_DIRECT 标志位，该标志位意味着绕过操作系统层的 OS Buffer，IO 直写到底层存储设备，不使用该标志位意味着将日志进行缓冲，缓冲到了一定容量，或者显式 fsync() 才会将缓冲中的刷到存储设备，使用该标志位意味着每次都要发起系统调用，比如写abcde，不使用 O_DIRECT 将只发起一次系统调用，使用O_DIRECT 将发起 5 次系统调用。     MySQL 支持用户自定义在 Commit 时如何将 Buffer 中的日志刷到 Log File 中，这种控制通过变量 innodb_flush_log_at_trx_commit 的值来决定，该变量有三种值，分别是 0、1、2，默认是 1，具体如下：\n  当设置为0的时候，事务提交时不会将 Log Buffer 中的日志写入到 OS Buffer，而是每秒写入 OS Buffer 并调用 fsync() 写入到 Log File on Disk 中，也就是说设置为 0 时是每秒刷新写入到磁盘中的，当系统崩溃，会丢失 1 秒钟的数据。\n  当设置为1的时候，事务每次提交都会将 Log Buffer 中的日志写入 OS Buffer，并调用 fsync() 刷到 Log File on Disk 中，这种方式及时系统崩溃也不会丢失任何数据，但是因为每次都写入磁盘，IO 性能较差。\n  当设置为2的时候，每次提交都仅写入到 OS Buffer，然后是每秒调用 fsync() 将 OS Buffer 中的日志写入到 Log File on Disk。\n     在主从复制结构中，要保证事务的持久性和一致性，需要对日志相关变量设置如下：\n 如果启用了 Binlog，则设置 sync_binlog = 1，即每提交一次事务，同步写到磁盘中。 总是设置 innodb_flush_log_at_trx_commit = 1，即每提交一次事务都写到磁盘中。 上面两项变量的设置保证了每次提交事务都写入二进制日志和事务日志，并在提交的时候将他们刷新到磁盘中。    2.3 测试刷日志的时间对数据修改性能的影响 #  选择刷日志的时间会严重影响数据修改时的性能，特别是刷到磁盘的过程，下面测试 innodb_flush_log_at_trx_commit 分别设置为 0、1、2 时的差距：\n  首先创建表和相应的存储过程：\n#创建测试表 drop table if exists test_flush_log; create table test_flush_log(id int,name char(50))engine=innodb; #创建插入指定行数的记录到测试表中的存储过程 drop procedure if exists proc; delimiter $$ create procedure proc(i int) begin declare s int default 1; declare c char(50) default repeat(\u0026#39;a\u0026#39;,50); while s\u0026lt;=i do start transaction; insert into test_flush_log values(null,c); commit; set s=s+1; end while; end$$ delimiter ;   当前环境下，innodb_flush_log_at_trx_commit 的值为 1，即每次提交都刷日志到磁盘，测试此时插入 10W 条记录的时间：\nmysql\u0026gt; call proc(10000); Query OK, 0 rows affected (1 min 4.79 sec) 用时 1 分 1.79 秒。\n  再测试值为 2 的时候，即每次提交都刷新到 OS Buffer，但每秒才刷入到磁盘中：\nmysql\u0026gt; set @@global.innodb_flush_log_at_trx_commit=2; mysql\u0026gt; truncate test_flush_log; mysql\u0026gt; call proc(10000); Query OK, 0 rows affected (29.84 sec) 结果插入时间大减，只需 20.84 秒。\n  最后测试值为 0 的时候，即每秒才刷到 OS Buffer 和磁盘：\nmysql\u0026gt; set @@global.innodb_flush_log_at_trx_commit=0; mysql\u0026gt; truncate test_flush_log; mysql\u0026gt; call proc(10000); Query OK, 0 rows affected (31.81 sec) 结果用时 31.81 秒。\n  最后可以发现，其实值为 2 和 0 的时候，他们的差距并不大，但 2 却比 0 要安全的多，他们都是每秒从 OS Buffer 刷到磁盘，他们之间的差距体现在 Log Buffer 刷到 OS Buffer 上，因为将 Log Buffer 中的日志刷新到 OS Buffer 只是内存数据的转移，并没有太大的开销，所以每次提交和每秒刷入差距并不大，但值为 1 的性能却差很多，尽管设置为 0 和 2 可以大幅度提升插入性能，但是在故障的时候可能会丢失 1 秒钟的数据，这 1 秒钟很可能有大量的数据，从上面的测试结果看，1W 条记录只消耗了 30 多秒，1 秒钟大约有 300-400 条数据，尽管上述插入数据的简单，但却说明了丢失数据的大量性，更好的插入数据的做法是将值设置为 1，然后修改存储过程，将每次循环都提交修改为只提交一次，这样既能保证数据的一致性，也能提升性能，修改如下：\ndrop procedure if exists proc; delimiter $$ create procedure proc(i int) begin declare s int default 1; declare c char(50) default repeat(\u0026#39;a\u0026#39;,50); start transaction; while s\u0026lt;=i DO insert into test_flush_log values(null,c); set s=s+1; end while; commit; end$$ delimiter ;   测试值为 1 的情况：\nmysql\u0026gt; set @@global.innodb_flush_log_at_trx_commit=1; mysql\u0026gt; truncate test_flush_log; mysql\u0026gt; call proc(10000); Query OK, 0 rows affected (1.06 sec)   2.4 Log Block #    InnoDB 存储引擎中，Redo Log 是以块为单位进行存储的，每个块占 512 字节，这称为Redo Log Block，所以不管是 Log Buffer、OS Buffer，还是 Redo Log File on Disk 中，都是这样以 512 字节的块存储的。\n  每个 Redo Log Block 由 3 部分组成，分别是日志块头、日志主体和日志块尾，其中日志块头占用 12 字节，日志主体占用 492 字节，日志块尾占用 8 字节。   因为 Redo Log 记录的是数据页的变化，当一个数据页产生的变化需要使用超过 492 字节的 Redo Log 记录，那么就会使用多个 Redo Log Block 来记录该数据页的变化。\n  日志块头包含 4 部分，具体如下：\n  log_block_hdr_no：占用 4 字节，表示该日志块在 Redo Log Buffer 中的位置 ID。\n  log_block_hdr_data_len：占用 2 字节，表示该日志块中已记录的日志大小，写满该日志块时为 0x200，表示 512 字节。\n  log_block_first_rec_group：占用 2 字节，表示该日志块中第一个日志的开始偏移位置。\n  log_block_checkpoint_no：占用 4 字节，表示写入检查点信息的位置。\n 关于日志块头的第三部分 log_block_first_rec_group：\n 因为有时候一个数据页产生的日志量超出了一个日志块，这时需要用多个日志块来记录该页的相关日志。 例如某一数据页产生了 552 字节的日志量，那么需要占用两个日志块，第一个日志块占用 492 字节，第二个日志块需要占用 60 个字节，那么对于第二个日志块来说，他的第一个日志的开始位置就是 73 字节（60+12），如果该部分的值和log_block_hdr_data_len 相等，则说明该日志块中没有新开始的日志块，即表示该日志块用来延续前一个日志块。       日志主体主要由 4 部分组成：\n redo_log_type：占用 1 字节，表示 Redo Log 的日志类型。 space：采用压缩的方式后，占用空间可能小于 4 字节，表示表空间的 ID。 page_no：同样是压缩过的，表示页的偏移量。 redo_log_body：表示每个 Redo Log 的数据部分，恢复时会调用相应的函数进行解析，不同类型的 SQL 语句写入 Redo Log 的内容是不一样的，例如下面的insert 和delete 的记录方式：   因为 InnoDB 存储引擎数据的单元是页，所以 Redo Log 也是基于页的格式来记录的。 默认情况下，InnoDB 的页大小是 16KB（由innodb_page_size 变量控制），一个页内可以存放非常多的 Log Block（每个 512 字节），而 Log Block 中记录的又是数据页的变化。       日志尾只有一个部分 log_block_trl_no，该值和日志块头的 log_block_hdr_no 相等。\n  上面所说的是一个日志块的内容，在 Redo Log Buffer 或者 Redo File on Disk 中，由很多日志块组成，如下图所示：\n   2.5 Log Group 和 Redo Log File #    Log Group 表示的是 Redo Log Group，一个组内由多个大小完全相同的 Redo Log File 组成，组内 Redo Log File 的数量由变量 innodb_log_files_group 决定，默认值为 2，即两个 Redo Log File。\n  这个组是一个逻辑的概念，并没有真正的文件来表示这是一个组，但是可以通过变量 innodb_log_group_home_dir 来定义组的目录，Redo Log File 都放在这个目录下，默认是在 datadir 下。\nmysql\u0026gt; show global variables like \u0026#39;innodb_log%\u0026#39;; +-----------------------------+------------------+ | Variable_name | Value | +-----------------------------+------------------+ | innodb_log_buffer_size | 8388608 | | innodb_log_checksums | ON | | innodb_log_compressed_pages | ON | | innodb_log_file_size | 5242880 | | innodb_log_files_in_group | 2 | | innodb_log_group_home_dir | /www/server/data | | innodb_log_write_ahead_size | 8192 | +-----------------------------+------------------+ mysql\u0026gt; system ls -l /www/server/data/ib* -rw-r----- 1 mysql mysql 381 6 月 28 10:38 /www/server/data/ib_buffer_pool -rw-r----- 1 mysql mysql 77594624 6 月 28 10:38 /www/server/data/ibdata1 -rw-r----- 1 mysql mysql 5242880 6 月 28 10:38 /www/server/data/ib_logfile0 -rw-r----- 1 mysql mysql 5242880 6 月 27 17:28 /www/server/data/ib_logfile1 -rw-r----- 1 mysql mysql 12582912 6 月 28 10:38 /www/server/data/ibtmp1 可以看到在默认的数据目录下，有两个 ib_logfile 开头的文件，他们就是 Log Group 中的 Redo Log File，而且他们的大小完全一致，且等于变量 innodb_log_file_size 定义的值，第一个文件 ibdata1 是在没有开启 innodb_file_per_table 时的共享表空间文件，对应于开启 innodb_file_per_table 时的 .ibd 文件。\n  在 InnoDB 将Log Buffer 中的 Redo Log Block 刷到这些 Log File 中时，会以追加写入的方式循环轮询写入，即先在第一个 Log File（即 ib_logfile0）的尾部追加写入，直到满了之后再向第二个 Log File（即 ib_logfile1）写，当第二个 Log File 满了之后会清空一部分第一个 Log File，然后继续写入，由于是将 Log Buffer 中的日志刷到 Log File，所以在 Log File 中记录日志也是 Log Block 的方式。\n  在每组的第一个 Redo Log File 中，前 2KB 记录 4 个特定的部分，从 2KB 之后才开始记录 Log Block，除了第一个 Redo Log File 中会记录，Log Group 中的其它 Log File 不会记录这 2KB，但是却会腾出这 2KB 的空间，如下图所示：\n   Redo Log File 的大小对 InnoDB 的性能影响非常大，设置的太大，恢复的时候就会时间越长，设置的太小，就会导致在写 Redo Log 的时候循环切换 Redo Log File。\n  2.6 日志刷盘规则 #  Log Buffer 中未刷到磁盘的日志称为脏日志，刷日志到磁盘主要有以下几种规则：\n 发出 Commit 动作，Commit 发出后是否刷日志由变量innodb_flush_log_at_trx_commit 控制，具体可参考 2.2 含义。 每秒刷一次，这个刷日志的频率由变量innodb_flush_log_at_timeout 值决定，默认是 1 秒，需要注意的是，这个刷日志的频率和 Commit 动作无关。 Log Buffer 已经使用的内存超过一半。 发生 Checkpoint。  2.7 LSN #  2.7.1 含义 #    LSN 称为日志的逻辑序列号（Log Sequence Number），在 InnoDB 存储引擎中，LSN 占用 8 个字节，LSN 的值会随着日志的写入而逐渐增大。\n  根据 LSN，我们可以获取到几个有用的信息：\n 数据页的版本信息。 写入的日志总量（通过 LSN 开始号码和结束号码可以计算出写入的日志总量）。 检查点的位置。    LSN 不仅存在于重做日志中，还存在于每个页中：\n 在每个页的头部，有一个值FIL_PAGE_LSN，记录了该页的 LSN。 在页中，LSN 表示该页最后刷新时 LSN 的大小。 因为重做日志记录的是每个页的日志，因此页中的 LSN 用来判断页是否需要进行恢复操作：  例如，页 P1 的 LSN 为 10000，而数据库启动时，InnoDB 检测到写入重做日志中的 LSN 为 13000，并且该事务已经提交，那么数据库需要进行恢复操作，将重做日志应用到 P1 页中。 同样的，对于重做日志中 LSN 小于 P1 页的 LSN，那么不需要进行重做，因为 P1 页中的 LSN 表示页已经被刷新到该位置。      Redo Log 的 LSN 信息可以通过 show engine innodb status \\G; 来查看，具体如下：\nmysql\u0026gt; show engine innodb status \\G; --- LOG --- Log sequence number 200876324 Log flushed up to 200876324 Pages flushed up to 200876324 Last checkpoint at 200876315 0 pending log flushes, 0 pending chkp writes 10 log i/o\u0026#39;s done, 0.00 log i/o\u0026#39;s/second 其中：\n Log sequence number：表示当前的 Redo Log In Buffer 中的 LSN。 Log flushed up to：表示刷到 Redo Log File On Disk 中的 LSN。 Pages flushed up to：表示已经刷到磁盘数据页上的 LSN。 Last checkpoint at：表示上一次检查点所在位置的 LSN。    2.7.2 修改机制 #  2.7.2.1 简单说明 #  InnoDB 从执行修改语句开始：\n 首先修改内存中的数据页，并在数据页中记录 LSN，暂且称之为data_in_buffer_lsn。 在修改数据页的同时向 Redo Log In Buffer 中写入 Redo Log，并记录下对应的 LSN，暂且称之为redo_log_in_buffer_lsn。 写完 Buffer 中的日志后，当触发了日志刷盘的几种规则时，会向 Redo Log File On Disk 刷入重做日志，并在该文件中记下对应的 LSN，暂且称之为redo_log_on_disk_lsn。 数据页不可能永远只停留在内存中，在某些情况下，会触发 Checkpoint 来将内存中的脏页（数据脏页和日志脏页）刷到磁盘，所以会在本次 Checkpoint 脏页刷盘结束时，在 Redo Log 中记录 Checkpoint 的 LSN 位置，暂且称之为checkpoint_lsn。 要记录 Checkpoint 所在位置很快，只需简单的设置一个标志即可，但是刷数据页并不一定很快，例如这一词 Checkpoint 要刷入的数据页非常多，也就是说要刷入所有的数据页需要一定的时间来完成，中途刷入的每个数据页都会记下当前页所在的 LSN，暂且称之为data_page_on_disk_lsn。  2.7.2.2 详细说明 #  详细说明如下图：\n   假设最初时（12:00:00）所有的日志和数据页都完成了刷盘，也记录好了检查点的 LSN，这时他们的 LSN 是完全一致的。\n  假设此时开启了一个事务，并立即执行了一个 update 操作，执行完成后，Buffer 中的数据页和 Redo Log 都记录好了更新后的 LSN 值，假设为 110，这时候如果执行 show engine innodb status 查看各 LSN 的值，即图中 ① 处的位置状态，结果会是：\nlog sequence number(110) \u0026gt; log flushed up to(100) = page flushed up to = last checkpoint at   之后又执行了一个 delete 操作，LSN 增长到 150，等到 12:00:01 时，触发 Redo Log 刷盘的规则（其中有一个规则是 innodb_flush_log_at_timeout 控制的默认日志刷盘频率为 1 秒），这时Redo Log On Disk 中的 LSN 会更新到和 Redo Log Buffer 的 LSN 一样，都等于 150，这是 show engine innodb status，即图 ② 中的位置，结果会是：\nlog sequence number(150) = log flushed up to(150) \u0026gt; page flushed up to(100) = last checkpoint at   之后又执行了一个 update 操作，缓存中的 LSN 将增长到 300，即图 ③ 的位置。\n  假设随后检查点出现，即图 ④ 的位置，此时会触发数据页和日志页刷盘，但需要一定的时间来完成，所以在数据页刷盘还未完成时，检查点的 LSN 还是上一次检查点的 LSN，但此时磁盘上数据页和日志页的 LSN 已经增长了，结果如下：\nlog sequence number \u0026gt; log flushed up to 和 pages flushed up to \u0026gt; last checkpoint at 但是 log flushed up to 和 pages flushed up to 的大小无法确定，因为日志刷盘可能快于、慢于或等于数据刷盘，但是Checkpoint 机制有保护数据刷盘速度是慢于日志刷盘的，当数据刷盘速度超过日志刷盘时，将会暂时停止数据刷盘，等待日志刷盘进度超过数据刷盘。\n  等到数据页和日志页刷盘完毕，即到了位置 ⑤ 的时候，所有的 LSN 都等于 300。\n  随着时间的推移到了 12:00:02，即图中位置 ⑥，此时又触发了日志刷盘规则，但此时Buffer 中的日志 LSN 和磁盘中的日志 LSN 是一致的，所以不执行日志刷盘，几次是所有的 LSN 都是相等的。\n  随后执行了一个 insert 操作，假设 Buffer 中的 LSN 增长到了 800，即图中位置 ⑦，此时各种 LSN 的大小和位置 ① 时一样。\n  随后执行了提交动作，即位置 ⑧，默认情况下，提交动作会触发日志刷盘，但不会触发数据刷盘，所以 show engine innodb status 的结果是：\nlog sequence number = log flushed up to \u0026gt; pages flushed up to = last checkpoint at   最后随着时间的推移，检查点再次出现，即图中位置 ⑨，但是这次检查点不会触发日志刷盘，因为日志的 LSn 在检查点出现之前已经同步了，假设这次数据刷盘速度极快，快到一瞬间内完成而无法捕捉状态的变化，这时 show engine innodb status 的结果是所有的 LSN 都是相等的。\n  3 Undo Log #  3.1 含义 #   重做日志记录了事务的行为，可以很好地通过其对页进行“重做操作”，但是事务有时还需要进行回滚操作，这时就需要 Undo，因此在对数据库进行修改时，InnoDB 存储引擎不但会产生 Redo Log，还会产生一定量的 Undo Log，这样如果用户执行的事务或语句由于某种原因失败了，或者用户用一条 ROLLBACK 请求回滚，就可以利用这些 Undo 信息将数据回滚到修改之前的样子。 Redo Log 存放在重做日志文件中，Undo Log 存放在数据库内部一个特殊段（Segment）中，这个段称为Undo 段（Undo Segment），位于共享表空间内。 Undo Log 是逻辑日志，只是将数据库逻辑地恢复到原来的样子，所有修改都被逻辑地取消了，但是数据结构和页本身在回滚之后可能大不相同：  这是因为在多用户并发系统中，可能会有数十、数百、甚至数千个并发事务，数据库的主要任务就是协调对数据记录的并发访问，比如，一个事务在修改当前一个页中某几条记录，同时还有别的事务在对同一个页中另几条记录进行修改，因此不能将一个页回滚到事务开始的样子，因为这样会影响到其他事务正在进行的工作。 假如用户执行了一个INSERT 10W 条记录的事务，这个事务会导致分配一个新的段，即表空间会增大，在用户执行 ROLLBACK 时，会将插入的事务进行回滚，但是表空间的大小并不会因此而收缩，因此，当InnoDB 存储引擎回滚时，它实际上做的是与之前相反的工作：  对于每个 INSERT，InnoDB 存储引擎会完成一个 DELETE。 对于每个 DELETE，InnoDB 存储引擎会执行一个 INSERT。 对于每个 UPDATE，InnoDB 存储引擎会执行一个相反的 UPDATE，将修改前的行放回去。     除了回滚操作，Undo Log 的另一个作用是MVCC，即在InnoDB 存储引擎中 MVCC 的实现是通过 Undo Log 来完成，当用户读取一行记录时，若该记录已经被其他事务占用，当前事务可以通过 Undo Log 读取之前的行版本信息，以此实现非锁定读取。 Undo Log 也会产生 Redo Log，也就是 Undo Log 的产生会伴随着 Redo Log 的产生，这是因为 Undo Log 也需要持久性的保护。  3.2 存储管理 #   InnoDB 存储引擎对 Undo Log 的管理同样采用段的方式，InnoDB 存储引擎有回滚段（Rollback Segment），每个回滚段中记录了 1024 个 Undo Log Segment，然后在每个 Undo Log Segment 中进行 Undo 页的申请。 在InnoDB 1.1 版本之前，只有一个 Rollback Segment，因此支持同时在线的事务限制为 1024，从 InnoDB 1.1 版本开始，InnoDB支持最大 128 个 Rollback Segment，因此支持同时在线的事务限制提高到了 128*1024。 可以通过如下参数对 Rollback Segment 做进一步的设置：  innodb_undo_directory：用于设置 Rollback Segment 文件所在的路径，默认值为“.”，表示当前 InnoDB 存储引擎的目录。 innodb_undo_logs：用来设置 Rollback Segment 的个数，默认值为 128，在 InnoDB 1.2 版本中，该参数用来替换之前版本的参数innodb_rollback_segments。 innodb_undo_tablespaces：用来设置构成 Rollback Segment 文件的数量，这样Rollback Segment 可以较为平均地分布在多个文件中，设置该参数后，会在路径innodb_undo_directory 看到undo 为前缀的文件，该文件就代表 Rollback Segment 文件。   事务在 Undo Log Segment 分配页写入 Undo Log 的这个过程同样需要写入重做日志，当事务提交时，Innodb 存储引擎会做以下两件事情：  将 Undo Log 放入链表中，以供之后的 purge 操作：  事务提交之后并不能马上删除 Undo Log 以及 Undo Log 所在的页，这是因为可能还有其他事务需要通过 Undo Log 来得到行记录之前的版本，因此事务提交时将 Undo Log 放入一个链表中，是否可以最终删除 Undo Log 和 Undo Log 所在的页由 purge 线程来判断。   判断 Undo Log 所在的页是否可以重用，若可以，则分配给下个事务使用：  若为每一个事务分配一个单独的 Undo 页会非常浪费存储空间，特别是对于 OLTP 的应用类型，因为在事务提交时，可能并不能马上释放页，因此，在 InnoDB 存储引擎的设计中对 Undo 页可以进行重用：  当事务提交时，首先将 Undo Log 放入链表中，然后判断 Undo 页的使用空间是否小于$\\frac34$，若是，则表示该 Undo 页可以被重用，之后新的 Undo Log 记录在当前 Undo Log 的后面。 由于存放 Undo Log 的列表是以记录进行组织的，而Undo 页可能存放着不同事务的 Undo Log，因此purge操作需要涉及磁盘的离散读取操作，是一个比较缓慢的过程。        3.3 格式 #  在 InnoDB 存储引擎中，Undo Log 分为两种，分别是Insert Undo Log、Update Undo Log。\n3.3.1 Insert Undo Log #   Insert Undo Log 是指在Insert 操作中产生的 Undo Log，因为Insert 操作的记录，只对事务本身可见，对其他事务不可见（这是事务隔离性的要求），因此Insert Undo Log 可以在事务提交后直接删除。 Insert Undo Log 的格式如下图所示，详细信息如下：   next：2 个字节，表示下一个 Undo Log 的位置。\n  type_cmpl：1 字节，表示 Undo 的类型，对于 Insert Undo Log，该值总是 11。\n  undo_no：表示事务的 ID，采用压缩存储。\n  table_id：表示 Undo Log 所对应的表对象，采用压缩存储。\n  lenN、colN：表示主键的列和值，在进行 ROLLBACK 操作时，根据这些值可以定位到具体的记录，然后进行删除即可。\n  start：2 字节，表示 Undo Log 的开始位置。\n     3.3.2 Update Undo Log #   Update Undo Log 记录的是对 delete 和 update 操作所产生的 Undo Log，该 Undo Log可能需要提供 MVCC 机制，因此不能在事务提交时就删除，提交时放入 Undo Log 链表，等待 purge 线程进行最后的删除。 Update Undo Log 的结构如下图所示，详细信息如下（Update Undo Log 相对于 Insert Undo Log，记录的内容更多，所需占用的空间也更大，其中next、start、undo_no、table_id 与 Insert Undo Log 相同，这里不再叙述。）：   type_cmpl：表示 Undo 的类型，由于 Update Undo Log 本身还有其他类型，因此其可能的值如下：\n 12：TRX_UNDO_UPD_EXIST_REC，表示更新 non-delete-mark 的记录。 13：TRX_UNDO_UPD_DEL_REC，表示将 delete 的记录标记为 not delete。 14：TRX_UNDO_DEL_MARK_REC，表示将记录标记为 delete。    update_vector：表示 update 操作导致发生改变的列，每个修改的列信息都要记录在 Undo Log 中，对于不同的 Undo Log 类型，可能还需要记录对索引列所做的修改。\n     3.4 delete/update 操作的内部机制 #  3.4.1 delete #   delete操作并不直接删除记录，而是将记录标记为已删除，也就是将记录的 delete flag 设置为 1，而记录最终的删除是在 purge 操作中完成的。  3.4.2 update #  update 分为两种情况：\n 如果update的是非主键，在 Update Undo Log 中直接反向记录是如何 update 的即可。 如果update的是主键，则主要分两个步骤进行，首先将原主键记录标记为已删除，然后插入一条新的纪录。  3.5 purge #  3.5.1 含义 #    假如存在如下的表：\nCREATE TABLE t( a INT, b VARCHAR(32), PRIMARY KEY(a), KEY(b) )ENGINE=InnoDB;   delete 和 update 操作可能并不直接删除原有的数据：\n  例如，对表 t 执行如下的 SQL 语句：\nDELETE FROM t WHERE a = 1; 表 t 上列 a 有聚集索引，列 b 上有辅助索引。\n  对于上述的 delete 操作，仅是将主键列等于 1 的记录 delete flag 设置为 1，记录并没有被删除，即记录还是存在于 B+ 树中。\n  其次，对辅助索引上 a 等于 1，b 等于 1 的记录同样没有做任何处理，甚至没有产生 Undo Log，而真正删除这行记录的操作其实被延时了，最终在 purge 操作中完成。\n    purge用于最终完成 delete 和 update 操作：\n 这样设计是因为InnoDB 存储引擎支持 MVCC，所以记录不能在事务提交时立即进行处理，这时其他事务可能正在引用这行，故 InnoDB 存储引擎需要保存记录之前的版本，而是否可以删除该条记录通过 purge 来进行判断，若该行记录已不被任何其他事务引用，那么就可以进行真正的 delete 操作。 可见，purge 操作是清理之前的delete 和update 操作，将上述操作最终完成，而实际执行的操作为delete 操作，清理之前行记录的版本。    为了节省存储空间，InnoDB 存储引擎的 Undo Log 设计是这样的：\n 一个页上允许多个事务的 Undo Log 存在，虽然这不代表事务在全局过程中提交的顺序，但是后面的事务产生的 Undo Log 总在最后。 此外，InnoDB 存储引擎还有一个**history 列表**，他**根据事务提交的顺序**，**将 Undo Log 进行链接**，如下面的一种情况：   如下图所示，History List 表示按照事务提交的顺序将 Undo Log 进行组织，在 InnoDB 存储引擎的设计中，先提交的事务总在尾端，\n  Undo Page 存放了 Undo Log，由于可以重用，因此一个 Undo Page 中可能存放了多个不同事务的 Undo Log，trx5 的灰色阴影表示该 Undo Log 还被其他事务引用。\n  在执行 purge 的过程中，InnoDB 存储引擎首先从 History List 中找到第一个需要被清理的记录，这里为 trx1，清理之后InnoDB 存储引擎会在 trx1 的 Undo Log 所在的页中继续寻找是否存在可以被清理的记录，这里会找到事务 trx3，接着找到 trx5，但是发现 trx5 被其他事务所引用而不能清理，故会再次去 History List 中查找，发现这时最尾端的记录为 trx2，接着找到 trx2 所在的页，然后依次再把事务 trx6、trx4 的记录进行清理，由于 Undo Page2 中所有的页都被清理了，因此该 Undo Page 可以被重用。\n  InnoDB 存储引擎这种先从 History List 中找 Undo Log，然后再从 Undo Page 中找 Undo Log 的设计模式是为了避免大量的随机读取操作，从而提高 purge 的效率。\n       3.5.2 参数 #  3.5.2.1 innodb_purge_batch_size #   全局动态参数innodb_purge_batch_size 用来设置每次 purge 操作需要清理的 Undo Page 数量： 在InnoDB 1.2 之前，该参数的默认值为 20，从1.2 版本开始，该参数的默认值为 300。 通常来说，该参数设置的越大，每次回收的 Undo Page 也就越多，这样可供重用的 Undo Page 就越多，减少了磁盘存储空间与分配的开销。 不过，该参数设置的越大，则每次需要处理更多的 Undo Page，从而导致 CPU 和磁盘 IO 过于集中于对 Undo Log 的处理，使性能下降。 因此，该参数的调整需要长期观察数据库的运行状态，一般情况下不需要调整该参数。  3.5.2.2 innodb_max_purge_lag #    当InnoDB 存储引擎的压力非常大时，并不能高效地进行 purge 操作，那么History List 的长度会变得越来越长。\n  全局动态参数 innodb_max_purge_lag 用来控制 History List 的长度，若长度大于该参数时，其会延缓 DML 的操作，该参数的默认值为 0，表示不对 History List 做任何限制，当大于 0 时，就会延缓 DML 的操作，其延缓的算法为：\n$$ delay = ((length(history_list) - innodb_max_purge_lag)*10-5 $$\n$delay$ 的单位是毫秒，此外，需要特别注意的是，$delay$ 的对象是行，而不是一个 DML 操作，例如当一个 update 操作需要更新 5 行数据时，每行数据的操作都会被 $delay$，故总的延时时间为 $5*delay$，而 $delay$ 的统计会在每一次 purge 操作完成后重新计算。\n  InnoDB 1.2 版本引入了新的全局动态参数 innodb_max_purge_lag_delay，其用来控制 $delay$ 的最大毫秒数，也就是当上述计算得到的 $delay$ 值大于该参数时，将 $delay$ 设置为 innodb_max_purge_lag_delay，避免由于 purge 操作缓慢导致其他 SQL 线程出现无限制的等待。\n  参考文献 #   《MySQL 技术内幕（InnoDB 存储引擎）第 2 版》  详细分析 MySQL 事务日志(redo log 和 undo log)。  "},{"id":77,"href":"/school-recruitment/docs/database/3Redis/3.1-%E6%A6%82%E8%BF%B0/3.1.4-%E8%BF%87%E6%9C%9F%E6%9C%BA%E5%88%B6%E5%92%8C%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/","title":"3.1.4 过期机制和内存淘汰策略","section":"3.1 概述","content":"过期机制和内存淘汰策略 #  1 过期机制 #  1.1 设置键的生存时间或过期时间 #  1.1.1 设置过期时间 #   Redis 中有四个不同的命令可以用于设置键的生存时间（键可以存在多久）或过期时间（键什么时候被删除）：  EXPIRE \u0026lt;key\u0026gt; \u0026lt;ttl\u0026gt;：命令用于将键 key 的生存时间设置为 ttl 秒。 PEXPIRE \u0026lt;key\u0026gt; \u0026lt;ttl\u0026gt;：命令用于将 键 key 的生存时间设置为 ttl 毫秒。 EXPIREAT \u0026lt;key\u0026gt; \u0026lt;timestamp\u0026gt;：命令用于将键 key 的过期时间设置为 timestamp 所指定的秒数时间戳。 PEXPIREAT \u0026lt;key\u0026gt; \u0026lt;timestamp\u0026gt;：命令用于将键 key 的过期时间设置为 timestamp 所指定的毫秒数时间戳。   虽然有多种不同单位和不同形式的设置命令，但实际上EXPIRE、PEXPIRE、EXPIREAT三个命令都是使用 PEXIREAT 命令来实现的，无论客户端执行的是以上四个命令中的哪一个，经过转换之后，最终的执行效果都和执行 PEXPIREAT 命令一样：   首先，EXPIRE命令可以转换为 PEXPIRE：\ndef EXPIRE(key, ttl_in_sec): # 将 TTL 转换成毫秒 ttl_in_ms = sec_to_ms(ttl_in_sec) PEXPIRE(key, ttl_in_ms)   接着，PEXPIRE 命令又可以换成 PEXPIREAT 命令：\ndef PEXPIRE(key, ttl_in_ms): # 获取以毫秒计算的当前 UNIX 时间戳 now_ms = get_current_unix_timestamp_in_ms() # 当前时间加上 TTL，得出毫秒格式的键过期时间 PEXPIREAT(key, now_ms + ttl_in_ms)   并且，EXPIREAT 命令也可以转换成 PEXPIREAT 命令：\ndef EXPIREAT(key, expire_time_in_sec): # 将过期时间从秒转换为好毫秒 expire_time_in_ms = sec_to_ms(expire_time_in_sec) PEXPIREAT(key, expire_time_in_ms)   最终，EXPIRE、PEXPIRE 和 EXPIREAT 三个命令都会转换成 PEXPIREAT 命令来执行，如下图所示：\n     1.1.2 保存过期时间 #    redisDb结构的 expires 字典保存了数据库中所有键的过期时间，我们称这个字典为过期字典，其中 redisDb 的结构为：\ntypedef struct redisDb { dict *dict; /* The keyspace for this DB */ dict *expires; /* Timeout of keys with a timeout set */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP) */ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ struct evictionPoolEntry *eviction_pool; /* Eviction pool of keys */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */ } redisDb;  过期字典的键是一个指针，这个指针指向键空间中的某个键对象。 过期字典的值是一个 long long 类型的整数，这个整数保存了键所指向的数据库键的过期时间（一个毫秒精度的 UNIX 时间戳）。    如下图所示，过期字典保存了两个键值对：\n  第一个键值对的键为 alphabet 键对象，值为 1385877600000，这表示数据库键 alphabet 的过期时间为 1385877600000（2013 年 12 月 1 日零时）。\n  第二个键值对的键为 book 键对象，值为 1388556000000，这表示数据库键 book 的过期时间为 1388556000000（2014 年 1 月 1 日零时）。\n 为了展示方便，下图中的键空间和过期字典中重复出现了两次 alphabet 键对象和 book 对象，在实际中，键空间的键和过期字典的键都指向一个键对象，所以不会出现任何重复对象，也不会浪费任何空间。\n      当客户端执行 PEXPIREAT（或者其他三个会转换为 PEXIREAT 命令的命令）为一个数据库键设置过期时间时，服务器会在数据库的过期字典中关联给定的数据库键和过期时间，例如如果数据库当前的状态为上图所示，那么在服务器执行以下命令之后，过期字典将新增一个键值对，其中键为 message 对象，而值为 1391234400000（2014 年 2 月 1 日零时），如下图所示：\nredis\u0026gt; PEXPIREAT message 1391234400000    1.1.3 移除过期时间 #    PERSIST 命令可以移除一个键的过期时间：\nredis\u0026gt; PEXPIREAT message 1391234400000 (integer) 1 redis\u0026gt; TTL message (integer) 63862337 redis\u0026gt; PERSIST message (integer) 1 redis\u0026gt; TTL message (integer) -1   PERSIST 命令就是 PEXPIREAT命令的反操作，PERSIST 命令在过期字典中查找给定的键，并解除键和值（过期时间）在过期字典中的关联，例如，数据库当前状态如上面第二张图所示，那么当服务器执行以下命令之后，数据库将更新为如下图所示的状态：\nredis\u0026gt; PERSIST book (integer) 1    可以看到，当 PERSIST 命令执行之后，过期字典中原来的 book 键值对消失了，这代表数据库键 book 的过期时间已经被移除。\n TTL 命令以秒为单位返回键的剩余生存时间，PTTL 以毫秒为单位返回键的剩余生存时间。\n   1.2 过期键删除策略 #   过期删除策略主要有三种，分别是定时删除、惰性删除、定期删除。 Redis 服务器实际使用的是惰性删除和定期删除两种策略，通过配合使用这两种删除策略，服务器可以很好地在合理使用 CPU 时间和避免浪费内存空间之间取得平衡。  1.2.1 定时删除 #  1.2.1.1 含义 #   定时删除是指对于每一个设置了过期时间的 key 都会创建一个定时器，一旦到达过期时间就立即删除。  1.2.1.2 优缺点 #  1.2.1.2.1 优点 #   该策略可以立即清除过期的数据，对内存较友好。  1.2.1.2.2 缺点 #   占用了大量的 CPU 资源去处理过期的数据，会影响 Redis 的吞吐量和响应时间。  1.2.2 惰性删除 #  1.2.2.1 含义 #   惰性删除是指当键的过期时间到达的时候，并不立即进行删除，而是当访问到这个键的时候，检查这个键是否过期，如果已过期就删除并返回 NULL。  1.2.2.2 实现原理 #    Redis 过期键的删除策略由 db.c/expireIfNeeded 函数实现，expireIfNeeded 函数的源码如下：\nint expireIfNeeded(redisDb *db, robj *key) { mstime_t when = getExpire(db,key); mstime_t now; if (when \u0026lt; 0) return 0; /* No expire for this key */ /* Don\u0026#39;t expire anything while loading. It will be done later. */ if (server.loading) return 0; /* If we are in the context of a Lua script, we claim that time is * blocked to when the Lua script started. This way a key can expire * only the first time it is accessed and not in the middle of the * script execution, making propagation to slaves / AOF consistent. * See issue #1525 on Github for more information. */ now = server.lua_caller ? server.lua_time_start : mstime(); /* If we are running in the context of a slave, return ASAP: * the slave key expiration is controlled by the master that will * send us synthesized DEL operations for expired keys. * * Still we try to return the right information to the caller, * that is, 0 if we think the key should be still valid, 1 if * we think the key is expired at this time. */ if (server.masterhost != NULL) return now \u0026gt; when; /* Return when this key has not expired */ if (now \u0026lt;= when) return 0; /* Delete the key */ server.stat_expiredkeys++; propagateExpire(db,key); notifyKeyspaceEvent(REDIS_NOTIFY_EXPIRED, \u0026#34;expired\u0026#34;,key,db-\u0026gt;id); return dbDelete(db,key); }   所有读写数据库的 Redis 命令在执行之前都会调用 expireIfNeeded 函数对输入键进行检查：\n  如果输入键已经过期，那么 expireIfNeeded 函数将输入键从数据库中删除。\n  如果输入键未过期，那么 expireIfNeeded 函数不作任何动作。\n     expireIfNeeded函数就像一个过滤器，他可以在命令真正执行之前，过滤掉过期的输入键，从而避免命令接触到过期键。\n  每个被访问的键都可能因为过期而被 expireIfNeeded 函数删除，所以每个命令的实现函数都必须能同时处理键存在以及键不存在两种情况：\n  当键存在时，命令按照键存在的情况执行。\n  当键不存在或者因为过期而被 expireIfNeeded 函数删除时，命令按照键不存在的情况执行。\n     1.2.2.3 优缺点 #  1.2.2.3.1 优点 #   不会占用过多的 CPU，保证 Redis 的执行效率。  1.2.2.3.2 缺点 #   会有大量的过期键还保存在内存中，占用额外的内存，并且如果一个过期键不会再访问了，那么这个过期键就会一直保存在内存中，可以看作是内存泄漏。  1.2.3 定期删除 #  1.2.3.1 含义 #   定期删除是前两种策略的一种折中方案。 定期删除会每隔一段时间（默认是 1s 扫描 20 次），从数据库中取出一定数量的随机键进行检查（默认是 20 个），然后删除其中的过期键，如果其中已经过期的键超过 25%，那么就立即再次执行一次扫描删除操作。 定期删除会通过限制删除操作执行的时长和频率来防止无限制的执行定期删除操作导致 CPU 占用的影响。  1.2.3.2 实现原理 #    Redis 过期键的定期删除策略由 src/redis.c/activeExpireCycle 函数实现，每当 Redis 的服务周期性操作 src/redis.c/serverCron 函数执行时，activeExpireCycle函数就会被调用，他在规定的时间内，分多次遍历服务器中的各个数据库，从数据库的 expires 字典中随机检查一部分键的过期时间，并删除其中的过期键，整个过程可用伪代码描述如下：\n   activeExpireCycle 函数的工作模式可以总结如下：\n 函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查，并删除其中的过期键。 全局变量 current_db 会记录当前 activeExpireCycle 函数检查的进度，并在下一次 activeExpireCycle 函数调用时，接着上一次的进度进行处理，例如，当前activeExpireCycle 函数在遍历 10 号数据库时返回了，那么下次activeExpireCycle 函数执行时，将从 11 号数据库开始查找并删除过期键。 随着activeExpireCycle 函数的不断执行，服务器中的所有数据库都会被检查一遍，这时函数将 current_db 变量置为 0，然后再开始新一轮的检查工作。    1.2.3.3 优缺点 #  1.2.3.3.1 优点 #   不会占用过多的 CPU，而且可以防止内存浪费。  1.2.3.3.2 缺点 #   如果定期删除执行的时长和频率不合理，就会退化为定时删除，占用过多的 CPU 时间。  1.3 RDB、AOF 和主从复制功能对过期键的处理 #  1.3.1 RDB 对过期键的处理 #  对于 RDB 的相关介绍可参考 2.1 RDB。\n1.3.1.1 生成 RDB 文件 #   在执行 SAVE 命令或者 BGSAVE 命令创建一个新的 RDB 文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的 RDB 文件中。 因此，数据库中包含过期键不会对生成新的 RDB 文件造成影响。  1.3.1.2 载入 RDB 文件 #   在启动 Redis 服务器时，如果服务期开启了 RDB 功能，那么服务器将对 RDB 文件进行载入：  如果服务器以主服务器模式运行，那么在载入 RDB 文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，而过期键则会被忽略，所以过期键对载入 RDB 文件的主服务器不会造成影响。 如果服务器以从服务器模式运行，那么在载入 RDB 文件时，文件中保存的所有键，无论是否过期，都会被载入到数据库中，不过，因为主从服务器在进行数据同步的时候，从服务器的数据库就会被清空，所以一般来讲，过期键对载入 RDB 文件的从服务器也不会有影响。    1.3.2 AOF 对过期键的处理 #  对于 AOF 的相关介绍可参考 2.2 AOF。\n1.3.2.1 AOF 文件写入 #   当服务器以 AOF 持久化模式运行时，如果数据库中的某个键已经过期，但他还没有被惰性删除或定期删除，那么AOF 文件不会因为这个过期键而产生任何影响。 当过期键被惰性删除或者定期删除之后，程序会向 AOF 文件追加（append）一条 DEL 命令，来显式地记录该键已被删除。 例如，如果客户端使用GET message 命令试图访问过期的message 键，那么服务器将执行以下三个动作：  从数据库中删除message 键。 追加一条DEL message 命令到 AOF 文件。 向执行GET 命令的客户端返回空回复。    1.3.2.2 AOF 重写 #   和生成 RDB 文件类似，在执行 AOF 重写的过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的 AOF 文件中。 因此，数据库中包含过期键不会对生成新的 AOF 文件造成影响。  1.3.3 主从复制对过期键的处理 #   当服务器运行在主从复制模式下时，从服务器的过期键删除动作由主服务器控制：  主服务器在删除一个过期键之后，会显式地向所有从服务器发送一个 DEL 命令，告知从服务器删除这个过期键。 从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，而是继续向处理未过期的键一样来处理过期键。 从服务器只有在接到主服务器发来的 DEL 命令之后，才会删除过期键。   通过由主服务器统一地删除过期键，可以保证主从服务器数据的一致性，也正是因为这个原因，当一个过期键仍然存在于主服务器的数据库时，这个过期键在从服务器里的复制品也会继续存在。 具体的示例如下：   假如有一对主从服务器，他们的数据库中都保存着同样的三个键 message、xxx 和 yyy，其中 message 为过期键，如下图所示：\n   如果这时有客户端向从服务器发送命令 GET message，那么从服务器将发现 message 键已经过期，但从服务器并不会删除 message 键，而是继续将 message 键的值返回给客户端，就好像 message 键并没有过期一样，如下图所示：\n   假设在此之后，有客户端向主服务器发送 GET message，那么主服务器将发现键 message 已经过期，因此，主服务器会删除 message 键，向客户端返回空回复，并向从服务器发送 DEL message 命令，如下图所示：\n   从服务器在接收到主服务器发来的 DEL message 命令之后，也会从数据库中删除 message 键，在这之后，主从服务器都不再保存过期键 message 了，如下图所示：\n     2 内存淘汰策略 #  2.1 为什么需要内存淘汰策略 #   因为不管是定期采样删除还是惰性删除都不是一种完全精准的删除，就还是会存在 key 没有被删除掉的场景，所以就需要内存淘汰策略进行补充。  2.2 含义 #   内存淘汰策略针对的是内存不足的情况下的一种 Redis 处理机制，例如，当前的 Redis 存储已经超过内存限制了，然而我们的业务还在继续往 Redis 里面追加缓存内容，这时候 Redis 的淘汰机制就起到作用了。  2.3 Redis 常见的六种淘汰策略 #   noeviction：不删除策略，达到最大内存限制时，如果需要更多内存，直接返回错误信息。 alllkeys-lru：所有 key 通用，达到最大内存限制时，如果需要更多内存，优先删除最近最少使用（Least Recent Used, LRU）的 key。 volatile-lru：只限于设置了过期时间的部分，达到最大内存限制时，如果需要更多内存，优先从设置了过期时间的 key 中选择最近最少使用的 key 进行删除。 allkeys-random：所有 key 通用，达到最大内存限制时，如果需要更多内存，随即删除一部分 key。 volatile-random：只限于设置了过期时间的部分，达到最大内存限制时，如果需要更多内存，优先从设置了过期时间的 key 中随机选择一部分 key 进行删除。 volatile-ttl：只限于设置了过期时间的部分，达到最大内存限制时，如果需要更多内存，优先删除剩余时间（Time to Live, TTL）短的 key。  参考文献 #    理解 Redis 的内存回收机制和过期淘汰策略。  最详细的一篇关于 Redis 的过期键机制介绍。 redis 设计与实现（第二版）。  彻底弄懂 Redis 的内存淘汰策略。  Redis 的过期策略\u0026amp;内存淘汰策略。  "},{"id":78,"href":"/school-recruitment/docs/java/3JVM/3.4-%E7%B1%BB%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%92%8C%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","title":"3.4 类的生命周期和加载过程","section":"3、 Jvm","content":"类的生命周期和加载过程 #   一个类在 JVM 里的生命周期有 7 个阶段，分别是：\n 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading）  其中前 5 个部分（加载、验证、准备、解析、初始化）统称为类加载，具体过程如下：\n  加载： 这个阶段的主要操作是根据明确知道的 class 完全限定名，来获取二进制 classfile 格式的字节流，简单点说就是找到文件系统中 jar 包或存在于任何地方的\u0026quot;class 文件\u0026quot;。如果找不到，则会抛出 NoClassDefFound 错误。加载阶段并不会检查 classfile 的语法和格式，整个过程主要由 JVM 和 Java 的具体某一个 类加载器 协作完成。\n  验证： 这个阶段主要是确保 class 文件里的字节流信息符合当前虚拟机的要求，不会危害虚拟机的安全。验证过程中会检查 classfile 的语义，判断常量池中的符号，并执行类型检查，主要目的是判断字节码的合法性。\n  准备： 这个阶段会创建静态字段，并将其初始化为标准默认值（比如 null 或者 0 值），并分配方发表，即在方法区中分配这些变量所使用的的内存空间，需要注意的是，准备阶段并未执行任何 Java 代码。 例如：对于 public static int i = 1;，后面在类初始化阶段才会执行赋值为 1，但是如果使用 final 作为静态常量，则会在准备阶段就会被赋值为 1。\n  解析： 这个阶段主要是解析符号引用，也就是解析常量池，主要有以下四种：\n 类或接口的解析。 字段解析。 类方法解析。 接口方法解析。 简单的来说就是我们编写的代码中，当一个变量引用某个对象的时候，这个引用在 .class 文件中是以符号引用的来存储的（相当于做了一个索引记录）。在解析阶段就需要将其解析并链接为直接引用（相当于指向实际对象）。    初始化： JVM 规范明确规定，必须在类的首次主动引用时才能执行类初始化。初始化的过程包括执行：\n 类构造器方法。 static 静态变量赋值语句。 static 静态代码块。 如果是一个子类进行初始化会先对其父类进行初始化，保证其父类在子类之前进行初始化。    "},{"id":79,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.5-%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/","title":"1.1.5 编辑距离","section":"1.1 动态规划","content":"编辑距离 #  1 题目 #  给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。\n你可以对一个单词进行如下三种操作：\n 插入一个字符 删除一个字符 替换一个字符  示例 1：\n输入：word1 = \u0026quot;horse\u0026quot;, word2 = \u0026quot;ros\u0026quot; 输出：3 解释： horse -\u0026gt; rorse (将 'h' 替换为 'r') rorse -\u0026gt; rose (删除 'r') rose -\u0026gt; ros (删除 'e') 示例 2：\n输入：word1 = \u0026quot;intention\u0026quot;, word2 = \u0026quot;execution\u0026quot; 输出：5 解释： intention -\u0026gt; inention (删除 't') inention -\u0026gt; enention (将 'i' 替换为 'e') enention -\u0026gt; exention (将 'n' 替换为 'x') exention -\u0026gt; exection (将 'n' 替换为 'c') exection -\u0026gt; execution (插入 'u') 提示：\n 0 \u0026lt;= word1.length, word2.length \u0026lt;= 500 word1 和 word2 由小写英文字母组成  2 解题思路 #  2.1 递归 #  2.1.1 问题解析 #  解决两个字符串的动态规划问题，一般都是用两个指针 i、j 分别指向两个字符串的最后，然后一步一步往前走，缩小问题的规模。\n设两个字符串分别为 rad 和 apple，为了把 s1 变成 s2，算法会这样进行：\n 根据上面的 GIF，可以发现操作不止有三个，其实还有第四个操作，就是什么都不要做（skip），比如这个情况：\n 因为这两个字符本来就相同，为了使编辑距离最小，显然不应该对他们有任何操作，直接往前移动 i、j 即可。\n还有一个很容易处理的情况，就是 j 走完 s2 时，如果 i 还没走完 s1，那么只能用删除操作把 s1 缩短为 s2，比如这个情况：\n 类似的，如果 i 走完 s1 时 j 还没走完 s2，那就只能用插入操作把 s2 剩下的字符全部插入 s1，这两种情况就是算法的base case。\n2.1.2 代码解析 #  先梳理下之前的思路：\nbase case是 i 走完 s1 或 j 走完 s2，可以直接返回另一个字符串剩下的长度。\n对于每队字符 s1[i] 和 s2[j]，可以有四种操作：\nif s1[i] == s2[j]: 啥都别做（skip） i, j 同时向前移动 else: 三选一： 插入（insert） 删除（delete） 替换（replace） 对于这三个操作，需要全试一遍，哪个操作最后得到的编辑距离小，就选谁，具体代码如下：\ndef minDistance(s1, s2) -\u0026gt; int: def dp(i, j): # base case  if i == -1: return j + 1 if j == -1: return i + 1 if s1[i] == s2[j]: return dp(i - 1, j - 1) # 啥都不做 else: return min( dp(i, j - 1) + 1, # 插入 dp(i - 1, j) + 1, # 删除 dp(i - 1, j - 1) + 1 # 替换 ) # i，j 初始化指向最后一个索引  return dp(len(s1) - 1, len(s2) - 1) 下面对这段递归代码进行以下解释。\n dp[i][j]表示将 s1[i] 之前的字符修改为 s2[j] 之前的字符的最小编辑距离。  def dp(i, j) -\u0026gt; int 如果 s1[i]==s2[j]：  说明这两个字符本来就相等，不需要进行任何操作。 此时 s1[0..i] 和 s2[0..j] 的最小编辑距离等于 s1[0..i-1] 和 s2[0..j-1] 的最小编辑距离。 dp(i,j) 等于 dp(i-1,j-1)。    if s1[i] == s2[j]: return dp(i - 1, j - 1) 如果 s1[i]!=s2[j]，此时就需要对三个操作递归了：   插入： 直接在 s1[i] 插入一个和 s2[j] 相同的字符，那么 s2[j] 就被匹配了，前移 j，继续跟 i 对比，同时操作数加 1。\ndp(i, j - 1) + 1, # 插入    删除： 直接把 s[i] 这个字符删掉，前移 i，继续跟 j 对比，同时操作数加 1。\ndp(i - 1, j) + 1, # 删除    替换：直接把 s1[i] 替换成 s2[j]，这样他俩就匹配了，然后前移 i、j，并将其继续对比，同时操作数加 1。\ndp(i - 1, j - 1) + 1 # 替换      2.1.3 参考代码 #  /** * 返回三个数之间的最小值 * @param a 第一个参数 * @param b 第二个参数 * @param c 第三个参数 * @return 三个数之间的最小值 */ int min(int a, int b, int c) { return Math.min(a, Math.min(b, c)); } /** * dp 函数（版本 1） * @param s1 第一个字符串 * @param s2 第二个字符串 * @param i s1 的最后一个字符下标 * @param j s2 的最后一个字符下标 * @return s1 前 i 个字符替换成 s2 前 j 个字符所需要的最小编辑次数 */ int dpV1(String s1, String s2, int i, int j) { // base case // 1. 如果第一个单词遍历到最左边，则把 s2 剩余测字符全插入到 s1 前面 if (i == -1) {return j + 1;} // 2. 如果第二个单词遍历到最左边，则把 s1 剩余字符都删除 if (j == -1) {return i + 1;} // 如果两个字符串当前字符相同，则啥也不做 if (s1.charAt(i) == s2.charAt(j)) {return dpV1(s1, s2, i - 1, j - 1);} else { // 否则，一次尝试插入、删除、替换三种操作，并返回编辑距离最小的距离 return min( // 插入 dpV1(s1, s2, i, j - 1) + 1, // 删除 dpV1(s1, s2, i - 1, j) + 1, dpV1(s1, s2, i - 1, j - 1) + 1 ); } } /** * 72. 编辑距离（版本 1：动态规划） * 给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 * 你可以对一个单词进行如下三种操作： * 插入一个字符 * 删除一个字符 * 替换一个字符 * @param word1 第一个单词 * @param word2 第二个单词 * @return 最少操作数 */ public int minDistanceV1(String word1, String word2) { return dpV1(word1, word2, word1.length() - 1, word2.length() - 1); } 2.2 动态规划优化 #  动态规划的优化方法主要有两种，一种是备忘录，另一种是DP Table。\n2.2.1 备忘录优化 #  2.2.1.1 问题解析 #  备忘录优化主要是把上面递归方法中的 dp(i,j) 的数据保存在备忘录（实质是一个二维数组）中，每次递归时先判断 dp(i,j) 的数据有没有在备忘录中，如果有的话直接返回，没有的话再进行计算即可。\n2.2.1.2 参考代码 #  /** * 返回三个数之间的最小值 * @param a 第一个参数 * @param b 第二个参数 * @param c 第三个参数 * @return 三个数之间的最小值 */ int min(int a, int b, int c) { return Math.min(a, Math.min(b, c)); } /** * dp 函数（版本 2：备忘录优化） * @param s1 第一个字符串 * @param s2 第二个字符串 * @param i s1 的最后一个字符下标 * @param j s2 的最后一个字符下标 * @return s1 前 i 个字符替换成 s2 前 j 个字符所需要的最小编辑次数 */ int dpV2(String s1, String s2, int i, int j, int[][] memo) { // base case  // 1. 如果第一个单词遍历到最左边，则把 s2 剩余测字符全插入到 s1 前面  if (i == -1) {return j + 1;} // 2. 如果第二个单词遍历到最左边，则把 s1 剩余字符都删除  if (j == -1) {return i + 1;} if (memo[i][j] != -1) {return memo[i][j];} // 如果两个字符串当前字符相同，则啥也不做  if (s1.charAt(i) == s2.charAt(j)) {memo[i][j] = dpV2(s1, s2, i - 1, j - 1, memo);} else { // 否则，一次尝试插入、删除、替换三种操作，并返回编辑距离最小的距离  memo[i][j] = min( // 插入  dpV2(s1, s2, i, j - 1, memo) + 1, // 删除  dpV2(s1, s2, i - 1, j, memo) + 1, dpV2(s1, s2, i - 1, j - 1, memo) + 1 ); } // 返回结果  return memo[i][j]; } /** * 72. 编辑距离（版本 2：动态规划（备忘录优化）） * 给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 * 你可以对一个单词进行如下三种操作： * 插入一个字符 * 删除一个字符 * 替换一个字符 * @param word1 第一个单词 * @param word2 第二个单词 * @return 最少操作数 */ public int minDistanceV2(String word1, String word2) { // 定义备忘录，并将其里面的每个元素初始化为 -1  int[][] memo = new int[word1.length()][word2.length()]; for (int i = 0; i \u0026lt; memo.length; i++) { Arrays.fill(memo[i], -1); } // 递归获取最小编辑距离  return dpV2(word1, word2, word1.length() - 1, word2.length() - 1, memo); } 2.2.2 DP Table 优化 #  2.2.2.1 问题解析 #    定义 dp[i][j]：\n dp[i][j] 代表 word1 中前 i 个字符，变换到 word2 中前 j 个字符，最短需要操作的次数。 需要考虑 word1 或 word2 一个字母都没有，即全增加或全删除的情况，所以预留 dp[0][j] 和 dp[i][0]。    状态转移：\n 增：dp[i][j] = dp[i][j-1] + 1 删：dp[i][j] = dp[i-1][j] + 1 改：dp[i][j] = dp[i-1][j-1] 按顺序计算，当计算 dp[i][j] 时，dp[i][j-1]、dp[i-1][j]、dp[i-1][j-1] 均已经确定。 配合增删改这三种操作，需要对应的 dp 把操作数加 1，取三种的最小。 如果刚好这两个字母相同，即 word1[i-1]=word2[j-1]，那么可以直接参考 dp[i-1][j-1]，操作不用加 1。    具体的图解如下：\n 绿色： 增。 红色： 删。 黄色： 改。        2.2.2.2 参考代码 #  /** * 返回三个数之间的最小值 * @param a 第一个参数 * @param b 第二个参数 * @param c 第三个参数 * @return 三个数之间的最小值 */ int min(int a, int b, int c) { return Math.min(a, Math.min(b, c)); } /** * 72. 编辑距离（版本 3：动态规划（DP Table 优化）） * 给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 * 你可以对一个单词进行如下三种操作： * 插入一个字符 * 删除一个字符 * 替换一个字符 * @param word1 第一个单词 * @param word2 第二个单词 * @return 最少操作数 */ public int minDistanceV3(String word1, String word2) { int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; // base case  for (int i = 1; i \u0026lt;= m; i++) { dp[i][0] = i; } for (int j = 1; j \u0026lt;= n; j++) { dp[0][j] = j; } // 自底向上求解  for (int i = 1; i \u0026lt;= m; i++) { for (int j = 1; j \u0026lt;= n; j++) { if (word1.charAt(i - 1) == word2.charAt(j - 1)) {dp[i][j] = dp[i - 1][j - 1];} else { dp[i][j] = min( dp[i][j - 1] + 1, dp[i - 1][j] + 1, dp[i - 1][j - 1] + 1 ); } } } // 储存着整个 word1 和 word2 的最小编辑距离  return dp[m][n]; } 3 参考文献 #    72. 编辑距离。  经典动态规划：编辑距离。  【编辑距离】入门动态规划，你定义的 dp 里到底存了啥。  "},{"id":80,"href":"/school-recruitment/docs/database/1%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/1.5-MVCC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"1.5 Mvcc实现原理","section":"1、数据库基础","content":"MVCC实现原理 #  1 含义 #   MVCC 是在并发访问数据库时，通过对数据做多版本管理，避免因为写锁的阻塞而造成数据的并发阻塞问题。 通俗的讲就是 MVCC通过保存数据的历史版本，根据比较版本号来处理数据的是否显示，从而达到读取数据的时候不需要加锁就可以保证事务隔离性的效果。  2 核心知识点 #  2.1 事务版本号 #   每次事务开启前都会从数据库获得一个自增长的事务 ID，可以从事务 ID判断事务执行的先后顺序。  2.2 隐藏列 #   DB_TRX_ID： 记录操作该数据事务的事务 ID。 DP_POLL_PTR： 指向上一个版本数据在 Undo Log 里的位置指针。 DB_ROW_ID：隐藏 ID，当建表没有合适的索引作为聚集索引时，会用该隐藏 ID 创建聚集索引。  2.3 Undo Log #  2.3.1 含义 #   Undo Log 主要用于记录数据被修改之前的日志，在表信息修改之前会先把数据拷贝到 Undo Log 里，在事务进行回滚时可以通过 Undo Log 里的日志进行数据还原，具体可参考 Undo Log。  2.3.2 用途 #   保证事务进行 ROLLBACK 时的原子性和一致性，当事务进行回滚的时候可以用 Undo Log 的数据进行恢复。 在MVCC 版本控制中，通过读取 Undo Log 的历史版本数据可以实现不同事务版本号都拥有自己独立的快照数据版本。  2.3.3 事务的版本号、隐藏列、Undo Log 的关系 #  我们模拟一次数据修改的过程来了解下事务版本号、隐藏列和 Undo Log 他们之间的关系：\n  首先准备一张原始数据表 user_info：   参考文献开启一个事务 $A$，对 user_info 表执行 update user_info set name = '李四' where id = 1，这个过程的详细流程如下：\n 首先获得一个事务编号 104。 把user_info 表修改前的数据拷贝到 Undo Log。 修改user_info 表id = 1 的数据。 把修改后的数据事务版本号改成当前事务版本号，并把DB_ROLL_PTR 地址指向 Undo Log 数据地址。    最后执行完结果如下图所示：\n   2.4 Read View #  2.4.1 含义 #   在 InnoDB 中每个 SQL 语句执行前都会得到一个 Read View，主要保存了当前数据库系统中正处于活跃（没有 Commit）的事务的 ID 号。 简单的说就是保存系统中当前不应该被本事务看到的其他事务 ID 列表。  2.4.2 属性 #   trx_ids：当前系统活跃（未提交）事务版本号集合。 low_limit_id：创建当前 Read View 时当前系统最大事务版本号 +1。 up_limit_id：创建当前 Read View 时系统正处于活跃事务最小版本号。 creator_trx_id：创建当前 Read View 的事务版本号。  2.4.3 匹配条件 #  2.4.3.1 数据事务 ID \u0026lt; up_limit_id 则显示 #  如果数据事务 ID 小于 Read View 中的最小活跃事务 ID，则可以肯定该数据是在当前事务开启之前就已经存在了的，所以可以显示。\n2.4.3.2 数据事务 ID \u0026gt;= low_limit_id 则不显示 #  如果数据事务 ID 大于 Read View 中当前系统的最大事务 ID，则说明该数据是在当前 Read View 创建之后才产生的，所以数据不予显示。\n2.4.3.3 up_limit_id \u0026lt;= 数据事务 ID \u0026lt; low_limit_id，则与活跃事务集合 trx_ids 里匹配 #  如果数据的事务 ID 大于最小的活跃事务 ID，同时又小于等于系统最大的事务 ID，这种情况就说明这个数据有可能是在当前事务开始的时候还没有提交的，所以此时我们就需要把数据的事务 ID 与当前 Read View 中的活跃事务稽核 trx_ids 匹配。\n 如果事务 ID 不存在于 trx_ids 集合，则说明Read View 产生的时候事务已经 Commit 了，这种情况数据则可以显示。 如果事务 ID存在于 trx_ids 集合，则说明Read View 产生的时候数据还没有提交：  如果数据事务 ID 等于 creator_trx_id，那么说明这个数据就是当前事务自己生成的，自己生成的数据自己当然能看见，所以这种情况下数据也是可以显示的。 如果数据事务 ID 不等于 creator_trx_id，那么说明Read View 产生的时候事务还没有提交，又不是自己生成的，所以这种情况下数据不能显示。    2.4.3.4 不满足 Read View 条件的时候，从 Undo Log 里面获取数据 #  当数据事务 ID 不满足 Read View 条件的时候，从 Undo Log 里面获取数据的历史版本，然后根据数据的历史版本号回头再和 Read View 条件匹配，直到找到一条满足条件的历史数据，或者找不到则返回空结果。\n3 InnoDB 实现 MVCC 的原理 #    创建user_info 表，插入一条初始化数据。  事务 $A$ 和事务 $B$ 同时对 user_info 进行修改和查询操作：  事务 $A$：update user_info set name = '李四'; 事务 $B$：select * from user_info where id = 1;   先开启事务 $A$，在事务 $A$ 修改数据后但未进行 Commit，此时执行事务 B，在这期间产生的具体流程如下：   事务 $A$：开启事务，首先得到一个事务编号102。\n  事务 $B$：开启事务，得到事务编号103。\n  事务 $A$：进行修改操作，首先把原数据拷贝到 Undo Log，然后对数据进行修改，标记事务编号和上一个数据版本在 Undo Log 的地址。\n   事务 $B$：此时事务 $B$ 获得一个 Read View，Read View 对应的值如下：\n   事务 $B$：执行查询语句，此时得到的是事务 $A$ 修改后的数据：\n   事务 $B$：把数据与 Read View 进行匹配，事务 ID 为 102：\n 不小于up_limit_id。 小于low_limit_id。 存在于trx_ids。 不等于creator_trx_id。    发现不满足 Read View 条件，所以从 Undo Log 获取历史版本的数据，然后再和 Read View 进行匹配，最后返回数据如下：\n     4 各种事务隔离级别下的 Read View 的工作方式 #  事务隔离级别的详细信息可参考 事务隔离级别。\n4.1 Read Commit #  在读提交级别下同一事务里面的每一次查询都会获得一个新的 Read View 副本，这样就可能造成同一个事务里前后读取数据可能不一致的问题（重复读）。\n 4.2 Repetable Read #  在可重复读级别下同一事务里面只会获取一次Read View副本，从而保证每次查询的数据都是一样的，因此在可重复读级别下不存在幻读问题。\n 4.3 Read Uncommitted #  在读未提交级别下事务不会获取Read View副本。\n5 快照读和当前读 #  5.1 快照读 #   快照读是指读取数据时不是读取最新版本的数据，而是基于历史版本读取的一个快照信息（MySQL读取Undo Log历史版本）。 快照读可以使普通的 SELECT读取数据时不用对表数据进行加锁，从而解决了因为对数据库表加锁而导致的两个问题：  解决了因加锁导致的修改数据时无法对数据进行读取的问题。 解决了因加锁导致的读取数据时无法对数据进行修改的问题。   快照读可以避免幻读问题。 快照读包括的操作主要包括：  SELECT。    5.2 当前读 #   当前读是读取数据库最新的数据。 当前读和快照读不同，因为要读取最新的数据，而且要保证事务的隔离性，所以当前读是需要加锁的。 在当前读的情况下需要使用间隙锁来解决幻读问题。 当前读包括的操作主要包括：  UPDATE。 DELETE。 INSERT。 SELECT ... LOCK IN SHARE MODE。 SELECT ... FOR UPDATE。    参考文献 #    数据库基础（四）Innodb MVCC 实现原理。  "},{"id":81,"href":"/school-recruitment/docs/java/3JVM/3.5-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%97%B6%E6%9C%BA/","title":"3.5 类加载时机","section":"3、 Jvm","content":"类加载时机 #  1 类的初始化触发的情况 #   当虚拟机启动时，初始化用户指定的主类，就是启动执行的main方法所在的类。 当遇到用以新建目标类实例的new指令时，初始化new指令的目标类，就是new一个类的时候要初始化。 当遇到调用静态方法的指令时，初始化该静态方法所在的类。 当遇到访问静态字段的指令时，初始化该静态字段所在的类。 子类的初始化会触发父类的初始化。 如果一个接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，会触发该接口的初始化。 使用反射API对某个类进行反射调用时，初始化这个类，其实跟前面一样，反射调用要么是已经有实例了，要么是静态方法，都需要初始化。 当初次调用MethodHandle实例时，初始化该MethodHandle指向的方法所在的类。  2 不会指定类初始化的情况 #   通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。 定义对象数组，不会触发该类的初始化。 常量在编译期间会存入调用类的常量池中，本质上没有直接引用常量的类，不会触发定义常量所在的类。 通过类名获取Class对象，不会触发类的初始化，Hello.class不会让Hello类初始化。 通过ClassLoader默认的loadClass方法，也不会触发初始化动作（加载了，但是不初始化）。  "},{"id":82,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.6-%E9%AB%98%E6%A5%BC%E6%89%94%E9%B8%A1%E8%9B%8B/","title":"1.1.6 高楼扔鸡蛋","section":"1.1 动态规划","content":"高楼扔鸡蛋 #  1 题目 #  给你 k 枚相同的鸡蛋，并可以使用一栋从第 1 层到第 n 层共有 n 层楼的建筑。\n已知存在楼层 f ，满足 0 \u0026lt;= f \u0026lt;= n ，任何从 高于 f 的楼层落下的鸡蛋都会碎，从 f 楼层或比它低的楼层落下的鸡蛋都不会破。\n每次操作，你可以取一枚没有碎的鸡蛋并把它从任一楼层 x 扔下（满足 1 \u0026lt;= x \u0026lt;= n）。如果鸡蛋碎了，你就不能再次使用它。如果某枚鸡蛋扔下后没有摔碎，则可以在之后的操作中 重复使用 这枚鸡蛋。\n请你计算并返回要确定 f 确切的值 的 最小操作次数 是多少？\n示例 1：\n输入：k = 1, n = 2 输出：2 解释： 鸡蛋从 1 楼掉落。如果它碎了，肯定能得出 f = 0 。 否则，鸡蛋从 2 楼掉落。如果它碎了，肯定能得出 f = 1 。 如果它没碎，那么肯定能得出 f = 2 。 因此，在最坏的情况下我们需要移动 2 次以确定 f 是多少。 示例 2：\n输入：k = 2, n = 6 输出：3 示例 3：\n输入：k = 3, n = 14 输出：4 提示：\n 1 \u0026lt;= k \u0026lt;= 100 1 \u0026lt;= n \u0026lt;= 104  2 解题思路 #  2.1 动态规划 #  2.1.1 问题分析 #  我们可以考虑使用动态规划来做这道题，状态可以表示成 $(k,n)$，其中 $k$ 为鸡蛋数，$n$ 为楼层数。当我们从第 $x$ 楼扔鸡蛋的时候：\n 如果鸡蛋不碎，那么状态变成 $(k,n-x)$，即我们鸡蛋的数目不变，但答案只可能在上方的 $n-x$ 层楼了。也就是说，我们把原问题缩小成了一个规模为 $(k,n-x)$ 的子问题； 如果鸡蛋碎了，那么状态变成 $(k-1,x-1)$，即我们少了一个鸡蛋，但我们直到答案只可能在 $x$ 楼下方的 $x-1$ 层楼中了。也就是说，我们把原问题缩小成了一个规模为 $(k-1,x-1)$ 的子问题。   这样一来，我们定义 $dp(k,n)$ 为在状态 $(k,n)$ 下最少需要的步数。根据以上分析，我们可以列出状态转移方程：\n$$ dp(k,n)=1+\\min_{1 \\leq x \\leq n}(max(dp(k-1,x-1),dp(k,n-x))) $$\n这个状态转移方程是如何得来的呢？对于 $dp(k,n)$ 而言，我们像上面分析的那样，枚举第一个鸡蛋扔在的楼层数 $x$。由于我们并不知道真正的 $f$ 值，因此我们必须保证鸡蛋碎了之后接下来需要的步数和鸡蛋没碎之后需要的步数二者最大值最小，这样就保证了在最坏情况下（也就是无论 $f$ 的值如何）$dp(k,n)$ 的值最小。\n2.1.2 解题方法 #  动态规划的解法有递归、备忘录优化、DP Table 优化。\n2.1.2.1 递归方法 #  /** * 最坏情况下扔鸡蛋的次数（版本 1：递归） * * @param k 鸡蛋个数 * @param n 总楼层数 * @return 最坏情况下扔鸡蛋的次数 */ public int dpV1(int k, int n) { // 如果只有 1 个鸡蛋，则所有楼层都需要试一下  if (k == 1) { return n; } // 如果楼层数为 0，则不需要进行尝试，直接返回 0 即可  if (n == 0) { return 0; } int res = n; for (int i = 1; i \u0026lt;= n; i++) { res = Math.min( res, Math.max( // 鸡蛋碎了  dpV1(k - 1, i - 1), // 鸡蛋没碎  dpV1(k, n - i) ) + 1 ); } return res; } /** * 887. 鸡蛋掉落（版本 1：递归） * 给你 k 枚相同的鸡蛋，并可以使用一栋从第 1 层到第 n 层共有 n 层楼的建筑。 * 已知存在楼层 f ，满足 0 \u0026lt;= f \u0026lt;= n ，任何从 高于 f 的楼层落下的鸡蛋都会碎，从 f 楼层或比它低的楼层落下的鸡蛋都不会破。 * 每次操作，你可以取一枚没有碎的鸡蛋并把它从任一楼层 x 扔下（满足 1 \u0026lt;= x \u0026lt;= n）。如果鸡蛋碎了，你就不能再次使用它。如果某枚鸡蛋扔下后没有摔碎，则可以在之后的操作中 重复使用 这枚鸡蛋。 * 请你计算并返回要确定 f 确切的值 的 最小操作次数 是多少？ * * @param k 目标楼层 * @param n 总楼层 * @return 要确定 f 确切的值 的 最小操作次数 */ public int superEggDropV1(int k, int n) { return dpV1(k, n); } 2.1.2.2 备忘录优化 #  /** * 最坏情况下扔鸡蛋的次数（版本 2：备忘录优化） * * @param k 鸡蛋个数 * @param n 总楼层数 * @return 最坏情况下扔鸡蛋的次数 */ public int dpV2(int k, int n, int[][] memo) { // 如果只有 1 个鸡蛋，则所有楼层都需要试一下  if (k == 1) { return n; } // 如果楼层数为 0，则不需要进行尝试，直接返回 0 即可  if (n == 0) { return 0; } // 如果数据在备忘录中已经存在的话，直接返回即可  if (memo[k][n] != Integer.MAX_VALUE) { return memo[k][n]; } for (int i = 1; i \u0026lt;= n; i++) { memo[k][n] = Math.min( memo[k][n], Math.max( // 鸡蛋碎了  dpV2(k - 1, i - 1, memo), // 鸡蛋没碎  dpV2(k, n - i, memo) ) + 1 ); } return memo[k][n]; } /** * 887. 鸡蛋掉落（版本 2：备忘录优化） * 给你 k 枚相同的鸡蛋，并可以使用一栋从第 1 层到第 n 层共有 n 层楼的建筑。 * 已知存在楼层 f ，满足 0 \u0026lt;= f \u0026lt;= n ，任何从 高于 f 的楼层落下的鸡蛋都会碎，从 f 楼层或比它低的楼层落下的鸡蛋都不会破。 * 每次操作，你可以取一枚没有碎的鸡蛋并把它从任一楼层 x 扔下（满足 1 \u0026lt;= x \u0026lt;= n）。如果鸡蛋碎了，你就不能再次使用它。如果某枚鸡蛋扔下后没有摔碎，则可以在之后的操作中 重复使用 这枚鸡蛋。 * 请你计算并返回要确定 f 确切的值 的 最小操作次数 是多少？ * * @param k 目标楼层 * @param n 总楼层 * @return 要确定 f 确切的值 的 最小操作次数 */ public int superEggDropV2(int k, int n) { // 备忘录  int[][] memo = new int[k + 1][n + 1]; for (int i = 0; i \u0026lt; memo.length; i++) { Arrays.fill(memo[i], Integer.MAX_VALUE); } return dpV2(k, n, memo); } 2.1.2.3 二分查找优化 #  如果我们直接暴力求解每个状态的 $dp$ 值，时间复杂度为 $O(kn^2)$，即一共有 $O(kn)$ 个状态，对于每个状态枚举扔鸡蛋的楼层 $x$，需要 $O(n)$ 的时间。这无疑在当前数据范围下是会超出时间限制的，因此我们需要想办法优化枚举的时间复杂度。\n随我们观察到 $dp(k,n)$ 是一个关于 $n$ 的单调递增函数，也就是说在鸡蛋数 $k$ 固定的情况下，楼层数 $n$ 越多，需要的步数一定不会变少。在2.1.1 问题分析中的状态转移方程中，第一项 $T_1(x)=dp(k-1,x-1)$ 是一个随 $x$ 的增加而单调递增的函数，第二项 $T_2(x)=dp(k,n-x)$ 是一个随着 $x$ 的增加而单调递减的函数。\n 如上图所示，如果这两个函数都是连续函数，那么我们只需要找出这两个函数的交点，在交点处就能保证这两个函数的最大值最小。但在本题中，$T_1(x)$和$T_2(x)$都是离散函数，也就是说$x$的值只能取1、2、3等等。在这种情况下，我们需要找到最大的满足$T_1(x) \\lt T_2(x)$中的$x_0$，以及最小的满足$T_1(x) \\ge T_2(x)$的$x_1$，对应到上图中，就是离这两个函数（想象中的）交点左右两侧最近的整数。\n我们只需要比较在$x_0$和$x_1$处两个函数的最大值，取一个最小的作为$x$即可。\n参考代码如下：\n/** * 最坏情况下扔鸡蛋的次数（版本3：二分查找优化） * * @param k 鸡蛋个数 * @param n 总楼层数 * @return 最坏情况下扔鸡蛋的次数 */ public int dpV3(int k, int n, int[][] memo) { // 如果只有 1 个鸡蛋，则所有楼层都需要试一下  if (k == 1) { return n; } // 如果楼层数为 0，则不需要进行尝试，直接返回 0 即可  if (n == 0) { return 0; } // 如果数据在备忘录中已经存在的话，直接返回即可  if (memo[k][n] != Integer.MAX_VALUE) { return memo[k][n]; } int left = 1, right = n + 1, res = Integer.MAX_VALUE; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; // 鸡蛋碎了  int broken = dpV3(k - 1, mid - 1, memo); // 鸡蛋没碎  int notBroken = dpV3(k, n - mid, memo); if (broken \u0026gt; notBroken) { right = mid - 1; res = Math.min(res, broken + 1); } else if (broken \u0026lt; notBroken) { left = mid + 1; res = Math.min(res, notBroken + 1); } else if (broken == notBroken) { right = mid - 1; res = Math.min(res, broken + 1); } } memo[k][n] = res; return res; } 3 参考文献 #    887. 鸡蛋掉落。  《鸡蛋掉落》官方题解。  "},{"id":83,"href":"/school-recruitment/docs/java/3JVM/3.6-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E7%9A%84%E7%A7%8D%E7%B1%BB/","title":"3.6 类加载器的种类","section":"3、 Jvm","content":"类加载器的种类 #  类加载过程可以描述为“通过一个类的全限定名 a.b.c.XXClass来获取描述此类的 Class对象”，这个过程由类加载器（CLassLoader） 来完成。这样的好处在于，子类加载器可以复用父类加载器加载的类。 类加载器主要分为三种：\n   启动类加载器（BootstrapClassLoader）：\n 他用来加载Java的核心类，是用C++代码来实现的，并不继承自java.lang.ClassLoader（负责加载JDK中jre/lib/tr.jar里所有的class）。 他可以看做是JVM自带的，我们在代码层面无法直接获取到启动类加载器的引用，所以不允许直接操作他，如果打印出来就是个null，但是后面可以通过命令行参数影响他加载什么。    扩展类加载器（ExtCLassLoader）：\n 他负责加载JRE的扩展目录，lib/ext或者由java.ext.dirs系统属性指定的目录中的JAR包的类。 代码里直接获取他的父类加载器为null，因为无法拿到启动类加载器。    应用类加载器（AppClassLoader）：\n 他负责在JVM启动时加载来自Java命令的-classpath或者-cp选项、java.class.path系统属性指定的jar包和类路径。 在应用程序代码里可以通过ClassLoader的静态方法getSystemClassLoader()来获取应用类加载器。 如果没有特别指定，则在没有使用自定义类加载器情况下，用户自定义的类都由此类加载器加载。    自定义类加载器（CustomClassLoader）：\n 除了前面三种系统自带的类加载器，用户还可以自己定义类加载器，如果用户自定义了类加载器，则自定义类加载器都以应用类加载器作为父加载器。    "},{"id":84,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.7-%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E5%BA%8F%E5%88%97/","title":"1.1.7 最长回文子序列","section":"1.1 动态规划","content":"最长回文子序列 #  1 题目 #  给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。 示例 1: 输入:\n\u0026quot;bbbab\u0026quot; 输出:\n4 一个可能的最长回文子序列为 \u0026ldquo;bbbb\u0026rdquo;。\n示例 2: 输入:\n\u0026quot;cbbd\u0026quot; 输出:\n2 一个可能的最长回文子序列为 \u0026ldquo;bb\u0026rdquo;。\n提示：\n 1 \u0026lt;= s.length \u0026lt;= 1000 s 只包含小写英文字母  2 解题思路 #  2.1 子序列问题处理模板 #  对于这种子序列问题，我们一般需要使用动态规划的方法来解决：\n 找状态关系（通过数学归纳获得）。 定义 dp 数组（根据状态转移方程获得）。  dp 数组的定义主要有两种方式，一种是定义一个一维数组，另一种是定义一个二维数组。\n2.1.1 一维 dp 数组 #  例如，在 最长递增子序列中，我们就是定义了一个一维数组，其含义为：在子数组 $array[0..i]$ 中，我们要求的子序列（最长递增子序列）的长度是 $dp[i]$。\nint n = array.length; int[] dp = new int[n]; for (int i = 1; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; i; j++) { dp[i] = 最值(dp[i], dp[j] + ...) } } 2.1.2 二维 dp 数组 #  这种思路运用相对多一些，尤其是涉及两个字符串/数组的子序列。本思路中 dp 数组含义又分为只涉及一个字符串和涉及两个字符串两种情况。\n 只涉及一个字符串/数组时： 在子数组 $array[i..j]$ 中，我们要求的子序列（最长回文子序列）的长度为 $dp[i][j]$。 涉及两个字符串/数组：在子数组 $arr1[0..i]$ 和 $arr2[0..j]$ 中，我们要求的子序列（最长公共子序列）长度为 $dp[i][j]$。  2.2 问题解析 #   dp 函数的定义为：在子串 $s[i..j]$ 中，最长回文子序列的长度为 $dp[i][j]$。\n如果我们想求 $dp[i][j]$，假设我们已经知道了子问题 $dp[i+1][j-1]$ 的结果，即 $s[i+1..j-1]$ 中最长回文子序列的长度，那么我们就可以想办法算出 $dp[i][j]$ 的值，即 $s[i..j]$ 中最长回文子序列的长度，这主要取决于 $s[i]$ 和 $s[j]$ 的字符。\n 如果 $s[i]==s[j]$： 则他俩加上 $s[i+1..j-1]$ 中的最长回文子序列就是 $s[i..j]$ 的最长回文子序列。   如果 $s[i]!=s[j]$： 说明他俩不可能同时出现在 $s[i..j]$ 的最长回文子序列中，那么把他俩分别加入 $s[i+1..j-1]$ 中，看看哪个子串产生的回文子序列更长即可。   代码模板如下：\nif (s[i] == s[j]) // 它俩⼀定在最⻓回⽂⼦序列中  dp[i][j] = dp[i + 1][j - 1] + 2; else // s[i+1..j] 和 s[i..j-1] 谁的回⽂⼦序列更⻓？  dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]); 至此，状态转移方程就写出来了，根据dp 数组的定义，我们要求的就是 $dp[0][n-1]$，也就是整个 s 的最长回文子序列的长度。\n2.3 参考代码 #   首先明确一下 $base \\space case$，如果只有一个字符，显然最长回文子序列的长度为 1，即 $dp[i][j]=1 \\space (i==j)$。 因为 $i$ 肯定小于 $j$，所以对于那些 $i\u0026gt;j$ 的位置，根本不存在什么子序列，应该初始化为 0。 根据我们刚才的状态转移方程，想求 $dp[i][j]$ 需要知道 $dp[i+1][j-1]$，$dp[i+1][j]$，$dp[i][i-1]$ 这三个位置，将其填入 dp 数组后是这样：   为了保证每次计算 $dp[i][j]$，左、下、右方向的位置已经被计算出来了，只能斜着遍历或者反着遍历。\n 我选择反着遍历，参考代码如下：\npackage com.grayson.top; import java.util.Arrays; /** * @author peng.wei * @version 1.0 * @date 2021/4/8 14:50 * @Description 最长回文子序列 */ public class L516 { /** * 516. 最长回文子序列 * 给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。 * @param s 字符串 * @return 最长回文子序列的长度 */ public int longestPalindromeSubseq(String s) { int n = s.length(); // dp table: s[i...j] 子串的回文子序列的最大长度  // 最终的结果为 dp[0][n - 1]  int[][] dp = new int[n][n]; // base case: 单个字符的回文子序列的最大长度为 1  for (int i = 0; i \u0026lt; n; i++) { dp[i][i] = 1; } for (int i = n - 1; i \u0026gt;= 0; i--) { for (int j = i + 1; j \u0026lt; n; j++) { if (s.charAt(i) == s.charAt(j)) { // 两个字符相等，将 dp[i + 1][j - 1] + 1  dp[i][j] = dp[i + 1][j - 1] + 2; } else { // 两个字符不相等，则 dp[i][j] = max(dp[i][j - 1], dp[i + 1][j])  dp[i][j] = Math.max(dp[i][j - 1], dp[i + 1][j]); } } } // 返回最终的结果 dp[0][n - 1]  return dp[0][n - 1]; } } 3 参考文献 #    最长回文子序列。  动态规划之子序列问题解题模板。  "},{"id":85,"href":"/school-recruitment/docs/java/3JVM/3.7-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E7%9A%84%E7%89%B9%E7%82%B9/","title":"3.7 类加载机制的特点","section":"3、 Jvm","content":"类加载机制的特点 #  类加载机制有三个特点，具体如下：\n 双亲委托：  当一个自定义类加载器需要加载一个类，比如java.lang.String，他很懒，不会一上来就直接加载他，而是先委托自己的父类加载器去加载。父加载器如果发现自己还有父加载器，会一直往前找。 这样只要上级加载器比如启动类加载器，已经加载了某个类比如java.lang.String，所有的子类加载器都不需要自己加载了。 如果几个类加载器都没有加载到指定名称的类，那么会抛出ClassNotFoundException 异常。   负责依赖： 如果一个加载器在加载某个类的时候，发现这个类依赖于另外几个类或接口，也会去尝试加载这些依赖项。 缓存加载： 为了提升加载效率，消除重复加载，一旦某个类被一个类加载器加载，那么他会缓存这个加载结果，不会重复加载。  "},{"id":86,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.8-%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/","title":"1.1.8 最大子序和","section":"1.1 动态规划","content":"最大子序和 #  1 题目 #  给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。\n示例 1：\n输入：nums = [-2,1,-3,4,-1,2,1,-5,4] 输出：6 解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 示例 2：\n输入：nums = [1] 输出：1 示例 3：\n输入：nums = [0] 输出：0 示例 4：\n输入：nums = [-1] 输出：-1 示例 5：\n输入：nums = [-100000] 输出：-100000 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 3 * 104 -105 \u0026lt;= nums[i] \u0026lt;= 105  进阶： 如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的 分治法 求解。\n2 解题思路 #  2.1 暴力法 #  2.1.1 问题解析 #   依次遍历单个元素、两个元素、\u0026hellip;、N 个元素，然后从中选取最大子序和。   2.1.2 参考代码 #  /** * 53. 最大子序和（版本 1：暴力法） * * @param nums 数组 * @return 最大子序和 */ public int maxSubArrayV1(int[] nums) { int len = nums.length; int max = nums[0]; for (int i = 0; i \u0026lt; len; i++) { for (int j = 0; j \u0026lt; len - i; j++) { int sum = 0; for (int k = 0; k \u0026lt;= i; k++) { sum += nums[j + k]; } max = Math.max(sum, max); } } return max; } 2.2 动态规划 #  2.2.1 问题解析 #   定义 $dp$ 数组：  $dp[i]$ 表示 $nums$ 中以 $nums[i]$ 结尾的最大子序和。 $dp[i]$ 中最大的元素即为 $nums$ 的最大子序和。 $dp[0] = nums[0]$。   列出状态转移方程： $$ dp[i] = max(dp[i-1] + nums[i], nums[i]) $$   2.2.2 参考代码 #  /** * 53. 最大子序和（版本 2：动态规划） * * @param nums 数组 * @return 最大子序和 */ public int maxSubArrayV2(int[] nums) { int len = nums.length, max; // dp 数组，其中 dp[i] 表示以 nums[i] 结尾的 nums[0...i] 序列中最大子序和  // 则最终 dp 数组中的最大值便是整个数组的最大子序和  int[] dp = new int[len]; dp[0] = nums[0]; max = dp[0]; for (int i = 1; i \u0026lt; len; i++) { dp[i] = Math.max(nums[i] + dp[i - 1], nums[i]); max = Math.max(max, dp[i]); } return max; } 2.2.3 题目扩展 #  2.2.3.1 打家劫舍 #  2.2.3.1.1 问题分析 #    该题目中 $dp$ 数组的含义为 $dp[i]$ 表示以从第 $i$ 家开始偷窃，在不触动警报装置的情况下，一夜之内能够偷窃得到的最高金额，且：\n$$ dp[i] = max(dp[i + 1], dp[i + 2] + nums[i]) $$\n  2.2.3.1.2 参考代码 #  public int rob(int[] nums) { int m = nums.length; int[] dp = new int[m]; // base case  if (m \u0026gt;= 1) {dp[m - 1] = nums[m - 1];} if (m \u0026gt;= 2) {dp[m - 2] = Math.max(nums[m - 1], nums[m - 2]);} for (int i = m - 3; i \u0026gt;= 0; i--) { dp[i] = Math.max(dp[i + 1], dp[i + 2] + nums[i]); } return dp[0]; } 2.2.3.2 乘积最大子数组 #  2.2.3.2.1 问题分析 #   对于这种含有不定状态的最值问题，一般可以通过设置多个 $dp$ 数组来求解，分别用不同的 $dp$ 数组来表示不同的状态。 在本题中，假如我们直接使用一个 $dp$ 数组，其中 $dp[i]$表示以第 $i$ 个元素结尾的最大连续子数组的乘积，此时当前位置的最优解未必是由前一个位置的最优解转移得到。 因此，我们可以根据正负性进行讨论：  如果当前位置是一个负数的话，那么我们希望以他前一个位置结尾的某个段的积也是个负数，这样就可以负负得正，并且我们希望这个积尽可能负得多，即尽可能小。 如果当前位置是一个正数的话，那么我们希望以他前一个位置结尾的某个段的积也是个正数，并且我们希望这个积尽可能大。   因此我们需要维护两个 $dp$ 数组，**分别是 $dp_{max}$ 和**$dp_{min}$：   $dp_{max}$ 表示**以第 $i$ 个元素结尾的最大连续子数组的乘积**，且：\n$$ dp_{max} = max(dp_{max}[i - 1] \\times nums[i], dp_{min}[i - 1] \\times nums[i], nums[i]) $$\n  $dp_{min}$ 表示**以第 $i$ 个元素结尾的最小连续子数组的乘积**，且：\n$$ dp_{min} = min(dp_{max}[i - 1] \\times nums[i], dp_{min}[i - 1] \\times nums[i], nums[i]) $$\n    2.2.3.2.2 参考代码 #  /** * 152. 乘积最大子数组（版本 1：动态规划（优化前）） * * @param nums 数组 * @return 数组中乘积最大的连续子数组的乘积 */ public int maxProductV1(int[] nums) { int m = nums.length; // dp 数组，dpMax[i] 表示以第 i 个元素结尾的最大连续子数组的乘积  int[] dpMax = new int[m]; // dp 数组，dpMin[i] 表示以第 i 个元素结尾的最小连续子数组的乘积  int[] dpMin = new int[m]; int res; dpMax[0] = nums[0]; dpMin[0] = nums[0]; res = dpMax[0]; for (int i = 1; i \u0026lt; m; i++) { int item = nums[i]; // dpMax[i] = max(dpMax[i - 1] * nums[i], dpMin[i - 1] * nums[i], nums[i])  dpMax[i] = Math.max( dpMax[i - 1] * item, Math.max( dpMin[i - 1] * item, item ) ); // dpMin[i] = min(dpMax[i - 1] * nums[i], dpMin[i - 1] * nums[i], nums[i])  dpMin[i] = Math.min( dpMax[i - 1] * item, Math.min( dpMin[i - 1] * item, item ) ); // 去 dpMax 中的最大值  res = Math.max(res, dpMax[i]); } // 返回最后结果  return res; } 由于第 $i$ 个状态只和第 $i - 1$ 个状态相关，根据滚动数组思想，我们可以只用两个变量来维护 $i - 1$ 时刻的状态，一个维护 $dpMax$，一个维护 $dpMin$。\n/** * 152. 乘积最大子数组（版本 2：动态规划（优化后）） * * @param nums 数组 * @return 数组中乘积最大的连续子数组的乘积 */ public int maxProductV2(int[] nums) { int m = nums.length; // dpMax 表示以第 i 个元素结尾的最大连续子数组的乘积  int dpMax = nums[0]; // dpMin 表示以第 i 个元素结尾的最小连续子数组的乘积  int dpMin = nums[0]; int res; res = dpMax; for (int i = 1; i \u0026lt; m; i++) { int item = nums[i]; int dpMaxTemp = dpMax, dpMinYemp = dpMin; // dpMax = max(dpMaxTemp * nums[i], dpMinYemp * nums[i], nums[i])  dpMax = Math.max( dpMaxTemp * item, Math.max( dpMinYemp * item, item ) ); // dpMin = min(dpMaxTemp * nums[i], dpMinYemp * nums[i], nums[i])  dpMin = Math.min( dpMaxTemp * item, Math.min( dpMinYemp * item, item ) ); // 去 dpMax 中的最大值  res = Math.max(res, dpMax); } // 返回最后结果  return res; } 2.2.3.3 三角形最小路径和 #  2.2.3.3.1 问题分析 #    该题目中 $dp$ 数组的含义为 $dp[i][j]$表示从顶点到 $triangle.get(i).get(j)$ 的最小路径和，且：\n$$ dp[i][j] = Math.min(dp[i - 1][j], dp[i - 1][j - 1]) + triangle.get(i).get(j) $$\n  2.2.3.3.2 参考代码 #  /** * 120. 三角形最小路径和 * * @param triangle 三角形顶点列表 * @return 三角形自顶向下的最小路径和 */ public int minimumTotal(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; triangle) { int m = triangle.size(); int n = triangle.get(m - 1).size(); // dp 数组，其中 dp[i][j] 表示从顶点到 triangle.get(i).get(j) 的最小路径和  int[][] dp = new int[m][n]; int res = Integer.MAX_VALUE; for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; triangle.get(i).size(); j++) { int item = triangle.get(i).get(j); // 转移关系为：dp[i][j] = Math.min(dp[i - 1][j], dp[i - 1][j - 1]) + triangle.get(i).get(j)  // 需要确保数组下标不要越界，即：i - 1 \u0026gt;= 0 \u0026amp;\u0026amp; j - 1 \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; triangle.get(i - 1).size()  if (i - 1 \u0026gt;= 0) { if (j - 1 \u0026gt;= 0) { if (j \u0026lt; triangle.get(i - 1).size()) { dp[i][j] = Math.min(dp[i - 1][j], dp[i - 1][j - 1]) + item; } else { dp[i][j] = dp[i - 1][j - 1] + item; } } else { dp[i][j] = dp[i - 1][j] + item; } } else { dp[i][j] = item; } } } // 三角形最后一行中的最小路径和即为整个三角形自顶向下的最小路径和  for (int i = 0; i \u0026lt; triangle.get(m - 1).size(); i++) { res = Math.min(res, dp[m - 1][i]); } return res; } 2.2.3.4 解码方法 #  2.2.3.4.1 问题分析 #    动态规划中：\n 对于一维$dp$数组一般有两种思路，一种是以 $nums[i]$ 开头，另一种是以 $nums[i]$ 结尾，我们在定义 $dp$ 数组时可以从这两个方面去考虑即可。 对于二维$dp$数组一般可以从中间进行截取，例如 1.1.7 最长回文子序列中$dp$数组的定义为$dp[i][j]$表示$s[i\u0026hellip;j]$中包含的最长回文子序列的长度。    该题目中 $dp$ 数组的含义为 $dp[i]$ 表示以 $s.charAt(i)$ 开头的字符串的解码方法的总数，且：\n$$ dp[i] = dp[i + 1] + dp[i + 2] $$\n  2.2.3.4.2 参考代码 #  /** * 91. 解码方法 * * @param s 消息字符串 * @return 消息字符串解码方法的总数 */ public int numDecodings(String s) { int m = s.length(); // dp 数组，其中 dp[i] 表示以 s.charAt(i) 开头的消息字符串解码方法的总数  int[] dp = new int[m]; // base case  if (m \u0026gt;= 1) { dp[m - 1] = (s.charAt(m - 1) == \u0026#39;0\u0026#39; ? 0 : 1); } if (m \u0026gt;= 2) { if (s.charAt(m - 2) == \u0026#39;0\u0026#39;) { dp[m - 2] = 0; } else { if ((s.charAt(m - 2) \u0026lt; \u0026#39;2\u0026#39;) || s.charAt(m - 2) == \u0026#39;2\u0026#39; \u0026amp;\u0026amp; s.charAt(m - 1) \u0026lt;= \u0026#39;6\u0026#39;) { dp[m - 2] = dp[m - 1] + 1; } else { dp[m - 2] = dp[m - 1]; } } } for (int i = m - 3; i \u0026gt;= 0; i--) { if (s.charAt(i) == \u0026#39;0\u0026#39;) { // 如果一个字符串以 0 开头，那么该字符串解码方法的总数为 0  dp[i] = 0; } else if ((s.charAt(i) == \u0026#39;2\u0026#39; \u0026amp;\u0026amp; s.charAt(i + 1) \u0026gt;= \u0026#39;7\u0026#39;) || (s.charAt(i) \u0026gt;= \u0026#39;3\u0026#39;)) { // 如果当前字符串的第 i 个字符和第 i + 1 个字符组成的数字超过 26，则这两个字符不可以拆开，因此其解码的方法总数和以第 i+2 个字符开头的字符串解码的方法总数一样  dp[i] = dp[i + 1]; } else { // 如果当前字符串的第 i 个字符和第 i + 1 个字符组成的数字不超过 26，则这两个字符可以拆开，因此其解码的方法总数和以第 i+1 个字符开头的字符串及以第 i+2 个字符开头的字符串解码的方法总数一样  dp[i] = dp[i + 1] + dp[i + 2]; } } // 最后的结果即以第 0 个字符开头的字符串对应的解码方法总数  return dp[0]; } 2.2.3.5 打家劫舍 II #  2.2.3.5.1 问题分析 #    对于这种圆环型的问题，我们可以把他拆分成两部分，并对其分别去求结果，然后再将两部分的结果取最值即可。\n  该题目中 $dp$ 数组的含义为 $dp[i]$ 表示第 $i$ 户及之后所能偷到的最大金额，且：\n$$ dp[i] = max(dp[i + 1], dp[i + 2] + nums[i]); $$\n  2.2.3.5.2 参考代码 #  /** * 213. 打家劫舍 II * @param nums 每个房屋存放金额的非负整数数组 * @return 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额 */ public int rob(int[] nums) { int m = nums.length; // 将整个数组拆分成两部分，分别为 nums[0, nums.length - 2] 和 nums[1, nums.length - 1]，然后对这两部分分别求能够偷窃到的最大金额，并取二者的最大值即可  return m \u0026gt; 1 ? Math.max(subRob(nums, 0, m - 1), subRob(nums, 1, m)) : nums[0]; } /** * 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额 * @param nums 每个房屋存放金额的非负整数数组 * @param start 起始位置 * @param end 结束位置 * @return 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额 */ public int subRob(int[] nums, int start, int end) { int m = nums.length; // dp 数组，其中 dp[i] 表示从第 i 户及后面住户中所能偷窃到的最高金额  int[] dp = new int[m]; // base case  if (end \u0026gt;= 1) { dp[end - 1] = nums[end - 1]; } if (end \u0026gt;= 2) { if (nums[end - 2] \u0026gt;= nums[end - 1]) { dp[end - 2] = nums[end - 2]; } else { dp[end - 2] = nums[end - 1]; } } for (int i = end - 3; i \u0026gt;= start; i--) { // 第 i 户及之后所能偷到的最大金额 等于 第 i 户及之后所能偷到的最大金额 与 第 i + 2 户所能偷到的最大金额和第 i 户金额之和 的最大值  dp[i] = Math.max(dp[i + 1], dp[i + 2] + nums[i]); } return dp[start]; } 2.3 贪心法 #  2.3.1 问题解析 #   如果 sum 小于 0，说明他对于下一个 sum 起副作用，所以将 sum 重置为当前元素。 否则的话，直接将当前元素累加到 sum 上。   2.3.2 参考代码 #  /** * 53. 最大子序和（版本 3：贪心算法） * * @param nums 数组 * @return 最大子序和 */ public int maxSubArrayV3(int[] nums) { int sum = nums[0], max = nums[0]; int len = nums.length; for (int i = 1; i \u0026lt; len; i++) { if (sum \u0026lt; 0) { // 如果 sum 小于 0，说明他对于下一个 sum 起副作用，所以将 sum 重置为当前元素  sum = nums[i]; } else { // 否则的话，直接将当前元素累加到 sum 上  sum += nums[i]; } max = Math.max(max, sum); } return max; } 3 参考文献 #    53. 最大子序和。  198. 打家劫舍。  最大子序和 c++ 实现四种解法 暴力法、动态规划、贪心法和分治法 图示讲解。  152. 乘积最大子数组。  乘积最大子数组。  120. 三角形最小路径和。  91. 解码方法。  213. 打家劫舍 II。  "},{"id":87,"href":"/school-recruitment/docs/java/3JVM/3.8-JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","title":"3.8 Jvm内存结构","section":"3、 Jvm","content":"1 概述 #  JVM 内部使用Java 内存模型，在逻辑上将内存划分为线程栈（Thread Stacks）和堆内存（Heap）两个部分，如下图所示：\n 1.1 线程栈 #  JVM 中，每个正在运行的线程，都有自己的线程栈。\n 线程栈包含了当前正在执行的方法链或调用链上的所有方法的状态信息， 所以线程栈又被称为方法栈或调用栈。 线程栈里面保存了调用链上正在执行的所有方法中的局部变量：  每个线程都只能访问自己的线程栈。 每个线程都不能访问其他线程的局部变量。   即使两个线程正在执行完全相同的代码，但每个线程都会在自己的线程栈内创建对应代码中声明的局部变量，所以每个线程都有一份自己的局部变量副本：  所有原生类型的局部变量都存储在线程栈中，因此对其他线程是不可见的。 线程可以将一个原生变量值的副本传给另一个线程，但不能共享原生局部变量本身。    1.2 堆内存 #  堆内存又称为共享堆，堆中的所有对象，可以被所有线程访问，只要他们能拿到对象的引用地址：\n 如果一个线程可以访问某个对象时，也就可以访问该对象的成员变量。 如果两个线程同时调用某个对象的同一个方法，则他们都可以访问到这个对象的成员变量，但每个线程的局部变量副本是独立的。  虽然各个线程自己使用的局部变量都在自己的栈上，但是大家可以共享堆上的对象，各个不同线程访问同一个对象实例的基础类型的成员变量，会给每个线程一个变量的副本。\n 1.3 总结 #   如果是原生数据类型的局部变量，那么他的内容就全部保留在线程栈中。 如果是对象引用，在对象的引用地址保存在栈中，而实际的对象内容保存在堆中。 对象的成员变量与对象本身一起存储在堆上，不管成员变量的类型是原生数值，还是对象引用。 类的静态变量则和类定义一样都保存在堆中。  总结一下：\n 原始数据类型的局部变量和对象引用地址在栈上。 对象、对象成员与类定义、静态变量在堆上。   2 线程栈 #    每启动一个线程，JVM 就会在栈空间分配对应的线程栈。 线程栈也叫做Java 方法栈，如果使用了JNI（Java Native Interface）方法，则会分配一个单独的本地方法栈（Native Stack）。 线程执行过程中，一般会有多个方法组成调用栈（Stack Trace），比如 A 调用 B，B 调用 C\u0026hellip;\u0026hellip;。每执行到一个方法，就会创建对应的栈帧（Frame）。  栈帧是一个逻辑上的概念，具体的大小在一个方法完成后基本上就能确定。 比如返回值需要有一个空间存放；每个局部变量都需要对应的地址空间；此外还有给指令使用的操作数栈；以及Class 指针（标识这个栈帧对应的是哪个类的方法，指向非堆里面的 Class 对象）。     3 堆 #    堆内存是所有线程共用的内存空间，理论上大家都可以访问。 逻辑上的 Java 堆划分为 堆（Heap）和 堆外（Non-Heap）两个部分：  3.1 堆内存（Heap Memory） #  JVM 将 Heap 内存分为 年轻代（Young Generation）和 老年代（Old Generation）两部分。\n3.1.1 年轻代 #  年轻代划分为三个内存池，分别为 新生代（Eden Space）和 存活区（Survivor Spaces）。\n3.1.1.1 新生代（Eden Space） #  新生代，也叫伊甸区，用来分配新创建的对象。\n 通常会有多个线程同时创建对象，所以 Eden 区被划分为多个线程本地分配缓冲区（Thread Local Allocation Buffer, TLAB）。通过这种缓冲区的划分，大部分对象直接由 JVM 在对应线程的TLAB中分配，避免与其他线程的同步操作。  如果 TLAB 中没有足够的内存空间时，就会在共享 Eden 区（Shared Eden Space）之中分配。  如果共享 Eden 区也没有足够的空间，就会触发一次年轻代 GC来释放内存空间。  如果 GC 之后 Eden 区依然没有足够的空闲内存区域，则对象就会被分配到老年代空间（Old Generation）。       当 Eden 区进行垃圾回收时，GC 将从所有 root 可达的对象过一遍，并标记为存活对象。 标记阶段完成后，Eden 区所有存活的对象都会被复制到存活区（Survivor Spaces）里面，整个 Eden 区就可以被认为是空的，然后就能用来分配新对象， 这种方法称为标记-复制算法（Mark and Copy）。  3.1.1.2 存活区（Survivor Spaces） #  Eden 区的旁边是两个存活区（Survivor Spaces），称为 from 空间和to 空间，任意时刻总有一个存活区是空的。\n 空的那个存活区用于在下一次年轻代 GC 时存放收集的对象，年轻代中所有的存活对象（包括Eden 区和非空的那个 from 存活区）都会被复制到to 存活区。GC 过程完成后，to 区有对象，而 from 区没有对象，二者的角色正好进行切换，from 变成 to，to 变成 from。 存活的对象会在两个存活区之间复制多次，直到某些对象的存活时间达到一定阈值，因为根据分带理论假设，存活超过一定时间的对象很可能会继续存活更长时间，因此这些年老的对象将会被提升到老年代，提升的时候，存活的对象不再是复制到另一个存活区，而是迁移到老年代，并在老年代一直驻留，直到变为不可达对象。 为了确定一个对象是否足够老，GC 模块会跟踪记录每个存活区对象存活的次数，每次分代 GC 完成后，存活对象的年龄就会增长，当年龄超过提升阈值（Tenuring Threshold），就会被提升到老年代区域。  具体的提升阈值由 JVM 动态调整，但也可以用参数 -XX:+MaxTenuringThreshold 来指定上限。 如果设置 -XX:+MaxTenuringThreshold=0，则 GC 时存活对象不在存活区之间复制，直接提升到老年代。 现代 JVM 中，这个阈值默认设置为 15 个 GC 周期，这也是 HotSpot JVM 中允许的最大值。 如果存活区空间不够存放年轻代中的存活对象，提升也可能更早的进行。     3.1.2 老年代 #   老年代的 GC 实现要复杂的多，其空间通常会更大，GC 发生的频率比年轻代要小很多。 因为预期老年代中的对象大部分是存活的，所以不再使用标记-复制算法，而是采用移动对象的方式来实现最小化内存碎片。 老年代空间的清理算法通常是建立在不同基础上的，原则上，会执行以下这些步骤：  通过标志位，标记所有 GC Roots 可达的对象。 删除所有不可达的对象。 整理老年代空间中的内容，方法是将所有的存活对象复制，从老年代开始的地方依次存放。    3.2 堆外内存（No-Heap Memory） #  Non-Heap 的本质还是 Heap，只是一般不归 GC 管理，里面划分为三个内存池：Metaspace、Compressed Class Space、Code Cache。\n3.2.1 Metaspace #    以前叫持久代（Permanent Generation），主要用于存放以下信息：\n JVM 中类的元数据在 Java 堆中的存储区域。 Java 类对应的 HotSpot 虚拟机中的内部表示也存储在这里。 类的层级信息，字段，名字。 方法的编译信息及字节码。 变量。 常量池和符号解析。    由于很难预测这块区域到底需要占用多少内存空间，预测失败可能会导致内存溢出错误，因此 Java8 直接删除了永久代，改用 Metaspace，同时将方法区移至 Metaspace。\n  Metaspace 并不在虚拟机中，而是使用本地内存，因此，默认情况下，元空间的大小仅受本地内存限制。\n  参考文献 #    Java8 Non-Heap 中的 metaspace 和 compressed class space 解释。  Metaspace 之一：Metaspace 整体介绍（永久代被替换原因、元空间特点、元空间内存查看分析方法）。  "},{"id":88,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.9-%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98%E7%B3%BB%E5%88%97%E9%80%9A%E8%A7%A3/","title":"1.1.9 股票问题系列通解","section":"1.1 动态规划","content":" 本篇文章转载自 股票问题系列通解（转载翻译）。\n 1 前言 #   股票问题一共有六道题，链接如下：   121. 买卖股票的最佳时机。  122. 买卖股票的最佳时机 II。  123. 买卖股票的最佳时机 III。  188. 买卖股票的最佳时机 IV。  309. 最佳买卖股票时机含冷冻期。  714. 买卖股票的最佳时机含手续费。   每个问题都有优质的题解，但是大多数题解没有建立起这些问题之间的联系，也没有给出股票问题系列的通解，这篇文章给出适用于全部股票问题的通解，以及对于每个特定问题的特解。  2 通用情况 #   这个想法基于如下问题，给定一个表示每天股票价格的数组，什么因素决定了可以获得的最大收益，相信大多数人可以很快给出答案，例如在那些天进行交易以及允许多少次交易，这些因素当然也很重要，在问题描述中也有这些因素，然而还有一个隐藏但是关键的因素决定了最大收益，下文将阐述这一点。 首先介绍一些符号：  用 $n$表示股票价格的数组。 用 $i$表示第 $i$ 天（$i$ 的取值范围是 0 到 $n - 1$）。 用 $k$表示允许的最大交易次数。 用 $T[i][k]$表示在第 $i$ 天结束时，最多进行 $k$ 次交易的情况下可以获得的最大收益。   基准情况为 $T[-1][k] = T[i][0] = 0$，表示没有进行股票交易时没有收益（注意第一天对应 $i = 0$，因此 $i = -1$ 表示没有股票交易）。 现在开始将 $T[i][k]$ 关联到子问题，得到状态转移方程：  第 $i$ 天可能有三个操作，分别为买入、卖出、休息。 我们并不知道哪个操作是最好的，但是可以通过计算得到选择每个操作可以得到的最大收益，假设没有别的限制条件，则可以尝试每一种操作，并选择可以最大化收益的一种操作，但是，题目中确实有限制条件，规定不能同时进行多次交易，因此如果决定在第 $i$ 天买入，在买入之前必须持有 0 份股票，如果决定在第 $i$ 天卖出，在卖出之前必须恰好持有 1 份股票，持有股票的数量是上文提及到的隐藏因素，该因素影响第 $i$ 天可以进行的操作，进而影响最大收益。 因此对 $T[i][k]$ 的定义需要分成两项：  $T[i][k][0]$表示第 $i$ 天结束时，最多进行 $k$ 次交易且在进行操作后持有 0 份股票的情况下可以获得的最大收益。 $T[i][k][1]$表示在第 $i$ 天结束时，最多进行 $k$ 次交易且在进行操作后持有 1 份股票的情况下可以获得的最大收益。   使用新的状态表示之后，可以得到基准情况和状态转移方程如下：   基准情况：\n$$ T[-1][k][0] = 0, T[-1][k][1] = -Infinity $$\n$$ T[i][0][0] = 0, T[i][0][1] = -Infinity $$\n 基准情况中，$T[-1][k][0] = T[i][0][0] = 0$ 的含义和上文相同，$T[-1][k][1] = T[i][0][1]$ 的含义是在没有进行股票交易时不允许持有股票。    状态转移方程：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1]) + prices[i] $$\n$$ T[i][k][1] = max(T[i - 1][k][0] - prices[i], T[i - 1][k][1]) $$\n 对于状态转移方程中的$T[i][k][0]$，第 $i$ 天进行的操作只能是休息或卖出，因为在第 $i$ 天结束时持有的股票数量是 0，$T[i - 1][k][0]$是休息操作可以得到的最大收益，$T[i - 1][k][1] + prices[i]$是卖出操作可以得到的最大收益，注意到允许的最大交易次数是不变的，因为每次交易包含两次成对的操作，分别为买入和卖出，只有买入操作会改变允许的最大交易次数。 对于状态转移方程中的 $T[i][k][1]$，第 $i$ 天进行的操作只能是休息或卖出，因为在第 $i$ 天结束时持有的股票数量是 0，$T[i - 1][k][1] + prices[i]$是卖出操作可以得到的最大收益，$T[i - 1][k - 1][0] - prices[i]$是买入操作可以得到的最大收益，注意到允许的最大交易次数减少了一次，因为每次买入操作会使用一次交易。     为了得到最后一天结束时的最大收益，可以遍历股票价格数组，根据状态转移方程计算 $T[i][k][0]$ 和 $T[i][k][1]$ 的值，最终答案是 $T[n - 1][k][0]$，因为结束时持有 0 份股票的收益一定大于持有 1 份股票的收益。    3 应用于特殊情况 #  上述六个股票问题是根据 $k$ 的值进行分类的，其中 $k$是允许的最大交易次数，最后两个问题有附加限制，包括【冷冻期】和【手续费】，通解可以应用于每个股票问题。\n3.1 情况一：$k = 1$ #    情况一对应的题目是 121. 买卖股票的最佳时机。\n  对于情况一，每天有两个未知变量，分别为 $T[i][1][0]$ 和 $T[i][1][1]$，状态转移方程如下：\n$$ T[i][1][0] = max(T[i - 1][1][0], T[i - 1][1][1] + prices[i]) $$\n$$ T[i][1][1] = max(T[i - 1][1][1], T[i - 1][0][0] - prices[i]) = max(T[i - 1][1][1], -prices[i]) $$\n第二个状态转移方程利用了 $T[i - 1][0][0] = 0$\n  根据上述状态转移方程，可以写出时间复杂度为 $O(n)$ 和空间复杂度为 $O(n)$ 的解法。\n/** * 121. 买卖股票的最佳时机（版本 3：动态规划） * @param prices 股票价格 * @return 最大利润 */ public int maxProfit(int[] prices) { if (prices == null || prices.length == 0) {return 0;} int m = prices.length; int[][] dp = new int[m + 1][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for (int i = 1; i \u0026lt; m; i++) { dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][1] + prices[i]); dp[i][1] = Math.max(-prices[i], dp[i - 1][1]); } return dp[m - 1][0]; }   3.2 情况二：$k$ 为正无穷 #    情况二对应的题目是 122. 买卖股票的最佳时机 II。\n  如果 $k$ 为正无穷，则 $k$ 和 $k - 1$ 可以看成是相同的，因此有$T[i - 1][k - 1][0] = T[i - 1][k][0]$和 $T[i - 1][k - 1][1] = T[i - 1][k][1]$，每天仍然有两个未知变量，分比为 $T[i][k][0]$ 和 $T[i][k][1]$，其中 $k$为正无穷，状态转移方程如下：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1] + prices[i]) $$\n$$ T[i][k][1] = max(T[i - 1][k - 1][0] - prices[i], T[i - 1][k][1]) = max(T[i - 1][k][0] - prices[i], T[i - 1][k][1]) $$\n第二个状态转移方程利用了 $T[i - 1][k - 1][0] = T[i - 1][k][0]$\n  根据上述状态转移方程，可以写出时间复杂度为 $O(n)$ 和空间复杂度为 $O(n)$ 的解法：\n/** * 122. 买卖股票的最佳时机 II * @param prices 股票价格 * @return 最大利润 */ public int maxProfit(int[] prices) { if (prices == null || prices.length == 0) {return 0;} int m = prices.length; int[][] dp = new int[m + 1][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for (int i = 1; i \u0026lt; m; i++) { dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][1] + prices[i]); dp[i][1] = Math.max(dp[i - 1][0] - prices[i], dp[i - 1][1]); } return dp[m - 1][0]; }   3.3 情况三：$k = 2$ #    情况三对应的题目是 123. 买卖股票的最佳时机 III。\n  情况三和情况一相似，区别之处是，对于情况三，每天有四个未知变量，分别为 $T[i][1][0]$、$T[i][1][1]$、$T[i][2][0]$、$T[i][2][1]$，状态转移方程如下：\n$$ T[i][1][0] = max(T[i - 1][1][0], T[i - 1][1][1] + prices[i]) $$\n$$ T[i][1][1] = max(T[i - 1][0][0] - prices[i], T[i - 1][1][1]) = max(-prices[i], T[i - 1][1][1]) $$\n$$ T[i][2][0] = max(T[i - 1][2][0], T[i - 1][2][1] + prices[i]) $$\n$$ T[i][2][1] = max(T[i - 1][1][0] - prices[i], T[i - 1][2][1]) $$\n第二个状态转移方程利用了 $T[i][0][0] = 0$。\n  根据上述状态转移方程，可以写出时间复杂度为 $O(n)$ 和空间复杂度为 $O(n)$ 的解法：\n/** * 123. 买卖股票的最佳时机 III * @param prices 股票价格 * @return 最大利润 */ public int maxProfit(int[] prices) { if (prices == null || prices.length == 0) {return 0;} int m = prices.length; int[][][] dp = new int[m + 1][3][2]; dp[0][1][0] = 0; dp[0][1][1] = -prices[0]; dp[0][2][0] = 0; dp[0][2][1] = -prices[0]; for (int i = 1; i \u0026lt; m; i++) { dp[i][1][0] = Math.max(dp[i - 1][1][0], dp[i - 1][1][1] + prices[i]); dp[i][1][1] = Math.max(dp[i - 1][1][1], dp[i - 1][0][0] - prices[i]); dp[i][2][0] = Math.max(dp[i - 1][2][0], dp[i - 1][2][1] + prices[i]); dp[i][2][1] = Math.max(dp[i - 1][2][1], dp[i - 1][1][0] - prices[i]); } return dp[m - 1][2][0]; }   3.4 情况四：$k$ 为任意值 #    情况四对应的题目是 188. 买卖股票的最佳时机 IV。\n  情况四是最通用的情况，对于每一天需要使用不同的 $k$ 值更新所有的最大收益，对应持有 0 份股票或 1 份股票，如果 $k$ 超过一个临界值，最大收益就不在取决于允许的最大交易次数，而是取决于股票价格数组的长度，因此可以进行优化。\n  一个由有收益的交易至少需要两天（在前一天买入，在后一天卖出，前提是买入价格低于卖出价格），如果股票价格数组的长度为 $n$，则有收益的交易的数量最多为 $n / 2$（整数除法），因此 $k$的临界值是 $n / 2$，如果给定的 $k$ 不小于临界值，即 $k \u0026gt;= n / 2$，则可以将 $k$ 扩展为正无穷，此时问题等价于 情况二。\n  根据状态转移方程，可以写出时间复杂度为 $O(nk)$ 和空间复杂度为 $O(nk)$ 的解法：\n/** * 188. 买卖股票的最佳时机 IV * @param prices 股票价格 * @param k 交易最大笔数 * @return 最大利润 */ public int maxProfit(int k, int[] prices) { if (prices == null || prices.length == 0) {return 0;} if (k \u0026gt;= prices.length / 2) {return maxProfit(prices);} int m = prices.length; int[][][] dp = new int[m + 1][k + 1][2]; for (int i = 0; i \u0026lt;= k; i++) { dp[0][i][0] = 0; dp[0][i][1] = -prices[0]; } for (int i = 1; i \u0026lt; m; i++) { for (int j = k; j \u0026gt; 0; j--) { dp[i][j][0] = Math.max(dp[i - 1][j][0], dp[i - 1][j][1] + prices[i]); dp[i][j][1] = Math.max(dp[i - 1][j][1], dp[i - 1][j - 1][0] - prices[i]); } } return dp[m - 1][k][0]; } /** * 当 k 趋近于无穷大时买卖股票的最佳时机 * @param prices 股票价格 * @return 最大利润 */ public int maxProfit(int[] prices) { if (prices == null || prices.length == 0) {return 0;} int m = prices.length; int[][] dp = new int[m + 1][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for (int i = 1; i \u0026lt; m; i++) { dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][1] + prices[i]); dp[i][1] = Math.max(dp[i - 1][0] - prices[i], dp[i - 1][1]); } return dp[m - 1][0]; }   3.5 情况五：$k$为正无穷但有冷却时间 #    情况五对应的题目是 309. 最佳买卖股票时机含冷冻期。\n  由于具有相同的$k$值，因此情况五和情况二非常类似，不同之处在于情况五有「冷却时间」的限制，因此需要对状态转移方程进行一些修改。\n  情况二的状态转移方程如下：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1] + prices[i]) $$\n$$ T[i][k][1] = max(T[i - 1][k][0] - prices[i], T[i - 1][k][1]) $$\n  但是在有「冷却时间」的情况下，如果在第$i - 1$天卖出了股票，就不能在第$i$天买入股票，因此，如果要在第$i$天买入股票，第二个状态转移方程就不能使用$T[i - 1][k][0]$，而应该使用$T[i - 2][k][0]$，状态转移方程中的别的项保持不变，新的状态转移方程如下：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1] + prices[i]) $$\n$$ T[i][k][1] = max(T[i - 2][k][0] - prices[i], T[i - 1][k][1]) $$\n  根据上述状态转移方程，可以写出时间复杂度为$O(n)$和空间复杂度为$O(n)$的解法：\n/** * 309. 最佳买卖股票时机含冷冻期 * @param prices 股票价格 * @param k 交易最大笔数 * @return 最大利润 */ public int maxProfit(int k, int[] prices) { if (prices == null || prices.length == 0) {return 0;} int m = prices.length; int[][] dp = new int[m + 1][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for (int i = 1; i \u0026lt; m; i++) { dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][1] + prices[i]); dp[i][1] = Math.max((i \u0026gt;= 2 ? dp[i - 2][0] : 0) - prices[i], dp[i - 1][1]); } return dp[m - 1][0]; }   3.6 情况六：$k$为正无穷但有手续费 #    情况六对应的题目是 714. 买卖股票的最佳时机含手续费。\n  由于具有相同的$k$值，因此情况六和情况二非常相似，不同之处在于情况六有「手续费」，因此需要对状态转移方程进行一些修改。\n  情况二的状态转移方程如下：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1] + prices[i]) $$\n$$ T[i][k][1] = max(T[i - 1][k][0] - prices[i], T[i - 1][k][1]) $$\n  由于需要对每次交易付手续费，因此在每次买入或卖出股票之后的收益需要扣除手续费，新的状态转移方程有两种表示方法：\n  第一种表示方法，在每次买入股票时扣除手续费：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1] + prices[i]) $$\n$$ T[i][k][1] = max(T[i - 1][k][0] - prices[i] - fee, T[i - 1][k][1]) $$\n  第二种表示方法，在每次卖出股票时扣除手续费：\n$$ T[i][k][0] = max(T[i - 1][k][0], T[i - 1][k][1] + prices[i] - fee) $$\n$$ T[i][k][1] = max(T[i - 1][k][0] - prices[i], T[i - 1][k][1]) $$\n    根据上述状态转移方程，可以写出时间复杂度为$O(n)$和空间复杂度为$O(n)$的解法：\n/** * 714. 买卖股票的最佳时机含手续费 * @param prices 股票价格 * @param fee 手续费 * @return 最大利润 */ public int maxProfit(int[] prices, int fee) { if (prices == null || prices.length == 0) {return 0;} int m = prices.length; int[][] dp = new int[m + 1][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for (int i = 1; i \u0026lt; m; i++) { dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][1] + prices[i] - fee); dp[i][1] = Math.max(dp[i - 1][0] - prices[i], dp[i - 1][1]); } return dp[m - 1][0]; }   参考文献 #    121. 买卖股票的最佳时机。  122. 买卖股票的最佳时机 II。  123. 买卖股票的最佳时机 III。  188. 买卖股票的最佳时机 IV。  309. 最佳买卖股票时机含冷冻期。  714. 买卖股票的最佳时机含手续费。  股票问题系列通解（转载翻译）。  "},{"id":89,"href":"/school-recruitment/docs/java/3JVM/3.9-%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B/","title":"3.9 常见的垃圾收集算法有哪些","section":"3、 Jvm","content":"常见的垃圾收集算法有哪些 #  1 引用计数法 #  2.1 原理 #   这种算法会在每一个对象上记录这个对象被引用的次数，只要有任何一个对象引用了该对象，这个对象的计数器就加 1，取消对这个对象的引用时，这个对象的计数器就减 1。 任何一个时刻，如果该对象的计数器为 0，那么这个对象就是可以回收的。  2.2 缺点 #   如果出现了对象之间的相互引用，这样单纯的引用计数就会出现问题，导致循环引用的对象没办法回收，这时就会出现内存泄漏，该释放的没释放，该回收的没回收。 如果依赖关系复杂，计算机的内存资源很可能用满，或者说不够用，这样就可能会导致内存溢出。 由于以上原因，在JVM 中没有使用引用计数法。  2 JVM 常用 GC 算法 #  各种垃圾收集器的实现细节虽然不尽相同，但总体而言，垃圾收集器都专注于两件事情：\n 查找所有存活对象。 抛弃其他部分，即死对象，不再使用的对象。  2.1 标记可达对象（Marking Reacable Objects） #  现代 JVM 中所有的 GC 算法，第一步都是找出所有存活的对象，如下图所示：\n  首先，有一些特定的对象被指定为GC 根元素（Garbage Collection Roots），包括：  当前正在执行的方法里的局部变量和输入参数。 活动线程。 内存中所有类的静态字段。 JNI 引用。   其次，GC 遍历内存中整体的对象关系图，从 GC 根元素开始扫描，到直接引用，以及其他对象（通过对象的属性域），所有 GC 访问到的对象都被标记为存活对象。  存活对象在上图中用蓝色表示，标记阶段完成后，所有存活对象都被标记了。 而其他对象（上图中灰色部分）就是根元素不可达的，也就是程序不能再使用这些不可达的对象，这样的对象被认为是垃圾，GC 会在接下来的阶段清除他们。   在标记阶段有几个需要注意的地方：  标记阶段需要暂停所有的应用线程，以遍历所有对象的引用关系，因为不暂停就没法跟踪一直在变化的引用关系图，这种情景叫做全线停顿（Stop The World Pause），而可以安全暂停线程的点叫做安全点，然后，JVM 就可以专心执行清理工作了，安全点可能由多种因素触发，GC 是触发安全点最常见的原因。 暂停的时间与堆内存大小、对象的总数没有直接关系，而是由存活对象的数量来决定，所以增加堆内存的大小并不会直接影响标记阶段占用的时间。    2.2 清除（Sweeping） #  标记阶段完成后，GC 进行下一步操作，删除不可达对象。\n各种 GC 算法在删除不可达对象时略有不同，但总体可分为三类：清除（Sweeping）、整理（Compacting）和复制（Copying）。\n2.3.1 标记-清除算法（Mark and Speep） #  算法的概念非常简单：直接忽略所有的垃圾。也就是说在标记阶段完成后，所有不可达对象占用的内存空间都被认为是空闲的，因此可以用来分配新对象。\n该算法的缺点如下：\n 这种算法需要使用空闲列表（Free List）来记录所有的空闲区域，以及每个区域的大小，维护空闲表增加了对象分配时的开销。 命名有很多空闲内存，却可能没有一个区域的大小能够存放需要分配的对象，从而导致分配失败（在 Java 中就是 OutOfMemoryError）。   2.3 整理（Compacting） #  2.3.1 标记-清除-整理算法（Mark-Sweep-Compact） #  算法的原理是将所有被标记的对象（存活对象）迁移到内存空间的起始处，这消除了标记-清除算法的缺点。\n该算法的优缺点如下：\n 优点：  碎片整理之后，分配新对象就很简单，只需要通过指针碰撞（Pointer Bumping）即可。 使用这种算法，内存空间剩余的容量一直是清楚的，不会再导致内存碎片问题。   缺点：  GC 暂停时间会增加，因此需要将所有对象复制到另一个地方，然后修改这些对象的引用。     2.4 复制（Copying） #  2.4.1 标记-复制算法（Mark and Copy） #  标记-复制算法和标记-整理算法十分相似。\n 相同点：两者都会移动所有存活的对象。 区别：标记-复制算法是将内存移动到另外一个空间：存活区，标记-清除算法只是认为所有不可达对象的空间都是空闲的，可以用来分配新对象。  标记-复制算法的优缺点如下：\n 优点：标记和复制可以同时进行。 缺点：需要一个额外的内存空间，来存放所有的存活对象。   "},{"id":90,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.10-%E6%9C%80%E9%95%BF%E9%87%8D%E5%A4%8D%E5%AD%90%E6%95%B0%E7%BB%84/","title":"1.1.10 最长重复子数组","section":"1.1 动态规划","content":"最长重复子数组 #  1 题目 #  给两个整数数组 A 和 B ，返回两个数组中公共的、长度最长的子数组的长度。\n示例：\n输入： A: [1,2,3,2,1] B: [3,2,1,4,7] 输出：3 解释： 长度最长的公共子数组是 [3, 2, 1] 。 提示：\n 1 \u0026lt;= len(A), len(B) \u0026lt;= 1000 0 \u0026lt;= A[i], B[i] \u0026lt; 100  2 解题思路 #  2.1 暴力解法 #  2.1.1 问题分析 #   首先将 $nums2$ 中的元素对应的下标保存在 $map$ 中，这样便于查找 $nums1$ 中的元素是否在 $nums2$ 中出现。 然后遍历 $nums1$，假设当前遍历的元素为 $item$：  如果 $item$ 没有在 $nums2$ 中出现，则直接返回。 如果 $item$ 在 $nums2$ 中出现，则以 $item$ 为起点，同步遍历 $nums1$ 和 $nums2$ 的后续元素，直到两个数组中对应元素不相等，或达到任意数组的边界，计算两个数组相应部分的的重复子数组的大小，并对最终的结果进行更新。   最后返回最终结果即可。  2.1.2 参考代码 #  /** * 718. 最长重复子数组（版本 1：暴力解法） * @param nums1 数组 1 * @param nums2 数组 2 * @return 两个数组中公共的、长度最长的子数组的长度 */ public int findLengthV1(int[] nums1, int[] nums2) { Map\u0026lt;Integer, Map\u0026lt;Integer, Integer\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int n1 = nums1.length; int n2 = nums2.length; int maxLength = 0; // 将 nums2 中的元素对应的下标保存在 map 中，这样便于查找 nums1 中的元素是否在 nums2 中出现  for (int i = 0; i \u0026lt; n2; i++) { int item = nums2[i]; if (!map.containsKey(item)) { Map\u0026lt;Integer, Integer\u0026gt; mapTemp = new HashMap\u0026lt;\u0026gt;(); mapTemp.put(i, 0); map.put(item, mapTemp); } else { Map\u0026lt;Integer, Integer\u0026gt; mapTemp = map.get(item); mapTemp.put(i, 0); map.put(item, mapTemp); } } // 遍历 nums1  for (int i = 0; i \u0026lt; n1; i++) { int item = nums1[i]; // item 没有在 nums2 中出现，直接返回  if (!map.containsKey(item)) {continue;} // item 在 nums2 中出现，获取其在 nums2 中对应的下标  Map\u0026lt;Integer, Integer\u0026gt; mapTemp = map.get(item); // 依次以 item 在 nums2 中的下标的下一个位置为起点，计算两个数组的重复子数组的大小  for (Map.Entry\u0026lt;Integer, Integer\u0026gt; entry: mapTemp.entrySet()) { int tempLength = 1; int key = entry.getKey(); for (int j = key + 1; j \u0026lt; n2; j++) { if (i + tempLength \u0026gt;= n1) {break;} if (nums1[i + tempLength] != nums2[j]) {break;} tempLength++; } maxLength = Math.max(maxLength, tempLength); } } // 返回结果  return maxLength; } 2.2 动态规划 #  2.2.1 问题分析 #    对于求最值的题目，都可以思考一下看是否可以用动态规划来求解，而动态规划的核心就是定义 $dp$ 数组，寻找状态转移方程。\n  $dp$数组的定义有一维和二维数组两种，这个需要根据具体的题目来具体分析。\n  本题中 $dp$ 数组可以定义为 $dp[i][j]$，表示 $nums1[i]$ 和 $nums2[j]$ 的最长公共前缀的长度，这样 $dp$数组中最大的元素即为 $nums1$ 和 $nums2$ 的最长重复子数组的长度。\n  然后分别遍历 $nums1$ 和 $nums2$：\n 如果 $nums1[i] = nums2[j]$，则 $dp[i][j] = dp[i + 1][j + 1] + 1$。 否则，$dp[i][j] = 0$。     2.2.2 参考代码 #  /** * 718. 最长重复子数组（版本 2：动态规划） * @param nums1 数组 1 * @param nums2 数组 2 * @return 两个数组中公共的、长度最长的子数组的长度 */ public int findLengthV2(int[] nums1, int[] nums2) { int n1 = nums1.length; int n2 = nums2.length; int res = 0; // dp 数组，其中 dp[i][j] 表示 nums1[i:] 和 nums2[j:] 的最长公共前缀的长度，则 dp 数组中最大的元素即为 nums1 和 nums2 的最长重复子数组的长度  int[][] dp = new int[n1 + 1][n2 + 1]; // 分别遍历 nums1 和 nums2，计算最长重复子数组的长度  for (int i = n1 - 1; i \u0026gt;= 0; i--) { for (int j = n2 - 1; j \u0026gt;=0; j--) { // 如果 nums1[i] = nums2[j]，则 dp[i][j] = dp[i + 1][j + 1]，否则，dp[i][j] = 0  dp[i][j] = (nums1[i] == nums2[j] ? dp[i + 1][j + 1] + 1 : 0); res = Math.max(res, dp[i][j]); } } // 返回结果  return res; } 2.2.3 问题延伸 #  2.2.3.1 最长公共子序列 #  2.2.3.1.1 问题分析 #    类似的解法还可用于 1143. 最长公共子序列，不过这里和求最长重复子数组不同的一点是子序列中的元素不一定在原数组中连续，因此，在$dp$数组的转换上稍微会有一定区别，具体如下：\n 如果 $nums1[i] = nums2[j]$，则 $dp[i][j] = dp[i + 1][j + 1] + 1$。 否则，$dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])$。     2.2.3.1.2 参考代码 #  /** * 1143. 最长公共子序列 * @param text1 数组1 * @param text2 数组2 * @return 两个数组中公共的、长度最长的子数组的长度 */ public int longestCommonSubsequence(String text1, String text2) { int n1 = text1.length(); int n2 = text2.length(); int res = 0; // dp 数组，其中 dp[i][j] 表示 nums1[i:] 和 nums2[j:] 的最长公共子序列的长度，则 dp 数组中最大的元素即为 nums1 和 nums2 的最长公共子序列的长度  int[][] dp = new int[n1 + 1][n2 + 1]; // 分别遍历 nums1 和 nums2，计算最长公共子序列的长度  for (int i = n1 - 1; i \u0026gt;= 0; i--) { for (int j = n2 - 1; j \u0026gt;=0; j--) { // 如果 nums1[i] = nums2[j]，则 dp[i][j] = dp[i + 1][j + 1]，否则，dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])  if (text1.charAt(i) == text2.charAt(j)) { dp[i][j] = dp[i + 1][j + 1] + 1; } else { dp[i][j] = Math.max(dp[i + 1][j], dp[i][j + 1]); } res = Math.max(res, dp[i][j]); } } // 返回结果  return res; } 2.3 滑动窗口 #  2.3.1 问题分析 #    对于两个数组的遍历，可以通过滑动窗口的方法来减少遍历的次数，因为每次比较的只是滑动窗口内部相同区域的元素，相比于暴力解法而言，可以显著减少遍历的次数。\n  本题目中可以先把 $nums1$ 放在上面，$nums2$放在下边，然后将 $nums1$ 的第一个元素和 $nums2$ 的最后一个元素对齐，然后将 $nums2$ 从做往右滑动，直到 $nums1$ 的第一个元素和 $nums2$ 的第一个元素对齐，且每滑动一次，都对两个数组滑块内部相同区域的元素进行比较。\n  然后把 $nums2$ 放在上面，$nums1$放在下面，并且把 $nums2$ 的第一个元素和 $nums1$ 的第一个元素对齐，然后把 $nums1$ 从右往左滑动，直到 $nums2$ 的第一个元素和 $nums1$ 的最后一个元素对齐，且每滑动一次，都对两个数组滑块内部相同区域的元素进行比较。\n  其实第三步可以合到第二步里面，即在第二步中一直把 $nums2$ 滑动到第一个元素和 $nums1$ 的第一个元素对齐，但这样不太好实现，因此后面一步拆分成等价的第四步来实现。\n   2.3.2 参考代码 #  /** * 718. 最长重复子数组（版本 3：滑动窗口） * @param nums1 数组 1 * @param nums2 数组 2 * @return 两个数组中公共的、长度最长的子数组的长度 */ public int findLengthV3(int[] nums1, int[] nums2) { int n1 = nums1.length; int n2 = nums2.length; int res = 0; // nums1 的第一个元素和 nums2 的最后一个元素对齐，然后将 num2 从左往右滑动，直到 nums2 的第一个元素和 nums1 的第一个元素对齐  for (int i = n2 - 1; i \u0026gt;= 0; i--) { int minLen = Math.min(n1, n2 - i); int tempRes = 0; // 遍历 nums1 和 nums2 交叉的部分，并计算这一部分的最长重复子数组的长度  for (int j = 0; j \u0026lt; minLen; j++) { if (tempRes != 0 \u0026amp;\u0026amp; nums1[j] != nums2[i + j]) { res = Math.max(res, tempRes); tempRes = 0; } if (nums1[j] == nums2[i + j]) { tempRes++; } } res = Math.max(res, tempRes); } // nums2 的第一个元素和 nums1 的第一个元素对齐，然后将 num1 从右往左滑动，直到 nums2 的第一个元素和 nums1 的最后一个元素对齐  for (int i = 0; i \u0026lt; n1; i++) { int minLen = Math.min(n1 - i, n2); int tempRes = 0; // 遍历 nums1 和 nums2 交叉的部分，并计算这一部分的最长重复子数组的长度  for (int j = 0; j \u0026lt; minLen; j++) { if (tempRes != 0 \u0026amp;\u0026amp; nums1[i + j] != nums2[j]) { res = Math.max(res, tempRes); tempRes = 0; } if (nums1[i + j] == nums2[j]) { tempRes++; } } res = Math.max(res, tempRes); } return res; } 3 参考文献 #    718. 最长重复子数组。  1143. 最长公共子序列。  最长重复子数组。  滑动窗口解法。  "},{"id":91,"href":"/school-recruitment/docs/java/3JVM/3.10-%E6%8C%87%E9%92%88%E7%A2%B0%E6%92%9E%E5%92%8C%E7%A9%BA%E9%97%B2%E5%88%97%E8%A1%A8/","title":"3.10 指针碰撞和空闲列表","section":"3、 Jvm","content":"指针碰撞和空闲列表 #  1 原理分析 #  为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来：\n 假设 Java 堆中的内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配的内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为指针碰撞（Pointer Bumping）。 如果 Java 堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List）。 选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的的垃圾收集器是否带有压缩整理功能决定。 因此：  在使用Serial、ParNew等带有Compact过程的收集器时，系统采用的分配算法时指针碰撞。 在使用CMS这种基于标记-清除算法的收集器时，通常采用空闲列表。    指针碰撞：\n 空闲列表：\n 2 参考文献 #    指针碰撞和空闲列表。  "},{"id":92,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.1.11-%E6%9C%80%E9%95%BF%E6%9C%89%E6%95%88%E6%8B%AC%E5%8F%B7/","title":"1.1.11 最长有效括号","section":"1.1 动态规划","content":"最长有效括号 #  1 题目 #  给你一个只包含 \u0026lsquo;(\u0026rsquo; 和 \u0026lsquo;)\u0026rsquo; 的字符串，找出最长有效（格式正确且连续）括号子串的长度。\n示例 1：\n输入：s = \u0026#34;(()\u0026#34; 输出：2 解释：最长有效括号子串是 \u0026#34;()\u0026#34; 示例 2：\n输入：s = \u0026#34;)()())\u0026#34; 输出：4 解释：最长有效括号子串是 \u0026#34;()()\u0026#34; 示例 3：\n输入：s = \u0026#34;\u0026#34; 输出：0 提示：\n 0 \u0026lt;= s.length \u0026lt;= 3 * 104 s[i] 为 \u0026lsquo;(\u0026rsquo; 或 \u0026lsquo;)\u0026rsquo;  2 解题思路 #  2.1 动态规划 #  2.1.1 问题解析 #   对于这种最值型题目一般采用动态规划的方法来求解。 动态规划题目的分析分为以下 4 个步骤：  确定状态：  研究最优策略的最后一步。 化为子问题。   转移方程：  根据子问题定义得到。   初始条件和边界情况。 计算顺序。   首先，我们定义一个 $dp$ 数组，其中 $dp[i]$表示以下标为 $i$ 的字符结尾的最长有效子字符串的长度。 然后进行动态规划的求解：  确定状态：  对于最优的策略，一定有最后一个元素 $s[i]$，所以，我们先看第 $i$ 个位置，这个位置的元素 $s[i]$ 有两种情况：  $s[i] = \u0026lsquo;('$：这时 $s[i]$无法和其之前的元素组成有效的括号对，所以 $dp[i] = 0$。 $s[i] = \u0026lsquo;)'$：这时，需要看其前面一个元素来判断是否为有效括号对：   $s[i - 1] = \u0026lsquo;('$：即 $s[i]$和 $s[i - 1]$ 组成一对有效括号，有效括号长度新增 2，此时以 $i$位置的字符结尾的最长有效括号长度为以 $(i - 2)$ 位置的字符结尾的最长有效括号长度加 2，我们无需知道 $(i - 2)$ 位置的字符是否可以组成有序括号对，此时有：\n$$ dp[i] = dp[i - 2] + 2 $$\n   $s[i - 1] = \u0026lsquo;)'$：这种情况下，如果前面有和 $s[i]$ 组成有效括号对的字符，即形如 $((\u0026hellip;)) $，这样的话，就要求 $s[i - 1]$ 位置必然是有效的括号对，否则 $s[i]$无法和前面对字符组成有效括号对，这时，我们只需找到和 $s[i]$ 配对的字符的位置（$i - dp[i - 1] - 1$），并判断其是否可以和 $s[i]$ 配对即可：\n  如果 $s[i - dp[i - 1] - 1] = \u0026lsquo;('$，即 $s[i - dp[i -1] - 1]$可以和 $s[i]$ 配对，则以 $s[i]$ 结尾的最长有序括号长度为以 $s[i - 1]$ 为结尾的最长有序括号长度加 2，此时有：\n$$ dp[i] = dp[i - 1] + 2 $$\n需要注意的是，$s[i - dp[i - 1] - 1]$ 和 $s[i]$ 组成了有序括号对，这将是一段独立的有序括号对，如果之前的子序列是形如 $(\u0026hellip;)$ 这种序列，那么当前位置的最长有序括号对的长度还需加上这一段，即：\n$$ dp[i] = dp[i - 1] + dp[i - dp[i - 1] - 2] + 2 $$\n          子问题：   根据上面的分析，我们得到了如下两个计算公式：\n$$ dp[i] = dp[i - 1] + 2 $$\n$$ dp[i] = dp[i - 1] + dp[i - dp[i - 1] - 1] + 2 $$\n  那么，求 $dp[i]$ 就变成了求 $dp[i - 1]$、$dp[i - 2]$、$dp[i - dp[i - 1] - 1]$ 的子问题。\n  这样状态也明确了：设 $dp$ 数组，其中第 $i$ 个元素表示以下标为 $i$ 的字符结尾的最长有效字符串的长度。\n   转移方程：   子问题明确后，转移方程直接由子问题得到：\nif (s.charAt(i) == \u0026#39;(\u0026#39;) { dp.charAt(i) = 0 } if (s.charAt(i) == \u0026#39;)\u0026#39;) { if (s.charAt(i - 1) == \u0026#39;(\u0026#39;) { dp.charAt(i) = dp[i - 2] + 2 // 要保证 i - 2 \u0026gt;= 0  } if (s.charAt(i - 1) == \u0026#39;)\u0026#39; \u0026amp;\u0026amp; s[i - dp.charAt(i - 1) - 1] == \u0026#39;(\u0026#39;) { dp.charAt(i) = dp.charAt(i - 1) + dp[i - dp.charAt(i - 1) - 2] + 2 // 要保证 i - dp.charAt(i - 1) - 2 \u0026gt;= 0  } }    初始条件和边界情况：  初始条件：$dp[i] = 0$。 边界情况：需要保证计算过程中 $i - 2 \u0026gt;= 0$ 和 $i - dp[i - 1] - 2 \u0026gt;= 0$。   计算顺序：  无论第一个字符是什么，都有$dp[0] = 0$。 然后依次计算$dp[1]、dp[2],\u0026hellip;,dp[n - 1]$。 最后结果是 $max(dp[i])$。      2.1.2 参考代码 #  /** * 32. 最长有效括号（版本1：动态规划） * * @param s 字符串 * @return 字符串中最长有效括号子串的长度 */ public int longestValidParenthesesV1(String s) { int m = s.length(); // dp 数组，其中 dp[i] 表示以 s.charAt(i) 结尾的最长有效括号子串的长度  int[] dp = new int[m + 1]; int res = 0; for (int i = 1; i \u0026lt; m; i++) { if (s.charAt(i - 1) == \u0026#39;(\u0026#39; \u0026amp;\u0026amp; s.charAt(i) == \u0026#39;)\u0026#39;) { // 第 i - 1 个元素为 (，第 i 个元素为 )，即第 i - 1 个元素和第 i 个元素可以组成一个有序括号，则 dp[i] = dp[i - 2] + 2  dp[i] = (i - 2 \u0026gt;= 0 ? dp[i - 2] : 0) + 2; } else if (s.charAt(i - 1) == \u0026#39;)\u0026#39; \u0026amp;\u0026amp; s.charAt(i) == \u0026#39;)\u0026#39;) { // 第 i - 1 个元素为 )，第 i 个元素为 )  if (i - dp[i - 1] - 1 \u0026gt;= 0 \u0026amp;\u0026amp; s.charAt(i - dp[i - 1] - 1) == \u0026#39;(\u0026#39;) { // 如果 s.charAt(i - dp[i - 1] - 1) 为 (，即该元素和 s.charAt(i) 配对，则 dp[i] = dp[i - 1] + dp[i - dp[i - 1] - 2] + 2，其中 s.charAt(i - dp[i - 1] - 2) 为与 s.charAt(i) 配对的前一个元素  dp[i] = dp[i - 1] + 2; if (i - dp[i - 1] - 2 \u0026gt;= 0) { dp[i] = dp[i - 1] + dp[i - dp[i - 1] - 2] + 2; } } } res = Math.max(res, dp[i]); } // 返回最终结果  return res; } 2.2 栈 #  2.2.1 问题分析 #   对于这种符号匹配的题目，我们一般可以采用栈的方法来求解。 具体做法是我们始终保持栈底元素为当前已遍历过的元素中最后一个没有被匹配的右括号的下标，这样的做法主要是考虑了边界条件的处理，栈里其他元素维护左括号的下标：  对于遇到的每个 (，我们将他的下标放入栈中。 对于遇到的每个 )，我们先弹出栈顶元素表示匹配了当前右括号：  如果栈为空，说明当前的右括号为没有被匹配的右括号，我们将其下标放入栈中来更新我们之前提到的最后一个没有被匹配的右括号的下标。 如果栈不为空，当前右括号的下标减去栈顶元素即为以该括号为结尾的最长有效括号的长度。     然后我们从前往后遍历字符串并更新答案即可。 需要注意的是，如果一开始栈为空，第一个字符为左括号的时候，我们会将其放入栈中，这样就不满足提及的最后一个没有被匹配的右括号的下标，为了保持统一，我们在一开始的时候往栈中放入一个值为-1的元素。 具体演示动画可参考 最长有效括号。  2.2.2 参考代码 #  /** * 32. 最长有效括号（版本2：栈） * * @param s 字符串 * @return 字符串中最长有效括号子串的长度 */ public int longestValidParenthesesV2(String s) { int m = s.length(); // 栈底元素始终为当前已经遍历过的元素中 最后一个没有被匹配的右括号的下标  Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); int maxLength = 0; // 如果一开始栈为空，第一个字符为左括号的时候我们会将其放入栈中，这样栈就不满足栈底元素始终为当前已遍历过的元素中 最后一个没有被匹配的右括号的下标，为了保持统一，我们在一开始的时候往栈中放入一个值为 -1 的元素  stack.push(-1); for (int i = 0; i \u0026lt; m; i++) { char item = s.charAt(i); if (item == \u0026#39;(\u0026#39;) { // 如果当前遍历的元素为 (，则把当前元素的下标放入栈中  stack.push(i); } else if (item == \u0026#39;)\u0026#39;) { // 当前遍历的元素为 )  if (!stack.empty()) { // 如果栈不为空，则将栈顶元素弹出  stack.pop(); } if (stack.size() == 0) { // 如果栈为空，则把当前元素的下标放入栈中  stack.push(i); } else { // 如果栈不为空，则 当前遍历元素的下标 减去 栈顶元素 即为以该右括号为结尾的最长有效括号的长度  maxLength = Math.max(maxLength, i - stack.peek()); } } } // 返回结果  return maxLength; } 3 参考文献 #    32. 最长有效括号。  最长有效括号。  动态规划思路详解（c++）——32.最长有效括号。  "},{"id":93,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.1-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%89%E5%A4%A7%E7%89%B9%E6%80%A7/","title":"2.1 并发编程的三大特性","section":"2、 Java多线程","content":"并发编程的三大特性 #  并发编程主要包含三大特性，分别是 原子性、可见性 和 有序性。\n1 原子性 #  1.1 概念 #  原子性是指在一次或者多次操作时，要么所有操作都被执行，要么所有操作都不执行。\n1.2 示例 #  i = 0; //1 j = i ; //2 i++; //3 i = j + 1; //4 上面的四个操作中，只有 1 是原子操作，其余绝不是：\n 1 在 Java 中，对基本数据类型的变量的赋值操作都是原子性。 2 中包含了两个操作：  读取 i。 将 i 结果赋值给 j。   3 中包含了三个操作：  读取 i。 i+1。 将加 1 结果赋值给 i。   4 中同 3 一样。  1.3 保证原子性的方法 #   如果要保证多个操作的原子性，需要使用 synchronized 关键字或者 lock 相关的工具类。 如果要使 int、long 等类型的自增操作具有原子性，可以使用 java.util.concurrent.atomic 包下的工具类，如：AtomInteger、AtomLong 等。 需要注意的是，volatile 关键字不具有保证原子性的语义。  2 可见性 #  2.1 概念 #  可见性是指当一个线程对共享变量进行修改后，另外一个线程可以立即看到该变量修改后的最新值。\n2.2 示例 #  /** * @author peng.wei * @version 1.0 * @date 2021/4/19 16:12 * @Description JUC 可见性测试 */ public class VisibilityTest { public static int count = 0; public static void main(String[] args) { final SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;HH:mm:ss.SSS\u0026#34;); //读取 count 值的线程  new Thread(() -\u0026gt; { System.out.println(\u0026#34;开始读取 count...\u0026#34;); int i = count;//存放 count 的更新前的值  while (count \u0026lt; 3) { if (count != i) {//当 count 的值发生改变时，打印 count 被更新  System.out.println(sdf.format(new Date()) + \u0026#34; count 被更新为\u0026#34; + count); i = count;//存放 count 的更新前的值  } } }).start(); //更新 count 值的线程  new Thread(() -\u0026gt; { for (int i = 1; i \u0026lt;= 3; i++) { //每隔 1 秒为 count 赋值一次新的值  try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(sdf.format(new Date()) + \u0026#34; 赋值 count 为\u0026#34; + i); count = i; } }).start(); } } 输出结果如下：\n开始读取 count... 16:14:33.186 赋值 count 为 1 16:14:34.190 赋值 count 为 2 16:14:35.194 赋值 count 为 3 从运行的输出结果可以看出，读取 count 值的线程一直没有读取到 count 的最新值，这是因为在读取 count 值的线程中，第一次是从主内存中读取 count 值后写入工作内存，再从工作内存中读取，之后都是从自己的工作内存中读取的 count 值，因此并没有发现更新 count 值的线程对 count 值的修改。\n2.3 保证可见性的方法 #  在 Java 中可以用以下 3 种方式保证可见性：\n 使用 volatile 关键字： 当一个变量被 volatile 关键字修饰时，其他线程对该变量进行了修改后，会导致当前线程在工作内存中的变量副本失效，必须从主内存中再次获取，当前线程修改工作内存中的变量后，同时也会将其修改刷新到主内存中。 使用 synchronized 关键字：synchronized 关键字能够保证同一时刻只有一个线程获得锁，然后执行同步方法或者代码块，并且确保在锁释放之前，把变量的修改刷新到主内存中。 使用 Lock 相关的工具类：Lock 相关的工具类的 lock 方法能够保证同一时刻只有一个线程获得锁，然后执行同步代码块，必须确保执行 unlock 方法之前，把变量的修改刷新到主内存中。  3 有序性 #  3.1 概念 #  有序性指的是程序执行的顺序按照代码的先后顺序执行。\n3.2 示例 #  package top.grayson.juc.characteristic; /** * @author peng.wei * @version 1.0 * @date 2021/4/19 16:34 * @Description JUC 有序性测试 */ public class Singleton { private Singleton (){} private static boolean isInit = false; private static Singleton instance; public static Singleton getInstance() { if (!isInit) {//判断是否初始化过  instance = new Singleton();//初始化  isInit = true;//初始化标识赋值为 true  } return instance; } }  这是一个有问题的单例模式示例。 假如在编译期或运行期时指令重排，把 isInit = true; 重新排序到 instance = new Singleton(); 的前面，在单线程运行时，程序重排后的执行结果和代码顺序的执行的结果是完全一样的，但是在多线程运行时就极有可能出现问题。 比如，一个线程先判断 isInit 为 false 进行初始化，本应在初始化后再把 isInit 赋值为 true，但是因为指令重排后没初始化就把 isInit 赋值为 true，恰好此时另外一个线程在判断是否初始化过，isInit 为 true 就执行返回了 instance，这是一个没有初始化的 instance，肯定造成不可预知的错误。  3.3 如何保证有序性 #   使用 volatile 关键字保证有序性。 使用 synchronized 关键字保证有序性。 使用 Lock 相关的工具类保证有序性。  4 参考文献 #    学妹教你并发编程的三大特性：原子性、可见性、有序性。  "},{"id":94,"href":"/school-recruitment/docs/java/3JVM/3.11-%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E6%9C%89%E5%93%AA%E4%BA%9B/","title":"3.11 常见的垃圾收集器有哪些","section":"3、 Jvm","content":"常见的垃圾收集器有哪些 #  JVM 中常见的垃圾收集器主要包括串行 GC 收集器（Serial GC）、ParNew 收集器、并行 GC 收集器（Parallel GC）、CMS 收集器（Most Concurrent Mark and Sweep Garbage Collector）、G1 收集器（Garbage First）五种。\n1 串行 GC 收集器（Serial GC） #  Serial 收集器主要包括Serial 收集器（用于年轻代）、Serial Old 收集器（用于老年代）两种。\n2.1 特点 #   串行 GC在年轻代使用标记-复制算法，在老年代使用标记-清除-整理算法。 两者都是单线程的垃圾收集器，不能进行并行处理，所以都会触发全线暂停（STW），停止所有的应用线程。  2.2 缺点 #   不能充分利用多核 CPU，因为不管有多少 CPU 内核，JVM 在垃圾收集时都只能使用单个核心。  2.3 启用方法 #  -XX:+UseSerialGC 2 ParNew 收集器 #  ParNew 收集器是Serial 收集器的多线程版本，它是许多运行在 Server 模式下的虚拟机中首选的新生代收集器，因为除了 Serial 收集器外，目前只有他能与CMS 收集器配合工作。\n3 并行 GC 收集器（Parallel GC） #  Parallel 收集器主要包括Parallel 收集器（用于年轻代）、Parallel Old 收集器（用于老年代）两种。\n3.1 特点 #   并行垃圾收集器这一类组合，在年轻代使用标记-复制算法，在老年代使用标记-清除-整理算法。 年轻代和老年代的垃圾回收都会触发STW事件，暂停所有的应用线程来执行垃圾收集。 两者在执行标记和复制/整理阶段都使用多个线程，通过并行执行，使得 GC 时间大幅减少。  3.2 优缺点 #  3.2.1 优点 #   并行垃圾收集器适用于多核服务器，主要目标是增加吞吐量，因为对系统资源的有效使用，能达到更多的吞吐量：  在 GC 期间，所有 CPU 内核都在并行清理垃圾，所以总暂停时间更短。 在两次 GC 周期的间隔期，没有 GC 线程在运行，不会消耗任何系统资源。    3.2.2 缺点 #   并行垃圾收集器 GC 的所有阶段都不能中断，所以并行 GC 很容易出现长时间的卡顿（注：这里说的长时间也很短，一般来说 Minor GC 是毫秒级别，Full GC 是几十或几百毫秒级别）。如果系统的主要目标是最低的停顿时间/延迟，而不是整体的吞吐量最大，那么就应该选择其他垃圾收集器组合。  3.3 启用方法 #  -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+UseParallelGC -XX:+UseParallelOldGC 4 CMS 收集器（Most Concurrent Mark and Sweep Garbage Collector） #  4.1 特点 #   CMS在年轻代采用并行 STW 方式的标记-复制算法，在老年代使用并发标记-清除算法。 CMS 的设计目标是避免在老年代垃圾收集时出现长时间的停顿，主要通过两种手段来达成此目标：  不对老年代进行整理，而是采用空闲列表来管理内存空间的回收。 在标记-清除阶段的大部分工作和应用线程一起并发执行，在这些阶段没有明显的应用暂停，但他仍然和应用线程争抢 CPU 时间，因此CMS 会比并行 GC 的吞吐量差一些。    4.2 优缺点 #  4.2.1 优点 #   并发收集，低停顿。  4.2.2 缺点 #   对 CPU 资源非常敏感，CMS 默认的回收线程数等于(CPU 数量 +3)/4。 无法处理浮动垃圾，并发清理阶段用户程序运行产生的垃圾过了标记阶段而无法在本次收集中清理掉，称为浮动垃圾。 基于标记-清除算法会产生大量空间碎片。  4.3 各个阶段 #  4.3.1 初始标记（Initial Mark） #   这个阶段会伴随着 STW 暂停。 初始标记的目标是标记所有的根对象（包括根对象直接引用的对象以及被年轻代中所有存活对象所引用的对象）。   4.3.2 并发标记（Concurrent Mark） #   在此阶段，CMS遍历老年代，标记所有存活的对象，从前一阶段初始标记找到的的根对象开始算起。 并发标记阶段与应用程序同时运行，不用暂停。 并非所有老年代中存活的对象都在此阶段被标记，因为在标记过程中对象的引用关系还在发生变化，例如，在下面的示意图中，当前处理的对象的一个引用就被应用线程给断开了，即这个部分的对象关系发生了变化。   4.3.3 并发预清理（Concurrent Preclean） #   此阶段同样是与应用线程并发执行的，不需要停止应用线程。 因为前一阶段并发标记与应用程序并发运行，可能有一些引用关系已经发生了变化，如果在并发标记过程中引用关系发生了变化，JVM 会通过“卡片”的方式将发生了改变的区域标记为“脏”区，这就是所谓的“卡片标记”。 在预清理阶段，这些脏对象会被统计出来，他们所引用的对象也会被标记。此阶段完成后，用于标记的卡片也会被清空。 这个阶段是为最终标记阶段做准备。    4.3.4 可取消的并发预清理（Concurrent Abortable Preclean） #   这个阶段也是为最终标记阶段做准备。 在进入最终标记阶段前，最好能进行一个Minor GC，将年轻代清理一遍，这样可以清除大部分年轻代的对象，尽量缩短重新标记阶段停顿时间。 CMS 还提供了CMSScavengeBeforeRemark 参数，可以在进入最终标记之前强制进行一次 Minor GC。  4.3.5 最终标记（Final Remark） #   最终标记阶段是此次 GC 事件中的第二次（也是最后一次）STW 停顿。 本阶段的目标是完成老年代中所有存活对象的标记，因为之前的预清理阶段是并发执行的，有可能 GC 线程跟不上应用程序的修改速度，所以需要一次 STW 暂停来处理各种复杂的情况。 通常 CMS 会尝试在年轻代尽可能空的情况下执行最终标记阶段，以免连续触发多次 STW 事件。  4.3.6 并发清除（Concurrent Sweep） #   在前面 5 个标记阶段完成之后，老年代中所有的存活对象都被标记了，然后该阶段将清除所有不使用的对象来回收老年代空间。 此阶段与应用程序并发执行，不需要 STW 停顿。   4.3.7 并发重置（Concurrent Reset） #   此阶段与应用程序并发执行，重置 CMS 算法相关的内部数据，为下一次 GC 循环做准备。  5 G1 收集器（Garbage First） #   G1 的全称是 Garbage First，意为垃圾优先，哪一块的垃圾最多就优先清理他。\n 5.1 特点 #   G1 收集器的最主要的设计目标是将 STW 停顿的时间和分布，变成可预期且可配置的。 堆不再分成年轻代和老年代，而是划分为多个（通常是 2048 个）可以存放对象的小块堆区域（Region）。 每个小块可能一会被定义成 Eden 区，一会被指定为 Survivor 区或者 Old 区。在逻辑上所有的 Eden 区和 Survivor 区合起来就是年轻代，所有的 Old 区拼在一起就是老年代，如下图所示：   这样划分之后，使得 G1 不必每次都去收集整个堆空间，而是以增量的方式进行处理：每次只处理一部分内存块，称为这次 GC 的回收集，每次 GC 暂停都会收集所有年轻代的内存块，但是一般只包含部分老年代的内存块。   在并发阶段估算每个小堆块存活对象的总数，构件回收集的原则是垃圾最多的小块会被优先收集。 G1 适合大内存，需要较低延迟的场景。  5.2 各个阶段 #  5.2.1 初始标记（Intial Mark） #   此阶段只是标记一下 GC Root 能直接关联到的对象，并且修改 TAMS 指针，让下一阶段用户线程并发执行时能正确的在可用内存区域中分配新对象。 这个阶段需要一次STW 暂停，但耗时很短，因为其是在进行Minor GC的时候同步完成的，所以 G1 收集器在这个阶段实际上没有额外的停顿。   TAMS 是什么？\n要达到 GC 与用户线程并发运行，必须要解决回收过程中新对象的分配，所以 G1 为每一个 Region 区域设计了两个名为TAMS（Top at Mark Start）的指针，从 Region 区域划出一部分空间用于记录并发回收过程中的新对象，这样的对象认为他们是存活的，不纳入垃圾回收范围。\n 5.2.2 并发标记（Concurrent Mark） #   此阶段是标记所有从 GC Root 可达的存活对象。 这一阶段耗时较长，但可与应用程序并发完成。 并发时引用变动的对象会产生漏标问题，G1 中会使用起始快照（Snapshot at The Beginning, SATB）算法来解决。  5.2.3 最终标记（Final Mark） #   此阶段需要一次STW 暂停，用于处理并发标记开始时未被标记的存活对象。  5.2.4 筛选回收（Live Data Counting and Evacuation） #   负责更新 Region 的统计数据，对各个 Region 的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划。 可以自由选择任意多个 Region 构成回收集，然后把决定回收的那一部分 Region 的存活对象复制到空的 Region 中，再清理掉整个旧 Region 的全部空间。 这里的操作涉及存活对象的移动，因此需要一次STW 暂停。 该阶段是由多个收集器线程并行完成的。  5.3 记忆集（Remembered Set） #  5.3.1 特点 #   堆空间通常被分为年轻代和老年代。由于年轻代的垃圾收集通常很频繁，如果老年代对象引用了年轻代的对象，那么回收年轻代的话，需要扫描所有从老年代到年轻代的所有引用，所以要避免每次年轻代垃圾回收时扫描整个老年代，减少开销。\n  记忆集（Remembered Set, RSet）是用来记录从其他 Region 中的对象到本 Region 的引用，是一种抽象的数据结构。每一个 Region 都有一个 RSet，有了这个数据结构，在回收某个 Region 的时候，就不必对整个堆内存的对象进行扫描了，他使得部分收集成为了可能。 对于年轻代的 Region，他的 RSet 只保存了来自老年代的引用，这是因为年轻代的回收是针对所有年轻代 Region 的，没必要画蛇添足，所以说年轻代 Region 的 RSet 有可能是空的。 对于老年代的 Region，他的 RSet 也只会保存老年代对它的引用，这是因为老年代回收之前，会先对年轻代进行回收，这时 Eden 区变空了，而在回收过程中会扫描 Survivor 分区，所以也没必要保存来自年轻代的引用。  5.3.2 缺点 #   RSet 通常会占用很大的空间，大约 5% 或者更高（最高可能 20%）。 不仅是空间方面，很多计算开销也是比较大的。  5.3.3 RSet 辅助 GC 的原理 #   在做YGC的时候，只需要选定年轻代的 RSet 作为 GC Roots，这些 RSet 记录了 $Old \\rightarrow Young$ 的跨代引用，避免扫描整个老年代。 而在Mixed GC的时候，老年代中记录了 $Old \\rightarrow Old$ 的 RSet，$Young \\rightarrow Young$ 的引用从Survivor获取（老年代回收之前，会先对年轻代进行回收，存活的对象放在 Survivor 区），这样也不用扫描全部老年代了。 所以 RSet 的引入大大减少了 GC 的工作量。  5.4 三色标记算法 #    在三色标记算法之前有一个算法叫做标记-清除算法（Mark and Sweep），这个算法会设置一个标志位来记录对象是否被引用。最开始，所有的标志位都是 0，如果发现对象是可达的就会置为 1，一步步下去就会呈现一个类似树状的结果。等标记步骤完成后，会将未被标记的对象统一清理，再次把所有标记位设置成 0，方便下次清理。 这个算法最大的问题是GC 执行期间需要把整个程序完全暂停，不能实现用户线程和 GC 线程并发执行。因为在不同标记阶段标记-清除算法的标志位 0 和 1 有不同的含义，那么新增的对象无论标记为什么都有可能被意外删除。对实时性要求高的系统来说，这种需要长时间挂起的标记-清除算法是不可接受的，所以就需要一个算法来解决 GC 运行程序长时间挂起的问题，那就是三色标记算法。    三色标记算法的最大好处是可以异步执行，从而可以以中断时间极少的代价或者完全没有中断来进行整个 GC。 三色标记法很简单，首先将对象用三种颜色表示，分别是黑色、灰色、白色：  黑色： 表示根对象，或者该对象与他引用的对象都已经被扫描过了。 灰色： 该对象本身已经被标记，但是他引用的对象还没有扫描完。 白色：未被扫描的对象，如果扫描完所有对象之后，最终为白色的为不可达对象，也就是垃圾对象。     5.4 漏标问题 #    假设此时对象 A 及其引用的对象都已经扫描完，那么对象 A 将会被标记为黑色。 用户线程将对象 B 和对象 C 之间的引用断开，将对象 A 指向对象 C，此时由于对象 A 已经被标记为黑色，所以不会再被扫描，因此对象 C 就会被当成垃圾对象，产生漏标问题。  漏标问题在CMS和G1 收集器中有着不同的解决方案：\n CMS： 采用增量更新（Incremental Update）算法，在并发标记阶段如果一个白色对象被黑色对象引用时，会将黑色对象重新标记为灰色，让垃圾收集器在重新标记阶段重新扫描。 G1： 采用SATB（Snapshot at The Beginning），在初始标记时做一个快照，当B 和 C 之间的引用消失时，把这个引用推到 GC 的堆栈，保证 C 还能被 GC 扫描到，在最终标记阶段扫描 SATB 记录。  两种漏标解决方案的对比：\n 增量更新算法关注的是引用的增加（$A \\rightarrow C$ 的引用），需要重新扫描，效率低。 SATB 算法关注的是引用的删除（$B \\rightarrow C$ 的引用）。  6 垃圾收集器的选择方法 #   如果系统考虑吞吐量优先，CPU 资源都用来最大程度处理业务，用Parallel GC。 如果系统考虑低延迟优先，每次GC 时间尽量短，用CMS GC。 如果系统堆内存较大，同时希望整体来看平均 GC 时间可控，使用G1 GC。  7 参考文献 #    GC 回收之二：4 种垃圾收集算法及 7 种垃圾收集器。  CMS 垃圾收集器。  G1 垃圾收集器详解。  "},{"id":95,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.2-%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92/","title":"2.2 指令重排","section":"2、 Java多线程","content":"指令重排 #  在 Java 中看似顺序的代码在 JVM 中，可能会出现编译器或者 CPU 对这些操作指令进行了重新排序，在特定情况下，指令重排会给我们的程序带来不确定的结果。\n1 概念 #  在虚拟机层面，为了尽可能减少内存操作速度远慢于CPU运行速度所带来的的CPU空置的影响，虚拟机会按照自己的一些规则将程序编写顺序打乱，即写在后面的代码在时间顺序上可能会先执行，而写在前面的代码可能会后执行，以尽可能充分的利用CPU，从 Java源代码到最终实际执行的指令序列，会分别经历下面三种重排序：\n 编译器优化重排序： 编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行重排序： 现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统重排序： 处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。   2 数据依赖性 #   如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。     名称 代码示例 说明     写后读 a = 1;b = 1 写一个变量之后，再读这个变量   写后写 a = 1;a = 2; 写一个变量之后，再写这个变量   读后写 a = b;b = 1; 读一个变量之后，再写这个变量    上面 3 中情况中，只要将两个操作的顺序进行重排序，程序的执行结果就会被改变。 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。不过这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。  3 as-if-serial 语义 #  3.1 含义 #   as-if-serial 语义的意思是不管怎么重排序，单线程的执行结果不能被改变，编译器、Runtime 和处理器都必须遵守as-if-serial 语义。 为了遵守as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种排序会改变执行结果，但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 as-if-serial 语义把单线程程序保护了起来，遵守as-if-serial 语义的编译器、Runtime 和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的，无需担心重排序会干扰他们，也无需担心内存可见性问题，但核心点还是单线程，多线程情况下不遵守此原则。  3.2 示例 #  double pi = 3.14; // A double r = 1.0; // B double area = pi * r * r; // C   如图所示，A 和C 之间存在数据依赖关系，同时B 和C 之间也存在数据依赖关系。 因此，在最终执行的指令序列中，C 不能被重排序到A 和B 前面，但是A 和B 之间没有数据依赖关系，编译器和处理器可以对A 和B 之间的执行顺序进行重排序。   4 happens-before规则 #  4.1 定义 #   如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行，如果重排序之后的结果与按照 happens-before关系来执行的结果一致，那么这种重排序并不非法。  4.2 规则 #   程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作。 锁定规则： 一个unlock操作先行发生于后面对同一个锁的 unlock操作。 volatile变量规则： 对一个变量的写操作先行发生于后面对这个变量的读操作。 传递规则： 如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个操作。 线程中断规则： 对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生。 线程终结规则： 线程中所有的操作都先行发生于线程的中止检测。 对象终结规则： 一个对象的初始化完成先行发生于他的 finalize()方法的开始。  5 内存屏障 #  5.1 含义 #   内存屏障（Memory Barrier）是硬件之上，操作系统或 JVM之下，对并发做出的最后一层支持，它是一种标准，不同的厂商会采用不同的实现。 通过volatile标记，可以解决编译器层面的可见性与重排序的问题，而内存屏障则解决了硬件层面的可见性与重排序问题。  5.2 标准 #    Store：将处理器缓存的数据刷新到内存中。 Load：将内存存储的数据拷贝到处理器的缓存中。      屏障类型 指令示例 说明     LoadLoad Barriers Load1;LoadLoad;Load2 该屏障确保Load1数据的装载先于Load2及其后所有装载指令的操作。   StoreStore Barriers Store1;StoreStore;Store2 该屏障确保Store1立刻刷新数据到内存的操作先于Store2及其后所有存储指令的操作。   LoadStore Barriers Load1;LoadStore;Store2 确保Load1的数据装载先于Store2及其后所有的存储指令刷新数据到内存的操作。   StoreLoad Barriers Store1;StoreLoad;Load2 该屏障确保Store1立即刷新数据到内存的操作先于Load2及其后所有装载指令的操作，他会使该屏障之前的所有内存访问指令完成之后，才执行该屏障之后的内存访问指令。    StoreLoad Barriers同时具备其他三个屏障的效果，因此也被称之为全能屏障（mfence），是目前大多数处理器所支持的，但是相对其他屏障，该屏障的开销相对昂贵。\n6 参考文献 #    JVM 之指令重排分析。  数据依赖性。  指令重排序。  【死磕Java并发】\u0026mdash;\u0026ndash;Java内存模型之happens-before。  一文解决内存屏障。  "},{"id":96,"href":"/school-recruitment/docs/java/3JVM/3.12-%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E4%B8%8E%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"3.12 内存溢出与内存泄漏的区别","section":"3、 Jvm","content":"内存溢出与内存泄漏的区别 #  1 基本概念 #  1.1 内存溢出 #  内存溢出（Out Of Memory）是指程序在申请内存时，没有足够的内存空间供其使用。\n1.2 内存泄漏 #  内存泄漏（Memory Leak）是指不再使用的对象持续占有内存，使得这部分内存得不到及时释放，从而造成内存空间浪费。\n2 二者关系 #   如果存在严重的内存泄漏问题，随着时间的推移，则必然会引起内存溢出。 内存泄漏一般是资源管理问题或程序 Bug，内存溢出则是内存空间不足和内存泄漏的最终结果。  3 内存溢出的常见情况 #  3.1 持久代溢出 #   因为运行常量池在方法区，而方法区在持久代中，因此出现持久代溢出的原因可能是运行时常量池溢出。 也可能是程序中使用了大量的 jar 或 class，使得方法区中保存的 class 对象没有被及时回收或class 信息使用的内存超过了配置的大小。  3.2 堆溢出 #   发生这种溢出的原因一般是创建的对象太多，在进行垃圾回收之前对象数量达到了最大堆的容量限制。 解决这个区域异常的方法是通过内存映像分析工具对 Dump 出来的转储快照进行分析，看到底是内存溢出还是内存泄漏：  如果是内存泄漏，可以通过工具查看泄漏对象到 GC Roots 的应用链，定位泄漏代码的位置，修改程序或算法。 如果不是内存泄漏，说明内存中的对象确实都还必须存活，那就应该检查虚拟机的堆参数 -Xmx（最大堆大小）和 -Xms（初始堆大小），与机器物理内存对比看是否可以调大。    3.3 虚拟机栈和本地方法栈溢出 #    如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出 StackOverFlowError。 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出 OutOfMemoryError。  4 内存泄漏 #  4.1 根本原因 #  内存泄漏的根本原因是长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象已经不再需要，但由于长生命周期对象持有它的引用而导致不能被回收。\n4.2 分类 #  以发生的方式来分类，内存泄漏可以分为 4 类：\n 常发性内存泄漏： 发生内存泄漏的代码会被多次执行到，每次被执行的时候都会导致一块内存泄漏。 偶发性内存泄漏： 发生内存泄漏的代码只有在某些特定环境或操作过程下才会发生。常发性和偶发性是相对的，对于特定的环境，偶发性的也许就变成了常发性的，所以测试环境和测试方法对检测内存泄漏至关重要。 一次性内存泄漏： 发生内存泄漏的代码只会被执行一次，或者由于算法上的缺陷，导致总会有一块且仅有一块内存发生泄漏，比如，在类的构造函数中分配内存，在析构函数中却没有释放该内存，所以内存泄漏只会发生一次。  4.3 常见情况 #  4.3.1 静态集合类引起的内存泄漏 #  像 HashMap、Vector 等的使用最容易出现内存泄漏，这些静态变量的生命周期和应用程序一致，他们所引用的所有对象也不能被释放，从而造成内存泄漏，因为他们也将一直被 Vecotr 等引用着。\nVector\u0026lt;Object\u0026gt; v=new Vector\u0026lt;Object\u0026gt;(100); for (int i = 1; i\u0026lt;100; i++) { Object o = new Object(); v.add(o); o = null; } 在这个例子中，循环申请 Object 对象，并将所申请的对象放入一个 Vector 中，如果仅仅释放引用本身（o=null），那么 Vector 仍然引用该对象，所以这个对象对 GC 来说是不可回收的。因此，如果对象加入到 Vector 后，还必须从 Vector 删除，最简单的方法就是将 Vector 对象设置为 null。\n4.3.2 修改 HashSet 中对象的参数值，且参数是计算哈希值的字段 #  当一个对象被存储到 HashSet 集合中以后，修改了这个对象中那些参与计算哈希值的字段后，这个对象的哈希值与最初存储在集合中的就不同了，这种情况下，用 contains 方法在集合中检索对象是找不到的，这将会导致无法从 HashSet 中删除当前对象，造成内存泄漏。\npublic static void main(String[] args){ Set\u0026lt;Person\u0026gt; set = new HashSet\u0026lt;Person\u0026gt;(); Person p1 = new Person(\u0026#34;张三\u0026#34;,\u0026#34;1\u0026#34;,25); Person p2 = new Person(\u0026#34;李四\u0026#34;,\u0026#34;2\u0026#34;,26); Person p3 = new Person(\u0026#34;王五\u0026#34;,\u0026#34;3\u0026#34;,27); set.add(p1); set.add(p2); set.add(p3); System.out.println(\u0026#34;总共有:\u0026#34;+set.size()+\u0026#34; 个元素!\u0026#34;); //结果：总共有:3 个元素!  p3.setAge(2); //修改 p3 的年龄,此时 p3 元素对应的 hashcode 值发生改变  set.remove(p3); //此时 remove 不掉，造成内存泄漏  set.add(p3); //重新添加，可以添加成功  System.out.println(\u0026#34;总共有:\u0026#34;+set.size()+\u0026#34; 个元素!\u0026#34;); //结果：总共有:4 个元素!  for (Person person : set){ System.out.println(person); } } 4.3.3 监听器 #  在 Java 编程中，通常一个应用中会用到很多监听器，我们会调用诸如 addXXXListener() 等方法来增加监听器，但往往释放对象的时候却没有记住去删除这些监听器，从而增加了内存泄漏的机会。\n4.3.4 各种连接 #   比如数据库连接、网络连接和IO 连接，除非其显示的调用了 close 方法将其连接关闭，否则是不会自动被 GC 回收的。 对于ResultSet 和Statement 对象来说，可以不进行显示回收，但Connection 一定要显示回收，因为Connection 在任何时候都无法自动回收，而Connection 一旦回收，ResultSet 和Statement 对象就会立即为NULL。 如果要使用连接池，除了要显示关闭连接，还必须显示关闭ResultSet 和Statement 对象（关闭其中一个，另外一个也会关闭），否则就会造成大量的Statement 对象无法释放，从而引起内存泄漏，这种情况一般都会在try 里面去连接，在finally 里面释放连接。  4.3.5 单例模式 #   不正确使用单例模式是引起内存泄漏的一个常见问题，单例对象在被初始化后将在JVM 的整个生命周期存在（以静态变量的方式），如果单例对象持有外部对象的引用，那么这个外部对象将不能被 JVM 正常回收，导致内存泄漏。  class A{ public A(){ B.getInstance().setA(this); } .... } //B 类采用单例模式 class B{ private A a; private static B instance=new B(); public B(){} public static B getInstance(){ return instance; } public void setA(A a){ this.a=a; } //getter... } B 采用 singleton 模式，他持有一个 A 对象的引用，因此这个 A 类的对象将不能被回收。\n4.4 建议 #   尽早释放无用对象的引用。 避免在循环中创建对象。 使用字符串处理时避免使用String，应使用StringBuffer。 尽量少使用静态变量，因为静态变量存放在永久代，基本不参与垃圾回收。  5 参考文献 #    内存溢出与内存泄漏。  内存泄漏和内存溢出的关系和一般解决问题思路。  "},{"id":97,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.3-Volatile%E5%8E%9F%E7%90%86/","title":"2.3 Volatile原理","section":"2、 Java多线程","content":"Volatile原理 #  1 定义 #   Volatile 可以保证线程的可见性、有序性，但是无法保证线程的原子性。 加入Volatile 关键字时，会多出一个 lock 前缀指令，该指令实际相当于一个内存屏障，他会提供 3 个功能：  确保指令重排序时不会把后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面，即在执行到内存屏障这句指令时，在他前面的操作已经全部完成。 他会强制将缓存的修改操作立即写入主内存。 如果是写操作，他会导致其他 CPU 中对应的缓存行无效。    2 实现原理 #  2.1 可见性 #   如果对声明了 Volatile 变量进行写操作时，JVM 会向处理器发送一条 Lock 前缀指令，将这个变量所在的缓存行的数据写回到系统内存，这一步确保了如果有其他线程对声明了 Volatile 变量进行修改时，则立即更新主内存中的数据。 但这时候其他处理器的缓存还是旧的，所以在多处理器环境下，为了保证各个处理器缓存一致，每个处理器会通过嗅探在总线上的传播的数据来检查自己的缓存是否过期，当处理器发现自己缓存行对应的内存地址被修改了，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作时，会强制重新从系统内存把数据读到处理器缓存里，这一步确保了其他线程获得的声明了 Volatile 变量都是从主内存中获取最新的。  2.2 有序性 #   Lock 前缀指令实际上相当于一个内存屏障，它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面，即在执行到内存屏障这句指令时，在他前面的操作已经全部完成。  3 示例 #  3.1 保证可见性 #  先看一段代码，假如线程 1 先执行，线程 2 后执行：\n//线程 1 boolean stop = false; while(!stop){ doSomething(); } //线程 2 stop = true;  上述代码的运行不一定完全正确，线程 1 不一定会中断：  线程 1 在运行的时候，会将stop 变量的值拷贝一份放在自己的工作内存当中。 当线程 2 更改了stop 变量的值之后，但是还没来得及写入主内存中，线程 2 转去做其他事情了。 那么线程 1 由于不知道线程 2 对stop 变量的更改，因此还会一直循环下去。   但是用Volatile 修饰之后就变得不一样了：  使用Volatile 关键字会强制将修改的值立即写入主内存。 使用Volatile 关键字时的话，当线程 2 修改 stop 变量的值时，会导致线程 1 的工作内存中缓存变量 stop 的缓存行无效。 由于线程 1 的工作内存中缓存变量 stop 的缓存行无效，所以线程 1 再次读取变量 stop 的值时会去主内存读取。    3.2 不能保证原子性 #  package top.grayson.juc.myvolatile; /** * @author peng.wei * @version 1.0 * @date 2021/4/20 14:28 * @Description Volatile 不能保证原子性测试 */ public class VolatileAtomTest { public volatile int inc = 0; public void increase() { inc++; } public static void main(String[] args) { final VolatileAtomTest test = new VolatileAtomTest(); for(int i=0;i\u0026lt;10;i++){ new Thread(){ @Override public void run() { for(int j=0;j\u0026lt;1000;j++) { test.increase(); } }; }.start(); } // 保证前面的线程都执行完  while(Thread.activeCount()\u0026gt;1) { Thread.yield(); } System.out.println(test.inc); } } 输出结果：\n9810  虽然Volatile 关键子能保证可见性，但可见性只能保证每次读取的是最新值，Volatile 没法保证对变量的操作的原子性。 自增操作不具备原子性，它包括读取变量的原始值、进行加 1 操作、写入工作内存三步，因此自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现：  假如某个时刻inc 的值为 10，线程 1 对变量进行自增操作，线程 1 先读取了变量 inc 的原始值，然后线程 1 被阻塞了。 然后线程 2 对变量进行自增操作，也去读取变量inc 的原始值，由于线程 1 只是对变量 inc 进行读取操作，而没有对变量进行修改操作，所以不会导致线程 2 的工作内存中缓存变量 inc 的缓存行无效，所以线程 2 会直接去主内存中读取 inc 的值，发现 inc 的值是 10，然后进行加 1 操作，并把 11 写入工作内存，最后写入主内存。 然后线程 1 接着进行加 1 操作，由于已经读取了 inc 的值，注意此时在线程 1 的工作内存中 inc 的值仍然为 10，所以线程 1 对 inc 进行加 1 操作后 inc 的值为 11，然后将 11 写入工作内存，最后写入主内存。    把上面的代码改成以下任何一种都可以达到原子性效果。\n3.2.1 采用 Synchronized #  package top.grayson.juc.myvolatile; /** * @author peng.wei * @version 1.0 * @date 2021/4/20 15:10 * @Description Synchronized 原子性测试 */ public class SynchronizedAtomTest { public int inc = 0; public synchronized void increase() { inc++; } public static void main(String[] args) { final SynchronizedAtomTest test = new SynchronizedAtomTest(); for(int i=0;i\u0026lt;10;i++){ new Thread(){ @Override public void run() { for(int j=0;j\u0026lt;1000;j++) { test.increase(); } }; }.start(); } // 保证前面的线程都执行完  while(Thread.activeCount()\u0026gt;1) { Thread.yield(); } System.out.println(test.inc); } } 3.2.2 采用 Lock #  package top.grayson.juc.myvolatile; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; /** * @author peng.wei * @version 1.0 * @date 2021/4/20 15:10 * @Description Lock 原子性测试 */ public class LockAtomTest { public int inc = 0; Lock lock = new ReentrantLock(); public void increase() { lock.lock(); try { inc++; } finally { lock.unlock(); } } public static void main(String[] args) { final LockAtomTest test = new LockAtomTest(); for (int i = 0; i \u0026lt; 10; i++) { new Thread() { @Override public void run() { for (int j = 0; j \u0026lt; 1000; j++) { test.increase(); } } ; }.start(); } // 保证前面的线程都执行完  while (Thread.activeCount() \u0026gt; 1) { Thread.yield(); } System.out.println(test.inc); } } 3.2.3 采用 AtomInteger #  package top.grayson.juc.myvolatile; import java.util.concurrent.atomic.AtomicInteger; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; /** * @author peng.wei * @version 1.0 * @date 2021/4/20 15:10 * @Description AtomInteger 测试 */ public class AtomIntegerTest { public AtomicInteger inc = new AtomicInteger(); Lock lock = new ReentrantLock(); public void increase() { inc.incrementAndGet(); } public static void main(String[] args) { final AtomIntegerTest test = new AtomIntegerTest(); for (int i = 0; i \u0026lt; 10; i++) { new Thread() { @Override public void run() { for (int j = 0; j \u0026lt; 1000; j++) { test.increase(); } } ; }.start(); } // 保证前面的线程都执行完  while (Thread.activeCount() \u0026gt; 1) { Thread.yield(); } System.out.println(test.inc); } } 3.3 保证有序性 #  Volatile 关键字能禁止指令重排序，所以 Volatile 能在一定程度上保证有序性，Volatile 关键字禁止指令重排序有两层意思：\n 当程序执行到 Volatile 变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行。 再进行指令优化时，不能将 Volatile 变量访问的语句放在其后面执行，也不能把 Volatile 变量后面的语句放到其前面执行。  具体的例子如下：\n//x、y 为非 volatile 变量 //flag 为 volatile 变量  x = 2; //语句 1 y = 0; //语句 2 flag = true; //语句 3 x = 4; //语句 4 y = -1; //语句 5  由于flag 变量为Volatile 变量，那么在进行指令重排序的过程中，不会将语句 3 放到语句 1、语句 2 前面，也不会将语句 3 放到语句 4、语句 5 后面，但是语句 1 和语句 2 的顺序、语句 4 和语句 5 的顺序是不作任何保证的。 并且Volatile 关键字能保证执行到语句 3 时，语句 1 和语句 2 可能执行完了，且语句 1 和语句 2 的执行结果对语句 3、语句 4、语句 5 是可见的。  3.4 适用场景 #  3.4.1 使用条件 #  我们只能在有限的一些情形下使用 Volatile 变量代替锁，要使 Volatile 变量提供理想的线程安全，必须同时满足下面两个条件：\n  对变量的写操作不依赖于当前值。\n 该条件的限制使Volatile 变量不能用作线程安全计数器。    该变量没有包含在具有其他变量的不变式中。\n  该条件的限制使 Volatile 变量不能用于约束条件中，例如下面是一个非线程安全的数值范围类，它包含了一个不变式 $\\rightarrow$下界总是小于等于上界：\npackage top.grayson.juc.myvolatile; /** * @author peng.wei * @version 1.0 * @date 2021/4/20 16:01 * @Description 非线程安全的数值范围类 */ public class NumberRange { private int lower, upper; public int getLower() { return lower; } public int getUpper() { return upper; } public void setLower(int value) { if (value \u0026gt; upper) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } lower = value; } public void setUpper(int value) { if (value \u0026lt; lower) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } upper = value; } } 将 lower 和 upper 字段定义为 Volatile 类型不能够充分实现类的线程安全，仍然需要使用 synchronized 使 setLower() 和 setUpper() 操作原子化。\n否则，如果凑巧两个线程在同一时间使用不一致的值执行 setLower 和 setUpper 的话，就会使范围处于不一致的标志。例如，如果初始状态是 $(0,5)$，同一时间内，线程 A 调用 setLower(4) 并且线程 B 调用 setUpper(3)，显然这两个操作交叉存入的值是不符合条件的，那么这两个线程都会通过用于保护不变式的检查，时的最后的范围是 $(4,3)$，这是一个无效的范围。\n    3.4.2 适用场景 #  3.4.2.1 状态标志 #  也许实现 Volatile 变量的规范使用仅仅是使用一个布尔状态标志，用于指示发生了一个重要的一次性事件，例如完成初始化或请求停机。\nvolatile boolean shutdownRequested; ... public void shutdown() { shutdownRequested = true; } public void doWork() { while (!shutdownRequested) { // do stuff  } }  线程 1 执行doWork() 的过程中，可能有另外的线程 2 调用了shutDown()，所以boolean 变量必须使Volatile。 而如果使用synchronized 块编写循环要比使用Volatile 状态标志麻烦很多，由于Volatile 简化了编码，并且状态标志并不依赖于程序内任何其他状态，因此此处非常适合使用Volatile。 这种类型的状态标记的一个公共特性是通常只有一种状态转换，shutdownRequested 标志从false 转换为true，然后程序停止，这种模式可以扩展到来回转换的状态标志，但是只有在转换周期不被察觉的情况下才能扩展（从false 到true，再转换到false）。  3.4.2.2 一次性安全发布（One-Time Safe Publication） #  public class UnsafeDCLFactory { private Singleton instance; public Singleton get() { if (instance == null) { // read 1, check 1  synchronized (this) { if (instance == null) { // read 2, check 2  instance = new Singleton(); } } } return instance; // read 3  } } 上面的程序在多线程情况下并不能正常执行：\n 即使check1 成功执行了，此时instance 可能还没有完全被初始化，这是因为Singleton 的内容仅仅对构造线程（Constructing Thread）可见，并不能保证其在其他线程里面也可见，因为其他线程正在和初始化线程（Intializer Thread）竞争资源。而且，即使我们得到了一个非空的instance，也不意味着其内部的属性都已经完全实例化，因为在JMM 中，Singleton 的构造函数和其他属性的初始化之间并没有 happens-before 关系。 这样就可能导致某个线程获得一个未完全初始化的实例。  可以通过如下方法来改进：\npublic class SafeDCLFactory { private volatile Singleton instance; public Singleton get() { if (instance == null) { // check 1  synchronized(this) { if (instance == null) { // check 2  instance = new Singleton(); } } } return instance; } } 因为当程序执行到 Volatile 变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见，因此可以保证调用 get() 的线程将会得到完全构造的 instance。\n3.4.2.3 独立观察（Independent Observation） #   安全使用Volatile 的另一种简单模式是定期发布观察结果供程序内部使用。 假设有一种环境传感器能够感觉环境温度，一个后台线程可能会每隔几秒读取一次该传感器，并更新当前文档的Volatile 变量，然后其他线程可以读取这个变量，从而随时看到最新的温度值。 使用该模式的另一种应用程序就是收集程序的统计信息，下面的程序展示了身份验证机制如何记忆最近一次登录的用户的名字，将反复使用lastUser 引用来发布值，以供程序的其他部分使用。  public class UserManager { public volatile String lastUser; //发布的信息  public boolean authenticate(String user, String password) { boolean valid = passwordIsValid(user, password); if (valid) { User u = new User(); activeUsers.add(u); lastUser = user; } return valid; } } 3.4.2.4 Volatile Bean 模式 #   Volatile Bean 模式的基本原理是很多框架为易变数据的持有者（例如 HttpSession）提供了容器，但是放入这些容器中的对象必须是线程安全的。 在Volatile Bean 模式中，JavaBean 的所有数据成员都是 Volatile 类型的，并且getter 和setter 方法必须非常普通，即不包含约束。  @ThreadSafe public class Person { private volatile String firstName; private volatile String lastName; private volatile int age; public String getFirstName() { return firstName; } public String getLastName() { return lastName; } public int getAge() { return age; } public void setFirstName(String firstName) { this.firstName = firstName; } public void setLastName(String lastName) { this.lastName = lastName; } public void setAge(int age) { this.age = age; } } 3.4.2.5 开销较低的读-写锁策略 #   如果读操作远远超过写操作，我们可以结合使用内部锁和 Volatile 变量来减少公共代码路径的开销。 如下显示的是线程安全的计数器：  使用 synchronized 确保增量操作是原子的，并使用 Volatile 保证当前结果的可见性。  使用synchronized 进行所有变化的操作，使用Volatile 进行只读操作。 synchronized一次只允许一个线程访问值，Volatile允许多个线程执行读操作。   如果更新不频繁的话，该方法可以实现更好的性能，因为读路径的开销仅涉及 Volatile 读操作，这通常要优于一个无竞争的锁获取的开销。    @ThreadSafe public class CheesyCounter { // Employs the cheap read-write lock trick  // All mutative operations MUST be done with the \u0026#39;this\u0026#39; lock held  @GuardedBy(\u0026#34;this\u0026#34;) private volatile int value; //读操作，没有 synchronized，提高性能  public int getValue() { return value; } //写操作，必须 synchronized。因为 x++ 不是原子操作  public synchronized int increment() { return value++; } } 参考文献 #    深入分析 Volatile 的实现原理。  【Java 线程】volatile 的适用场景。  Safe Publication and Safe Initialization in Java。  "},{"id":98,"href":"/school-recruitment/docs/java/3JVM/3.13-%E5%B8%B8%E7%94%A8%E7%9A%84JVM%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B/","title":"3.13 常用的 Jvm启动参数有哪些","section":"3、 Jvm","content":"常用的JVM启动参数有哪些 #  # JVM 启动参数不换行 # 设置堆内存 -Xmx4g -Xms4g # 指定 GC 算法 -XX:+UseG1GC -XX:MaxGCPauseMillis=50 # 指定 GC 并行线程数 -XX:ParallelGCThreads=4 # 打印 GC 日志 -XX:+PrintGCDetails -XX:+PrintGCDateStamps # 指定 GC 日志文件 -Xloggc:gc.log # 指定 Meta 区的最大值 -XX:MaxMetaspaceSize=2g # 设置单个线程栈的大小 -Xss1m # 指定堆内存溢出时自动进行 Dump -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/ # 指定默认的连接超时时间 -Dsun.net.client.defaultConnectTimeout=2000 -Dsun.net.client.defaultReadTimeout=2000 # 指定时区 -Duser.timezone=GMT+08 # 设置默认的文件编码为 UTF-8 -Dfile.encoding=UTF-8 # 指定随机数熵源(Entropy Source) -Djava.security.egd=file:/dev/./urandom "},{"id":99,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.4-CAS%E5%8E%9F%E7%90%86/","title":"2.4 Cas原理","section":"2、 Java多线程","content":"CAS原理 #  1 CAS 含义 #    CAS 全称是 Compare and Swap，即比较并交换。\n  它是一种无锁原子算法，同时也是一种乐观机制。\n  CAS 映射到操作系统就是一条 CPU 原子指令，实现方式是基于硬件平台的汇编指令，在 Intel 的 CPU 中，使用的是 cmpxchg 指令，就是说 CAS 是靠硬件实现的，从而在硬件层面提升效率。\n  CAS 包含 3 个参数 V、E、N：\n V： Value，即要更新的值。 E： Expect，即预期值。 N：New，即新值。  只有当 V 值等于 E 值时，才会将 V 的值设为 N，如果 V 值和 E 值不同，则说明已经有其他线程完成更新，则当前线程什么都不做，最后 CAS 返回当前 V 的真实值。\n  当多个线程同时使用 CAS 操作一个变量时，最多只会有一个会胜出，并成功更新，其余均会失败。失败的线程不会挂起，仅是被告知失败，并且允许再次尝试（自旋），当然也允许实现的线程放弃操作。基于这样的原理，CAS 操作即使没有锁，也可以避免其他线程对当前线程的干扰。\n  与锁相比，使用 CAS 会使程序看起来更加复杂一些，但是使用无锁的方式完全没有锁竞争带来的线程间频繁调度的开销和阻塞，他对死锁问题天生免疫，因此他要比基于锁的方式拥有更好的性能。\n  简单的说，CAS 需要我们额外给出一个期望值，也就是我们认为这个变量现在应该是什么样子，如果变量不是我们想象的那样，说明他已经被别人修改过了，我们就需要重新拉取，再次尝试修改就好了。\n  2 CAS 底层原理 #  2.1 AtomicInteger.getAndIncrement()实现原理 #  AtomicInteger.getAndIncrement() 源码如下所示：\n/** * Atomically increments by one the current value. * * @return the previous value */ public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } 可知该方法最终调用了 Unsafe 类的 unsafe.getAndAddInt() 方法，该方法的具体定义如下：\n/** * Atomically adds the given value to the current value of a field * or array element within the given object {@code o} * at the given {@code offset}. * * @param o object/array to update the field/element in * @param offset field/element offset * @param delta the value to add * @return the previous value * @since 1.8 */ // @HotSpotIntrinsicCandidate public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!compareAndSwapInt(o, offset, v, v + delta)); return v; } 而 unsafe.getAndAddInt() 方法最终调用了 compareAndSwapInt() 方法，该方法的具体定义如下：\n/** * Performs a compare-and-set operation on an \u0026lt;code\u0026gt;int\u0026lt;/code\u0026gt; * field within the given object. * * @param obj non-null; object containing the field * @param offset offset to the field within \u0026lt;code\u0026gt;obj\u0026lt;/code\u0026gt; * @param expectedValue expected value of the field * @param newValue new value to store in the field if the contents are * as expected * @return \u0026lt;code\u0026gt;true\u0026lt;/code\u0026gt; if the new value was in fact stored, and * \u0026lt;code\u0026gt;false\u0026lt;/code\u0026gt; if not */ public native boolean compareAndSwapInt(Object obj, long offset, int expectedValue, int newValue); 该方法是一个 native 方法，含有 obj、offset、expectedValue、newValue，每个参数的具体含义如下：\n obj： 包含要更新的字段的对象。 offset：该字段的内存偏移地址。 expectedValue： 期望更新的值。 newValue： 要更新的最新值。  如果原子变量中该字段的值等于 expectedValue，则使用 newValue 值更新该值并返回 true，否则返回 false。\n假设线程 A 和线程 B 两个线程同时执行 getAndAddInt 操作（分别在不同的 CPU 上）：\n AtomicInteger 里面的value 原始值为 3，即主内存中AtomicInteger 的value 为 3，根据JMM 模型，线程A 和线程B 各自持有一份值为 3 的value 副本分别到各自的工作内存。 线程A 通过getIntVolatile(o, offset) 拿到value 值为 3，这时线程A 被挂起。 线程B 也通过getIntVolatile(o, offset) 拿到value 值为 3，刚好线程 B 没有被挂起并执行 compareAndSwapInt 方法，比较内存中的值也为 3，成功修改内存值为 4，线程B 执行完毕。 这时线程A 恢复，执行compareAndSwapInt 方法比较，发现自己工作内存中的value 值（3）和主内存中的值（4）不一样，说明该值已经被其他线程抢先一步修改过了，线程 A 本次修改失败，只能重新读取再来一遍了。 线程A 重新获取value 值，因为变量 value 被 volatile 修饰，所以其他线程对它的修改，线程 A 总能看到，线程A 继续执行compareAndSwapInt 进行比较替换，直到成功。  2.2 Unsafe 应用解析 #  2.2.1 Unsafe 介绍 #   Unsafe 是位于sun.misc 包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等。这些方法在提升 Java 运行效率、增强 Java 语言底层资源操作能力方面起到了很大的作用。 但由于Unsafe 类使Java 语言拥有了类似C 语言指针一样操作内存空间的能力，这也增加了程序发生相关指针问题的风险，在程序中过度、不正确使用Unsafe 类会使得程序出错的概率变大，因此对Unsafe 的使用一定要慎重。  2.2.2 Unsafe 源码解析 #  Unsafe 源码如下所示：\n/** * A collection of methods for performing low-level, unsafe operations. * Although the class and all methods are public, use of this class is * limited because only trusted code can obtain instances of it. * * @author John R. Rose * @see #getUnsafe */ public final class Unsafe { private static native void registerNatives(); static { registerNatives(); sun.reflect.Reflection.registerMethodsToFilter(Unsafe.class, \u0026#34;getUnsafe\u0026#34;); } private Unsafe() {} private static final Unsafe theUnsafe = new Unsafe(); /** * Provides the caller with the capability of performing unsafe * operations. * * \u0026lt;p\u0026gt; The returned \u0026lt;code\u0026gt;Unsafe\u0026lt;/code\u0026gt; object should be carefully guarded * by the caller, since it can be used to read and write data at arbitrary * memory addresses. It must never be passed to untrusted code. * * \u0026lt;p\u0026gt; Most methods in this class are very low-level, and correspond to a * small number of hardware instructions (on typical machines). Compilers * are encouraged to optimize these methods accordingly. * * \u0026lt;p\u0026gt; Here is a suggested idiom for using unsafe operations: * * \u0026lt;blockquote\u0026gt;\u0026lt;pre\u0026gt; * class MyTrustedClass { * private static final Unsafe unsafe = Unsafe.getUnsafe(); * ... * private long myCountAddress = ...; * public int getCount() { return unsafe.getByte(myCountAddress); } * } * \u0026lt;/pre\u0026gt;\u0026lt;/blockquote\u0026gt; * * (It may assist compilers to make the local variable be * \u0026lt;code\u0026gt;final\u0026lt;/code\u0026gt;.) * * @exception SecurityException if a security manager exists and its * \u0026lt;code\u0026gt;checkPropertiesAccess\u0026lt;/code\u0026gt; method doesn\u0026#39;t allow * access to the system properties. */ @CallerSensitive public static Unsafe getUnsafe() { Class\u0026lt;?\u0026gt; caller = Reflection.getCallerClass(); if (!VM.isSystemDomainLoader(caller.getClassLoader())) throw new SecurityException(\u0026#34;Unsafe\u0026#34;); return theUnsafe; } }  Unsafe 类为一单例实现，提供静态方法getUnsafe() 获取Unsafe 实例。 当且仅当getUnsafe() 方法的类为引导类加载器所加载时才合法，否则会抛出SecurityException 异常。 如果我们想使用这个类，可以通过如下方法获取其实例：   从 getUnsafe() 方法的使用限制条件出发，通过 Java 命令行命令 -Xbootclasspath/a 把调用 Unsafe 相关方法的类 A 所在 jar 包路径追加到默认的 bootstrap 路径中，使得 A 被引导类加载器加载，从而通过 Unsafe.getUnsafe() 方法安全的获取 Unsafe 实例，具体命令如下：\njava -Xbootclasspath/a: ${path} // 其中 path 为调用 Unsafe 相关方法的类所在 jar 包路径   通过反射获取单例对象 theUnsafe：\nprivate static Unsafe reflectGetUnsafe() { try { Field field = Unsafe.class.getDeclaredField(\u0026#34;theUnsafe\u0026#34;); field.setAccessible(true); return (Unsafe) field.get(null); } catch (Exception e) { log.error(e.getMessage(), e); return null; } }     2.2.3 功能介绍 #   如上图所示，Unsafe 提供的 API 大致可分为内存操作、CAS、Class 相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类。\n2.2.3.1 内存操作 #  2.2.3.1.1 特点 #  //分配内存, 相当于 C++ 的 malloc 函数 public native long allocateMemory(long bytes); //扩充内存 public native long reallocateMemory(long address, long bytes); //释放内存 public native void freeMemory(long address); //在给定的内存块中设置值 public native void setMemory(Object o, long offset, long bytes, byte value); //内存拷贝 public native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes); //获取给定地址值，忽略修饰限定符的访问限制。与此类似操作还有: getInt，getDouble，getLong，getChar 等 public native Object getObject(Object o, long offset); //为给定地址设置值，忽略修饰限定符的访问限制，与此类似操作还有: putInt,putDouble，putLong，putChar 等 public native void putObject(Object o, long offset, Object x); //获取给定地址的 byte 类型的值（当且仅当该内存地址为 allocateMemory 分配时，此方法结果为确定的） public native byte getByte(long address); //为给定地址设置 byte 类型的值（当且仅当该内存地址为 allocateMemory 分配时，此方法结果才是确定的） public native void putByte(long address, byte x);  这部分主要包含堆外内存的分配、拷贝、释放、给定地址值操作等方法。 通常，我们在Java 中创建的对象都处于堆内内存（heap）中，堆内内存是由 JVM 所管控的 Java 进程内存，并且他们遵循 JVM 的内存管理机制，JVM 会采用垃圾回收机制统一管理内存。 与之相对的是堆外内存，存在于JVM 管控之外的内存区域，Java 中对堆外内存的操作，依赖于 Unsafe 提供的操作堆外内存的 native 方法。  2.2.3.1.2 使用堆外内存的原因 #   对垃圾回收停顿的改善： 由于堆外内存是直接受操作系统管理而不是JVM，所以当我们使用堆外内存时即可保持较小的堆内内存的规模，从而在GC 时减少回收停顿对于应用的影响。 提升程序或 I/O 操作的性能： 通常在 I/O 通信过程中，会存在堆内内存到堆外内存的拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。  2.2.3.1.3 典型应用 #   DirectByteBuffer 是Java 实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA 等NIO 框架中应用广泛。 DirectByteBuffer 对于堆外内存的创建、使用、销毁等逻辑均由Unsafe 提供的堆外内存API 来实现。 下图为DirectByteBuffer 构造函数：  创建DirectBuffer 的时候，通过Unsafe.allocateMemory() 分配内存、Unsafe.setMemory() 进行内存初始化。 而后构建Cleaner 对象用于跟踪DirectByteBuffer 对象的垃圾回收，以实现DirectByteBuffer 被垃圾回收时，分配的堆外内存一起被释放。      如何通过构建垃圾回收追踪对象 Cleaner 实现堆外内存释放呢？\n Cleaner 继承自Java 四大引用类型之一的虚引用（PhantomReference）（众所周知，无法通过虚引用获取与之关联的对象实例，且当对象仅被虚引用引用时，在任何发生GC 的时候，其均可被回收），通常PhantomReference 与引用队列ReferenceQueue 结合使用，可以实现虚引用关联对象被垃圾回收时能够进行系统通知、资源清理等功能。 如下图所示，当某个被Cleaner 引用的对象回收时，JVM 垃圾收集器会将此对象的引用放入到对象引用的pending 链表中，等待ReferenceHandler 进行相关处理。 其中ReferenceHandler 为一个拥有最高优先级的守护线程，会循环不断的处理pending 链表中的对象引用，执行Cleaner 的clean 方法进行相关清理工作。 所以当DirectByteBuffer 仅被Cleaner 引用（虚引用）时，其可以在任意GC 时段被回收。当DirectByteBuffer 实例对象被回收时，在ReferenceHandler 线程操作中，会调用Cleaner 的clean 方法根据创建Cleaner 时传入的Deallocator 来进行堆外内存的释放。       2.2.3.2 CAS #  2.2.3.2.1 特点 #  /** * Performs a compare-and-set operation on an \u0026lt;code\u0026gt;int\u0026lt;/code\u0026gt; * field within the given object. * * @param obj non-null; object containing the field * @param offset offset to the field within \u0026lt;code\u0026gt;obj\u0026lt;/code\u0026gt; * @param expectedValue expected value of the field * @param newValue new value to store in the field if the contents are * as expected * @return \u0026lt;code\u0026gt;true\u0026lt;/code\u0026gt; if the new value was in fact stored, and * \u0026lt;code\u0026gt;false\u0026lt;/code\u0026gt; if not */ public native boolean compareAndSwapInt(Object obj, long offset, int expectedValue, int newValue); public native boolean compareAndSwapLong(Object obj, long offset, long expectedValue, long newValue); public native boolean compareAndSwapObject(Object obj, long offset, Object expectedValue, Object newValue); 上述部分主要为 CAS 相关操作，其底层实现即为 CPU 指令 cmpxchg。\n2.2.3.2.2 典型应用 #   如下图所示，在AtomicInteger 的实现中，静态字段valueOffset 即为字段value 的内存偏移地址，valueOffset 的值在AtomicInteger 初始化时，在静态代码块中通过Unsafe 的objectFieldOffset() 方法获取。 在AtomicInteger 中提供的线程安全方法中，通过字段valueOffset 的值可以定位到AtomicInteger 对象中value 的内存地址，从而可以根据CAS 实现对value 字段的原子操作。  下面以 AtomicInteger 中的 incrementAndGet() 方法具体分析一下其执行过程。\n 根据Unsafe 的objectFieldOffset() 方法获取value 的偏移地址valueOffset。   调用unsafe.getAndAddInt() 方法，获取被更新的值，然后加 1，返回更新后的值。   unsafe.getAndAddInt() 方法中根据value 的偏移量valueOffset 获取获取value 的值，然后通过CAS 方法获取被更新的值。   下图为某个 AtomInteger 对象自增操作前后的内存示意图：\n 对象的基地址baseAddress=\u0026quot;0x110000\u0026quot;。 通过baseAddress + valueOffset 得到value 的内存地址valueAddress = \u0026quot;0x11000c\u0026quot;。 然后通过CAS 进行原子性的更新操作，成功则返回，否则继续重试，直到更新成功为止。   2.2.3.3 线程调度 #  2.2.3.3.1 特点 #  //取消阻塞线程 public native void unpark(Object thread); //阻塞线程 public native void park(boolean isAbsolute, long time); //获得对象锁（可重入锁） @Deprecated public native void monitorEnter(Object o); //释放对象锁 @Deprecated public native void monitorExit(Object o); //尝试获取对象锁 @Deprecated public native boolean tryMonitorEnter(Object o);  这部分，主要包括线程挂起、恢复、锁机制等方法。 方法park() 和unpark() 即可实现线程的挂起和恢复：  将一个线程进行挂起是通过park() 方法实现的，调用park() 方法后，线程将一直阻塞直到超时或者中断条件出现。 unpark() 方法可以终止一个挂起的线程，使其恢复正常。    2.2.3.3.2 典型应用 #  Java 锁和同步器框架的核心类 AbstractQueuedSynchronizer，就是通过调用 LockSupport.park() 和 LockSupport.unpark() 实现线程的阻塞和唤醒的，而 LockSupport 的 park() 和 unpark() 方法实际上是调用 Unsafe 的 park() 和 unpark() 方式来实现的。\n2.2.3.4 Class 相关 #  2.2.3.4.1 特点 #  //获取给定静态字段的内存地址偏移量，这个值对于给定的字段是唯一且固定不变的 public native long staticFieldOffset(Field f); //获取一个静态类中给定字段的对象指针 public native Object staticFieldBase(Field f); //判断是否需要初始化一个类，通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。 当且仅当 ensureClassInitialized 方法不生效时返回 false。 public native boolean shouldBeInitialized(Class\u0026lt;?\u0026gt; c); //检测给定的类是否已经初始化。通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。 public native void ensureClassInitialized(Class\u0026lt;?\u0026gt; c); //定义一个类，此方法会跳过 JVM 的所有安全检查，默认情况下，ClassLoader（类加载器）和 ProtectionDomain（保护域）实例来源于调用者 public native Class\u0026lt;?\u0026gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain); //定义一个匿名类 public native Class\u0026lt;?\u0026gt; defineAnonymousClass(Class\u0026lt;?\u0026gt; hostClass, byte[] data, Object[] cpPatches); 这部分主要提供 Class 和他的静态字段的操作相关方法，包含静态字段内存定位、定义类、定义匿名类、检验\u0026amp;确保初始化等。\n2.2.3.4.2 典型应用 #   从Java 8 开始，JDK 使用invokDynamic 及VM Anonymous Class 结合来实现Java 语言层面上的Lambda 表达式：  invokdynamic：invokdynamic 是Java 7为了实现在 JVM 上运行动态语言而引入的一条新的虚拟机指令，他可以实现在运行期动态解析出调用点限定符所引用的方法，然后再执行该方法，invokedynamic 指令的分派逻辑是由用户设定的引导方法决定。 VM Anonymous Class：  可以看作是一种模板机制，针对于程序生成很多结构相同、仅若干常量不同的类时，可以先创建包含常量占位符的模板类，然后通过Unsafe.defineAnonymousClass() 方法定义具体类时填充模板的占位符并生成具体的匿名类。 生成的匿名类不显式挂在任何Class Loader 下面，只有当该类没有实例对象、且没有强引用来引用该类的 Class 对象时，该类就会被 GC 回收。 因此VM Anonymous Class 相比于Java 语言层面的匿名内部类无需通过 ClassClassLoader 进行类加载且更容易回收。     Lambda 表达式的实现主要包括以下几个步骤：  通过invokedynamic 指令调用引导方法生成调用点，在此过程中，会通过 ASM 动态生成字节码。 然后利用Unsafe 的defineAnonymousClass 方法定义实现相应的函数式接口的匿名类。 接着实例化此匿名类，并返回与此匿名类中函函数式方法的方法句柄关联的调用点。 最后通过此调用点实现相应 Lambda 表达式定义逻辑的功能。    2.2.3.4 对象操作 #  2.2.3.4.1 特点 #  //返回对象成员属性在内存地址相对于此对象的内存地址的偏移量 public native long objectFieldOffset(Field f); //获得给定对象的指定地址偏移量的值，与此类似操作还有：getInt，getDouble，getLong，getChar 等 public native Object getObject(Object o, long offset); //给定对象的指定地址偏移量设值，与此类似操作还有：putInt，putDouble，putLong，putChar 等 public native void putObject(Object o, long offset, Object x); //从对象的指定偏移量处获取变量的引用，使用 volatile 的加载语义 public native Object getObjectVolatile(Object o, long offset); //存储变量的引用到对象的指定的偏移量处，使用 volatile 的存储语义 public native void putObjectVolatile(Object o, long offset, Object x); //有序、延迟版本的 putObjectVolatile 方法，不保证值的改变被其他线程立即看到。只有在 field 被 volatile 修饰符修饰时有效 public native void putOrderedObject(Object o, long offset, Object x); //绕过构造方法、初始化代码来创建对象 public native Object allocateInstance(Class\u0026lt;?\u0026gt; cls) throws InstantiationException; 此部分主要包含对象成员属性相关操作及非常规的对象实例化等相关方法。\n2.2.3.4.2 典型应用 #   常规对象实例化方式：  我们通常所用到的创建对象的方式，从本质上讲，都是通过new机制来实现对象的创建。 但是，new 机制有个特点就是当类只提供有参的构造函数且无显示声明无参构造函数时，则必须使用有参构造函数进行对象构造，而使用有参构造函数时，必须传递相应个数的参数才能完成对象实例化。   非常规的实例化方式：  Unsafe 中提供allocateInstance 方法，仅通过 Class 对象就可以创建此类的实例对象，而不需要调用其构造函数、初始化代码、JVM 安全检查等等。 他抑制修饰符检测，也就是即使构造器是private 修饰的也能通过此方法实例化，只需提供类对象即可创建相应对象。 由于这种特性，allocateInstance 在java.lang.invoke、Objenesis（提供绕过类构造器的对象生成方式）、Gson（反序列化时用到）中都有相应应用。    如下图所示，在 Gson 反序列化时，如果类有默认构造函数，则通过反射调用默认构造函数创建实例，否则通过 UnsafeAllocator 来实现对象实例的构造。UnsafeAllocator 通过调用 Unsafe 的 allocateInstance() 实现对象的实例化，保证在目标类无默认构造函数时，反序列化不受影响。\n 2.2.3.5 数组相关 #  2.2.3.5.1 特点 #  //返回数组中第一个元素的偏移地址 public native int arrayBaseOffset(Class\u0026lt;?\u0026gt; arrayClass); //返回数组中一个元素占用的大小 public native int arrayIndexScale(Class\u0026lt;?\u0026gt; arrayClass); 这部分与数组相关的方法主要有 arrayBaseOffset 和 arrayIndexScale 两个方法，二者配合起来使用，即可定位数组中每个元素在内存中的位置。\n2.2.3.5.2 典型应用 #   这两个与数据操作相关的方法，在java.util.concurrent.atomic 包下的AtomicIntegerArray（可以实现对Integer 数组中每个元素的原子性操作）中有典型的应用。 如下图AtomicIntegerArray 源码所示，通过Unsafe 的arrayBaseOffset、arrayIndexScale 分别获取数组首元素的偏移地址base 及单个元素大小因子scale。 后续相关原子性操作，均依赖于这两个值进行数组中元素的定位，如下图所示的getAndAdd 方法即通过checkedByteOffset 方法获取某数组元素的偏移地址，而后通过 CAS 实现原子性操作。   2.2.3.6 内存屏障 #  2.2.3.6.1 特点 #  //内存屏障，禁止 load 操作重排序。屏障前的 load 操作不能被重排序到屏障后，屏障后的 load 操作不能被重排序到屏障前 public native void loadFence(); //内存屏障，禁止 store 操作重排序。屏障前的 store 操作不能被重排序到屏障后，屏障后的 store 操作不能被重排序到屏障前 public native void storeFence(); //内存屏障，禁止 load、store 操作重排序 public native void fullFence(); 这部分与内存屏障相关的主要包括禁止 load 操作重排序、禁止 store 操作重排序、禁止 load 和 store 重排序。\n2.2.3.6.2 典型应用 #   在Java 8 中引入了一种锁的新机制-StampedLock，他可以看成是读写锁的一个改进版本。 StampedLock 提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程“饥饿”现象。 由于StampedLock 提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存 load 到线程工作内存时，会存在数据不一致的问题。 所以，当使用StampedLock 的乐观读锁时，需要遵从如下图用例中使用的模式来确保数据的一致性：  在方法distanceFromOrigin 中，首先，通过 tryOptimisticRead 方法获取乐观读标记。 然后从主内存中加载点的坐标值 (x, y)。 然后通过 StampedLock 的 validate 方法校验锁的状态，判断坐标点(x, y)从主内存加载到线程工作内存过程中，主内存的值是否已经通过其他线程通过 move 方法修改：  如果validate 返回值为true，证明(x, y) 的值未被修改，可参与后续计算。 否则，需加悲观读锁，再次从主内存加载 (x, y) 的最新值，然后再进行距离计算。   其中，校验锁状态这步操作至关重要，需要判断锁状态是否发生改变，从而判断之前 copy 到线程工作内存中的值是否与主内存的值存在不一致。 StampedLock.validate 方法通过锁标记与相关常量进行位运算、比较来校验锁状态，在校验逻辑之前，或通过Unsafe.loadFence 方法注入一个load 内存屏障，目的是避免步骤 2和StampedLock.validate中锁状态校验运算发生重排序导致锁状态校验不准确的问题。      2.2.3.7 系统相关 #  2.2.3.7.1 特点 #  //返回系统指针的大小。返回值为 4（32 位系统）或 8（64 位系统）。 public native int addressSize(); //内存页的大小，此值为 2 的幂次方。 public native int pageSize(); 这部分包含两个获取系统相关信息的方法。\n2.2.3.7.2 典型应用 #  java.nio 下的工具类 Bits 中计算待申请内存所需内存页数量的静态方法，其依赖于 Unsafe 中 pageSize 方法获取系统内存页大小实现后续计算逻辑。\n 2.2.4 总结 #  Unsafe 提供了很多便捷、有趣的 API 方法，同时对于 Unsafe 中所包含的大量的自主操作内存的方法，如果使用不当，会对程序带来许多不可控的灾难，因此对它的使用我们需要慎之又慎。\n3 CAS 缺点 #  3.1 自旋问题 #    从源码可以看出所谓的自选无非就是操作结果失败后继续循环操作，这种操作也称为自旋锁，是一种乐观锁机制，一般来说都会给一个限定的自选次数，防止进入死循环。 自旋锁的优点是不需要休眠当前线程，因为自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是休眠当前线程是提高并发性能的关键点，这是因为减少了很多不必要的线程上下文切换开销。 但是，如果 CAS 一致操作不成功，会造成长时间原地自旋，会给 CPU 带来非常大的执行开销。  3.2 只能保证一个共享变量的原子性 #   因为**Java 中的CAS 操作只是对CPU 的cmpxchgq 指令的一层封装**，它的功能就是**一次只原子地修改一个变量**。 因此当对一个共享变量执行操作时，我们可以使用循环CAS 的方式来保证原子操作。 但是，对多个共享变量操作时，循环CAS 就无法保证操作的原子性，这个时候就需要用锁来保证原子性了。  3.3 ABA 问题 #  3.3.1 简介 #  在多线程场景下会出现 ABA 问题，具体如下：\n 假如有 2 个线程同时对同一个值（初始值为A）进行CAS 操作，这三个线程如下：  线程 1：期望值为A，欲更新的值为B。 线程 2： 期望值为A，欲更新的值为B。   线程 1 抢先获得时间片，而线程 2 因为其他原因阻塞了。 线程 1 取值与期望的A 值比较，发现相等然后将值更新为B。 这个时候出现了线程 3，期望值为 B，欲更新的值为 A，线程 3 取值与期望的B 值比较，发现相等则将值更新为 A。 此时线程 2 从阻塞中恢复，并且获得了 CPU 时间片，这时候线程 2 取值与期望的值A 比较，发现相等则将值更新为 B，虽然线程 2 也完成了操作，但是线程 2 并不知道值已经经过了$A \\rightarrow B \\rightarrow A$ 的变化过程。  3.3.2 带来的危害 #   小明在提款机，提取了 50 元，因为提款机问题，有两个线程，同时把余额从 100 变成 50：  线程 1（提款机）： 获取当前值 100，期望更新为 50。 线程 2（提款机）： 获取当前值 100，期望更新为 50.   线程 1 成功执行，线程 2 因为某种原因 block 了，这时，某人给小明汇款 50：  线程 3（提款机）： 获取当前值 50，期望更新为 100。   这个时候线程 3 成功执行，余额变为 100。 线程 2 从 block 中恢复，获取到的也是 100，compare 之后，继续更新余额为 50。 此时可以看到，实际余额应该为 100，但是实际上变为了 50，这就是ABA问题带来的成功提交。  3.3.3 解决方法 #  在变量前面加上版本号，每次变量更新的时候变量的版本号都加 1，即 $A \\rightarrow B \\rightarrow A$ 变成了 $1A \\rightarrow 2B \\rightarrow 3A$。\n在 Java 中，AtomicStampedReference 也实现了这个处理方式，具体如下：\n AtomicStampedReference 的内部类Pair：   其中：\n reference：维护对象的引用。 stamp：维护修改的版本号。  compareAndSet 方法：   从 compareAndSet 方法得知，如果要修改内存中的值，不仅要值相同，还要版本号相同。\n4 参考文献 #    一文彻底搞懂 CAS 实现原理。  搞定 CAS 的原理，看这一篇就够了！  Java 并发之 CAS 原理分析。  Java 魔法类：Unsafe 应用解析。  Atomic 实现原子性源码分析：CAS（比较并交换）、Unsafe 类。  为什么 CAS 只能保证一个共享变量的原子操作？  CAS 原理分析及 ABA 问题详解。  "},{"id":100,"href":"/school-recruitment/docs/java/3JVM/3.14-%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6/","title":"3.14 反射机制","section":"3、 Jvm","content":"反射机制 #  1 含义 #    一般情况下，我们使用某个类时必定知道他是什么类，是用来做什么的，于是我们直接对这个类进行实例化，之后使用这个类对象进行操作，这种进行类对象的初始化的方法，我们可以理解为正射，例如：\nApple apple = new Apple(); apple.setPrice(4.0);   而反射只有在运行时才知道要操作的类是什么，并且可以在运行时获取类的完整构造，并调用对应的方法，例如：\nClass\u0026lt;?\u0026gt; cls = Class.forName(\u0026#34;top.grayson.jvm.reflect.Apple\u0026#34;); Method setPriceMethod = cls.getMethod(\u0026#34;setPrice\u0026#34;, double.class); Constructor\u0026lt;?\u0026gt; constructor = cls.getConstructor(); Object instance = constructor.newInstance(); setPriceMethod.invoke(instance, 5.0);   上面两段代码的执行结果是完全一样的，但是其思路完全不一样：\n 第一段代码在未运行时就已经确定了要运行的类（Apple）。 第二段代码在运行时通过字符串值才得知要运行的类（top.grayson.jvm.reflect.Apple）。    完整的代码及运行结果如下：\npublic class ReflectTest1 { public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { // 直接初始化，正射  Apple apple = new Apple(); apple.setPrice(4.0); System.out.println(apple.getPrice()); // 反射  Class\u0026lt;?\u0026gt; cls = Class.forName(\u0026#34;top.grayson.jvm.reflect.Apple\u0026#34;); Method setPriceMethod = cls.getMethod(\u0026#34;setPrice\u0026#34;, double.class); Constructor\u0026lt;?\u0026gt; constructor = cls.getConstructor(); Object instance = constructor.newInstance(); setPriceMethod.invoke(instance, 5.0); Method getPriceMethod = cls.getMethod(\u0026#34;getPrice\u0026#34;); Object price = getPriceMethod.invoke(instance); System.out.println(price); } } 4.0 5.0   从上面的例子中我们可以看出，一般情况下我们使用反射获取一个对象的步骤为：\n  获取类的 Class 对象实例：\nClass\u0026lt;?\u0026gt; cls = Class.forName(\u0026#34;top.grayson.jvm.reflect.Apple\u0026#34;);   根据 Class 对象实例获取 Constructor 对象：\nConstructor\u0026lt;?\u0026gt; constructor = cls.getConstructor();   根据 Constructor 对象的 newInstance() 方法获取反射类对象：\nObject instance = constructor.newInstance();     而如果要调用某一个方法，则需要经过下面的步骤：\n  获取方法的 Method 对象：\nMethod setPriceMethod = cls.getMethod(\u0026#34;setPrice\u0026#34;, double.class);   利用 invoke() 方法调用方法：\nsetPriceMethod.invoke(instance, 5.0);     2 常用 API #  2.1 获取反射中的 Class 对象 #   在反射中，要获取一个类或调用一个类的方法，我们首先需要获取到该类的 Class 对象。 在 Java API 中，获取Class 类对象有三种方法：   使用 Class.forName() 静态方法，当我们知道该类的全路径名时，可以使用该方法获取 Class 对象：\nClass cls = Class.forName(\u0026#34;java.lang.String\u0026#34;);   使用 .class 方法，这种方法只适合在编译前就知道操作的 Class：\nClass cls = String.class;   使用类对象的 getClass() 方法：\nString str = new String(\u0026#34;Hello\u0026#34;); Class cls = str.getClass();     2.2 通过反射创建类对象 #   通过反射创建类对象主要有两种方式：   通过 Class 对象的 newInstance() 方法，这种方法只能使用默认的无参构造方法：\nClass cls = Apple.class; Apple apple = (Apple)cls.newInstance();   通过 Constructor 对象的 newInstance() 方法，这种方法相比通过 Class 对象的 newInstance 方法，可以选择特定构造方法：\n// 无参构造方法 Class cls = Apple.class; Constructor constructor = cls.getConstructor(); Apple apple = (Apple)constructor.newInstance(); // 有参构造方法 Class cls = Apple.class; Constructor constructor = cls.getConstructor(String.class, int.class); Apple apple = (Apple)constructor.newInstance(\u0026#34;红富士\u0026#34;, 15);     2.3 通过反射获取类属性、方法、构造器 #    我们通过 Class 对象的 getFields() 方法可以获取 Class 类的属性，但无法获取私有属性：\nClass\u0026lt;?\u0026gt; cls = Class.forName(\u0026#34;top.grayson.jvm.reflect.Apple\u0026#34;); for (Field field: cls.getFields()) { System.out.println(field.getName()); } price 如果想获取包括私有属性在内的所有属性，可以使用 Class 对象的 getDeclaredFields() 方法：\nClass\u0026lt;?\u0026gt; cls = Class.forName(\u0026#34;top.grayson.jvm.reflect.Apple\u0026#34;); for (Field field: cls.getDeclaredFields()) { System.out.println(field.getName()); } name price   与获取类属性一样，当我们去获取类方法、类构造器时，如果要获取私有方法或私有构造器，则必须使用有 declared 关键字的方法。\n  3 源码解析 #    进入 Method 的 invoke() 方法我们可以看到，一开始是进行了一些权限的检查，最后是调用了 MethodAccessor 类的 invoke() 方法进行进一步处理，如下图所示：\n   MethodAccessor 是一个接口，定义了方法调用的具体操作：\n MethodAccrssor 有三个具体的实现类，分别为：\n   要看 ma.invoke() 到底调用的是哪个类的 invoke() 方法，则需要看看 MethodAccessor 对象返回的到底是哪个类对象，所以我们需要进入 acquireMethodAccessor() 方法中看一下：\n   从 acquireMethodAccessor() 方法中我们可以看到，代码先判断是否存在对应的 MethodAccessor 对象，如果存在那么就复用之前的 MethodAccessor 对象，否则调用 ReflectionFactory 对象的 newMethodAccessor() 方法生成一个 MethodAccessor 对象：\n   在 ReflectionFactory 类的 newMethodAccessor() 方法里，我们可以看到首先生成了一个 NativeMethodAccessorImpl 对象，然后将这个对象作为参数调用了 DelegatingMethodAccessorImpl 类的构造方法，这里的实现是使用了 代理模式，将 NativeMethodAccessorImpl 对象交给 DelegatingMethodAccessorImpl 对象代理，我们查看 DelegatingMethodAccessorImpl 类的构造方法可以知道，其实是将 NativeMethodAccessorImpl 对象赋值给 DelegatingMethodAccessorImpl 类的 delegate 属性：\n   所以说 ReflectionFactory类的 newMethodAccessor() 方法最终返回 DelegatingMethodAccessorImpl 对象，所以我们前面的 里，其将会进入 DelegatingMethodAccessorImpl 类的 invoke() 方法中，进入 DelegatingMethodAccessorImpl 类的 invoke() 方法后，这里调用了 delegate 属性的 invoke() 方法：\n   delegate 的类型为 MethodAccessorImpl，该类是一个抽象类，有两个子类，分别是 DelegatingMethodAccessorImpl 和 NativeMethodAccessorImpl：\n 按照我们前面说的，这里的 delegate 其实是一个 NativeMethodAccessorImpl 对象，所以这里会进入 NativeMethodAccessorImpl 的 invoke() 方法。\n  在 NativeMethodAccessorImpl 的 invoke() 方法中，会判断调用次数是否超过阈值（默认为 15 次），如果超过该阈值，那么就会生成另一个 MethodAccessor 对象，并将原来 DelegatingMethodAccessorImpl 对象中的 delegate 属性指向最新的 MethodAccessor 对象：\n   到这里，其实我们可以知道 MethodAccessor对象其实就是具体去生成反射类的入口，通过查看源码上的注释，我们可以了解到 MethodAccessor 对象的一些设计信息：\n 初次加载字节码实现反射，使用 Method.invoke() 和 Constructor.newInstance() 加载花费的时间是使用原生代码加载花费时间的 3-4 倍，这使得那些频繁使用反射的应用需要花费更长的启动时间。 为了避免这中痛苦的时间，我们在第一次加载的时候重用了 JVM 的入口，之后切换到字节码的实现，因此实际的 MethodAccessor 实现有两个版本，一个是 Native 版本，一个是 Java 版本：  Native 版本一开始启动快，但是随着运行时间变长，速度变慢；Java 版本一开始加载慢，但是随着运行时间变长，速度变快。 正是因为二者存在这样的差异，所以第一次加载的时候我们会发现使用的是 NativeMethodAccessorImpl，当反射调用次数超过 15 次之后，则使用 MethodAccessorGenerator 生成的 MethodAccessorImpl 对象去实现反射。      Method 类的 invoke() 方法的整个流程的时序图如下所示：\n   4 应用场景 #  反射常见的应用场景有以下几个：\n Spring 实例化对象：  当程序启动时，Spring 会读取配置文件 application.xml 并解析出里面的所有标签并实例化到 IOC 容器中。 具体示例如下：   假如有如下的上下文配置文件：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;smallpineapple\u0026#34; class=\u0026#34;top.grayson.jvm.domain.SmallPineapple\u0026#34;\u0026gt; \u0026lt;constructor-arg type=\u0026#34;java.lang.String\u0026#34; value=\u0026#34;小菠萝\u0026#34;/\u0026gt; \u0026lt;constructor-arg type=\u0026#34;int\u0026#34; value=\u0026#34;21\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt;   在定义好上面的文件后，通过 ClassPathXmlApplicationContext加载该配置文件，程序启动时，Spring会将该配置文件中的所有 bean都实例化，放入IOC容器中，IOC容器本身就是一个工厂，通过该工厂传入 \u0026lt;bean\u0026gt;标签的 id属性获取到对应的实例：\npublic class SpringIOCTest { public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(\u0026#34;applicationContext.xml\u0026#34;); SmallPineapple smallPineapple = (SmallPineapple)context.getBean(\u0026#34;smallpineapple\u0026#34;); smallPineapple.getInfo(); // 小菠萝的年龄是21  } }   Spring在实例化对象的过程经简化之后，可以理解为反射实例化对象的步骤：\n 获取 Class对象的构造器。 通过构造器调用 newInstance()实例化对象。       用反射实现 动态代理。 JDBC连接数据库：  使用JDBC连接数据库时，指定连接数据库的驱动类时用到反射加载驱动类。 具体的示例如下：   在导入第三方库时，JVM不会主动去加载外部导入的类，而是等到真正用时，才去加载需要的类，正是如此，我们可以在获取数据库连接时传入驱动类的全限定名，交给JVM加载该类：\npublic class DBConnectionTest { // 指定数据库的驱动类  private static final String DRIVER_CLASS_NAME = \u0026#34;com.mysql.jdbc.driver\u0026#34;; public static Connection getConnection() throws ClassNotFoundException, SQLException { Connection conn = null; // 加载驱动类  Class.forName(DRIVER_CLASS_NAME); // 获取数据库连接对象  conn = DriverManager.getConnection(\u0026#34;jdbc:mysql://···\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;root\u0026#34;); return conn; } }   在我们开发SpringBoot项目时，常见的 application.yml中的数据库配置，也用到了反射的原理：\nspring: dataSource: driver-class-name: com.mysql.cj.jdbc.Driver username: root password: root url: jdbc:mysql://··· 与1中的写法相比，这样写的好处是不需要修改源码，仅加载配置文件就可以完成驱动类的替换。\n      5 优缺点 #  5.1 优点 #   增加程序的灵活性：  面对需求变更时，可以灵活地实例化不同的对象。 例如，在SpringBoot中利用反射连接数据库，涉及到数据库的数据源，直接在application.yml里面配置即可，当涉及到需要更改数据源时，直接更改配置文件即可，无需修改源码。    5.2 缺点 #   破坏类的封装性：  反射可以获取类中被 private修饰的变量、方法和构造器，这违反了面向对象的封装特性，因为被 private修饰意味着不想对外暴露，只允许本类访问，而 setAccessable(true)可以无视访问修饰符的限制，外界可以强制访问。   性能损耗：  在直接 new对象并调用对象方法和访问属性时，编译器会在编译期提前检查可访问性，如果尝试进行不正确的访问，IDE会提前提示错误，例如参数传递类型不匹配、非法访问 private属性和方法。 而在利用反射操作对象时，编译器无法提前得知对象的类型，访问是否合法，参数传递类型是否匹配，只有在程序运行时调用反射的代码时才会从头开始检查、调用、返回结果，JVM也无法对反射的代码进行优化。  虽然反射具有性能损耗的特点，但是我们不能一概而论，产生了使用反射就会性能下降的思想，反射的慢，需要同时调用上100W次才可能体现出来，在几次、几十次的调用，并不能体现反射的性能低下，所以不要一味地戴有色眼镜看反射，在单次调用反射的过程中，性能损耗可以忽略不计，如果程序的性能要求很高，那么尽量不要使用反射。\n     参考文献 #    大白话说 Java 反射：入门、使用、原理。  Java 反射是什么？看这篇绝对会了【macrozheng】！  Java 反射：这是一份全面 \u0026amp; 详细的 Java 反射机制 学习指南。  "},{"id":101,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.5-Java%E7%9A%844%E7%A7%8D%E5%BC%95%E7%94%A8%E7%BA%A7%E5%88%AB/","title":"2.5 Java的4种引用级别","section":"2、 Java多线程","content":"Java的4种引用级别 #  1 介绍 #  从 JDK 1.2 版本开始，对象的引用被划分为 4 中级别，从而使程序能更加灵活地控制对象的生命周期，这 4 中级别从高到低依次为强引用（FinalReference）、软引用（SoftReference）、弱引用（WeakReference）和虚引用（PhantomReference）。\n 2 具体划分 #  2.1 强引用（FinalReference） #    强引用是使用最普遍的引用，如果一个对象具有强引用，那么垃圾回收器绝对不会回收他，如下：\nObject finalReference = new Object();   当空间不足时，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。\n  如果强引用对象不使用时，需要弱化从而使 GC 能够回收，显示地设置 finalReference 为 null，或让其超出对象的生命周期范围，则 GC 认为该对象不存在引用，这时就可以回收这个对象，具体什么时候回收取决于 GC 算法。\nfinalReference = null;   如果一个方法内部有一个强引用（局部变量），这个引用保存在 Java 栈中，真正的引用内容（Object）保存在 Java 堆中，当这个方法运行完成后，就会退出方法栈，则引用对象的引用数就会变为 0，这个对象就会被回收。\npublic void test() { Object finalReference = new Object(); // 省略其他操作 }   如果这个变量为全局变量时，就需要在不用这个对象时赋值为 null，因为强引用不会被垃圾回收。\n  2.2 软引用（SoftReference） #    如果一个对象只具有软引用，则在内存空间充足时，垃圾回收器就不会回收他；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收他，该对象就可以被程序使用。\n// 强引用 String strongReference = new String(\u0026#34;abc\u0026#34;); // 软引用 String str = new String(\u0026#34;abc\u0026#34;); SoftReference\u0026lt;String\u0026gt; softReference = new SoftReference\u0026lt;String\u0026gt;(str);   软引用可以和一个引用队列（ReferenceQueue）联合使用，如果引用队列所引用的对象被垃圾回收，Java 虚拟机就会把这个软引用加入到与之关联的引用队列中。\nReferenceQueue\u0026lt;String\u0026gt; referenceQueue = new ReferenceQueue\u0026lt;\u0026gt;(); String str = new String(\u0026#34;abc\u0026#34;); SoftReference\u0026lt;String\u0026gt; softReference = new SoftReference\u0026lt;\u0026gt;(str, referenceQueue); str = null; // Notify GC System.gc(); System.out.println(softReference.get()); // abc  Reference\u0026lt;? extends String\u0026gt; reference = referenceQueue.poll(); System.out.println(reference); //null  注意：软引用对象是在 JVM 内存不够的时候才会被回收，我们调用 System.gc() 方法只是起通知作用，JVM 什么时候扫描回收对象是 JVM 自己的状态决定的。就算扫描到软引用对象也不一定回收他，只有内存不够的时候才会回收。\n   当内存不足时，JVM 首先将软引用对象置为 null，然后通知垃圾回收器进行回收，也就是说，垃圾回收器会在虚拟机抛出 OutOfMemoryError 之前回收软引用对象，而且虚拟机会尽可能优先回收长时间闲置不用的软引用对象，对那些刚构建的或刚使用过的较新的软对象会被虚拟机尽可能保留，这就是引入引用队列（ReferenceQueue）的原因。\nif(JVM 内存不足) { // 将软引用中的对象引用置为 null  str = null; // 通知垃圾回收器进行回收  System.gc(); }   2.3 弱引用（WeakReference） #   弱引用与软引用的区别在于：弱引用的对象拥有更短的生命周期。 在垃圾回收器线程扫描他所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存，回收的时候，JVM 会首先将软引用中的对象引用置为 null，然后通知垃圾回收器进行回收。 不过，由于垃圾回收器是一个优先级很低的线程，因此他不一定会很快发现那些只具有弱引用的对象。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果引用队列所引用的对象被垃圾回收，Java 虚拟机就会把这个软引用加入到与之关联的引用队列中。 如果一个对象是偶尔（很少）使用，并且希望在使用时随时就能获取到，但又不想影响此对象的垃圾收集，那么我们可以用弱引用来记住此对象。 下面的代码会让一个弱引用再次变为一个强引用： String str = new String(\u0026#34;abc\u0026#34;); WeakReference\u0026lt;String\u0026gt; weakReference = new WeakReference\u0026lt;\u0026gt;(str); // 弱引用转强引用  String strongReference = weakReference.get();   2.4 虚引用（PhantomReference） #  2.4.1 特点 #    无法通过虚引用来获取一个对象的真实引用。\nReferenceQueue queue = new ReferenceQueue(); PhantomReference\u0026lt;byte[]\u0026gt; reference = new PhantomReference\u0026lt;byte[]\u0026gt;(new byte[1], queue); System.out.println(reference.get()); 运行结果：\nnull    虚引用必须与 ReferenceQueue 一起使用，当 GC 准备回收一个对象，如果发现他还有虚引用，就会在回收之前，把这个虚引用加入到与之关联的 ReferenceQueue 中，此时他的实例对象还在内存中。\n 运行结果：\n 我们简单分析下代码：\n 第一个线程往集合里面塞数据，随着数据越来越多，肯定会发生GC。 第二个线程死循环，从queue 里面拿数据，如果拿出来的数据不是null，就打印出来。  从运行结果可以看到，当发生 GC 时，虚引用就会被回收，并且把回收的通知放到 ReferenceQueue 中。\n  2.4.2 适用场景 #  2.4.2.1 确定对象从内存中回收的具体时间 #  虚引用针对一些内存敏感型的任务可以帮助我们确定对象从内存中回收的具体时间，例如延迟给新的对象分配内存（例如很大的图片），直到以前的内存被释放。\n2.4.2.2 替代 finalize 方法，保证对象在 finalize 时不会复活（Resurrect） #   虚引用可以用来代替 finalize 方法，保证对象在 finalize 时不会复活（Resurrect）。 这允许对象在一个周期内完成垃圾回收，而不需要等待下一个垃圾回收期以确保他没有复活。   3 总结 #   4 参考文献 #    理解 Java 的强引用、软引用、弱引用和虚引用。  强软弱虚引用，只有体会过了，才能记住。  Phantom References in Java.  虚引用。  "},{"id":102,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.6-Java%E4%B8%AD%E7%9A%84%E9%94%81/","title":"2.6 Java中的锁","section":"2、 Java多线程","content":"Java中的锁 #  1 公平锁和非公平锁 #  1.1 基本概念 #  1.1.1 公平锁 #  1.1.1.1 含义 #  公平锁是指多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列里面的第一位才能得到锁。\n1.1.1.2 优缺点 #  1.1.1.2.1 优点 #   所有的线程都能得到资源，不会饿死在队列中。  1.1.1.2.2 缺点 #   吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，CPU唤醒阻塞线程的开销会很大。  1.1.2 非公平锁 #  1.1.2.1 含义 #   非公平锁是指多个线程区获取锁的时候，会直接去尝试，获取不到，再去进入等待队列，如果能获取到，则直接获取锁。 ReentrantLock默认是非公平锁， 如果要使用公平锁，可以使用ReentrantLock lock = new ReentrantLock(true);   1.1.2.2 优缺点 #  1.1.2.2.1 优点 #   可以减少 CPU 唤醒线程的开销，整体的吞吐效率会高点，CPU 也不必去唤醒所有线程，会减少唤醒线程的数量。  1.1.2.2.2 缺点 #   可能导致队列中的线程一直获取不到锁或者长时间获取不到锁导致饿死。  1.2 具体实例 #   1.3 源码分析 #  下面以 ReentrantLock 为例。\n ReentrantLock 有一个内部类Sync，他继承自AbstractQueuedSynchronizer。 Sync 又有两个子类FairSync 和NoFairSync，分别对应公平锁和非公平锁。   公平锁和非公平锁中获取锁的具体实现如下：   分析以上代码，可知：\n 公平锁就是在获取锁之前会先判断等待队列是否为空或者自己是否位于队列头部，该条件通过才能继续获取锁。 若释放锁的时候没有新的线程来获取锁，则非公平锁等于公平锁。 若释放锁的时候正好有一个线程来获取锁，而此时位于队列头部的线程还没有被唤醒（因为线程上下文切换是需要不少开销的），此时后来的线程则优先获得锁，成功打破公平，称为非公平锁。 对于非公平锁，只要线程进入了等待队列，队列里面依然是 FIFO 的原则，跟公平锁的顺序是一样的，因为公平锁和非公平锁的 release() 部分是共用 AQS 的代码。   2 可重入锁与非可重入锁 #  2.1 基本概念 #  2.1.1 可重入锁 #  2.1.1.1 含义 #   可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者 class），不会因为之前已经获取过还没释放而阻塞。 Java 中ReentrantLock 和Synchronized 都是可重入锁。  2.1.1.2 实例 #  public class Widget { public synchronized void doSomething() { System.out.println(\u0026#34;方法 1 执行...\u0026#34;); doOthers(); } public synchronized void doOthers() { System.out.println(\u0026#34;方法 2 执行...\u0026#34;); } }  在上面的代码中，类中的两个方法都是被内置锁synchronized 修饰的，doSomething() 方法中调用doOthers() 方法。 因为内置锁是可重入的，所以同一个线程在调用 doOthers() 时可以直接获得当前对象的锁，进入 doOthers() 进行操作。 如果是一个不可重入锁，那么当前线程在调用 doOthers() 之前需要将执行 doSomething() 时获取当前对象的锁释放掉，实际上该对象锁已被当前线程所持有，且无法释放，所以此时会出现死锁。  2.1.1.3 优缺点 #  2.1.1.3.1 优点 #   可以在一定程度上避免死锁。  2.1.2 非可重入锁 #  2.1.2.1 含义 #   非可重入锁是指如果当前线程执行某个方法已经获取了该锁，那么在方法中尝试再次获取该锁时，就会因为获取不到而被阻塞。  2.1.2.2 实例 #   当线程执行 print() 方法时首先获取 lock，接下来执行 doAdd() 方法时就无法执行 doAdd() 中的逻辑，必须先释放锁。\n2.2 实例分析 #  2.2.1 可重入锁 #   当有很多人在排队打水，此时管理员允许锁和同一个人的多个水桶绑定。 这个人用多个水桶打水时，第一个水桶和锁绑定并打完水之后，第二个水桶也可以直接和锁绑定并开始打水，所有的水桶打完水之后，打水人才会将锁还给管理员。 这个人的所有打水流程都能够成功执行，后续等待的人也能够打到水，这就是可重入锁。   2.2.2 非可重入锁 #   如果是非可重入锁的话，此时管理员只允许锁和同一个人的一个水桶绑定。 第一个水桶和锁绑定打完水之后并不会释放锁，导致第二个水桶不能和锁绑定，也无法打水。 因此当前线程就会出现死锁，整个等待队列中的所有线程都无法被唤醒。   2.3 源码分析 #    首先 ReentrantLock 和 NonReentrantLock 都继承父类 AQS，其父类 AQS 中维护了一个同步状态 status 来计数重入次数，status初始值为 0。\n  获取锁时：\n 可重入锁先尝试获取并更新 status 值：  如果status == 0，表示没有其他线程在执行同步代码，则把 status 置为 1，当前线程开始执行。 如果status != 0，则判断当前线程是否是获取到这个锁的线程，如果是的话，执行status + 1，且当前线程可以再次获取锁。   非可重入锁直接去获取并尝试更新当前 status 的值：  如果status != 0，则会导致其获取锁失败，当前线程阻塞。      释放锁时：\n 可重入锁同样先获取当前 status 的值，在当前线程是持有锁的前提下，如果status - 1 == 0，则表示当前线程所有重复获取锁的操作都已经执行完毕，然后该线程才会真正释放锁。 非可重入锁则是在确定当前线程是持有锁的线程之后，直接将 status 置为 0，将锁释放。     3 乐观锁与悲观锁 #  3.1 基本概念 #   乐观锁与悲观锁是一种广义上的概念，体现了看待线程同步的不同角度。 对于同一个数据的并发操作：  悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改，Java 中synchronized 和lock 的实现类都是悲观锁。 乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据：  如果这个数据没有被更新，当前线程将自己修改的数据成功写入。 如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。   乐观锁在Java 中是通过使用无锁编程来实现，最常采用的是CAS 算法，Java 原子类中的递增操作就是通过CAS 自旋实现的。   乐观锁和悲观锁的使用场景如下：  悲观锁适合写操作多的场景，先加锁可以保证写操作时数据正确。 乐观锁适合读操作多的场景，不加锁的特点能够使其读操作的性能大幅提升。    3.2 实例分析 #  乐观锁和悲观锁获取同步资源数据的过程如下：\n 3.3 源码分析 #  乐观锁和悲观锁的调用方式示例如下：\n// ------------------------- 悲观锁的调用方式 ------------------------- // synchronized public synchronized void testMethod() { // 操作同步资源 } // ReentrantLock private ReentrantLock lock = new ReentrantLock(); // 需要保证多个线程使用的是同一个锁 public void modifyPublicResources() { lock.lock(); // 操作同步资源 \tlock.unlock(); } // ------------------------- 乐观锁的调用方式 ------------------------- private AtomicInteger atomicInteger = new AtomicInteger(); // 需要保证多个线程使用的是同一个 AtomicInteger atomicInteger.incrementAndGet(); //执行自增 1 通过调用方式示例，我们可以发现悲观锁基本都是在显式的锁定之后再操作同步资源，而乐观锁则直接去操作同步资源，而乐观锁之所以能够做到不锁定同步资源也可以正确的实现线程同步，主要是因为乐观锁主要采用 CAS 来实现，具体可参见 2.14 CAS 原理。\n4 独占锁与共享锁 #  4.1 基本概念 #  4.1.1 独占锁 #   独占锁也叫排他锁，是指该锁一次只能被一个线程所持有。 如果线程T 对数据A加上排他锁后，则其他线程不能再对 A 加任何类型的锁。 获得独占锁的线程即能读数据，又能修改数据。 JDK 中的synchronized 和JUC 中的Lock 的实现类就是独占锁。  4.1.2 共享锁 #   共享锁是指该锁可被多个线程所持有。 如果线程T 对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排他锁。 获得共享锁的线程只能读数据，不能修改数据。  4.2 源码分析 #     ReentrantReadWriteLock 有两把锁，分别是 ReadLock 和 WriteLock。\n  ReadLock 和 WriteLock 的主体都是 Sync，它是 AQS 的一个子类，但读锁和写锁的加锁方式不一样：读锁是共享锁，写锁是独占锁。\n  读写、写读、写写的过程互斥，因为读锁和写锁是分离的，所以 ReentrantReadWriteLock 的并发性比一般的互斥锁有了很大提升。\n  AQS 中有一个 state 字段（int 类型，32 位），该字段用来描述有多少线程持有锁：\n 在独占锁中，这个值通常是 0 或者 1（如果是重入锁的话state 就是重入的次数）。 在共享锁中，state 就是持有锁的数量。 但是在ReentrantReadWriteLock 中有读、写两把锁，所以需要在一个整形变量state 上分别描述读锁和写锁的状态，于是将state 变量按位切割分成了两个部分，高 16 位表示读锁状态，低 16 位表示写锁状态。     获取写锁的源码及过程如下图：\n   tryAcquire() 除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断，如果存在读锁，则写锁不能获取，原因在于必须确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他线程就无法感知到当前写线程的操作。 因此，只有等其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。 写锁的释放与ReentrantLock 的释放过程基本类似，每次释放均减少写状态，当写状态为 0 时，表示写锁已被释放，然后等待的读写线程才能够继续访问读写锁，同时前次写线程的修改对后续的读写线程可见。    获取读锁的源码如下图所示：\n  在tryAcquireShared 方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。 如果当前线程获取了写锁或者写锁未被获取，则当前线程增加读状态，成功获取读锁。 所以读写锁才能实现读读的过程共享，而读写、写读、写写的过程互斥。    5 自旋锁与适应性自旋锁 #  5.1 基本概念 #  5.1.1 自旋锁 #  5.1.1.1 含义 #   阻塞或唤醒一个 Java 线程需要操作系统切换 CPU 状态来完成，这种状态转换需要耗费处理器时间，如果同步代码块中的内容过于简单，状态转换消耗的时间可能比用户代码执行的时间还要长。 在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失，如果物理机器有多个处理器，能够让两个或两个以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃 CPU 的执行时间，看看持有锁的线程是否很快就会释放锁。 而为了让当前线程“稍等一下”，我们就需要让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销，这就是自旋锁。   5.1.1.2 优缺点 #  5.1.1.2.1 缺点 #   自旋锁本身是由缺点的，它本身不能代替阻塞。 自旋等待虽然避免了线程切换的开销，但他要占用处理器时间。 如果锁被占用的时间很短，自旋等待的效果就会非常好，反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。 所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是 10 次，可以使用-XX:PreBlockSpin 来更改）没有成功获得锁，就应当挂起线程。  5.1.2 适应性自旋锁 #  5.1.2.1 含义 #   自适应意味着自旋的时间不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也有可能再次成功，进而他将允许自旋等待持续相对更长的时间。 如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时，将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。  5.2 源码分析 #   自旋锁的实现原理同样也是CAS，AtomicInteger 中调用Unsafe 进行自增操作中的do-while 就是一个自旋操作，如果修改数值失败，则通过循环来执行自旋，直至修改成功。   6 无锁、偏向锁、轻量级锁、重量级锁 #  6.1 前言 #   这四种锁是指锁的状态，专门针对 Synchronized的。 目前锁一共有四种状态，级别从低到高依次是无锁、偏向锁、轻量级锁、重量级锁，锁状态只能升级，不能降级。 四种锁对应 Mark Word的内容如下：     锁状态 存储内容 标志位     无锁 对象的 hashCode、对象分代年龄、是否是偏向锁（0） 01   偏向锁 偏向线程 ID、偏向时间戳、对象分代年龄、是否是偏向锁（1） 01   轻量级锁 指向栈中锁记录的指针 00   重量级锁 指向互斥量（重量级锁）的指针 10    6.2 分类 #  6.2.1 无锁 #   无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源：  如果没有冲突就修改成功并退出，否则就会继续循环尝试。 如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断尝试直到修改成功。   无锁无法全面替代有锁，但无锁在某些场合下的性能是非常高的。  CAS原理及应用即是无锁的体现。  6.2.2 偏向锁 #  6.2.2.1 为什么要引入偏向锁 #   因为 HotSpot 作者经过研究实践发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得。 为了让线程获得锁的代价更低，引进了偏向锁，主要目的是在没有多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的加锁解锁操作是需要依赖多次 CAS 原子指令的，而偏向锁只需要在置换线程 ID 的时候依赖一次 CAS 原子指令。  6.2.2.2 含义 #   偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。 偏向锁是在单线程执行代码块时使用的机制，如果在多线程并发的环境下（即线程 A 尚未执行完同步代码块，线程 B 发起了申请锁的申请），则一定会转化为轻量级锁或者重量级锁。  6.2.2.3 原理 #  6.2.2.3.1 获得偏向锁 #  当一个线程访问同步块并获取锁时，会在 对象头和栈帧中的锁记录里存储锁偏向的线程 ID，以后进入和退出同步块时不需要花费 CAS 操作来争夺锁资源，只需要检查是否为偏向锁、锁标识以及线程 ID 即可，处理流程如下：\n 检查 Mark Word 是否为偏向锁、锁标识位是否为 01：  如果为可偏向状态，则检查线程 ID 是否为当前线程 ID：  如果是，则执行同步代码块。 如果不是，则通过 CAS 操作竞争锁：  如果竞争成功，则将 Mark Word 的线程 ID 替换为当前线程 ID。 如果竞争失败，说明当前存在多线程竞争的情况，当到达全局安全点（在这个时间点上没有正在执行的代码），获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被锁在安全点的线程继续往下执行同步代码块。        6.2.2.3.2 撤销偏向锁 #  偏向锁的释放采用了一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争，同时，偏向锁的撤销需要等待全局安全点，处理流程如下：\n 暂停拥有偏向锁的线程。 判断锁对象是否还处于被锁定状态：   如果是，则挂起持有锁的线程，并将指向当前线程的锁记录地址的指针放入对象头 Mark Word，升级为轻量级锁状态（00），然后恢复持有锁的当前线程，进入轻量级锁的竞争模式。\n  如果不是，则恢复到无锁状态（01），以允许其余线程竞争。\n 需要注意的是，此处将当前线程挂起再恢复的过程中并没有发生锁的转移，仍然在当前线程手中，只是穿插了个将对象头中的线程 ID 变更为指向锁记录地址的指针。\n    偏向锁的获得和撤销流程如下图所示：   6.2.3 轻量级锁 #  6.2.3.1 含义 #   轻量级锁的主要目的是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。 轻量级锁适用的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，必然会导致轻量级锁膨胀为重量级锁。  6.2.3.2 原理 #  6.2.3.2.1 获得轻量级锁 #  当关闭偏向锁功能或者多个线程竞争偏向锁导致偏向锁升级为轻量级锁，则会尝试获取轻量级锁，其步骤如下：\n 在线程进入同步块时，如果同步对象锁状态为无锁状态（锁标志位为 01，是否为偏向锁为 0），虚拟机首先在当前线程的栈帧中建立一个名为 Lock Record的空间，用于存储锁对象目前的 Mark Word的拷贝，此时线程堆栈与对象头的状态如下图所示：  拷贝对象头中的 Mark Word 到 Lock Record 中。 拷贝成功后，虚拟机将使用 CAS 操作尝试将对象 Mark Word 中的 Lock Word 更新为指向当前线程 Lock Record 的指针，并将 Lock Record 里的 Owner 指针指向 object mark word，如果更新成功，则执行步骤 4，否则，执行步骤 5。 如果这个动作更新成功了，那么当前线程就拥有了该对象的锁，并且对象 Mark Word 的锁标志位设置为 00，即表示此对象处于轻量级锁定状态，此时线程堆栈与对象头的状态如下图所示：  如果这个操作更新失败了，虚拟机首先会检查对象 Mark Word 中的 Lock Word 是否指向当前线程的堆栈：  如果是，说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。 否则，说明多个线程竞争锁，进入自旋执行 3，若自旋结束时仍未获得锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为 10，Mark Word 中存储的就是指向重量级锁的指针，当前线程以及后面等待锁的线程也要进入阻塞状态。    6.2.3.2.2 释放轻量级锁 #  轻量级锁的释放也是通过 CAS 操作来进行的，主要步骤如下：\n  通过 CAS 操作尝试把线程中复制的 Mark Word 对象替换为当前的 Mark Word：\n 如果替换成功，整个同步过程就完成了，恢复到无锁状态（01）。 如果替换失败，说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程。    轻量级锁的获得及释放过程如下图所示：   6.2.3.3 相关问题 #  6.2.3.3.1 为什么升级为轻量级锁时要把对象头里的 Mark Word 复制到线程栈的 Lock Record 中呢 #   因为在申请对象锁时需要以该值作为 CAS 的比较条件，同时，在升级到重量级锁的时候，能通过这个比较判定是否在持有锁的过程中此锁被其他线程申请过，如果被其他线程申请了，则在释放锁的时候要唤醒被挂起的线程。  6.2.3.3.2 为什么会尝试 CAS 不成功，以及什么情况下会不成功 #   CAS 本身是不带锁机制的，而是通过比较而来，假设如下场景：  线程 A 和线程 B 都在对象头里的锁标识为无锁状态下进入，那么如果线程 A 先更新对象头为其锁记录指针成功之后，线程 B 再用 CAS 去更新，就会发现此时的对象头已经不是其操作前的对象 HashCode 了，所以CAS 会失败，也就是说，只有两个线程并发申请锁的时候会发生 CAS 失败。 然后线程 B 进行 CAS 自旋，等待对象头的锁标识重新变回无锁状态，或对象头的内容等于对象 HashCode（因为这是线程 B 做 CAS 操作前的值），这也就意味着线程 A 执行结束（轻量级锁中只有线程 A 执行完毕撤销锁了才会重置对象头），此时线程 B 的 CAS 终于成功了，于是线程 B 获得了锁以及执行同步代码的权限。 如果线程 A 的执行时间较长，线程 B 经过若干次 CAS 时钟没有成功，则锁膨胀为重量级锁，即线程 B 被挂起阻塞，等待重新调度。    6.2.4 重量级锁 #  6.2.4.1 含义 #   重量级锁是指依赖于操作系统 Mutex Lock 实现的锁。 操作系统实现线程之间的切换需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，因此重量级锁一般效率比较低。  6.3 转换 #    6.4 选择 #   各种锁并不是相互替代的，而是在不同场景下的不同选择：  如果是单线程使用，那 偏向锁毫无疑问代价最小，并且他就能解决问题，仅仅在内存中比较下对象头就可以了。 如果是出现了其他线程竞争，则偏向锁就会升级为 轻量级锁。 如果其他线程通过一定次数的 CAS 尝试没有成功，则进入 重量级锁，在这种情况下，进入同步代码块就要做偏向锁建立、偏向锁撤销、轻量级锁建立、升级到重量级锁，最终还是得靠重量级锁来解决问题，那这样代价比直接用重量级锁要大不少。   所以，使用哪种技术，一定要看其所处的环境及场景，在绝大多数的情况下，偏向锁是有效的，这是基于HotSpot作者发现的大多数锁只会由同一线程并发申请的经验规律。 不同锁的优缺点及适用场景如下：     锁 优点 缺点 适用场景      偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法仅存在纳米级的差距。 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 适用于只有一个线程访问同步块的场景。    轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度。 如果始终得不到锁竞争的线程使用自旋会消耗CPU。 追求响应时间，同步块执行速度非常快。    重量级锁 线程竞争不使用自旋，不会消耗CPU。 线程阻塞，响应时间缓慢。 追求吞吐量，同步块执行速度较慢。    参考文献 #    一张图读懂非公平锁与公平锁。  阿里面试官：说一下公平锁和非公平锁的区别？  不可不说的 Java“锁”事。  Java 不可重入锁和可重入锁理解。  深入分析 Synchronized 原理(阿里面试题)。  "},{"id":103,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.7-Synchronized%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"2.7 Synchronized实现原理","section":"2、 Java多线程","content":"Synchronized实现原理 #  1 特性 #  Synchronized 具有 原子性、 可见性、 有序性、 可重入性。\n2 用法 #  Synchronized 可以修饰静态方法、成员函数，同时还可以直接定义代码块，但是归根结底他上锁的资源只有两类，一个是对象，一个是类。\n 关于 static 需要注意以下地方：\n static修饰的静态方法、静态属性都是归类所有，同时该类的所有实例对象都可以访问。 普通成员属性、成员方法是归实例化的对象所有，必须实例化之后才能访问，这也是为什么静态方法不能访问非静态属性的原因。   2.1 修饰成员函数 #   下面的代码均定义在 SynchronizedTest 类中，且该类中的变量定义如下：\nprivate int i = 0; private static int j = 0; private final SynchronizedTest instance = new SynchronizedTest();    具体的代码如下：\n// 对成员函数加锁，必须获得该类的实例对象的锁才能进入同步块 public synchronized void add1() { i++; }   该方法没有被 static 修饰，也就是说该方法是归实例化的对象所有，那么这个锁就是加给 SynchronizedTest 类所实例化的对象。\n  2.2 修饰静态方法 #    具体的代码如下：\n// 对静态方法加锁，必须获得类的锁才能进入同步块 public static synchronized void add2() { j++; }   该方法是静态方法，归 SynchronizedTest 类所有，所以这个锁是加给 SynchronizedTest 类的。\n  2.3 修饰代码块 #    具体的代码如下：\npublic void method() { synchronized (SynchronizedTest.class) { // 同步块，执行前必须获得 SynchronizedTest 类的锁  } synchronized (instance) { // 同步块，执行前必须先获得实例对象的锁  } }   method 方法中的两个同步代码块，第一个代码块锁定的是 SynchronizedTest.class，该锁是加给 SynchronizedTest 类的，第二个代码块锁定的是 instance，这个 instance是 SynchronizedTest 类的一个实例化对象，因此他所上的锁是给 insatnce 实例化对象的。\n  3 相关概念 #  3.1 Java 对象头 #   在 JVM 中，对象在内存中的布局分为三块区域，分别是对象头、实例数据和对齐填充，如下图所示：  实例数据：  存放类的属性数据信息，包括父类的属性信息。   对齐填充：  由于虚拟机要求，对象起始地址必须是 8 字节的整数倍，填充数据不是必须存在的，仅仅是为了字节对齐。   对象头：  Java 对象头一般占用 2 个机器码（在 32 位虚拟机中，1 个机器码等于 4 字节，也就是 32bit，在 64 位虚拟机中，1 个机器码是 8 个字节，也就是 64bit），但是如果对象是数组类型，则需要三个机器码，因为需要一块来记录数组长度。 Hotspot 虚拟机的对象头主要包括两部分数据，分别是Mark Word（标记字段）、Class Pointer（类型指针）：   Mark Word：\n  Mark Word 主要用于存储对象自身的运行时数据，如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等，是实现轻量级锁和偏向锁的关键。\n  对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构，以便在极小的空间内存储尽量多的数据，他会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，可能变化为存储以下四种数据（32 位虚拟机）：   对象头的最后两位存储了锁的标志位，01 是初始状态，未加锁，其对象头里存储的是对象本身的哈希码，随着锁级别的不同，对象头里会存储不同的内容：\n 偏向锁存储的是当前占用此对象的线程 ID。 轻量级锁存储的是指向线程栈中锁记录的指针。    从这里我们可以看到：\n 锁可能是个锁记录 + 对象头里的引用指针（判断线程是否拥有锁时将线程的锁记录地址和对象头里的指针地址比较）。 锁也可能是对象里头的线程 ID（判断线程是否拥有锁时将线程的 ID 和对象里存储的线程 ID 比较）。     锁也分为不同的状态，JDK 1.6 之前只有两个状态，分别是无锁、有锁（重量级锁），在 JDK 1.6 之后，对 Synchronized 进行了优化，新增了两种状态，总共就是四个状态，分别是无锁、偏向锁、轻量级锁、重量级锁，锁的类型和状态在对象头 Mark Word 中都有记录，在申请锁、锁升级等过程中 JVM 都需要读取对象的 Mark Word 数据。\n    Class Pointer：\n Class Pointer是对象指向他的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。          3.2 对象头中的 Mark Word 与线程中的 Lock Record #   在线程进入同步代码块的时候，如果此同步对象没有被锁定，即他的所标志位是 01，则虚拟机首先在当前线程的栈中创建我们称之为锁记录（Lock Record）的空间，用于存储锁对象的 Mark Word 拷贝。 Lock Record 是线程私有的数据结构，每一个线程都有一个可用 Lock Record 列表，同时还有一个全局列表，每一个被锁住的对象 Mark Word 都会和一个 Lock Record 关联（对象头的 Mark Word 中的 Lock Word 指向 Lock Record 的起始地址），同时 Lock Record 中有一个 Owner 字段存放拥有该锁的唯一标识，表示该锁被这个线程占用。 Lock Record 主要包括以下字段：  Owner：初始时为 null，表示当前没有任何线程拥有该 Lock Record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为 null。 EntryQ：关联一个系统互斥锁（Semaphore），阻塞所有试图锁住该 Lock Record 失败的线程。 RcThis：表示 blocked 或 waiting 在该 Lock Record 上的所有线程的个数。 Nest：用来实现重入锁的计数。 HashCode：保存从对象头拷贝过来的 HashCode 值。 Candidate：用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪，然后因为竞争锁失败又被阻塞），从而导致性能严重下降，Candidate只有两种可能的值，0 表示没有需要唤醒的线程，1 表示要唤醒一个继任线程来竞争锁。    3.3 Monitor #    Monitor是一个同步工具，每个对象都有一个 Monitor 与之关联，当一个 Monitor 被持有后，他将处于锁定状态。\n  Synchronized 在 JVM 里的实现都是基于进入和退出 Monitor 对象来实现方法同步和代码块同步，虽然具体实现细节不一样，但是都可以通过成对的 MonitorEnter 和 MonitorExit 指令来实现：\n MonitorEnter：插入在同步代码块的开始位置，当代码执行到该指令时，将会尝试获取该对象 Monitor 的所有权，即尝试获得该对象的锁。 MonitorExit：插入在方法结束处和异常处，JVM 保证每个 MonitorEnter 必须有对应的 MonitorExit。    在 Java 虚拟机（HotSpot）中，Monitor 是由 ObjectMonitor（src/share/vm/runtime/objectMonitor.hpp）实现，其主要数据结构如下：\nObjectMonitor() { _header = NULL; _count = 0; // 记录个数  _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; // 处于 wait 状态的线程，会被加入到 _WaitSet  _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; // 处于等待锁 block 状态的线程，会被加入到 _EntryList  _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; _previous_owner_tid = 0; }  ObjectMonitor 中有两个队列，分别是_WaitSet 和_EntryList，用来保存 ObjectWaiter 对象列表（每个等待锁的线程都会被封装成 ObjectWaiter 对象），_owner指向持有 ObjectMonitor 的线程，当多个线程同时访问同一段代码时：   首先会进入 _EntryList 列表，当线程获取到对象的 Monitor 后，进入 _owner 区域并把 _owner 变量设置为当前线程，同时Monitor 中的计数器 _count 加 1。\n  若线程调用 wait() 方法，将释放当前持有的 Monitor，_owner变量恢复为 null，_count减 1，同时该线程进入 _WaitSet 集合中等待被唤醒。\n  若当前线程执行完毕，也将释放当前持有的 Monitor，并复位 _count 的值，以便其他线程进入获取 Monitor。\n 因为 wait()、notify()需要借助 Monitor 对象来实现，所以必须要在同步方法或同步代码块中使用。\n     一个更形象的描述如下：  一个线程通过 1 号门进入 Entry Set（入口区）：  如果入口区没有线程等待，那么这个线程就会获取监视器成为监视器的 Owner，然后执行监视区域的代码。 如果在入口区中有其他线程等待，那么新来的线程也会和这些线程一起等待。   线程在持有监视器的过程中有两个选择：   一个是正常执行监视区域的代码，释放监视器，通过 5 号门退出监视器。\n  还有可能等待某个条件的出现，于是他会通过 3 号门到 Wait Set（等待区）休息，直到相应的条件满足后再通过 4 号门进入，重新获得监视器再执行。\n 需要注意的是：\n 当一个线程释放监视器时，在入口区和等待区的等待线程都会去竞争监视器：  如果入口区的线程赢了，会从 2 号门进入。 如果等待区的线程赢了，会从 4 号门进入。   只有通过 3 号门才能进入等待区，在等待区中的线程只有通过 4 号门才能退出等待区，也就是说，一个线程只有在持有监视器时才能执行 wait 操作，处于等待的线程只有再次获得监视器才能退出等待状态。           4 实现原理 #   Java 虚拟机是通过进入和退出 Monitor 对象来实现代码块同步和方法同步的：  代码块同步使用的是 monitorenter 和 monitorexit 指令实现的。 方法同步是通过 Access flags 后面的 ACC_SYNCHRONIZED 标志来隐式实现的。   这两种同步方式在本质上没有区别，只是方法的同步是一种隐式的方式来实现的，无需通过字节码来完成，两个指令的执行是 JVM 通过调用操作系统的互斥原语 mutex 来实现，被阻塞的线程会被挂起，等待重新调度，会导致用户态和和心态两个态之间来回切换，对性能有较大影响。  4.1 同步代码块 #   当一个线程访问同步代码块时，首先是需要得到锁才能执行同步代码，当退出或者抛出异常时必须要释放锁，具体的实现如下：   首先，我们定义一个同步代码块：\npublic class SynchronizedTest3 { public void method() { synchronized (this) { // 同步块，执行前必须获得 SynchronizedTest 类的锁  } } }   然后对该方法进行反编译（javac SynchronizedTest3.java），接着查看对应的字节码（javap -v -c -s -l SynchronizedTest3）：\n   从上述字节码中可以看到同步代码块的实现是由 monitorenter 和 monitorexit 指令完成的：\n monitorenter：  每个对象都是一个监视器锁，当 Monitor被占用时就会处于锁定状态，线程执行 monitorenter 指令时尝试获取 monitor 的所有权，过程如下：  如果monitor 的进入数为 0，则该线程进入 monitor，然后将进入数设置为 1，该线程即为 monitor 的所有者。 如果该线程已经占有该 monitor，只是重新进入，则进入 monitor 的进入数加 1。 如果其他线程已经占用了 monitor，则该线程进入阻塞状态，直到**monitor 的进入数为 0**，再**重新尝试获取 monitor 的所有权**。     monitorexit：  执行 monitorexit 的线程必须是对应的 monitor 的持有者。 指令执行时，monitor 的进入数减 1，如果减 1 后进入数为 0，则线程退出 monitor，不再是这个 monitor 的持有者，其他被这个 monitor 阻塞的线程可以尝试去获取这个 monitor 的所有权。 monitorexit 指令出现了两次，第一次为同步正常退出释放锁，第二次为发生异常退出释放锁。        4.2 同步方法 #    首先看方法上锁，我们新定义一个同步方法：\npublic class SynchronizedTest2 { private int i = 0; public synchronized void add1() { i++; } }   对该方法进行反编译（javac java_file），然后查看其字节码（javap -v -c -s -l class_file）：\n   从反编译的结果来看，方法的同步并没有通过指令 monitorenter 和 monitorexit 来完成，不过相对于普通方法，其常量池中多了 ACC_SYNCHRONIZED 标识符，JVM 就是根据该标识符来实现方法的同步的：\n 当方法调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取 monitor，获取成功之后才能执行方法体，方法执行完后再释放 monitor，在方法执行期间，其他任何线程都无法再获得同一个 monitor 对象。    5 JVM 对 Synchronized 的优化 #  5.1 为什么要进行优化 #   JVM 是通过进入和退出 Monitor 对象来实现代码块同步和方法同步的，而Monitor 是依靠底层操作系统的 Mutex Lock 来实现的，操作系统实现线程之间的切换需要 从用户态转换到核心态，这个切换成本比较高，对性能影响较大。  5.2 做了哪些优化 #   从JDK 1.5 引入了现代操作系统新增加的** CAS原子操作**。 从JDK 1.6开始，就对 Synchronized 的实现机制进行了较大调整，增加了自适应自旋锁、锁消除、锁粗化、偏向锁、轻量级锁这些优化策略，以此来减少锁的开销。 此时锁主要有四种状态，依次是无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，锁可以从偏向锁升级到轻量级锁，再升级到重量级锁，但是锁的升级是单向的，只能从低到高升级，不会出现锁的降级。  5.2.1 自旋锁 #  自旋锁的相关内容详见 5.1.1 自旋锁。\n5.2.2 适应性自旋锁 #  适应性自旋锁的相关内容详见 5.1.2 适应性自旋锁。\n5.2.3 锁消除 #    锁消除是 Java 虚拟机在 JIT 编译期间，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过锁消除，可以减少毫无意义的请求锁的时间。\n  比如下面代码的 method1 和 method2 的执行效率是一样的，因为 object 锁是私有变量，不存在锁竞争关系：\npublic class SynchronizedTest4 { public void method1() { Object object = new Object(); synchronized (object) { // 执行同步代码  System.out.println(\u0026#34;Hello World.\u0026#34;); } } // 优化后的方法，和上面 method1 的执行效率一样  public void method2() { Object object = new Object(); System.out.println(\u0026#34;Hello World.\u0026#34;); } }   5.2.4 锁粗化 #    锁粗化是指将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。\n  比如下面的 method3 经过锁粗化优化之后就和 method4 执行效率一样了：\npublic void method3() { for (int i = 0; i \u0026lt; 10000; i++) { synchronized (this) { System.out.println(\u0026#34;Hello World.\u0026#34;); } } } // 锁粗化，和上面一样 public void method5() { synchronized (this) { for (int i = 0; i \u0026lt; 10000; i++) { System.out.println(\u0026#34;Hello World.\u0026#34;); } } }   5.2.5 偏向锁 #  偏向锁的相关内容详见 6.2.2 偏向锁。\n5.2.6 轻量级锁 #  轻量级锁的相关内容详见 6.2.3 轻量级锁。\n5.2.7 重量级锁 #  重量级锁的相关内容详见 6.2.4 重量级锁。\n6 Synchronized 与 Lock 的区别 #   实现层面不一样：  Synchronized 是Java 关键字，在JVM 层面实现加锁和释放锁。 Lock 是一个接口，在代码层面实现加锁和释放锁。   是否自动释放锁：  Synchronized在线程代码执行完成或出现异常时自动释放锁。 Lock不会自动释放锁，需要在 finally{} 代码块中显式地释放锁。   是否一致等待：  Synchronized 会导致线程拿不到锁一直等待。 Lock可以设置尝试获取锁或者获取锁失败一定时间超时。   获取锁成功是否可知：  Synchronized无法得知是否获取锁成功。 Lock可以通过 tryLock 获得加锁是否成功。   功能复杂性：  Synchronized 加锁 可重入、不可中断、 非公平。 Lock 可重入、可中断、 可公平和 非公平，细分 读写锁提高效率。    参考文献 #    深入分析 Synchronized 原理(阿里面试题)。  面试官：请详细说下 synchronized 的实现原理。  深入 Synchronized 底层实现原理【架构师之巅】。  深入理解 synchronized 底层原理，一篇文章就够了【北风 IT 之路】。  Java 并发基石——所谓“阻塞”：Object Monitor 和 AQS（1）。  10 行代码理解 Java 锁消除。  synchronized 和 Lock 有什么区别？  "},{"id":104,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.8-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"2.8 线程池实现原理","section":"2、 Java多线程","content":"线程池实现原理 #  1 前言 #  1.1 什么是线程池 #   线程池（Thread Pool）是一种基于池化思想管理线程的工具，经常出现在多线程服务器中，如 MySQL。 线程过多会带来额外的开销，其中包括创建销毁线程的开销、调度线程的开销等等，同时也降低了计算机的整体性能。 线程池维护多个线程，等待监督管理者分配可并发执行的任务，这种做法，一方面避免了处理任务时创建销毁线程开销的代价，另一方面避免了线程数量膨胀导致的过分调度问题，保证了对内核的充分利用。  1.2 线程池有哪些优点 #   降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。 提高响应速度：任务到达时，无需等待线程创建即可立即执行。 提高线程的可管理性：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性，使用线程池可以进行统一的分配、调优和监控。 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能，比如延时定时线程池ScheduledThreadPoolExecutor，就允许任务延期执行或定期执行。  1.3 线程池解决的问题是什么 #   线程池解决的核心问题就是资源管理问题，在并发环境下，系统不确定在任意时刻中，有多少任务需要执行，有多少资源需要投入，这种不确定性将带来以下若干问题：  频繁申请、销毁资源和调度资源，将带来额外的损耗，可能会非常巨大。 对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。 系统无法合理管理内部的资源分布，会降低系统的稳定性。   为解决资源分配这个问题，线程池采用了池化（Pooling）思想，即为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。 除了线程池以外，池化思想在计算机领域的其他比较典型的几种使用策略包括：  内存池（Memory Pooling）：预先申请内存，提升内存申请速度，减少内存碎片。 连接池（Connection Pooling）：预先申请数据库连接，提升申请连接的速度，降低系统的开销。 实例池（Object Pooling）：循环使用对象，减少资源在初始化和释放时的昂贵损耗。    2 核心设计与实现 #  在前文中，我们了解到线程池是一种通过池化思想，帮助我们管理线程而获取并发性的工具，在 Java 中的体现是 ThreadPoolExecutor，下面我们将详细介绍该类的设计与实现。\n 如无特殊说明，下面内容的叙述基于的 JDK 版本为 JDK 1.8.0_181。\n 2.1 总体设计 #  ThreadPoolExecutor 的继承关系如下图所示：\n 2.1.1 Executor #   提供了一种思想，将任务提交和任务执行进行解耦，用户无需关注如何创建线程，如何调度线程来执行任务，只需要提供 Runnable 对象，将任务的运行逻辑提交到执行器 Executor 中，由 Executor 框架完成线程的调配和任务的执行部分。 void execute(Runnable command);   2.1.2 ExecutorService #    继承了 Executor，提供了管控线程池的方法，比如 shutdown()、submit()，可以说是真正的线程池接口。\nvoid shutdown(); /*启动一次有序的关闭，之前提交的任务执行，但不接受新的任务；这个方法不会等待之前提交的任务执行完毕*/ List\u0026lt;Runnable\u0026gt; shutdownNow(); /*试图停止所有正在执行的任务。暂停处理正在等待的任务，返回一个等待执行的任务列表；这个方法不会等待正在执行的任务终止*/ boolean isShutdown(); /*如果已经被 shutdown，返回 true*/ boolean isTerminated(); /*如果所有任务都已经被终止，返回 true*/ boolean awaitTermination(long timeout, TimeUnit unit) /*在一个 shutdown 请求后，阻塞的等待所有任务执行完毕，或者到达超时时间，或者当前线程被中断*/ throws InterruptedException;   扩充执行任务的能力，补充可以为一个或一批异步任务生成 Future 的方法。\nFuture\u0026lt;?\u0026gt; submit(Runnable task); /*提交一个可执行的任务，返回一个 Future 代表这个任务，等到任务成功执行，Future#get() 方法会返回 null*/ \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Runnable task, T result); /*提交一个可执行的任务，返回一个 Future 代表这个任务，等到任务成功执行，Future#get() 方法会返回这个给定的 result*/ \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task); /*提交一个有返回值的任务，并返回一个 Future 代表等待的任务执行的结果，等到任务成功执行，Future#get() 方法会返回任务执行的结果*/   2.1.3 AbstractExecutorService #   是上层的抽象类，将执行任务的流程串联了起来，保证下层的实现只需关注一个执行任务的方法即可。 public Future\u0026lt;?\u0026gt; submit(Runnable task) { /*提交一个可执行的任务，返回一个 Future 代表这个任务，等到任务成功执行，Future#get() 方法会返回 null*/ if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;Void\u0026gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } public \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Runnable task, T result) { /*提交一个可执行的任务，返回一个 Future 代表这个任务，等到任务成功执行，Future#get() 方法会返回这个给定的 result*/ if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;T\u0026gt; ftask = newTaskFor(task, result); execute(ftask); return ftask; } public \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task) { /*提交一个有返回值的任务，并返回一个 Future 代表等待的任务执行的结果，等到任务成功执行，Future#get() 方法会返回任务执行的结果*/ if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;T\u0026gt; ftask = newTaskFor(task); execute(ftask); return ftask; }   2.1.4 ThreadPoolExecutor #  2.1.4.1 含义 #    实现最复杂的运行部分，一方面维护自己的生命周期，另一方面同时管理线程和任务，使两者良好的结合从而执行并行任务，是线程池的具体实现。\n  ThreadPoolExecutor 的运行机制如下图所示：  线程池内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。 线程池的运行主要分成两部分，分别是任务管理、线程管理：  任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：  直接申请线程执行该任务。 缓冲到队列中等待线程执行。 拒绝该任务。   线程管理部分是消费者，他们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。      2.1.4.2 构造函数 #   ThreadPoolExecutor 的构造函数如下：\npublic ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler)   corePoolSize：\n 线程池的核心线程数，当提交一个任务时，线程池创建一个新线程执行任务，直到线程数等于 corePoolSize，然后继续提交的任务会被保存到阻塞队列中，等待被执行。 如果执行了线程池的 prestartAllCoreThreads() 方法，线程池会提前创建并启动所有核心线程。    maximumPoolSize：\n 线程池中允许的最大线程数，如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务，前提是当前线程数小于 maximumPoolSize。    keepAliveTime：\n 线程空闲时的存活时间，即当线程没有任务执行时，继续存活的时间。 默认情况下，该参数只在线程数大于 corePoolSize 时才有用。    workQueue：\n workQueue必须是 BlockingQueue 阻塞队列。 当线程池中的线程数超过他的 corePoolSize 的时候，线程会进入阻塞队列进行阻塞等待。 通过 workQueue，线程池实现了阻塞功能。 几种典型的阻塞队列及用法如下所示：   ArrayBlockingQueue：\n 一个数组实现的有界队列，按照先进先出（FIFO）的原则对元素进行排序，支持公平锁和非公平锁。    LinkedBlockingQueue：\n 一个由链表结构组成的有界队列，按照先进先出（FIFO）的原则对元素进行排序。 此队列的默认长度为 Integer.MAX_VALUE，所以默认创建的该队列有容量危险。    PriorityBlockingQueue：\n 一个支持线程优先级排序的无界队列，默认自然序进行排序，也可以自定义实现 compareTo() 方法来指定元素排序规则，不能保证同优先级元素的顺序。    DelayQueue：\n 一个实现 PriorityBlockingQueue实现延迟获取的无界队列，在创建元素时，可以指定多久才能从队列中获取当前元素，只有延时期满后才能从队列中获取元素。    SynchronousQueue：\n 一个不存储元素的阻塞队列，每一个 put 操作必须等待 take 操作，否则不能添加元素。 支持公平锁和非公平锁。 SynchronousQueue 的一个使用场景是在线程池里，Executors.newCachedThreadPool() 就使用了SynchronousQueue，这个线程池根据需要（新任务到来时）创建新的线程，如果有空闲线程则会重复使用，线程空闲了 60 秒后会被回收。    LinkedTransferQueue：\n 一个由链表结构组成的无界队列，相比于其他队列，LinkedTransferQueue 队列多了 transfer 和 tryTransfer 方法。    LinkedBlockingDeque：\n 一个由链表结构组成的双向队列，队列头部和尾部都可以添加和移除元素，多线程并发时，可以将锁的竞争最多降到一半。        threadFactory：\n  创建线程的工厂，通过自定义的线程工厂可以给每个新建的线程设置一个具有识别度的线程名。\n  Executors静态工厂里默认的 threadFactory，线程的命名规则是 pool-数字-thread-数字 ：\nstatic class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = \u0026#34;pool-\u0026#34; + poolNumber.getAndIncrement() + \u0026#34;-thread-\u0026#34;; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; } }   Executors 静态工厂创建的几种常用的线程池如下：\n newFixedThreadPool：   创建一个指定工作线程数的线程池，其中参数 corePoolSize和 maximumPoolSize 相等，阻塞队列基于 LinkedBlockingQueue。\n  他是一个典型且优秀的线程池，具有提高线程池执行效率和节省创建线程所耗开销的优点，但是在线程池空闲时，即线程池中没有可运行任务时，他也不会释放工作线程，还会占用一定的资源。\npublic static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory); }    newSingleThreadExecutor：   初始化的线程只有一个线程，如果该线程异常结束，会重新创建一个新的线程继续执行任务。\n  唯一的线程可以保证所提交任务的顺序执行，内部使用 LinkedBlockingQueue作为阻塞队列。\npublic static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory)); }    newCachedThreadPool：   创建一个可缓存工作线程的线程池，默认存活时间60秒。\n  线程池的线程数可达到 Integer.MAX_VALUE，内部使用 SynchronousQueue作为阻塞队列。\n  在没有任务执行时，当线程的空闲时间超过 keepAliveTime，则工作线程会停止，当提交新任务时，如果没有空闲线程，则创建新线程执行任务，会导致一定的系统开销。\npublic static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;(), threadFactory); }    newScheduledThreadPool：   初始化的线程池可以在指定的时间内周期性的执行所提交的任务，在实际的应用场景中可以使用该线程池定期的同步数据。\npublic static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) { return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory); }   ScheduledExecutorService中的定式策略有两种：\n scheduleAtFixedRate：   指的是以固定的频率执行，period指的是两次成功执行之间的时间。\n  比如 scheduleAtFixedRate(command, 5, 2, second)，第一次开始执行是5s后，假如执行耗时1s，那么下次开始执行是7s后，再下次开始执行是9s后。\npublic ScheduledFuture\u0026lt;?\u0026gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit);    scheduleWithFixedDelay：  指的是以固定的延时执行，delay指的是一次执行终止和下一次执行开始之间的延迟。 还是上例，scheduleAtFixedRate(command, 5, 2, second)，第一次开始执行是5s后，假如执行耗时1s，执行完成时间是6s后，那么下次开始执行是8s后，再下次开始执行是11s后。 public ScheduledFuture\u0026lt;?\u0026gt; scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit);             RejectedExecutionHandler：\n 线程池的拒绝策略，当阻塞队列满了，且没有空闲的工作线程，如果继续提交任务，必须采取一种策略处理该任务。 线程池提供了 4 种拒绝策略：  AbortPolicy：  丢弃任务并抛出 RejectedExecutionException，这是线程池默认的拒绝策略，在任务不能再提交的时候，抛出异常，及时反馈程序运行状态。 如果是比较关键的业务，推荐使用此拒绝策略，这样在系统不能承载更大的并发量的时候，能够及时的通过异常发现。   DiscardPolicy：  丢弃任务，但是不抛出异常。 使用此策略，可能会使我们无法发现系统的异常状态，建议是一些无关紧要的业务采用此策略。   DiscardOldestPolicy：  丢弃队列最前面的任务，然后重新提交被拒绝的任务。 是否要采用此种策略，还得根绝实际业务是否允许丢弃老任务来认真衡量。   CallerRunsPolicy：  由调用线程（提交任务的线程）处理该任务。 这种情况是需要让所有任务都执行完毕，比较适合大量计算的任务类型去执行，多线程仅仅是增大吞吐量的手段，最终必须要让每个任务都执行完毕。          2.2 生命周期管理 #    线程池的运行状态，并不是用户显示设置的，而是伴随着线程池的运行，由内部来维护，线程池内部使用一个变量维护两个值，分别为运行状态（runState）和线程池数量（workerCount），具体代码如下所示：\nprivate final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; /*Integer.SIZE = 32;*/ private static final int CAPACITY = (1 \u0026lt;\u0026lt; COUNT_BITS) - 1; // runState is stored in the high-order bits  private static final int RUNNING = -1 \u0026lt;\u0026lt; COUNT_BITS; private static final int SHUTDOWN = 0 \u0026lt;\u0026lt; COUNT_BITS; private static final int STOP = 1 \u0026lt;\u0026lt; COUNT_BITS; private static final int TIDYING = 2 \u0026lt;\u0026lt; COUNT_BITS; private static final int TERMINATED = 3 \u0026lt;\u0026lt; COUNT_BITS; // Packing and unpacking ctl  private static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } private static int ctlOf(int rs, int wc) { return rs | wc; } /*rs：runState，运行状态 wc：wordCount，线程池数量*/  ctl 这个AtomInteger 类型，是对线程池的运行状态和有效线程数量进行控制的一个字段，他同时包含两部分信息，分别是线程池的运行状态和线程池内有效线程的数量，其中，高 3 位用于维护线程池运行状态，低 29 位用于维护线程池中线程数量，两个变量之间互不干扰。 用一个变量去存储两个值，可以避免在做相关决策时出现不一致的情况，不必为了维护两者的一致而占用锁资源。 通过阅读线程池源代码也可以发现，经常出现需要同时判断线程池运行状态和线程数量的情况，线程池也提供了若干方法去供用户获得线程池当前的运行状态、线程个数，这里使用的都是位运算的方式，相比于基本运算，速度也会快很多。    ThreadPoolExecutor 的运行状态有 5 种，具体如下所示：\n RUNNING：能接受新提交的任务，并且也能处理阻塞队列中的任务。 SHUTDOWN：不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务。 STOP：不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。 TIDYING：所有的任务都已终止了，workCount（有效线程数）为 0。 TERMINATED：在 terminated() 方法执行完后进入该状态。    其生命周期转换如下图所示：\n 2.3 任务执行机制 #  2.3.1 任务调度 #   任务调度是线程池的主要入口，当用户提交了一个任务，接下来这个任务将如何执行都是由这个阶段决定的，了解这部分就相当于了解了线程池的核心运行机制。\n  首先，所有任务的调度都是由 execute() 方法完成的，这部分完成的工作是检查线程池的运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务。 当试图通过 execute() 方法将一个Runnable 任务添加到线程池中时，按照如下顺序来处理：   首先检测线程池运行状态，如果不是 RUNNING，则直接拒绝，线程池要保证在 RUNNING 的状态下执行任务。\n  然后检查运行线程数，决定接下来执行的流程：\n 如果线程池中的线程数量少于 corePoolSize，就创建新的线程来执行新添加的任务。 如果线程池中的线程数量大于等于 corePoolSize，但队列 workQueue 未满，则将新添加的任务放到 workQueue 中，按照 FIFO 的原则依次等待执行（线程池中有线程空闲出来后依次将队列中的任务交付给空闲的线程执行）。 如果线程池中的线程数量大于等于 corePoolSize，且队列 workQueue 已满，但线程池中的线程数量小于 maximumPoolSize，则会创建新的线程来处理被添加的任务。 如果线程池中的线程数量等于了 maximumPoolSize，就用 RejectedExecutionHandler 来做拒绝处理。      针对上面最后两张图的说明：\n 在第二张图中，线程池中有 $N$ 个任务，任务 1、任务 2、任务 3 这三个任务在执行，而任务 4 到任务 $N$ 在阻塞队列中等待，在执行任务的 Workers 集合中，包含 3 个Worker，每一个Worker 对应一个Thread 线程，Thread 线程每次处理一个任务。 当Worker 集合中处理完某一个任务之后，会从阻塞队列中取出一个任务来继续执行，如第三张图所示，第三张图表示任务 1 处理完毕之后，线程池将任务 4 从阻塞队列中取出，放到Workers 中进行处理。       2.3.2 任务缓冲 #   任务缓冲模块是线程池能够管理任务的核心部分，线程池的本质是对任务和线程的管理，而做到这一点的关键的思想就是将任务和线程两者解耦，不让两者直接关联，才可以做后续的分配工作。 线程池中是以生产者消费者模式，通过一个阻塞队列来实现的，阻塞队列缓存任务，工作线程从阻塞队列中获取任务。 阻塞队列是一个支持两个附加操作的队列，这两个附加的操作是：  在队列为空时，获取元素的线程会等待队列变为非空。 当队列满时，存储元素的线程会等待队列可用。   阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程，阻塞队列就是生产者存放元素的容器，而消费者也只是从容器里拿元素，如下图所示：  使用不同的队列可以实现不一样的任务存取策略，几种典型的阻塞队列及用法详见 2.1.4.2 构造函数。  2.3.3 任务申请 #   由上文的任务分配部分可知，任务的执行有两种可能：  任务直接由新创建的线程执行，这种情况仅出现在线程初始创建的时候。 线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行，这种情况出现在线程获取任务的绝大多数情况。   线程需要从任务缓存模块中不断地取任务执行，帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块之间的通信，这部分策略由 getTask 方法实现，其执行流程如下图所示：  getTask() 这部分进行了多次判断，为的是控制线程的数量，使其符合线程池的状态，如果线程池现在不应该持有那么多线程，则会返回 null，工作线程 Workder 会不断接收新任务去执行，而当工作线程 Worker 接收不到任务的时候，就会开始被回收。  2.3.4 任务拒绝 #   任务拒绝模块是线程池的保护部分，线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到 maximumPoolSize 时，就需要拒绝掉该服务，采取任务拒绝策略，保护线程池。 拒绝策略是一个接口，用户可以通过实现这个接口去定制拒绝策略，也可以选择 JDK 提供的四种已有拒绝策略，详见 2.1.4.2 构造函数。  2.3.5 Worker 线程管理 #  2.3.5.1 Worker 线程 #    线程池为了掌握线程的状态并维护线程的生命周期，设计了线程池内的工作线程 Worker，部分代码如下：\nprivate final class Worker extends AbstractQueuedSynchronizer implements Runnable { final Thread thread; /*Worker 持有的线程*/ Runnable firstTask; /*初始化的任务，可以为 null*/ }   Worker 这个工作线程，实现了 Runnable 接口，并持有一个线程 thread，一个初始化的任务 firstTask：\n thread：在调用构造方法时通过 ThreadFactory 来创建，可以用来执行任务。 firstTask：用来保存传入的第一个任务，这个任务可以有也可以为 null，如果这个值是非空的，那么线程就会在启动初期立即执行这个任务，也就对应核心线程创建时的情况，如果这个值是null，那么就需要创建一个线程去执行任务列表（workQueue）中的任务，也就是非核心线程的创建。     线程池需要管理线程的生命周期，在线程长时间不运行时需要对其进行回收，线程池使用一张 Hash 表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期，这个时候重要的就是如何判断线程是否在运行。\n  Worker 是通过继承 AQS，使用 AQS 来实现独占锁这个功能，通过实现不可重入的特性去反映线程现在的执行状态：\n lock 方法一旦获取了独占锁，表示当前线程正在执行任务中，此时不应该中断线程。 如果线程现在不是独占锁的状态，也就是空闲的状态，说明他没有在处理任务，这时可以对该线程进行中断。 线程池在执行 shutdown() 方法或 tryTerminate() 方法时会调用 interruptIdleWorkers() 方法来中断空闲的线程，interruptIdleWorkers() 方法会使用tryLock() 方法来判断线程池中的线程是否是空闲状态，如果线程是空闲状态，则可以安全回收。     2.3.5.2 Worker 线程增加 #   增加线程是通过线程池中的addWorker() 方法，该方法的功能就是增加一个线程，不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅是完成增加线程，并使他运行，最后返回是否成功这个结果。 addWorker() 方法有两个参数，分别是firstTask、core：  firstTask：指定新增的线程执行的第一个任务，该参数可以为空。 core：该参数为true，表示在新增线程时会判断当前活动线程数是否少于 corePoolSize，false 表示在新增线程时会判断当前活动线程数是否少于 maximumPoolSize。   addWorker() 方法的执行流程如下图所示：   2.3.5.3 Worker 线程回收 #    线程池中线程的销毁依赖 JVM 的自动回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被 JVM 回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。\n  Worker 被创建出来以后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务，当 Worker 无法获取到任务，也就是获取的任务为空时，循环会结束，Worker 会主动消除自身在线程池内的引用。\ntry { while (task != null || (task = getTask()) != null) { //执行任务  } } finally { processWorkerExit(w, completedAbruptly);//获取不到任务时，主动回收自己 }   需要注意的是：\n 线程回收的工作是在processWorkerExit() 方法中完成的，在这个方法中，将线程引用移除线程池就已经结束了线程销毁的部分，但是由于引起线程销毁的可能性有很多，因此线程池还要判断是什么引发了这次销毁，是否要改变线程池的现阶段状态，是否要根据新状态重新分配线程。     2.3.5.4 Worker 线程执行任务 #   在 Worker 类中的run() 方法调用了runWorker() 方法来执行任务，其执行过程如下：   while循环不断地通过 getTask() 方法获取任务。\n  getTask()方法从阻塞队列中获取任务：\n 如果获取到任务：  如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。 执行任务。   如果没有获取到任务，执行 processWorkerExit() 方法，销毁线程。       参考文献 #    Java 线程池实现原理及其在美团业务中的实践。  Java 线程池 ThreadPoolExecutor 使用和分析(一)。  Java线程池ThreadPoolExecutor使用和分析(二) - execute()原理。  "},{"id":105,"href":"/school-recruitment/docs/java/2Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/2.9-AQS/","title":"2.9 Aqs","section":"2、 Java多线程","content":"1 含义 #   AQS 全称是AbstractQueuedSynchronizer，即抽象对象同步器。 AQS 定义了两种资源共享模式：  独占式：  只能有一个线程占有锁资源，其他竞争资源的线程，在竞争失败后都会进入到等待队列中，等待占有锁资源的线程释放锁，然后再重新被唤醒竞争资源，例如 ReentrantLock 实现的就是独占式的锁资源。   共享式：  允许多个线程同时获取锁，并发访问共享资源，ReentrantWriteLock 和 CountDownLatch 等就是实现的这种模式。     AQS 内部维护了一个 volatile的 state 变量和一个 FIFO（先进先出）的队列：   state：\n 代表的是竞争资源的标识。 AQS 中提供了三种操作 state 的方法： protected final int getState() { return state; } protected final void setState(int newState) { state = newState; } protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this  return unsafe.compareAndSwapInt(this, stateOffset, expect, update); }     FIFO 队列：\n 代表的是竞争资源失败的线程排队时存放的容器。  /** * 竞争资源标识 */ private volatile int state; /** * FIFO 队列，代表的是竞争资源失败的线程排队时存放的容器 */ static final class Node {}    因为 AbstractQueuedSynchronizer是一个抽象类，采用模板方法的设计模式，规定了独占模式和共享模式需要实现的方法，并且将一些通用的功能已经进行了实现，所以不同模式的使用方式，只需要自己定义好实现共享资源的获取与释放即可，至于具体线程在等待队列中的维护（获取资源入队列、唤醒出队列等），AQS 已经实现好了，所以根据共享资源的模式一般实现的方法有如下几个：  boolean isHeldExclusively()：是否为独占模式，只有使用到了 Condition 的，才需要去实现他，例如 ReentrantLock。 boolean tryAcquire(int arg)：独占模式，尝试获取资源，成功返回 true，失败返回 false。 boolean tryRelease(int arg)：独占模式，尝试释放资源，成功返回 true，失败返回 false。 int tryAcquireShared(int arg)：共享模式，尝试获取资源，负数表示失败，0 表示成功，但是没有剩余可用资源了，整数表示成功，且有剩余可用资源。 boolean tryReleaseShared(int arg)：共享模式，尝试释放资源，若资源释放后允许唤醒后续等待节点返回 true，否则返回 false。 上面的这几个方法在 AbstractQueuedSynchronizer 抽象类中都没有被定义为 abstract，说明这些方法都是可以按需实现的，共享概念股模式下可以只实现共享模式的方法（例如 CountDownLatch），独占模式下可以只实现独占模式的方法（例如 ReentrantLock），也支持两种都实现，两种模式都使用（例如 ReentrantReadWriteLock）。    2 源码分析 #  我们先简单介绍 AQS 的两种模式的实现类的代表 ReentrantLock（独占模式）和 CountDownLatch（共享模式），是如何来共享资源的一个过程，然后再详细通过 AQS 的源码来分析整个实现过程：\n ReentrantLock：  在初始化的时候 state = 0，表示资源未被锁定，当 $A$ 线程执行 lock() 方法时，会调用 tryAcquire() 方法，将 AQS 中队列的模式设置为独占，并将独占线程设置为线程 $A$，以及将 state + 1。 这样在线程 $A$ 没有释放锁前，其他线程来竞争锁，调用 tryAcquire() 方法时都会失败，然后竞争锁失败的线程就会进入到队列中。 当线程 $A$ 调用执行 unlock() 方法将 state = 0 后，其他线程才有机会获取锁（注意 ReentrantLock 是可重入的，同一线程多次获取锁时 state 的值会进行累加的，在释放锁的时候也要释放相应的次数才算完全释放了锁）。   CountDownLatch：  CountDownLatch 会将任务分成 $N$ 个子线程去执行，state的初始值也是 $N$（state 与子线程数量一致），$N$个子线程是并行执行的，每个子线程执行完成后 countDown() 一次，state会通过 CAS方式减 1，直到所有子线程执行完成后（state = 0），会通过 unpark() 方法唤醒主线程，然后主线程就会从 await() 方法返回，继续后续操作。    2.1 独占模式 #  2.1.1 Node #    在 AbstractQueuedSynchronizer 的类里面，有一个静态内部类 Node，他代表的是队列中的每一个节点，其中 Node 节点有如下几个属性：\nvolatile int waitStatus; /*节点的状态*/ volatile Node prev; /*当前节点的前一个节点*/ volatile Node next; /*当前节点的后一个节点*/ volatile Thread thread; /*当前节点中所包含的线程对象*/ Node nextWaiter; /*等待队列中的下一个节点*/   waitStatus：\n  代表的是节点的状态，默认为 0。\n  该变量对应的值有以下几种：\nstatic final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3;  CANCELLED = 1：  代表的是当前节点从同步队列中取消。 当 timeout 或被中断（响应中断的情况下），会触发变更为此状态，进入该状态后的节点将不会再变化。   SIGNAL = -1：  代表后继节点处于等待状态。 后继节点入队时，会将前继节点的状态更新为 SIGNAL。   CONDITION = -2：  节点在等待队列中，节点线程等待在 Condition 上。 当其他线程对 Condition 调用了 signal() 方法后，该节点将会从等待队列中转移到同步队列中，加入到对同步状态的获取中。   PROPAGATE = -3：  表示在共享模式下，前继节点在释放资源后会唤醒后继节点，并将这种共享模式传播下去。      节点状态中通常负数值表示节点处于有效的等待状态，而正数值代表节点已经被取消了，源码中有很多地方通过节点状态的正负来判断队列中的节点是否正常。\n    prev：\n 代表的是当前节点的前一个节点。    next：\n 代表的是当前节点的后一个节点。    thread：\n 代表的是当前节点中所包含的线程对象。    nextWaiter：\n 代表的是等待队列中的下一个节点。      2.1.2 ReentrantLock #  ReentrantLock默认是非公平锁，就是说，线程在竞争锁的时候并不是按照先来后到的顺序来获取锁的，但是 ReentrantLock 也是支持公平锁的，在创建的时候传入一个参数值即可，如无特殊说明，下面对 ReentrantLock 加锁和解锁过程的分析是以 ReentrantLock 默认情况为基础。\n2.1.2.1 加锁过程 #    ReentrantLock 并没有直接继承 AQS 类，而是通过内部类来继承 AQS 类的。\n  我们在用 ReentrantLock 加锁的时候都是调用 lock() 方法，在默认非公平锁下，lock() 的源码如下：\n/** * Sync object for non-fair locks */ static final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } }   通过源码我们可以看到 lock() 方法首先是通过 CAS 的方式抢占锁，如果抢占成功，则将 state 的值设为 1，然后将对象独占线程设置为当前线程：\nprotected final void setExclusiveOwnerThread(Thread thread) { exclusiveOwnerThread = thread; }   如果抢占锁失败，就会调用 acquire() 方法，这个 acquire() 方法的实现就是在 AQS 类中，说明具体抢占锁失败后的逻辑，AQS 已经规定好了模板：\npublic final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); }   上面已经介绍了，独占模式是需要实现 tryAcquire() 方法的，这里首先就是通过 tryAcquire() 方法抢占锁，如果成功返回 true，失败返回 false，tryAcquire() 方法的具体实现，是在 ReentrantLock 里面的，AQS 类中默认是直接抛出异常的，tryAcquire() 方法的逻辑如下：\n  首先获取 state 值，如果 state 值为 0，说明无锁，那么通过 CAS 尝试加锁，成功后，将独占线程设置为当前线程。\n  如果 state 值不为 0，并且当前的独占线程和当前线程为同一线程，那么 state重入次数加 1。\n  如果 state 值不为 0，并且当前线程不是独占线程，直接返回 false。\n/** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); // 获取 state 值  int c = getState(); if (c == 0) { // 如果 state 值为 0，说明无锁，那么通过 CAS 尝试加锁，成功后，将独占线程设置为当前线程  if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { // 如果是同一个线程再次来获取锁，那么就将 state 的值进行加 1 处理（可重入锁的重入次数）  int nextc = c + acquires; if (nextc \u0026lt; 0) // overflow  throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } // state 值不为 0，并且当前线程不是独占线程，直接返回 false  return false; }     我们继续来看 acquire() 方法，在执行完 tryAcquire() 方法后，如果加锁失败那么就会执行 addWaiter() 方法和 acquireQueued() 方法，这两个方法的作用是将竞争锁失败的线程放入到等待队列中：\n addWaiter()：  该方法主要做了三件事：   将当前线程封装成 Node。\n  判断队列中尾部节点是否为空，若不为空，则将当前线程的 Node 节点通过 CAS 插入到尾部。\n  如果尾部节点为空或 CAS 插入失败，则通过 enq() 方法插入到队列中。\nprivate Node addWaiter(Node mode) { // 1. 将当前线程封装成 Node  Node node = new Node(Thread.currentThread(), mode); // 2. 判断队列中尾部节点是否为空，若不为空，则将当前线程的 Node 节点通过 CAS 插入到尾部  Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 3. 如果尾部节点为空或 CAS 插入失败，则通过 enq() 方法插入到队列中  enq(node); return node; }    enq() 方法主要就是通过自旋将数据插入到队列中：   当队列为空时，将当前节点设置为头结点和尾节点。\n  进入二次循环后，将 node 添加到尾部。\n/** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node\u0026#39;s predecessor */ private Node enq(final Node node) { // 看到死循环，就明白是通过自选咯  for (;;) { // 当 tail 节点为空时，直接将当前节点设置成尾部节点，并插入到队列中，以及设置他为 head 节点  Node t = tail; if (t == null) { if (compareAndSetHead(new Node())) tail = head; } else { // 若是因为在 addWaiter() 方法中插入失败或第二次进入循环，那么将当前线程的前级节点指向尾部节点，并通过 CAS 方式将尾部节点指向当前线程的节点  node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } }    这样 addWaiter() 方法就构造了一个队列，并将当前线程添加到了队列中了。   acquireQueued()：  该方法主要做了以下几件事：   首先获取节点的前级节点。\n  如果当前节点的前级节点是 head，那么就可以去抢占锁了。\n  抢占成功后就将新节点设置为 head，原来的 head 置为空。\n  如果抢占锁失败，则根据 waitStatus 值决定是否挂起线程。\n  最后，通过 cancelAcquire() 取消获取锁操作。\n/** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return {@code true} if interrupted while waiting */ final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { // 1. 获取前级节点，如果为 null，则抛出异常  final Node p = node.predecessor(); if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { // 2. 如果前级节点为 head，并且执行抢占锁成功，则  // 1. 将当前节点设置为新的 head 节点。  // 2. 将原来的 head 节点指向 null，方便进行垃圾回收  setHead(node); p.next = null; // help GC  failed = false; return interrupted; } // 3. 如果当前节点不为 head，或者抢占锁失败，就根据节点的状态决定是否需要挂起线程  if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) // 4. 如果获取锁失败，则取消获取锁操作  cancelAcquire(node); } } /** * Returns previous node, or throws NullPointerException if null. * Use when predecessor cannot be null. The null check could * be elided, but is present to help the VM. * * @return the predecessor of this node */ final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; }    下面看一下 shouldParkAfterFailedAcquire() 和 parkAndCheckInterrupt() 这两个方法是如何挂起线程的：  shouldParkAfterFailedAcquire()：   首先获取前级节点的 waitStatus。\n  如果前级节点的 waitStatus 值为 SIGNAL(-1)，说明当前节点也已经在等待唤醒了，直接返回 true。\n  如果前级节点的 waitStatus 大于 0，说明前级节点已经取消了，那么会继续向前找，直到找到的节点不是取消状态（waitStatus \u0026gt; 0），然后将其设置为当前节点的前级节点。\n  如果前级节点为 0 或者其他不为-1 的小于 0 的值，则将当前节点的前级节点设置为 SIGNAL(-1)。\n/** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev. * * @param pred node\u0026#39;s predecessor holding status * @param node the node * @return {@code true} if thread should block */ private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { // 获取前级节点的 waitStatus  int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 如果前级节点的 waitStatus 值为 SIGNAL(-1)，说明当前节点也已经在等待唤醒了，直接返回 true  return true; if (ws \u0026gt; 0) { // 如果前级节点的 waitStatus 大于 0，说明前级节点已经取消了，那么会继续向前找，直到找到的节点不是取消状态（waitStatus \u0026gt; 0），然后将其设置为当前节点的前级节点  do { node.prev = pred = pred.prev; } while (pred.waitStatus \u0026gt; 0); pred.next = node; } else { // 如果前级节点为 0 或者其他不为 -1 的小于 0 的值，则将当前节点的前级节点设置为 SIGNAL(-1)  compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; }    parkAndCheckInterrupt()：   该方法的作用就是挂起线程。\n  如果 shouldParkAfterFailedAcquire() 方法执行成功，会执行 parkAndCheckInterrupt() 方法，他通过 LockSupport.park() 方法，将当前线程挂起（WAITING），然后需要 LockSupport.unpark() 方法唤醒他，通过这样一种 FIFO 机制的等待，来实现 Lock 操作。\n/** * Convenience method to park and then check if interrupted * * @return {@code true} if interrupted */ private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); }   LockSupport 是 JDK 从 1.6 开始提供的一个线程同步源语工具类，在这里主要用到了他的两个方法，分别是挂起线程和唤醒线程：\npublic static void park(Object blocker) { Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); } public static void unpark(Thread thread) { if (thread != null) UNSAFE.unpark(thread); }   LockSupport 的挂起和唤醒线程都是不可重入的，他有一个许可标志，当调用 park() 时，会将许可设置为 0，挂起线程，如果再调用一次 park()，会阻塞线程，当调用 unpark() 时才会将许可标志设置成 1。\n          2.1.2.2 释放锁过程 #    ReentrantLock 释放锁的过程主要有两个阶段：\n 释放锁。 唤醒挂起的线程。    unlock() 方法的源码如下：\npublic void unlock() { sync.release(1); }   释放锁的方法是写在父类 AbstractQueuedSynchronizer 中的，主要包括如下过程：\n 尝试释放资源：   释放成功后，判断头结点的状态是否为无锁状态，如果不为无锁状态，就将头结点中的线程唤醒。\n  释放资源失败，直接返回 false。\npublic final boolean release(int arg) { // 尝试释放资源  if (tryRelease(arg)) { Node h = head; // 释放成功后，判断头结点的状态是否为无锁状态，如果不为无锁状态，就将头结点中的线程唤醒  if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h); return true; } // 释放资源失败，直接返回 false  return false; }       释放资源的过程如下：\n  从 state 中减去传入参数的相应值（一般为 1）。\n  判断释放资源的线程与独占锁现有线程是否一致，如果不一致，则为非法线程释放，直接抛出异常。\n  因为可重入机制，所以每次重入 state 值都加 1，所以在释放的时候也要相应的减 1，直到 state 的值为 0 才算完全的释放锁资源，完全释放锁资源后，将独占线程设置为 null，这样后面的竞争线程才有可能抢占。\n  最后对 state 重新赋值。\nprotected final boolean tryRelease(int releases) { // 从 state 中减去传入参数的相应值（一般为 1）  int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) // 当释放资源的线程与独占锁现有线程不一致时，为非法线程释放，直接抛出异常  throw new IllegalMonitorStateException(); boolean free = false; // 这里是处理重入锁的机制，因为可重入机制，所以每次重入 state 值都加 1  // 所以在释放的时候也要相应的减 1，直到 state 的值为 0 才算完全的释放锁资源  if (c == 0) { free = true; // 完全释放资源后，将独占线程设置为 null，这样后面的竞争线程才有可能抢占  setExclusiveOwnerThread(null); } // 重新赋值 state  setState(c); return free; }     释放了资源后，我们再看唤醒挂起线程时的过程，这个过程就在 unparkSuccessor() 方法中，主要过程如下：\n  首先获取当前节点的等待状态，一般是头结点，占有锁的节点是在头结点上，如果该节点没有处于取消状态，那么将当前节点的线程的状态值设为 0，成为无锁状态。\n  然后获取下一个需要唤醒的节点线程，如果获取到的节点线程为空或已经取消，就从队列的后面向前找，直到找到一个未取消的节点。\n 在寻找可以唤醒的节点时，为什么要从后向前找？ 线程唤醒的时候，通常是从当前线程的下个节点线程开始寻找，但是下个节点有可能已经取消了或者为 null 了，所以从后想起按找，直到找到一个非取消状态的节点线程。\n   最后如果我们获得的下一个可以唤醒的节点线程不为空，那么就唤醒他。\n/** * Wakes up node\u0026#39;s successor, if one exists. * * @param node the node */ private void unparkSuccessor(Node node) { // 获取当前节点的等待状态，一般是头结点，占有锁的节点是在头结点上  int ws = node.waitStatus; if (ws \u0026lt; 0) // 将当前节点的线程的状态值设为 0，成为无锁状态  compareAndSetWaitStatus(node, ws, 0); // 获取下一个需要唤醒的节点线程  Node s = node.next; if (s == null || s.waitStatus \u0026gt; 0) { // 如果获取到的节点线程为空或已经取消，就从队列的后面向前找，直到找到一个未取消的节点  s = null; for (Node t = tail; t != null \u0026amp;\u0026amp; t != node; t = t.prev) if (t.waitStatus \u0026lt;= 0) s = t; } if (s != null) // 如果获得的下一个可以唤醒的节点线程不为空，那么就唤醒他  LockSupport.unpark(s.thread); }     2.2 共享模式 #  2.2.1 CountDownLatch #  2.2.1.1 获取资源 #    在使用 CountDownLatch 时，是先创建 CountDownLatch 对象，然后在每次执行完一个任务后，就执行一次 countDown() 方法，直到通过 getCount() 获取到的值为 0 时才算执行完，如果 count 值不为 0，可通过 await() 方法让主线程进行等待，直到所有任务都执行完成，count的值被设为 0。\n  我们先来看创建 CountDownLatch 的方法：\npublic CountDownLatch(int count) { if (count \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;count \u0026lt; 0\u0026#34;); this.sync = new Sync(count); } Sync(int count) { setState(count); } 我们看到创建 CountDownLatch 的过程，其实就是将 count 值赋值给 state 的过程。\n  再来看 await() 方法的源码：\npublic void await() throws InterruptedException { // 可中断的获取共享资源的方法  sync.acquireSharedInterruptibly(1); } public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) // 如果线程已经中断，直接抛出异常结束  throw new InterruptedException(); if (tryAcquireShared(arg) \u0026lt; 0) // 尝试获取共享资源，获取失败后，自旋入队列  doAcquireSharedInterruptibly(arg); } 整个 await() 的等待过程是先尝试获取共享资源，获取成功则执行任务，获取失败，则调用方法自旋式进入队列。\n  最初在介绍 AQS 的时候就说过，共享模式下是需要自己去实现 tryAcquireShared() 方法来获取共享资源的，那么我们看看 CountDownLatch 是如何实现共享资源的：\nprotected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } 简单易懂，就一行代码，直接获取 state 值，等于 0 就是成功，不等于 0 就失败。\n  那么获取资源失败后，doAcquireSharedInterruptibly() 方法是如何执行的呢，源码如下：\nprivate void doAcquireSharedInterruptibly(int arg) throws InterruptedException { // addWaiter() 方法已经总结过了，这一步操作的目的就是将当前线程封装成节点加入队尾，并设置成共享模式  final Node node = addWaiter(Node.SHARED); boolean failed = true; try { for (;;) { // 获取前级节点  final Node p = node.predecessor(); if (p == head) { // 如果前级节点是头结点，直接尝试获取共享资源  int r = tryAcquireShared(arg); if (r \u0026gt;= 0) { // 如果获取共享资源成功，将 head 节点指向自己  setHeadAndPropagate(node, r); // 将原 head 节点指向空，方便垃圾回收  p.next = null; failed = false; return; } } // 如果前级节点不是 head 节点，就根据前级节点状态，判断是否需要挂起线程  if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) throw new InterruptedException(); } } finally { if (failed) // 如果执行失败，取消获取共享资源的操作  cancelAcquire(node); } } 这里的方法和独占模式下 acquireQueued() 方法很像，只是在设置头结点唤醒新线程的时候有所不同，在 setHeadAndPropagate() 方法里面：\nprivate void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below  setHead(node); // 如果在唤醒完下一个节点后，资源还有剩余，并且新唤醒的节点不为无效状态，就继续唤醒队列中的后面节点里的线程  if (propagate \u0026gt; 0 || h == null || h.waitStatus \u0026lt; 0 || (h = head) == null || h.waitStatus \u0026lt; 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); } } setHeadAndPropagate() 这个方法名称翻译成中文是「设置头结点并传播」，其实就是在获取共享锁资源的时候，如果资源除了用于唤醒下一个节点后，还有剩余，就会用于唤醒后面的节点，直到资源被用完，充分体现了共享模式的「共享」。\n  2.2.1.2 释放资源 #    我们再来看 countDown() 方法是如何释放资源的，源码如下：\npublic void countDown() { sync.releaseShared(1); }   CountDownLatch 中内部类 Sync 的 releaseShared()方法，是使用的 AQS 的 releaseShared() 方法：\npublic final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { // 尝试释放资源，如果释放资源成功，则唤醒节点  doReleaseShared(); return true; } return false; }   尝试释放资源方法 tryReleaseShared() 是 AQS 规定需要自己来实现的，CountDownLatch 的实现如下：\nprotected boolean tryReleaseShared(int releases) { // Decrement count; signal when transition to zero  for (;;) { int c = getState(); if (c == 0) // 如果 state 为 0，说明已经不需要释放资源了，直接返回 false  return false; int nextc = c-1; if (compareAndSetState(c, nextc)) // 真正的释放资源，是通过 CAS 的方式将 state 的值减 1  return nextc == 0; } }   释放资源成功后，就到了唤醒节点的过程了，在 doReleaseShared() 方法中：\nprivate void doReleaseShared() { for (;;) { Node h = head; if (h != null \u0026amp;\u0026amp; h != tail) { // 当头结点不为空，并且不等于尾节点时，从头开始唤醒  int ws = h.waitStatus; // 获取头结点的等待状态  if (ws == Node.SIGNAL) { // 如果头结点状态为等待唤醒，那么将头结点的状态设置为无锁状态，若 CAS 设置节点状态失败，就自旋  if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases  unparkSuccessor(h); // 唤醒头结点  } // 如果 head 节点的状态已经为无锁状态了，那么将 head 节点状态设置为可以向下传播唤醒的状态（PROPAGATE）  else if (ws == 0 \u0026amp;\u0026amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS  } // 若在执行过程中 head 节点发生变化，直接跳出循环  if (h == head) // loop if head changed  break; } }   参考文献 #    你来讲讲 AQS 是什么吧？都是怎么用的？  "},{"id":106,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.1-%E5%88%A0%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/","title":"2.3.1 删除有序数组中的重复项","section":"2.3 数组","content":"删除有序数组中的重复项 #  2.4.1 题目 #  给你一个有序数组 nums ，请你原地删除重复出现的元素，使每个元素只出现一次 ，返回删除后数组的新长度。\n不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。\n说明:\n为什么返回数值是整数，但输出的答案是数组呢?\n请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。\n你可以想象内部操作如下:\n// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝 int len = removeDuplicates(nums);// 在函数里修改输入数组对于调用者是可见的。 // 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。 for (int i = 0; i \u0026lt; len; i++) { print(nums[i]); } 示例 1：\n输入：nums = [1,1,2] 输出：2, nums = [1,2] 解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。 示例 2：\n输入：nums = [0,0,1,1,1,2,2,3,3,4] 输出：5, nums = [0,1,2,3,4] 解释：函数应该返回新的长度 5 ， 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4 。不需要考虑数组中超出新长度后面的元素。 2.4.2 问题分析 #  对于数组相关的算法问题，有一个通用的技巧：要尽量避免在中间删除元素，而是想办法把这个元素换到最后去。 这样的话，最终待删除的元素都拖在数组尾部，一个一个 pop 掉就行了，每次操作的时间复杂度也就降低到 O(1) 了。\n按照这个思路，又可以衍生出解决类似需求的通用方式：双指针技巧。具体一点说，应该是快慢指针。\n我们让慢指针 slow 走在后面，快指针 fast 走在前面探路，找到一个不重复的元素就告诉 slow 并让 slow 前进一步。这样当 fast 指针遍历完整个数组 nums 后，nums[0..slow]就是不重复元素，之后的所有元素都是重复元素。\n2.4.3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/22 18:55 * @Description 删除有序数组中的重复项 */ public class L26 { /** * 26.删除有序数组中的重复项 * * @param nums 数组 * @return 删除后数组的新长度 */ public static int removeDuplicates(int[] nums) { int slow = 0, fast = slow + 1; while (fast \u0026lt; nums.length) { if (nums[fast] != nums[fast - 1]) { slow++; nums[slow] = nums[fast]; } fast++; } return slow + 1; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[] nums = {1, 1, 2}; int len = removeDuplicates(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, len, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); nums = new int[]{0, 0, 1, 1, 1, 2, 2, 3, 3, 4}; len = removeDuplicates(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, len, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); nums = new int[]{1, 1, 2}; len = removeDuplicates(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, len, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } "},{"id":107,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.2-%E6%89%BE%E5%87%BA%E6%95%B0%E7%BB%84%E6%B8%B8%E6%88%8F%E7%9A%84%E8%B5%A2%E5%AE%B6/","title":"2.3.2 找出数组游戏的赢家","section":"2.3 数组","content":"找出数组游戏的赢家 #  1 题目 #  给你一个由 不同 整数组成的整数数组 arr 和一个整数 k 。\n每回合游戏都在数组的前两个元素（即 arr[0] 和 arr[1] ）之间进行。比较 arr[0] 与 arr[1] 的大小，较大的整数将会取得这一回合的胜利并保留在位置 0 ，较小的整数移至数组的末尾。当一个整数赢得 k 个连续回合时，游戏结束，该整数就是比赛的 赢家 。\n返回赢得比赛的整数。\n题目数据 保证 游戏存在赢家。\n示例 1：\n输入：arr = [2,1,3,5,4,6,7], k = 2 输出：5 解释：一起看一下本场游戏每回合的情况：因此将进行 4 回合比赛，其中 5 是赢家，因为它连胜 2 回合。 示例 2：\n输入：arr = [3,2,1], k = 10 输出：3 解释：3 将会在前 10 个回合中连续获胜。 示例 3：\n输入：arr = [1,9,8,2,3,7,6,4,5], k = 7 输出：9 示例 4：\n输入：arr = [1,11,22,33,44,55,66,77,88,99], k = 1000000000 输出：99 提示：\n 2 \u0026lt;= arr.length \u0026lt;= 10^5 1 \u0026lt;= arr[i] \u0026lt;= 10^6 arr 所含的整数 各不相同 。 1 \u0026lt;= k \u0026lt;= 10^9  2 解题思路 #  2.1 常规解法 #  2.1.1 问题解析 #  按照题目所述，每次将较小的元素移动到数组的最后，将较大的元素放到数组的第一位，但是这样做会超时。\n2.1.2 参考代码 #  /** * 将指定位置的数移到数组的某尾 * @param arr 数组 * @param i 指定位置 * @return 移动元素后的数组 */ public int[] moveToLast(int[] arr, int i) { int temp = arr[i]; int n = arr.length; for (int j = i;j \u0026lt;= n - 2;j++) { arr[j] = arr[j + 1]; } arr[n - 1] = temp; return arr; } /** * 1535. 找出数组游戏的赢家（版本 1：常规解法） * 给你一个由 不同 整数组成的整数数组 arr 和一个整数 k 。 * 每回合游戏都在数组的前两个元素（即 arr[0] 和 arr[1] ）之间进行。比较 arr[0] 与 arr[1] 的大小，较大的整数将会取得这一回合的胜利并保留在位置 0 ，较小的整数移至数组的末尾。当一个整数赢得 k 个连续回合时，游戏结束，该整数就是比赛的 赢家 。 * 返回赢得比赛的整数。 * 题目数据 保证 游戏存在赢家。 * @param arr 数组 * @param k 需要连续赢得回合的次数 * @return 赢得比赛的整数 */ public int getWinnerV1(int[] arr, int k) { // 定义赢取回合的元素和赢取回合的次数  int winNum = 0, winCount = 0; while (true) { if (arr[0] \u0026gt; arr[1]) { // 第一个元素大于第二个元素  if (arr[0] == winNum) { // 如果和上一个赢取回合的元素一样，则将其赢取回合的次数加 1  winCount++; // 如果一个元素赢取回合的次数等于 k，则返回第一个元素  if (winCount == k) {return arr[0];} } else { // 如果和上一个赢取回合的元素不一样，则将第一个元素置为赢取回合的元素，同时将赢取回合的次数加 1  winNum = arr[0]; winCount = 1; } // 将第 1 个元素移到最后一个位置  moveToLast(arr, 1); } else { // 第二个元素大于第一个元素  if (arr[1] == winNum) { // 如果和上一个赢取回合的元素一样，则将其赢取回合的次数加 1  winCount++; // 如果一个元素赢取回合的次数等于 k，则返回第二个元素  if (winCount == k) {return arr[1];} } else { // 如果和上一个赢取回合的元素不一样，则将第二个元素置为赢取回合的元素，同时将赢取回合的次数加 1  winNum = arr[1]; winCount = 1; } // 将第一个元素和第二个元素互换位置  int temp = arr[0]; arr[0] = arr[1]; arr[1] = temp; // 将第 2 个元素移到最后一个位置  moveToLast(arr, 1); } } } 2.2 单链表解法 #  2.2.1 问题解析 #  将数组中的元素先存储在单链表中，然后每次将较小的元素移动到单链表的尾部，将较大的元素移动到单链表的头部，相比于 2.1 中的常规解法来说，这种解法移动元素的时间复杂度较低，但是空间复杂度较高，因为需要多余的空间来存储数组中的元素，同时该算法也会超时。\n2.2.2 参考代码 #  /** * 1535. 找出数组游戏的赢家（版本 2：单链表） * 给你一个由 不同 整数组成的整数数组 arr 和一个整数 k 。 * 每回合游戏都在数组的前两个元素（即 arr[0] 和 arr[1] ）之间进行。比较 arr[0] 与 arr[1] 的大小，较大的整数将会取得这一回合的胜利并保留在位置 0 ，较小的整数移至数组的末尾。当一个整数赢得 k 个连续回合时，游戏结束，该整数就是比赛的 赢家 。 * 返回赢得比赛的整数。 * 题目数据 保证 游戏存在赢家。 * @param arr 数组 * @param k 需要连续赢得回合的次数 * @return 赢得比赛的整数 */ public int getWinnerV2(int[] arr, int k) { // 定义赢取回合的元素和赢取回合的次数  int winNum = 0, winCount = 0, n = arr.length;; ListNode head = new ListNode(), p = head, tail = null; for (int i = 0; i \u0026lt; n; i++) { ListNode node = new ListNode(arr[i]); p.next = node; p = node; if (i == n - 1) { tail = p; } } head = head.next; p = head; while (true) { int first = p.val; int second = p.next.val; if (first \u0026gt; second) { // 第一个元素大于第二个元素  if (first == winNum) { // 如果和上一个赢取回合的元素一样，则将其赢取回合的次数加 1  winCount++; // 如果一个元素赢取回合的次数等于 k，则返回第一个元素  if (winCount == k) {return first;} } else { // 如果和上一个赢取回合的元素不一样，则将第一个元素置为赢取回合的元素，同时将赢取回合的次数加 1  winNum = first; winCount = 1; } // 将第 2 个元素移到最后一个位置  ListNode q = p.next; p.next = q.next; tail.next = q; q.next = null; tail = q; } else { // 第二个元素大于第一个元素  if (second == winNum) { // 如果和上一个赢取回合的元素一样，则将其赢取回合的次数加 1  winCount++; // 如果一个元素赢取回合的次数等于 k，则返回第二个元素  if (winCount == k) {return second;} } else { // 如果和上一个赢取回合的元素不一样，则将第二个元素置为赢取回合的元素，同时将赢取回合的次数加 1  winNum = second; winCount = 1; } // 将第一个元素和第二个元素互换位置  ListNode q = p.next; p.next = q.next; q.next = p; p = q; // 将第 2 个元素移到最后一个位置  q = p.next; p.next = q.next; tail.next = q; q.next = null; tail = q; } } } 2.3 优化解法 #  2.3.1 问题解析 #  根据题目描述，每回合游戏之后，较小的整数移至数组的某尾，其实，并不需要对数组进行更新：\n 在第一回合游戏之后，无论 $arr[0]$ 和 $arr[1]$ 当中谁取得胜利，第二回合游戏的另一个整数一定是 $arr$ 中的下一个整数。 推广到一般的情况，当 $ 2 \\le i \\lt arr.length$ 时，第 $i$ 回合的游戏一定在第 $i-1$ 回合游戏中取得胜利的整数和 $arr[i]$ 之间进行。  因此，我们只需要记录一下第 $i-1$ 回合中取得胜利的整数 $winNum$ 以及截止到当前连胜的回合次数 $winCount$：\n 当 $arr[i]$ 大于 $winNum$ 时，将 $winNum$ 更新为 $arr[i]$，同时将 $winCount$ 置为 1。 当 $arr[i]$ 小于 $winNum$ 时，将 $winCount$ 加 1，同时判断 $winCount$ 是否大于等于 $k$，如果是的话，直接返回 $arr[i]$ 即可。  2.3.2 参考代码 #  /** * 1535. 找出数组游戏的赢家（版本 3：不用移动元素，挨个遍历） * 给你一个由 不同 整数组成的整数数组 arr 和一个整数 k 。 * 每回合游戏都在数组的前两个元素（即 arr[0] 和 arr[1] ）之间进行。比较 arr[0] 与 arr[1] 的大小，较大的整数将会取得这一回合的胜利并保留在位置 0 ，较小的整数移至数组的末尾。当一个整数赢得 k 个连续回合时，游戏结束，该整数就是比赛的 赢家 。 * 返回赢得比赛的整数。 * 题目数据 保证 游戏存在赢家。 * @param arr 数组 * @param k 需要连续赢得回合的次数 * @return 赢得比赛的整数 */ public int getWinnerV3(int[] arr, int k) { int winNum = arr[0], winCount = 0; for (int i = 1; i \u0026lt; arr.length; i++) { if (arr[i] \u0026gt; winNum) { winNum = arr[i]; winCount = 1; } else {winCount++;} if (winCount \u0026gt;= k) {break;} } return winNum; } 3 参考文献 #    1535. 找出数组游戏的赢家。  找出数组游戏的赢家『官方解法』。  "},{"id":108,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.4-%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E6%B6%88%E5%A4%B1%E7%9A%84%E5%85%83%E7%B4%A0/","title":"2.3.4 如何寻找消失的元素","section":"2.3 数组","content":"如何寻找消失的元素 #  1 题目 #  数组 nums 包含从 0 到 n 的所有整数，但其中缺了一个。请编写代码找出那个缺失的整数。你有办法在 O(n)时间内完成吗？\n注意： 本题相对书上原题稍作改动\n示例 1：\n输入：[3,0,1] 输出：2 示例 2：\n输入：[9,6,4,2,3,5,7,0,1] 输出：8 2 不同解法 #  2.1 方法一：位运算 #  对于异或运算（^），我们知道他有一个特殊的性质：一个数和他本身做异或运算结果为 0，一个数和 0 做异或运算还是他本身。\n而且异或运算满足交换律和结合律，也就是说：\n2^3^2 = 3^(2^2) = 3^0 = 3 而这道题就可以通过这些性质巧妙算出缺失的那个元素。比如说 nums = [0, 3, 1, 4]：\n 为了容易理解，我们假设先把索引补一位，让后让每个元素和自己相等的索引相对应：\n 这样做了之后，就可以发现除了缺失元素之外，所有的索引和元素都组成一对儿了，现在如果把这个落单的索引 2 找出来，也就找到了缺失的那个元素。\n如何找这个落单的数字呢，只要把所有的元素和索引做异或运算，成对儿的数字都会消为 0，只有这个落单的元素会剩下，这也就达到了我们的目的。\n/** * 面试题 17.04. 消失的数字（版本 2：位运算） * @param nums 数字集合 * @return */ public int missingNumberV2(int[] nums) { int res = 0; int n = nums.length; res ^= n; for (int i = 0; i \u0026lt; n; i++) { res ^= i ^ nums[i]; } return res; }  由于异或运算满足交换律和结合律，所以总是能把成对儿的数字消去，留下缺失的那个元素。\n2.2 方法二：等差数列 #  题目的意思我们可以这样理解：现在有个等差数列 [0,1,2..n]，其中少了某一个数字，需要我们把它找出来，这个数字就是 sum(0,1,2..n)-sum(nums)。\n/** * 面试题 17.04. 消失的数字（版本 3：等差数列） * @param nums 数字集合 * @return */ public int missingNumberV3(int[] nums) { int res = 0; int n = nums.length; // 求和公式：(首项 + 某项) * 元素个数 / 2  int expect = (0 + n) * (n + 1) / 2; for (int i = 0; i \u0026lt; n; i++) { res += nums[i]; } return expect - res; } 但是我们需要考虑的是在用求和公式计算 expect 时可能会出现相乘的结果太大导致整型溢出的情况。我们刚才的思路是把两个和都加起来然后相减，为了避免溢出，干脆一边求和一边减算了。很类似刚才位运算解法的思路，仍然假设 nums = [0, 3, 1, 4]，先补一位索引再让元素跟索引配对：\n 我们让每个索引减去其对应的元素，再把相减的结果加起来，就是那个缺失的元素。\n/** * 面试题 17.04. 消失的数字（版本 1） * @param nums 数字集合 * @return */ public int missingNumberV1(int[] nums) { int res = 0; int n = nums.length; res += n - 0; for (int i = 0; i \u0026lt; n; i++) { res += i - nums[i]; } return res; } "},{"id":109,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.5-%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E7%BC%BA%E5%A4%B1%E5%92%8C%E9%87%8D%E5%A4%8D%E7%9A%84%E5%85%83%E7%B4%A0/","title":"2.3.5 如何寻找缺失和重复的元素","section":"2.3 数组","content":"如何寻找缺失和重复的元素 #  1 题目 #  集合 s 包含从 1 到 n 的整数。不幸的是，因为数据错误，导致集合里面某一个数字复制了成了集合里面的另外一个数字的值，导致集合 丢失了一个数字 并且 有一个数字重复 。\n给定一个数组 nums 代表了集合 S 发生错误后的结果。\n请你找出重复出现的整数，再找到丢失的整数，将它们以数组的形式返回。\n示例 1：\n输入：nums = [1,2,2,4] 输出：[2,3] 示例 2：\n输入：nums = [1,1] 输出：[1,2] 2 问题解析 #  这个问题的特点是，每个元素和数组索引有一定的对应关系，。\n我们可以暂且将 nums 中的元素变为 [0..N-1]，这样每个元素就和一个数组索引完全对应了，这样方便理解一些。\n现在的问题是，有一个元素重复了，同时导致一个元素缺失了，这会导致有两个元素对应到了同一个索引，而且会有一个索引没有元素对应过去。\n所以我们如果可以通过某种方法，找到那个重复元素对应的索引，也就相当于找到了那个重复元素，找到那个没有元素对应的索引，也就找到了那个缺失的元素。\n我们可以通过将每个索引对应的元素变成负数，以表示这个索引被对应过一次了。 如果出现重复元素 4，直观结果就是，索引 4 所对应的元素已经是负数了。\n 对于缺失元素 3，直观结果就是，索引 3 所对应的元素是正数。\n 对于这种数组问题，关键点在于元素和索引是成对儿出现的，常用的方法是排序、异或、映射。\n3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.Arrays; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/26 14:32 * @Description 错误的集合 */ public class L645 { /** * 645. 错误的集合（版本 2） * 集合 s 包含从 1 到 n 的整数。不幸的是，因为数据错误，导致集合里面某一个数字复制了成了集合里面的另外一个数字的值，导致集合 丢失了一个数字 并且 有一个数字重复 。 * 给定一个数组 nums 代表了集合 S 发生错误后的结果。 * 请你找出重复出现的整数，再找到丢失的整数，将它们以数组的形式返回。 * @param nums 集合 * @return 重复出现的整数和丢失的整数组成的集合 */ public static int[] findErrorNumsV2(int[] nums) { int n = nums.length; int dup = -1, missing = -1; for (int i = 0; i \u0026lt; n; i++) { int index = Math.abs(nums[i]) - 1; // 相当于在集合后面又加了一个元素 0  if (nums[index] \u0026lt; 0) { dup = index + 1; } else { nums[index] *= -1; } } for (int i = 0; i \u0026lt; n; i++) { if (nums[i] \u0026gt; 0) { missing = i + 1; break; } } return new int[]{dup, missing}; } /** * 645. 错误的集合（版本 1） * 集合 s 包含从 1 到 n 的整数。不幸的是，因为数据错误，导致集合里面某一个数字复制了成了集合里面的另外一个数字的值，导致集合 丢失了一个数字 并且 有一个数字重复 。 * 给定一个数组 nums 代表了集合 S 发生错误后的结果。 * 请你找出重复出现的整数，再找到丢失的整数，将它们以数组的形式返回。 * @param nums 集合 * @return 重复出现的整数和丢失的整数组成的集合 */ public static int[] findErrorNumsV1(int[] nums) { // 结果集合  int[] res = new int[2]; // 1. 对数组进行排序  Arrays.sort(nums); // 2. 对数组进行遍历，找出重复出现的整数和丢失的整数  if (nums[0] \u0026gt; 1) {res[1] = 1;} if (nums[nums.length - 1] \u0026lt; nums.length) {res[1] = nums.length;} if (nums.length == 2 \u0026amp;\u0026amp; nums[1] == nums[0]) {res[0] = nums[0]; res[1] = (nums[1] == 1 ? 2 : 1);} for (int i = 1; i \u0026lt; nums.length; i++) { // 2.1 如果 nums[i] - nums[i - 1] == 0，则 nums[i - 1] 为重复的元素  if (0 == nums[i] - nums[i - 1]) {res[0] = nums[i - 1];} // 2.2 如果 nums[i] - nums[i - 1] == 2，则 nums[i] - 1 即为丢失的整数  if (nums[i] - nums[i - 1] == 2) {res[1] = nums[i] - 1;} // 2.3 如果重复出现的整数和丢失的整数都已经找到，则直接返回结果  if (res[0] != 0 \u0026amp;\u0026amp; res[1] != 0) {return res;} } // 3. 返回结果  return res; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); // int[] nums = {1, 3, 3}; // int[] nums = {3,2,3,4,6,5}; // int[] nums = {1,2,2,4};  int[] nums = {3,5,9,4,1,2,7,8,1}; int[] res = findErrorNumsV2(nums); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } "},{"id":110,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.6-%E6%9C%80%E5%A4%A7%E6%95%B0/","title":"2.3.6 最大数","section":"2.3 数组","content":"最大数 #  1 题目 #  给定一组非负整数 nums，重新排列每个数的顺序（每个数不可拆分）使之组成一个最大的整数。\n注意：输出结果可能非常大，所以你需要返回一个字符串而不是整数。\n示例 1：\n输入：nums = [10,2] 输出：\u0026quot;210\u0026quot; 示例 2：\n输入：nums = [3,30,34,5,9] 输出：\u0026quot;9534330\u0026quot; 示例 3：\n输入：nums = [1] 输出：\u0026quot;1\u0026quot; 示例 4：\n输入：nums = [10] 输出：\u0026quot;10\u0026quot; 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 100 0 \u0026lt;= nums[i] \u0026lt;= 109  2 解题思路 #  2.1 互换位置 #  2.1.1 问题分析 #   可以先遍历整个数组，然后比较两个数字组成字符串对应的数字的大小，然后将二者互换位置，将较大的数字放在数组前面。 在对数字组成字符串对应的数字进行比较时，可以通过将两个数字互换位置，然后比较组成的两个数字字符串的大小即可，在对两个数字字符串进行比较时需要注意，如果直接将其转换位数字来比较的话可能会越界，所以需要按照字符串的格式从高位到低位逐位比较，直到可以区分两个字符串的大小即可。 当所有数字比较完成后，需要判断一下是否所有数字全部为 0，因为如果都为 0 的话，按照正常的逻辑，后面生成的字符串为 \u0026quot;000...\u0026quot; 的格式，这样是不正确的，所以需要判断一下是否所有数字都为 0，如果都为 0 的话，直接返回 \u0026quot;0\u0026quot; 即可。  判断时只需要判断比较完的数组的第一个元素是否为 0 即可，因为如果第一个元素为 0 的话，后面的元素一定全部为 0。    2.1.2 参考代码 #  package com.grayson.top; /** * @author peng.wei * @version 1.0 * @date 2021/4/12 20:12 * @Description 最大数 */ public class L179 { /** * 比较两个数所包含元素的最大值的大小（版本 2：将两个数字根据位置的不同组成不同的字符串，然后比较两个字符串对应数字的大小） * * @param a 一个数 * @param b 另一个数 * @return 两个数所包含元素的最大值的大小 */ public int compare(int a, int b) { String combineAB = a + \u0026#34;\u0026#34; + b; String combineBA = b + \u0026#34;\u0026#34; + a; for (int i = 0; i \u0026lt; combineAB.length(); i++) { int ab = combineAB.charAt(i); int ba = combineBA.charAt(i); if (ab != ba) {return ab - ba;} } return 0; } /** * 179. 最大数（版本 2：将两个数字根据位置的不同组成不同的字符串，然后比较两个字符串对应数字的大小） * * @param nums 数组 * @return 数组中的元素组合成的最大整数 */ public String largestNumber(int[] nums) { String s = \u0026#34;\u0026#34;; for (int i = 0; i \u0026lt; nums.length - 1; i++) { for (int j = i + 1; j \u0026lt; nums.length; j++) { if (compare(nums[i], nums[j]) \u0026lt; 0) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; } } } boolean allZero = true; for (int i = 0; i \u0026lt; nums.length; i++) { if (nums[i] != 0) {allZero = false;} s += nums[i]; } return allZero ? \u0026#34;0\u0026#34; : s; } public static void main(String[] args) { L179 l179 = new L179(); // int[] nums = {10, 2};  int[] nums = {3,30,34,5,9}; // int[] nums = {1}; // int[] nums = {10}; // int[] nums = {34323,3432}; // int[] nums = {3, 29}; // int[] nums = {3, 31}; // int[] nums = {3,30,34}; // int[] nums = {3,34}; // int[] nums = {0,0}; // int[] nums = {3,42}; // String res = l179.largestNumber(nums);  System.out.println(res); } } 3 参考文献 #    179. 最大数。  "},{"id":111,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.7-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/","title":"2.3.7 二维数组中的查找","section":"2.3 数组","content":"二维数组中的查找 #  1 题目 #  在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n示例:\n现有矩阵 matrix 如下：\n[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ] 给定 target = 5，返回 true。\n给定 target = 20，返回 false。\n限制：\n 0 \u0026lt;= n \u0026lt;= 1000 0 \u0026lt;= m \u0026lt;= 1000  2 解题思路 #  如下图所示，我们将矩阵逆时针旋转 45 度，并将其转化为图形式，发现其类似于二叉搜索树，即对于每个元素，其左分支元素更小，右分支元素更大。因此，通过从“根节点”开始搜索，遇到比 target 大的元素就向左，反之向右，即可找到目标值 target。\n 3 参考代码 #  package com.grayson.top.codinginterviews; /** * @author peng.wei * @version 1.0 * @date 2021/4/27 20:05 * @Description */ public class CI4 { /** * 剑指 Offer 04. 二维数组中的查找 * @param matrix 二维数组 * @param target 目标值 * @return 二维数组中是否含有目标值 */ public boolean findNumberIn2DArray(int[][] matrix, int target) { if (matrix.length \u0026lt; 1) {return false;} int x = 0, y = matrix[0].length - 1; /** * 将矩阵逆时针旋转 45 度，然后将其视为二叉搜索树，然后按照二分查找法来查找元素即可 */ while (y \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; matrix.length) { if (matrix[x][y] \u0026gt; target) { y--; } else if (matrix[x][y] \u0026lt; target) { x++; } else if (matrix[x][y] == target) { return true; } } return false; } public static void main(String[] args) { CI4 ci4 = new CI4(); int[][] matrix = { {1, 4, 7, 11, 15}, {2, 5, 8, 12, 19}, {3, 6, 9, 16, 22}, {10, 13, 14, 17, 24}, {18, 21, 23, 26, 30} }; // int[][] matrix = { // {-1, 3} // };  int target = 5; boolean res = ci4.findNumberIn2DArray(matrix, target); System.out.println(res); } } 4 参考文献 #    剑指 Offer 04. 二维数组中的查找。  面试题 04. 二维数组中的查找（标志数，清晰图解）。  "},{"id":112,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.8-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/","title":"2.3.8 两数之和","section":"2.3 数组","content":"两数之和 #  1 题目 #  给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。\n你可以按任意顺序返回答案。\n示例 1：\n输入：nums = [2,7,11,15], target = 9 输出：[0,1] 解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2：\n输入：nums = [3,2,4], target = 6 输出：[1,2] 示例 3：\n输入：nums = [3,3], target = 6 输出：[0,1] 提示：\n 2 \u0026lt;= nums.length \u0026lt;= 103 -109 \u0026lt;= nums[i] \u0026lt;= 109 -109 \u0026lt;= target \u0026lt;= 109 只会存在一个有效答案  2 解题思路 #  2.1 暴力法 #  2.1.1 问题解析 #    设 $i \\in [0,nums.length-1]$ 从 0 开始，$j \\in [i+1,nums.length-1]$ 从 $i+1$ 开始遍历，当\n$$ nums[i] + nums[j] = target $$\n时，令\n$$ res[0] = i $$\n$$ res[1] = j $$\n然后返回 $res$ 即可。\n  2.1.2 参考代码 #  /** * 1. 两数之和（版本 1：暴力法） * @param nums 数组 * @param target 目标值 * @return 和为目标值的两个整数的下标 */ public int[] twoSumV1(int[] nums, int target) { int len = nums.length; int[] res = new int[2]; for (int i = 0; i \u0026lt; len - 1; i++) { for (int j = i + 1; j \u0026lt; len; j++) { if (nums[i] + nums[j] == target) { res[0] = i; res[1] = j; return res; } } } return res; } 2.2 HashMap #  2.2.1 问题解析 #   HashMap 的基本思想是当判断一个数组中是否存在两个数的和为目标值时，可以通过判断数组中是否存在和目标元素-当前元素的值。 因此我们可以通过Hash 映射的方法，其中 $key$ 为元素值，$value$ 为其对应的数组下表，从而减少查找的次数。  2.2.2 参考代码 #  /** * 1. 两数之和（版本 2：HashMap） * @param nums 数组 * @param target 目标值 * @return 和为目标值的两个整数的下标 */ public int[] twoSumV2(int[] nums, int target) { Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int len = nums.length; int[] res = new int[2]; for (int i = 0; i \u0026lt; len; i++) { map.put(nums[i], i); } for (int i = 0; i \u0026lt; len; i++) { int item = nums[i]; if (map.containsKey(target - item) \u0026amp;\u0026amp; map.get(target - item) != i) { res[0] = i; res[1] = map.get(target - item); return res; } } return res; } 2.3 改进 HashMap #  2.3.1 问题解析 #   上面方法中开始的时候是把所有的元素都对应放到 HashMap 中，后面直接进行查找，这样的话效率较低，因为因为实际查找的话可能部分元素不需要放到 HashMap 中。 所以我们可以对上面的方法进行改进：  开始的时候不放任何元素到 HashMap 中。 然后对数组中的元素进行遍历，判断 HashMap 中是否存在与当前元素之和等于目标元素的元素：  如果存在的话，直接将相应的结果返回。 如果不存在的话，说明对应的元素还没有遍历到，直接将当前元素对应的信息放入 HashMap 中。      2.3.2 参考代码 #  /** * 1. 两数之和（版本 3：改进 HashMap） * @param nums 数组 * @param target 目标值 * @return 和为目标值的两个整数的下标 */ public int[] twoSumV3(int[] nums, int target) { Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int len = nums.length; int[] res = new int[2]; for (int i = 0; i \u0026lt; len; i++) { int item = nums[i]; if (map.containsKey(target - item) \u0026amp;\u0026amp; map.get(target - item) != i) { res[0] = i; res[1] = map.get(target - item); return res; } map.put(item, i); } return res; } 3 参考文献 #    1. 两数之和。  "},{"id":113,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.9-%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/","title":"2.3.9 三数之和","section":"2.3 数组","content":"三数之和 #  1 题目 #  给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。\n注意： 答案中不可以包含重复的三元组。\n示例 1：\n输入：nums = [-1,0,1,2,-1,-4] 输出：[[-1,-1,2],[-1,0,1]] 示例 2：\n输入：nums = [] 输出：[] 示例 3：\n输入：nums = [0] 输出：[] 提示：\n 0 \u0026lt;= nums.length \u0026lt;= 3000 -105 \u0026lt;= nums[i] \u0026lt;= 105  2 解题思路 #  2.1 双指针法 #  2.1.1 问题解析 #   先对原来的数组进行排序。 假设数组 $nums$ 的长度为 $len$。 首先固定一个点 $nums[i]$，进行第一次去重，如果 $nums[i]==nums[i+1]$，则进行下一个循环。 然后分别定义左、右指针：  $$ left = i + 1 $$\n$$ right = len - 1 $$\n定义 $sum$：  $$ sum = nums[i] + nums[left] + nums[right] $$\n 对 $sum$ 进行判断：\n 如果 $sum\u0026gt;0$，$left++$。 如果 $sum \u0026lt; 0$，$right\u0026ndash;$。 如果 $sum = 0$，将 $[nums[i],nums[left],nums[right]]$ 添加到结果中，同时 $left++,right\u0026ndash;$，然后进行第二次去重：  $$ while(left \u0026lt; right \\space and \\space nums[left] == nums[left+1]) \\space left++ $$\n$$ while(left \u0026lt; right \\space and \\space nums[right] == nums[right-1]) \\space right— $$\n   2.1.2 参考代码 #  /** * 15. 三数之和 * @param nums 数组 * @return 和为 0 且不重复的三元组 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] nums) { int len = nums.length, left, right, sum; List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); // 如果数组的长度大于 3，说明不符合题意，直接返回空数组即可  if (len \u0026lt; 3) {return res;} Arrays.sort(nums); for (int i = 0; i \u0026lt; len; i++) { // 如果第一个元素都大于 0， 说明后面的元素肯定大于 0，则三者之和肯定大于 0，继续进行下一个循环即可  if (nums[i] \u0026gt; 0) {continue;} // 去重，如果第一个元素前后两个一样，则最后的结果可能和前面的结果重复  if (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1]) {continue;} left = i + 1; right = len - 1; while (left \u0026lt; right) { sum = nums[i] + nums[left] + nums[right]; if (sum \u0026lt; 0) { left++; } else if (sum \u0026gt; 0) { right--; } else if (sum == 0) { res.add(Arrays.asList(nums[i], nums[left], nums[right])); // 去重，如果左指针对应元素和其下一个元素相同，则最后的结果可能和前面的结果重复，直接将左指针向后面移动一位即可  while (left \u0026lt; right \u0026amp;\u0026amp; nums[left] == nums[left+1]) {left++;} // 去重，如果右指针对应元素和其上一个元素相同，则最后的结果可能和前面的结果重复，直接将右指针向前面移动一位即可  while (left \u0026lt; right \u0026amp;\u0026amp; nums[right] == nums[right-1]) {right--;} // 将左指针右移  left++; // 将右指针左移  right--; } } } return res; } 3 参考文献 #    15. 三数之和。  画解算法：15. 三数之和。  "},{"id":114,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.10-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84/","title":"2.3.10 合并两个有序数组","section":"2.3 数组","content":"合并两个有序数组 #  1 题目 #  给你两个有序整数数组 nums1 和 nums2，请你将 nums2 合并到 nums1 中，使 nums1 成为一个有序数组。\n初始化 nums1 和 nums2 的元素数量分别为 m 和 n 。你可以假设 nums1 的空间大小等于 m + n，这样它就有足够的空间保存来自 nums2 的元素。\n示例 1：\n输入：nums1 = [1,2,3,0,0,0], m = 3, nums2 = [2,5,6], n = 3 输出：[1,2,2,3,5,6] 示例 2：\n输入：nums1 = [1], m = 1, nums2 = [], n = 0 输出：[1] 提示：\n nums1.length == m + n nums2.length == n 0 \u0026lt;= m, n \u0026lt;= 200 1 \u0026lt;= m + n \u0026lt;= 200 -109 \u0026lt;= nums1[i], nums2[i] \u0026lt;= 109  2 解题思路 #  2.1 nums2 保存最大数据 #  2.1.1 问题分析 #    初始时 $i=0$，$j=0$。\n  如果 $i\u0026lt;m$\n 如果 $nums2[j]\u0026lt;nums1[i]$    将 $nums2$ 中的元素依次拼接到 $nums1$ 后面，这样 $nums1$ 中的元素就是全部的元素，同时是按照顺序排列的。\n   2.1.2 参考代码 #  /** * 打印数组 * @param obj 对象数组 */ public static void printArray(int[] obj) { for (int i = 0; i \u0026lt; obj.length; i++) { System.out.print(obj[i]); } System.out.println(); } /** * 88. 合并两个有序数组（版本 1：nums2 保存最大数据） * * @param nums1 * @param m * @param nums2 * @param n */ public void mergeV1(int[] nums1, int m, int[] nums2, int n) { int i = 0, j = 0; while (i \u0026lt; m \u0026amp;\u0026amp; j \u0026lt; n) { if (nums2[j] \u0026lt; nums1[i]) { // 如果 nums2[j] \u0026lt; nums1[i]，则交换 nums2[j] 和 nums1[i]  int temp = nums1[i]; nums1[i] = nums2[j]; nums2[j] = temp; // 调整 nums2[j] 在 nums2 中的位置，使得 nums2 依然是个有序数组  for (int k = 1; k \u0026lt; n; k++) { if (nums2[k - 1] \u0026gt; nums2[k]) { int temp2 = nums2[k - 1]; nums2[k - 1] = nums2[k]; nums2[k] = temp2; } else { break; } } // 将 j 复位，每次都从头比较 nums2 中的元素和 nums[i]，这样当 nums1 遍历结束时，nums2 中的元素就是两个数组中最大的 n 个元素  j = 0; } i++; } // 将 nums2 中的元素依次拼接到 nums1 的后面  for (int k = m; k \u0026lt; m + n; k++) { nums1[k] = nums2[k - m]; } printArray(nums1); } 2.2 从后向前 #  2.2.1 问题分析 #    初始时 $i=m-1$，$j=n-1$，$k=m+n-1$。\n  如果 $i\u0026gt;=0$ 并且 $j\u0026gt;=0$：\n 如果 $nums2[j]\u0026gt;=nums1[i]$：将 $nums2[j]$ 放到 $nums1[k]$ 的位置上，同时将 $j$ 的值减 1。 如果 $nums1[i]\u0026gt;nums2[j]$：将 $nums1[i]$ 放到 $nums1[k]$ 的位置上，同时将 $i$ 的值减 1。 最后统一将 $k$ 的值减 1。    如果最后 $i\u0026lt;0$，说明 $nums1$ 已经遍历完了，$nums2$ 还没有遍历完，此时 $nums1$ 的元素都已经移动到了对应的位置上，而且此时 $nums2$ 中剩余的元素都比 $nums1$ 中已经存在的元素小，因此将 $nums2$ 中还未遍历完的元素从 $nums1$ 的起始位置依次存放即可。\n   2.2.2 参考代码 #  /** * 打印数组 * @param obj 对象数组 */ public static void printArray(int[] obj) { for (int i = 0; i \u0026lt; obj.length; i++) { System.out.print(obj[i]); } System.out.println(); } /** * 88. 合并两个有序数组（版本 2：从后向前） * * @param nums1 * @param m * @param nums2 * @param n */ public void mergeV2(int[] nums1, int m, int[] nums2, int n) { int i = m - 1, j = n - 1, k = m + n - 1; // 从后向前依次遍历 nums1 和 nums2  while (i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026gt;= 0) { if (nums2[j] \u0026gt;= nums1[i]) { // 如果 nums2[j] \u0026gt;= nums1[i]，则将 nums2[j] 放到 nums1[k] 的位置上，同时将 j 的值减 1  nums1[k] = nums2[j]; j--; } else if (nums1[i] \u0026gt; nums2[j]) { // 如果 nums1[i] \u0026gt;= nums2[j]，则将 nums1[i] 放到 nums1[k] 的位置上，同时将 i 的值减 1  nums1[k] = nums1[i]; i--; } // 最后统一将 k 的值减 1  k--; } // 如果最后 nums1 已经遍历完了，nums2 还没有遍历完，说明此时 nums1 中的元素都已经移动到了 nums1 的对应的位置上，而且此时 nums2 中剩余的元素都小于 nums1 中已经存在的元素，因此直接将 nums2 中还未遍历完的元素从 nums1 的起始位置依次存放即可  if (i \u0026lt; 0) { int q = 0; for (int p = 0; p \u0026lt;= j; p++) { nums1[p] = nums2[q]; q++; } } printArray(nums1); } 2 参考文献 #    88. 合并两个有序数组。  "},{"id":115,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.11-%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%8E%92%E5%88%97/","title":"2.3.11 下一个排列","section":"2.3 数组","content":"1 题目 #  实现获取 下一个排列 的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列（即，组合出下一个更大的整数）。\n如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。\n必须 原地 修改，只允许使用额外常数空间。\n示例 1：\n输入：nums = [1,2,3] 输出：[1,3,2] 示例 2：\n输入：nums = [3,2,1] 输出：[1,2,3] 示例 3：\n输入：nums = [1,1,5] 输出：[1,5,1] 示例 4：\n输入：nums = [1] 输出：[1] 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 100 0 \u0026lt;= nums[i] \u0026lt;= 100  2 问题分析 #   这道题可以按照 维基百科上的解题思路来解决，具体如下：  先找出最大的索引 $k$，满足 $nums[k] \u0026lt; nums[k + 1]$，如果不存在，就翻转整个数组。 再找出另一个最大索引 $l$，满足 $nums[l] \u0026gt; nums[k]$。 交换 $nums[l]$ 和 $nums[k]$。 最后反转 $nums[k+1]$ 后面的元素。    3 参考代码 #  /** * 31. 下一个排列 * @param nums 数组 */ public void nextPermutation(int[] nums) { int m = nums.length; boolean exist = false; /*判断数字序列是否可以重新排列成下一个更大的序列*/ for (int i = m - 2; i \u0026gt;= 0; i--) { if (exist) { /*如果可以排列成下一个更大的序列，则中断 for 循环*/ break; } if (nums[i] \u0026lt; nums[i + 1]) { /*如果可以找到一个元素对 (nums[k], nums[k+1])，则表名可以重新排列成下一个更大的序列，令 exist = true*/ exist = true; for (int j = m - 1; j \u0026gt;= i + 1; j--) { /*循环遍历找到比 nums[i] 大的索引最大的元素*/ if (nums[j] \u0026gt; nums[i]) { int temp = nums[j]; /*交换 nums[i] 和 nums[j]*/ nums[j] = nums[i]; nums[i] = temp; sort(nums, i + 1, m); /*将 i + 1 后面的元素按照升序重新排列*/ break; } } } } if (!exist) { /*如果不能排列成下一个更大的序列，则对整个数组进行重新排序*/ sort(nums, 0, m); } } /** * 对数组指定区间的元素进行重新排序 * @param nums 数组 * @param start 起始位置 * @param end 结束位置 */ public void sort(int[] nums, int start, int end) { for (int k = start; k \u0026lt; end - 1; k++) { for (int l = k + 1; l \u0026lt; end; l++) { if (nums[k] \u0026gt; nums[l]) { int temp = nums[k]; nums[k] = nums[l]; nums[l] = temp; } } } } 参考文献 #    31. 下一个排列。  下一个排列。  Generation in lexicographic order.  "},{"id":116,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.12-%E7%BC%BA%E5%A4%B1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AD%A3%E6%95%B0/","title":"2.3.12 缺失的第一个正数","section":"2.3 数组","content":"1 题目 #  给你一个未排序的整数数组 nums ，请你找出其中没有出现的最小的正整数。\n请你实现时间复杂度为 O(n) 并且只使用常数级别额外空间的解决方案。 示例 1：\n输入：nums = [1,2,0] 输出：3 示例 2：\n输入：nums = [3,4,-1,1] 输出：2 示例 3：\n输入：nums = [7,8,9,11,12] 输出：1 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 5 * 105 -231 \u0026lt;= nums[i] \u0026lt;= 231 - 1  2 解题思路 #  2.1 哈希表 #  2.1.1 问题分析 #   对于一个长度为$N$的数组，其中没有出现的最小正整数只能在$[1, N + 1]$中，这是因为如果$[1, N]$都出现了，那么答案是$N + 1$，否则答案是$[1, N]$中没有出现的最小正整数。 因此，我们有了一种将数组设计成哈希表的思路：  我们对数组进行遍历，对于遍历到的数$x$，如果他在$[1, N]$的范围内，那么就将数组中的第$x - 1$个位置（数组下标从0开始）打上标记。 在遍历结束之后，如果所有位置都被打了标记，那么答案是$N + 1$，否则答案是最小的没有打上标记的位置加1。   由于我们只在意$[1, N]$的数，因此我们可以先对数组进行遍历，把不在$[1, N]$范围内的数修改成任意一个大于$N$的数（例如$N + 1$），这样一来，数组中的所有数就都是正数了，因此，我们可以将标记表示为负号。 具体的算法流程如下：   将数组中所有小于等于0的数修改为$N + 1$。\n  遍历数组中的每一个数$x$，他可能已经被打了标记，因此原本对应的数为$|x|$，如果$|x| \\in [1, N]$，并且，那么我们给数组中的第$|x| - 1$个位置的数添加一个负号，如果他已经有负号，不需要重复添加。\n  在遍历完成之后，如果数组中的每一个数都是负数，那么答案是$N + 1$，否则答案是第一个正数的位置$+ 1$。\n     2.1.2 参考代码 #  /** * 41. 缺失的第一个正数（版本1：哈希表） * @param nums 数组 * @return 数组中缺失的第一个正数 */ public int firstMissingPositiveV1(int[] nums) { int m = nums.length; for (int i = 0; i \u0026lt; m; i++) { // 将数组中小于等于 0 的数修改为 m + 1  if (nums[i] \u0026lt;= 0) {nums[i] = m + 1;} } for (int i = 0; i \u0026lt; m; i++) { // 假设遍历到的数的绝对值为 x，如果 nums[x] 为正数，则将其变为对应的相反数  int x = Math.abs(nums[i]); if (x \u0026lt;= m) {nums[x - 1] = -Math.abs(nums[x - 1]);} } for (int i = 0; i \u0026lt; m; i++) { // 数组中第一个正数所在的位置 + 1 即为数组中缺失的第一个正数  if (nums[i] \u0026gt; 0) {return i + 1;} } // 如果数组中全部都为负数，则数组中缺失的第一个正数为 m + 1  return m + 1; } 2.2 置换 #  2.2.1 问题分析 #   除了打标记外，我们还可以使用置换的方法，将给定的数组恢复成下面的形式：  如果数组中包含$x \\in [1, N]$，那么恢复后，数组的第$x - 1$个元素为$x$。   在恢复后，数组应当有$[1, 2,\u0026hellip;,N]$的形式，但其中有若干个位置的数是错误的，每一个错误的位置就代表了一个缺失的正数。 我们可以按照如下的方式对数组进行恢复：  对数组进行遍历，对于遍历到的数$x = nums[i]$，如果$x \\in [1, N]$，我们就知道$x$应当出现在数组中的$x - 1$的位置，因此交换$nums[i]$和$nums[x - 1]$，这样$x$就出现在了正确的位置，在完成交换后，新的$nums[i]$可能还在$[1, N]$的范围内，我们需要继续进行交换操作，直到$x \\in [1, N]$。 需要注意的是，上面的方法可能陷入死循环，如果$nums[i]$恰好与$nums[x - 1]$相等，那么就会无限交换下去，此时我们有$nums[i] = x = nums[x - 1]$，说明$x$已经出现在了正确的位置，因此我们可以跳出循环，开始遍历下一个数。     2.2.2 参考代码 #  /** * 41. 缺失的第一个正数（版本2：置换） * @param nums 数组 * @return 数组中缺失的第一个正数 */ public int firstMissingPositiveV2(int[] nums) { int m = nums.length; for (int i = 0; i \u0026lt; m; i++) { // 对于遍历到的数 x = nums[i]，如果 x 在 [1, m] 范围内，并且 x != nums[x - 1] （防止发生死循环），则交换 nums[i] 与 nums[x - 1]，如果交换后新的 nums[i] 还在 [1, m] 范围内，则继续执行交换操作，知道 x 不在 [1, m] 范围内  while (nums[i] \u0026gt;= 1 \u0026amp;\u0026amp; nums[i] \u0026lt;= m \u0026amp;\u0026amp; nums[i] != nums[nums[i] - 1]) { int temp = nums[nums[i] - 1]; nums[nums[i] - 1] = nums[i]; nums[i] = temp; } } for (int i = 0; i \u0026lt; m; i++) { // 交换后完后的数组中第 i 个位置的元素 nums[i]，如果 nums[i] != i + 1，则该元素所在的下一个位置 i + 1 即为数组中缺失的第一个正数  if (nums[i] != i + 1) { return i + 1; } } // 如果数组中的元素 nums[i] 都等于 i + 1，则数组中缺失的第一个正数为 m + 1  return m + 1; } 参考文献 #    41. 缺失的第一个正数。  缺失的第一个正数。  "},{"id":117,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.13-%E5%AF%BB%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%AD%A3%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/","title":"2.3.13 寻找两个正序数组的中位数","section":"2.3 数组","content":"1 题目 #  给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。\n示例 1：\n输入：nums1 = [1,3], nums2 = [2] 输出：2.00000 解释：合并数组 = [1,2,3] ，中位数 2 示例 2：\n输入：nums1 = [1,2], nums2 = [3,4] 输出：2.50000 解释：合并数组 = [1,2,3,4] ，中位数 (2 + 3) / 2 = 2.5 示例 3：\n输入：nums1 = [0,0], nums2 = [0,0] 输出：0.00000 示例 4：\n输入：nums1 = [], nums2 = [1] 输出：1.00000 示例 5：\n输入：nums1 = [2], nums2 = [] 输出：2.00000 提示：\n nums1.length == m nums2.length == n 0 \u0026lt;= m \u0026lt;= 1000 0 \u0026lt;= n \u0026lt;= 1000 1 \u0026lt;= m + n \u0026lt;= 2000 -106 \u0026lt;= nums1[i], nums2[i] \u0026lt;= 106  进阶： 你能设计一个时间复杂度为 O(log (m+n)) 的算法解决此问题吗？\n2 解题思路 #  2.1 二分查找 #  2.1.1 问题分析 #   给定两个有序数组，要求找到两个有序数组的中位数，最直观的思路有以下两种：  使用归并的方式，合并两个有序数组，得到一个大的有序数组，大的有序数组的中间位置的元素，即为中位数。 不需要合并两个有序数组，只要找到中位数的位置即可，由于两个数组的长度已知，因此中位数对应的两个数组的下标之和也是已知的：  维护两个指针，初始时分别指向两个数组的下标 0 的位置，每次将指向较小值的指针后移一位（如果一个指针已经到达数组末尾，则只需要移动另一个数组的指针），直到到达中位数的位置。     假设两个有序数组的长度分别为 $m$ 和 $n$，上述两种思路的复杂度如下：  第一种思路的时间复杂度为 $O(m + n)$，空间复杂度是 $O(m + n)$。 第二种思路虽然可以将空间复杂度降到 $O(1)$，但是时间复杂度依然是 $O(m + n)$。   本题也可以使用二分查找的方法来实现，具体思路如下：  根据中位数的定义，当 $m + n$ 是奇数时，中位数是两个有序数组中的第 $(m + n) / 2$ 个元素，当 $m + n$ 是偶数时，中位数是两个有序数组中的第 $(m + n) / 2$ 个元素和第 $(m + n) / 2 + 1$ 个元素的平均值，因此，这道题可以转化成寻找两个有序数组中第 $k$ 小的数，其中 $k$ 为 $(m + n) / 2$ 或 $(m + n) / 2 + 1$。 假设两个有序数组分别是 $A$ 和 $B$，要找到第 $k$ 个元素，我们可以比较 $A[\\frac{k}{2} - 1]$ 和 $B[\\frac{k}{2} - 1]$，由于 $A[\\frac{k}{2} - 1]$ 的前面分别有 $A[0..\\frac{k}{2} - 2]$ 和 $B[0..\\frac{k}{2} - 2]$，即 $\\frac{k}{2} - 1$ 个元素，对于 $A[\\frac{k}{2} - 1]$和 $B[\\frac{k}{2} - 1]$ 中的最小值，最多只会有 $(\\frac{k}{2} - 1) + (\\frac{k}{2} - 1) = k - 2 \u0026lt; k - 1$ 个元素比他小，那么他就不能是第 $k$ 小的数了（第 $k$ 小的数最多有 $k - 1$ 个数比他小）。 因此，我们可以归纳出三种情况：   如果$A[\\frac{k}{2} - 1] \u0026lt; B[\\frac{k}{2} - 1]$，则$A[0..\\frac{k}{2} - 1]$不可能是不可能是第$k$个数，可以全部排除。\n  如果$A[\\frac{k}{2} - 1] \u0026gt; B[\\frac{k}{2} - 1]$，则$B[0..\\frac{k}{2} - 1]$不可能是不可能是第$k$个数，可以全部排除。\n  如果$A[\\frac{k}{2} - 1] = B[\\frac{k}{2} - 1]$，归入第一种情况处理。\n    可以看到，比较$A[\\frac{k}{2} - 1]$之后，可以排除$\\frac{k}{2}$个不可能是第$k$小的数，查找范围缩小了一半，同时，我们将在排除后的新数组上继续二分查找，并且根据我们排除数的个数，减少$k$的值，这是因为我们排除的数都是不大于$k$的数。 有以下三种情况需要特殊处理：  如果$A[\\frac{k}{2} - 1]$或$B[\\frac{k}{2} - 1]$越界，那么我们可以选取对应数组中的最后一个元素，在这种情况下，我们必须根据排除数的个数减少$k$的值，而不能直接将$k$减去$\\frac{k}{2}$。 如果一个数组为空，说明该数组中的所有元素都被排除，我们可以直接返回另一个数组中第$k$小的元素。 如果$k = 1$，我们只要返回两个数组中的未排除下标范围内的首元素的最小值即可。   具体示例如下：   假设两个有序数组如下：\nA: 1 3 4 9 B: 1 2 3 4 5 6 7 8 9   两个有序数组的长度分别是4和9，长度之和是13，中位数是两个有序数组中的第7个元素，因此需要找到第$k = 7$的元素。\n  比较两个有序数组中下标为$\\frac{k}{2} - 1 = 2$的数，即$A[2]$和$B[2]$，如下面所示：\nA: 1 3 4 9 ↑ B: 1 2 3 4 5 6 7 8 9 ↑   由于$A[2] \u0026gt; B[2]$，因此排除$B[0..2]$，即数组$B$的下标偏移（offset）变为3，同时更新$k$的值$k = k - \\frac{k}{2} = 4$。\n  下一步寻找，比较两个有序数组中下标为$\\frac{k}{2} - 1 = 1$的数，即$A[1]$和$B[4]$，如下面所示，其中，方括号部分表示已经被排除的数：\nA: 1 3 4 9 ↑ B: [1 2 3] 4 5 6 7 8 9 ↑   由于$A[1] \u0026lt; B[4]$，因此排除$A[0..1]$，即数组$A$的下标偏移变为2，同时更新$k$的值$k = k - \\frac{k}{2} = 2$。\n  下一步寻找，比较两个有序数组中下标为$\\frac{k}{2} - 1 = 0$的数，即比较$A[2]$和$B[3]$，如下面所示：\nA: [1 3] 4 9 ↑ B: [1 2 3] 4 5 6 7 8 9 ↑   由于$A[2] = B[3]$，根据之前的规则，排除$A$中的元素，因此排除$A[2]$，即数组$A$的下标偏移变为3，同时更新$k$的值$k = k - \\frac{k}{2} = 1$，由于$k$的值变为1，因此比较两个有序数组中的未排除下标范围内的第一个数，其中较小的数即为第$k$个数，由于$A[3] = 9 \u0026gt; B[3] = 4$，因此第$k$个数是$B[3] = 4$。\n      2.1.2 参考代码 #  /** * 4. 寻找两个正序数组的中位数 * @param nums1 第一个正序数组 * @param nums2 第二个正序数组 * @return 两个正序数组的中位数 */ public double findMedianSortedArrays(int[] nums1, int[] nums2) { int length1 = nums1.length, length2 = nums2.length; int totalLength = length1 + length2; if (totalLength % 2 == 1) { // 数组长度之和为奇数，则中位数下标为 totalLength / 2  int midIndex = totalLength / 2; double median = getKthElement(nums1, nums2, midIndex + 1); return median; } else { // 数组长度之和为偶数，则中位数下标分别为 totalLength / 2 - 1，totalLength / 2  int midIndex1 = totalLength / 2 - 1, midIndex2 = totalLength / 2; double median = (getKthElement(nums1, nums2, midIndex1 + 1) + getKthElement(nums1, nums2, midIndex2 + 1)) / 2.0; return median; } } /** * 获取两个正序数组中第 k 小的元素 * @param nums1 第一个正序数组 * @param nums2 第二个正序数组 * @param k k 值 * @return 两个正序数组中第 k 小的元素 */ public int getKthElement(int[] nums1, int[] nums2, int k) { int length1 = nums1.length, length2 = nums2.length; int index1 = 0, index2 = 0; while (true) { // 边界情况  // 如果其中一个数组到达边界，则另一个数组中原来下标加上 k - 1 对应的下标即为两个数组中位数的下标  if (index1 == length1) {return nums2[index2 + k - 1];} if (index2 == length2) {return nums1[index1 + k - 1];} // 如果 k == 1，则两个数组中的较小数即为两个数组的中位数  if (k == 1) {return Math.min(nums1[index1], nums2[index2]);} // 正常情况  int half = k / 2; int newIndex1 = Math.min(index1 + half, length1) - 1; int newIndex2 = Math.min(index2 + half, length2) - 1; int pivot1 = nums1[newIndex1], pivot2 = nums2[newIndex2]; if (pivot1 \u0026lt;= pivot2) { // 如果 pivot1 \u0026lt;= pivot2，则 nums1[0, newIndex1] 都不可能是第 k 小的元素，把这些元素全部删除，剩下的作为新的 nums1 数组  k -= (newIndex1 - index1 + 1); // 由于我们删除了一些元素（这些元素都比第 k 小的元素小），因此需要修改 k 的值，减去删除的数的个数  index1 = newIndex1 + 1; } else { // 如果 pivot1 \u0026gt; pivot2，则 nums2[0, newIndex2] 都不可能是第 k 小的元素，把这些元素全部删除，剩下的作为新的 nums2 数组  k -= (newIndex2 - index2 + 1); // 由于我们删除了一些元素（这些元素都比第 k 小的元素小），因此需要修改 k 的值，减去删除的数的个数  index2 = newIndex2 + 1; } } } 参考文献 #    4. 寻找两个正序数组的中位数。  寻找两个有序数组的中位数。  "},{"id":118,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.14-%E5%A4%9A%E6%95%B0%E5%85%83%E7%B4%A0/","title":"2.3.14 多数元素","section":"2.3 数组","content":"1 题目 #  给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数 大于 ⌊ n/2 ⌋ 的元素。\n你可以假设数组是非空的，并且给定的数组总是存在多数元素。\n示例 1：\n输入：[3,2,3] 输出：3 示例 2：\n输入：[2,2,1,1,1,2,2] 输出：2 进阶：\n 尝试设计时间复杂度为 O(n)、空间复杂度为 O(1) 的算法解决此问题。  2 解题思路 #  2.1 计数 #  2.1.1 问题分析 #   最原始的思路是通过一个 $map$，其中 $key$为数组中的元素，$value$为对应元素出现的次数，添加完元素后，如果当前元素的出现次数大于 $\\frac{n}{2}$，则该元素即为多数元素，直接返回即可。  2.1.2 参考代码 #  /** * 169. 多数元素（版本 1：计数） * @param nums 数组 * @return 数组中出现次数 大于 ⌊ n/2 ⌋ 的元素 */ public int majorityElementV1(int[] nums) { Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int m = nums.length; if (m == 1) {return nums[0];} for (int i = 0; i \u0026lt; m; i++) { int key = nums[i]; if (!map.containsKey(key)) { map.put(key, 1); } else { int value = map.get(key); value++; if (value \u0026gt; m / 2) { return key; } else { map.put(key, value); } } } return -1; } 2.2 排序 #  2.2.1 问题分析 #   对数组中的元素进行升序排序，因为多数元素的个数大于 $\\frac{n}{2}$，因此排序后数组的中间位置的元素即为多数元素。  2.2.2 参考代码 #  /** * 169. 多数元素（版本 2：排序） * @param nums 数组 * @return 数组中出现次数 大于 ⌊ n/2 ⌋ 的元素 */ public int majorityElementV2(int[] nums) { Arrays.sort(nums); // 因为多数元素在数组中出现的次数大于 n / 2，因此位于中间位置的元素一定是中位数  return nums[nums.length \u0026gt;\u0026gt; 1]; } 2.3 摩尔排序 #  2.3.1 问题分析 #   开始时将投票人 $voteItem$ 初始化为 0，票数 $voteNum$ 初始化为 0，然后对数组 $nums$ 进行遍历，假设当前遍历到的元素 $nums[i]$ 为  如果 $voteNum = 0$：则令 $voteItem = nums[i], voteNum = 1$。 如果 $voteItem != nums[i]$，则 $voteNum = voteNum - 1$。 如果 $voteItem = nums[i]$，则 $voteNum = voteNum + 1$。   这种方法之所以行得通是因为投票法是遇到相同的则票数 +1，遇到不同的则票数-1，且多数元素的个数 $\u0026gt; ⌊\\frac{n}{2}⌋$，其余元素的个数总和 $\\le ⌊\\frac{n}{2}⌋$，因此多数元素的个数 - 其余元素的个数总和的结果一定 $\\ge1$，这就相当于每个多数元素和其他元素的两两相互抵消，抵消到最后肯定还剩余至少 1 个多数元素。  2.3.2 参考代码 #  /** * 169. 多数元素（版本 3：摩尔投票） * @param nums 数组 * @return 数组中出现次数 大于 ⌊ n/2 ⌋ 的元素 */ public int majorityElementV3(int[] nums) { int voteItem = 0, voteNum = 0; for (int i = 0; i \u0026lt; nums.length; i++) { if (voteNum == 0) { voteItem = nums[i]; voteNum = 1; } else if (voteItem == nums[i]) { voteNum++; } else { voteNum--; } } return voteItem; } 参考文献 #    169. 多数元素。  Java-3 种方法(计数法/排序法/摩尔投票法)。  "},{"id":119,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.3-%E6%95%B0%E7%BB%84/2.3.15-%E6%9C%80%E9%95%BF%E8%BF%9E%E7%BB%AD%E5%BA%8F%E5%88%97/","title":"2.3.15 最长连续序列","section":"2.3 数组","content":"1 题目 #  给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。\n请你设计并实现时间复杂度为 O(n) 的算法解决此问题。\n示例 1：\n输入：nums = [100,4,200,1,3,2] 输出：4 解释：最长数字连续序列是 [1, 2, 3, 4]。它的长度为 4。 示例 2：\n输入：nums = [0,3,7,2,5,8,4,6,0,1] 输出：9 提示：\n 0 \u0026lt;= nums.length \u0026lt;= 105 -109 \u0026lt;= nums[i] \u0026lt;= 109  2 解题思路 #  2.1 哈希表 #  2.1.1 问题分析 #   有意题目要求的时间复杂度为 $O(n)$，因此我们可以使用哈希表来解决这个问题。 我们使用哈希表来存储每个端点值对应连续区间的长度，其中 $key$为端点值，$value$为对应连续区间的长度。 然后按以下步骤计算每个端点值对应连续区间的长度：  若该端点已经在哈希表中存在，则直接跳过即可。 否则：  计算出该端点左右两个相邻端点 $nums[i] - 1$ 和 $nums[i] + 1$ 的连续区间的长度分别为 $left$ 和 $right$。 则当前端点对应连续区间的长度$currentLength$为 $left + right + 1$，然后更新最后的结果值 $res$ 为所有已计算端点连续区间长度的最大值。 更新当前端点 $nums[i]$ 及所在连续区间两边端点 $nums[i] - left$ 和 $nums[i] + right$ 的连续区间的长度为当前端点对应连续区间的长度 $currentLength$。       2.1.2 参考代码 #  /** * 128. 最长连续序列 * * @param nums 数组 * @return 数组中数字连续的最长序列（不要求序列元素在原数组中连续）的长度 */ public int longestConsecutive(int[] nums) { // 用于记录每个端点值对应连续区间的长度，其中 key 为端点值，value 为对应连续区间的长度  Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int res = 0, currentLength = 0; for (int i = 0; i \u0026lt; nums.length; i++) { /** * 计算每个端点值对应连续区间的长度： * 1. 若该端点已经在哈希表中存在，则直接跳过即可 * 2. 否则： * 2.1 计算出该端点左右两个相邻的端点 nums[i] - 1 和 nums[i] + 1 的连续区间的长度分别为 left 和 right * 2.2 则当前端点对应连续区间的长度为 left + right + 1，然后更新最后的结果值为所有已计算端点连续区间长度的最大值。 * 2.3 更新当前端点 nums[i] 及所在连续区间两边端点 nums[i] - left 和 nums[i] + right 的连续区间的长度为当前端点对应连续区间的长度 */ int key = nums[i]; if (map.containsKey(key)) { continue; } int left = map.getOrDefault(nums[i] - 1, 0); int right = map.getOrDefault(nums[i] + 1, 0); currentLength = left + right + 1; res = Math.max(res, currentLength); map.put(key, currentLength); map.put(key - left, currentLength); map.put(key + right, currentLength); } return res; } 参考文献 #    128. 最长连续序列。  【动态规划】Python 题解。  "},{"id":120,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.2-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/1.2.1-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E6%A1%86%E6%9E%B6/","title":"1.2.1 二分查找框架","section":"1.2 二分查找","content":"二分查找框架 #  package com.grayson.top; import java.util.List; /** * @author peng.wei * @version 1.0 * @date 2021/3/21 14:25 * @Description 二分查找算法框架 */ public class BinarySearch { /** * 二分查找算法 * @param nums 原始数组 * @param target 目标值 * @return 目标值在原始数组中的位置 */ public static int binarySearch(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left \u0026lt;= right) { // 为了防止 (left + right) 太大导致溢出  int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) {left = mid + 1;} else if (nums[mid] \u0026gt; target) {right = mid - 1;} // 直接返回  else if (nums[mid] == target) {return mid;} } // 直接返回  return -1; } /** * 寻找左侧边界的二分查找算法 * @param nums 原始数组 * @param target 目标值 * @return 目标值在原始数组中的位置 */ public static Integer leftBond(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) {left = mid + 1;} else if (nums[mid] \u0026gt; target) {right = mid - 1;} // 1.别返回，锁定左边界  else if (nums[mid] == target) {right = mid - 1;} } // 2.最后要检查 left 越界的情况  if (left \u0026gt;= nums.length || nums[left] != target) {return -1;} return left; } /** * 寻找右侧边界的二分查找算法 * @param nums 原始数组 * @param target 目标值 * @return 目标值在原始数组中的位置 */ public static Integer rightBond(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) {left = mid + 1;} else if (nums[mid] \u0026gt; target) {right = mid - 1;} // 1.别返回，锁定右边界  else if (nums[mid] == target) {left = mid + 1;} } // 2.最后要检查 right 越界的情况  if (right \u0026lt; 0 || nums[left] != target) {return -1;} return right; } } "},{"id":121,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.2-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/1.2.2-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/","title":"1.2.2 搜索旋转排序数组","section":"1.2 二分查找","content":"搜索旋转排序数组 #  1 题目 #  整数数组 nums 按升序排列，数组中的值 互不相同 。\n在传递给函数之前，nums 在预先未知的某个下标 k（0 \u0026lt;= k \u0026lt; nums.length）上进行了 旋转，使数组变为 [nums[k], nums[k+1], \u0026hellip;, nums[n-1], nums[0], nums[1], \u0026hellip;, nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2] 。\n给你 旋转后 的数组 nums 和一个整数 target ，如果 nums 中存在这个目标值 target ，则返回它的下标，否则返回 -1 。\n示例 1：\n输入：nums = [4,5,6,7,0,1,2], target = 0 输出：4 示例 2：\n输入：nums = [4,5,6,7,0,1,2], target = 3 输出：-1 示例 3：\n输入：nums = [1], target = 0 输出：-1 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 5000 -10^4 \u0026lt;= nums[i] \u0026lt;= 10^4 nums 中的每个值都 独一无二 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -10^4 \u0026lt;= target \u0026lt;= 10^4  进阶： 你可以设计一个时间复杂度为 O(log n) 的解决方案吗？\n2 解题思路 #  2.1 两段寻找 #  2.1.1 问题分析 #   这种方法的基本思想是将数组分成两部分，分别是前面一部分的升序数组和后面一部分的升序数组。 首先对前面一部分的升序数组进行遍历，找到两部分数组的边界，在遍历的过程中，如果找到了目标元素，那么直接返回对应的下标即可。 如果前一部分没有找到目标元素，并且已经找到了两部分数组的边界，此时直接对后面一部分的数组进行二分查找即可。  2.1.2 参考代码 #  /** * 对一个数组指定范围内的数据二分查找目标数据 * * @param nums 数组 * @param target 目标元素 * @return 目标元素在数组中的位置 */ public int binarySearch(int[] nums, int start, int end, int target) { int left = start, right = end; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) { left = mid + 1; } else if (nums[mid] \u0026gt; target) { right = mid - 1; } else if (nums[mid] == target) { return mid; } } return -1; } /** * 33. 搜索旋转排序数组（版本 1：两段寻找） * * @param nums 数组 * @param target 目标元素 * @return 目标元素在数组中的位置 */ public int searchV1(int[] nums, int target) { int len = nums.length; // 用于后面判断数组第一阶段升序范围  int start = nums[0]; int end = -1; if (start == target) {return 0;} // 首先判断数组的第一阶段升序范围，如果在这一范围内找到目标元素，则直接返回相应的下标  for (int i = 1; i \u0026lt; len; i++) { if (nums[i] == target) {return i;} if (nums[i] \u0026lt; start) { end = i; break; } start = nums[i]; } // 如果在数组第一阶段升序范围没有找到目标元素，则在后面一阶段升序范围采用二分查找法查找目标元素  return end == -1 ? -1 : binarySearch(nums, end, len - 1, target); } 2.2 二分查找 #  2.2.1 问题分析 #   这种方法的基本思想是直接在原来的数组上进行二分查找，但是由于基本的二分查找算法只能用于升序数组上，因此需要对基本的二分查找算法进行改进。 主要改进的地方在于在遍历的过程中先判断 $nums[mid]$ 位于左段还是右段：  如果 $nums[mid] \\ge nums[left]$，说明 $nums[mid]$ 位于左段，然后再判断 $target$ 的位置：  如果 $target \\ge nums[left]$ 并且 $target \\lt nums[mid]$，说明 $target$ 位于 $nums[mid]$左边，则 $right = mid - 1$。  否则，说明 $target$ 位于 $mid$右边，则 $left = mid + 1$。    如果 $nums[mid] \\le nums[right]$，说明 $nums[mid]$ 位于右段，然后再判断 $target$ 的位置：  如果 $target \\gt nums[mid]$ 并且 $target \\le nums[right]$，说明 $target$ 位于 $nums[mid]$右边，则 $left = mid + 1$。  否则，说明 $target$ 位于 $mid$左边，则 $right = mid - 1$。       2.2.2 参考代码 #  /** * 33. 搜索旋转排序数组（版本 2：二分查找） * * @param nums 数组 * @param target 目标元素 * @return 目标元素在数组中的位置 */ public int searchV2(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (nums[mid] == target) { return mid; } // 判断 nums[mid] 位于左段还是右段  if (nums[mid] \u0026gt;= nums[left]) { // nums[mid] 位于左段  if (target \u0026gt;= nums[left] \u0026amp;\u0026amp; target \u0026lt; nums[mid]) { right = mid - 1; } else { left = mid + 1; } } else if (nums[mid] \u0026lt;= nums[right]) { // nums[mid] 位于右段  if (target \u0026gt; nums[mid] \u0026amp;\u0026amp; target \u0026lt;= nums[right]) { left = mid + 1; } else { right = mid - 1; } } } return -1; } 3 参考文献 #    33. 搜索旋转排序数组。  多思路完全攻略，🤷‍♀️ 必须秒懂！  "},{"id":122,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.2-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/1.2.3-%E5%A6%82%E4%BD%95%E8%BF%90%E7%94%A8%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/","title":"1.2.3 如何运用二分查找算法","section":"1.2 二分查找","content":"如何运用二分查找算法 #  1 二分查找适用场景 #  在有序数组中搜索给定的目标值的索引，如果目标值重复，可以返回目标值的左侧边界索引或者右侧边界索引。\n2 场景示例 #  2.1 Koko 吃⾹蕉 #  2.1.1 题目 #  珂珂喜欢吃香蕉。这里有 N 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 H 小时后回来。\n珂珂可以决定她吃香蕉的速度 K （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 K 根。如果这堆香蕉少于 K 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉。\n珂珂喜欢慢慢吃，但仍然想在警卫回来前吃掉所有的香蕉。\n返回她可以在 H 小时内吃掉所有香蕉的最小速度 K（K 为整数）。\n示例 1：\n输入: piles = [3,6,7,11], H = 8 输出: 4 示例 2：\n输入: piles = [30,11,23,4,20], H = 5 输出: 30 示例 3：\n输入: piles = [30,11,23,4,20], H = 6 输出: 23 2.1.2 问题分析 #  题目的意思就是说 Koko 每小时最多吃一堆香蕉，如果吃不下的话会等到下一小时再吃；如果吃完了这一堆，也会等到下一小时才会吃下一堆。在这个条件下，让我们确定 Koko 吃香蕉的最小速度（根/小时）。\n首先，算法要求的是H 小时内吃完香蕉的最小速度，我们可以将其设为 speed，则 speed 的最小值为 1，最大值为 max(piles)，一旦发现某个值可以在 H 小时内吃完所有香蕉，这个值就是最小速度。\n因此我们需要从 1 遍历到 max(piles)，就是在连续的空间线性搜索，这就是二分查找法可以发挥作用的标志。由于我们要求的是最小速度，所以可以用搜索左侧边界的二分查找来代替线性搜索，提升效率。\n2.1.3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/21 15:43 * @Description 爱吃香蕉的珂珂 */ public class L875 { /** * 一堆香蕉吃完所用的时间： * 1.当 n 可以被 k 整除时，h = n / k * 2.当 n 不可以被 k 整除时，h = n / k + 1 * @param n 香蕉数量 * @param k 吃香蕉的速度 * @return 香蕉吃完所用的时间 */ public static int timeOf(int n, int k) { return (n / k) + ((n % k) \u0026gt; 0 ? 1 : 0); } /** * 判断能否在指定的时间吃完所有香蕉 * * @param piles 香蕉 * @param k 吃香蕉的速度 * @param h 指定时间 * @return 能否在指定的时间吃完所有香蕉 */ public static boolean canFinish(int[] piles, int k, int h) { int realH = 0; for (int pile: piles) { realH += timeOf(pile, k); } return realH \u0026lt;= h; } /** * 获取香蕉数量最大的一堆的橡胶数量 * * @param piles 香蕉 * @return 香蕉数量最大的一堆的橡胶数量 */ public static int getMax(int[] piles) { int max = -1; for (int i = 0; i \u0026lt; piles.length; i++) { max = Math.max(max, piles[i]); } return max; } /** * 875.爱吃香蕉的珂珂 * 珂珂喜欢吃香蕉。这里有 N 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 H 小时后回来。 * 珂珂可以决定她吃香蕉的速度 K （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 K 根。如果这堆香蕉少于 K 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉。 * 珂珂喜欢慢慢吃，但仍然想在警卫回来前吃掉所有的香蕉。 * 返回她可以在 H 小时内吃掉所有香蕉的最小速度 K（K 为整数）。 * * @param piles 每堆香蕉的数量 * @param h 时长 * @return */ public static int minEatingSpeed(int[] piles, int h) { int left = 1, right = getMax(piles) + 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (canFinish(piles, mid, h)) { right = mid - 1; } else { left = mid + 1; } } return left; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[] piles = new int[]{3, 6, 7, 11}; int h = 8; int k = minEatingSpeed(piles, h); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, k, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); piles = new int[]{30, 11, 23, 4, 20}; h = 5; k = minEatingSpeed(piles, h); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, k, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); piles = new int[]{30, 11, 23, 4, 20}; h = 6; k = minEatingSpeed(piles, h); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 3 结果为：%s，执行用时：%s 微秒\u0026#34;, k, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } 2.2 运送包裹 #  1 题目 #  传送带上的包裹必须在 D 天内从一个港口运送到另一个港口。\n传送带上的第 i 个包裹的重量为 weights[i]。每一天，我们都会按给出重量的顺序往传送带上装载包裹。我们装载的重量不会超过船的最大运载重量。\n返回能在 D 天内将传送带上的所有包裹送达的船的最低运载能力。\n示例 1：\n输入：weights = [1,2,3,4,5,6,7,8,9,10], D = 5 输出：15 解释： 船舶最低载重 15 就能够在 5 天内送达所有包裹，如下所示： 第 1 天：1, 2, 3, 4, 5 第 2 天：6, 7 第 3 天：8 第 4 天：9 第 5 天：10 请注意，货物必须按照给定的顺序装运，因此使用载重能力为 14 的船舶并将包装分成 (2, 3, 4, 5), (1, 6, 7), (8), (9), (10) 是不允许的。 示例 2：\n输入：weights = [3,2,2,4,1,4], D = 3 输出：6 解释： 船舶最低载重 6 就能够在 3 天内送达所有包裹，如下所示： 第 1 天：3, 2 第 2 天：2, 4 第 3 天：1, 4 示例 3：\n输入：weights = [1,2,3,1,1], D = 4 输出：3 解释： 第 1 天：1 第 2 天：2 第 3 天：3 第 4 天：1, 1 2 题目分析 #  要在 D 天内运输完所有货物，货物不可分割，如何确定运输的最小载重（cap）呢？\n题目本质和 Koko 吃香蕉  的问题是一样的，首先确定 cap 的最小值和最大值分别为 max(weights)，和 sum(weights)。\n我们要求最小载重，所以可以用搜索左侧边界的二分查找算法优化线性搜索。\n3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.ArrayList; import java.util.List; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/21 18:47 * @Description 在 D 天内送达包裹的能力 */ public class L1011 { /** * 判断 D 天内能否运完包裹 * @param weights 每个包裹的重量 * @param D 运送期限 * @param cap 载重量 * @return D 天内能否运完包裹 */ public static boolean canFinish(int[] weights, int D, int cap) { int realD = 0, weightSum = 0, index = 0; while (index \u0026lt; weights.length) { int weight = weights[index]; weightSum += weight; if (weightSum \u0026gt; cap) { realD++; weightSum = 0; } else { if (index == weights.length - 1) {realD++;} index++; } } return realD \u0026lt;= D; } /** * 获取货物的总重量 * @param weights 货物重量 * @return 货物的总重量 */ public static int getSum(int[] weights) { int sum = 0; for (int weight : weights) { sum += weight; } return sum; } /** * 获取货物中的最大值 * @param weights 货物重量 * @return 货物中的最大值 */ public static int getMax(int[] weights) { int max = -1; for (int weight : weights) { max = Math.max(max, weight); } return max; } /** * 1011.在 D 天内送达包裹的能力 * @param weights 每个包裹的重量 * @param D 运送期限 * @return 最小载重量 */ public static int shipWithinDays(int[] weights, int D) { int left = getMax(weights), right = getSum(weights); while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (canFinish(weights, D, mid)) {right = mid - 1;} else {left = mid + 1;} } return left; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); int[] weights = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; int D = 5; int cap = shipWithinDays(weights, D); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, cap, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); weights = new int[]{3,2,2,4,1,4}; D = 3; cap = shipWithinDays(weights, D); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, cap, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } "},{"id":123,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.3-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/1.3.1-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/","title":"1.3.1 反转链表","section":"1.3 数据结构","content":"反转链表 #  1 反转整个链表 #  1.1 题目 #  反转一个单链表。\n示例:\n输入: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL 输出: 5-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;1-\u0026gt;NULL 进阶: 你可以迭代或递归地反转链表。你能否用两种方法解决这道题？\n1.2 问题解析 #  对于递归算法，最重要的就是明确递归函数的定义。 具体来说，我们的函数定义是这样的：\n 输入一个节点 head，将以 head 为起点的链表反转，并返回反转之后的头结点。  示例代码如下：\n/** * 206.反转链表（版本 2-递归） * 反转一个单链表。 * @param head 单链表的头指针 * @return 反转后的单链表 */ public ListNode reverse(ListNode head) { // 如果输入的 head 为 null，则返回 null  if (head == null) {return null;} if (head.next == null) {return head;} ListNode last = reverse(head.next); head.next.next = head; head.next = null; return last; }  如上图所示，输入 reverse(head) 之后，会在这里进行递归。不要跳进递归，而是根据刚才函数的定义，来弄清楚这段代码会产生什么后果。\n 这个 reverse(head.next) 执行完成后，整个链表就成了这样：\n 根据函数的定义，reverse 函数会返回反转之后的头结点，我们用变量 last 接收了。\n现在来看下面这部分代码：\nhead.next.next = head;  接下来：\nhead.next = null; return last;  这样，整个链表就反转过来了，这里，有两个地方需要注意：\n 递归函数要使用base case。  if (head.next == null) return head; 当链表递归反转后，新的头结点是last，而之前的head 变成了最后一个节点，需要将链表的末尾指向null。  head.next = null; 1.3 参考代码 #  /** * 206.反转链表（版本 2-递归） * 反转一个单链表。 * @param head 单链表的头指针 * @return 反转后的单链表 */ public ListNode reverseListV2(ListNode head) { // 如果输入的 head 为 null，则返回 null  if (head == null) {return null;} if (head.next == null) {return head;} ListNode last = reverseListV2(head.next); head.next.next = head; head.next = null; return last; } /** * 206.反转链表（版本 1） * 反转一个单链表。 * @param head 单链表的头指针 * @return 反转后的单链表 */ public ListNode reverseListV1(ListNode head) { ListNode pre = head, suf = null, tmpNode; while (pre != null) { tmpNode = pre.next; pre.next = suf; suf = pre; pre = tmpNode; } return suf; } 2 反转链表前 N 个节点 #  这次我们实现一个这样的函数：\n// 将链表的前 n 个节点反转（n \u0026lt;= 链表⻓度） ListNode reverseN(ListNode head, int n) 比如说对于下图链表，执行 reverseN(head, 3)：\n 解决思路和反转整个链表差不多，只需稍加修改即可：\npublic static ListNode sucessor = null; /** * 反转前 N 个节点 * @param head 单链表的头指针 * @return 反转后的单链表 */ public static ListNode reverseN(ListNode head, int n) { // 如果输入的 head 为 null，则返回 null  if (head == null) {return null;} // 记录第 n + 1 个节点  if (n == 1) { sucessor = head.next; return head; } ListNode last = reverseN(head.next, n - 1); head.next.next = head; head.next = sucessor; return last; } 具体的区别如下：\n base case 变为n = 1，反转一个元素，就是他本身，同时要记录后驱节点。 刚才我们直接把head.next 设置为null，因为整个链表反转后原来的head 变成了整个链表的最后一个节点，。但现在head 节点在递归反转之后不一定是最后一个节点了，所以要记录后驱节点successor（第n+1 个节点），反转之后将head 连接上。   3 反转链表的一部分 #  3.1 题目 #  给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u0026lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。\n示例 1： 输入：head = [1,2,3,4,5], left = 2, right = 4 输出：[1,4,3,2,5] 示例 2：\n输入：head = [5], left = 1, right = 1 输出：[5] 3.2 问题解析 #  现在们们需要解决的问题时：给出一个索引区间 [m, n]（索引从 1 开始），仅仅反转区间中的链表元素。\nListNode reverseBetween(ListNode head, int m, int n)  首先，如果m = 1，就相当于反转链表开头的n 个元素，也就是我们刚才实现的功能。 如果m != 1，我们可以把head 的索引视为 1，那么我们就是从第m 个元素开始反转，如果把head.next 视为 1，那么相对于head.next，反转的区间应该是从第m - 1 个元素开始的，以此类推。  3.3 参考代码 #  public static ListNode sucessor = null; /** * 反转前 N 个节点 * @param head 单链表的头指针 * @return 反转后的单链表 */ public static ListNode reverseN(ListNode head, int n) { // 如果输入的 head 为 null，则返回 null  if (head == null) {return null;} // 记录第 n + 1 个节点  if (n == 1) { sucessor = head.next; return head; } ListNode last = reverseN(head.next, n - 1); head.next.next = head; head.next = sucessor; return last; } /** * 92.反转链表 II（版本 2-递归） * 给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u0026lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。 * * @param head 单链表的头指针 * @param left 起始位置 * @param right 结束位置 * @return 反转后的单链表 */ public static ListNode reverseBetweenV2(ListNode head, int left, int right) { // 如果 left == 1，则情况转变为反转前 N 个节点  if (left == 1) {return reverseN(head, right);} // 前进到反转的起点触发 base case  head.next = reverseBetweenV2(head.next, left - 1, right - 1); return head; } /** * 92.反转链表 II（版本 1） * 给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u0026lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。 * * @param head 单链表的头指针 * @param left 起始位置 * @param right 结束位置 * @return 反转后的单链表 */ public static ListNode reverseBetweenV1(ListNode head, int left, int right) { ListNode tmpHead = head, tmpHeadBefore, tmpLeftNode = null, tmpRightNode = null; // 1.计算出链表中需要移动的起点和终点对应的节点  // tmpLeftNode: left 对应的节点  // tmpRightNode: right 对应的节点  int index = 0; while (tmpHead != null) { if (index == 0) {tmpHead =head;} else {tmpHead = tmpHead.next;} index++; if (index == left) { tmpLeftNode = tmpHead; } if (index == right) { tmpRightNode = tmpHead; } } // 2.将 tmpLeftNode 和 tmpRightNode 节点中间的一段链表进行反转  tmpHead = head; index = 0; while (tmpHead != null) { tmpHeadBefore = tmpHead; if (index == 0) {tmpHead =head;} else {tmpHead = tmpHead.next;} index++; if (index == left) { ListNode pre = tmpLeftNode, suf = tmpRightNode.next, tmpNode; while (pre != tmpRightNode) { tmpNode = pre.next; pre.next = suf; suf = pre; pre = tmpNode; } pre.next = suf; if (index != 1) {tmpHeadBefore.next = pre;} else {head = pre;} break; } } return head; } 参考文献 #    206. 反转链表。  92. 反转链表 II。  递归反转链表的一部分。  "},{"id":124,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.3-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/1.3.2-%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E7%AC%ACk%E4%B8%AA%E6%9C%80%E5%A4%A7%E5%85%83%E7%B4%A0/","title":"1.3.2 数组中的第k个最大元素","section":"1.3 数据结构","content":"数组中的第k个最大元素 #  1 题目 #  在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。\n示例 1:\n输入: [3,2,1,5,6,4] 和 k = 2 输出: 5 示例 2:\n输入: [3,2,3,1,2,4,5,5,6] 和 k = 4 输出: 4 说明:\n你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。\n2 解题思路 #  下面代码中用到的 CommonUtils 工具类代码如下：\n/** * @author peng.wei * @version 1.0 * @date 2021/5/3 20:53 * @Description 通用工具类 */ public class CommonUtils { /** * 交换数组中两个指定位置的元素 * @param arr 数组 * @param i 下标 1 * @param j 下标 2 */ public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } 2.1 优先队列 #  2.1.1 问题解析 #   可以创建一个优先级队列，每次将数据添加到队列中。 当队列的大小超过 $k$ 时，就将队首的元素弹出，这样到最后，队列中就存放着前 $k$ 个较大的元素。 此时第 $k$ 的元素就在队首，直接将其弹出并返回即可。  2.1.2 参考代码 #  /** * 215. 数组中的第 K 个最大元素（版本 1：队列） * @param nums 数组 * @param k 元素下标 * @return 第 K 个最大元素 */ public int findKthLargestV1(int[] nums, int k) { Queue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;(); int m = nums.length; for (int i = 0; i \u0026lt; m; i++) { queue.add(nums[i]); if (queue.size() \u0026gt; k) { queue.poll(); } } return queue.poll(); } 2.2 堆排序 #  2.2.1 问题解析 #   我们可以构建一个大顶堆，然后在堆排序的过程中，每次调整大顶堆，我们都可以获取一个较大的元素，这样我们只需调整 $k$ 次，便可以将前 $k$ 大的元素排好位置，然后直接返回第 $arr.length - k$ 个元素即可。 对于求前 $k$ 大元素的题目，一般用堆排序来解决。  2.2.2 参考代码 #  /** * 调整堆 * @param arr 数组 * @param index 需要堆化处理的数据的索引 * @param length 未排序的数组的长度 */ public static void adjustHeap(int[] arr, int index, int length) { int left = (index \u0026lt;\u0026lt; 1) + 1; int right = left + 1; int max = left; if (left \u0026gt; length) return; if (right \u0026lt;= length \u0026amp;\u0026amp; arr[right] \u0026gt; arr[left]) {max = right;} if (arr[max] \u0026gt; arr[index]) { CommonUtils.swap(arr, max, index); adjustHeap(arr, max, length); } } /** * 堆排序 * @param arr 数组 * @param k 前几个数 */ public static void sort(int[] arr, int k) { // 1. 构造大顶堆  int length = arr.length; int beginIndex = (length \u0026gt;\u0026gt; 1) - 1; for (int i = beginIndex; i \u0026gt;= 0; i--) { adjustHeap(arr, i, length - 1); } // 2. 调整大顶堆  for (int i = length - 1; i \u0026gt;= length - k; i--) { CommonUtils.swap(arr, 0, i); adjustHeap(arr, 0, i - 1); } } /** * 215. 数组中的第 K 个最大元素（版本 2：堆排序） * @param nums 数组 * @param k 前几个数 * @return 第 K 个最大元素 */ public int findKthLargestV2(int[] nums, int k) { sort(nums, k); return nums[nums.length - k]; } 2.3 快速排序（通过 partition 减治） #  2.3.1 问题解析 #  本题目还可以借助快速排序过程中的 partition 思想来实现，借助 partition 操作定位到最终排定以后索引为 $arr.length - k$ 的那个元素即为第 $k$ 大元素，partition（切分）的原理如下：\n 对于某索引 $j$，$nums[j]$ 已经排定，即 $nums[j]$ 经过 partition 操作以后会放置在他最应该放置的地方。 $nums[left]$ 到 $nums[j - 1]$ 中的所有元素都不大于 $nums[j]$。 $nums[j + 1]$ 到 $nums[right]$ 中的所有元素都不小于 $nums[j]$。  partition 操作总能排定一个元素，还能够知道这个元素他最终所在的位置，这样每经过一次 partition 操作就能够缩小搜索的范围，这样的思想就是减而治之（是分而治之思想的特例）。\n2.3.2 参考代码 #  /** * 快速排序算法（递归） * @param arr 数组 * @param _left 左边界 * @param _right 右边界 * @param target 元素下标 * @return 第 K 个最大元素 */ public static void quickSort(int[] arr, int _left, int _right, int target) { int left = _left; int right = _right; int temp = 0; if (left \u0026lt;= right) { // 待排序的第一个元素作为基准元素  temp = arr[left]; // 从左到右交替扫描，直到 left = right  while (left != right) { // 从右往左扫描，找到第一个比基准元素小的元素  while (right \u0026gt; left \u0026amp;\u0026amp; arr[right] \u0026gt;= temp) { right--; } // 找到这种元素 arr[right] 后与 arr[left] 交换  arr[left] = arr[right]; // 从左往右扫描，找到第一个比基准元素大的元素  while (left \u0026lt; right \u0026amp;\u0026amp; arr[left] \u0026lt;= temp) { left++; } // 找到这种元素 arr[left] 后与 arr[right] 交换  arr[right] = arr[left]; } // 基准元素归为  arr[right] = temp; if (right \u0026gt; target) { // 对基准元素左边的元素进行递归排序  quickSort(arr, _left, left - 1, target); } else if (right \u0026lt; target) { // 对基准元素右边的元素进行递归排序  quickSort(arr, right + 1, _right, target); } else if (right == target) { return; } } } /** * 215. 数组中的第 K 个最大元素（版本 3：快速排序） * @param nums 数组 * @param k 元素下标 * @return 第 K 个最大元素 */ public int findKthLargestV3(int[] nums, int k) { int length = nums.length; quickSort(nums, 0, length - 1, length - k); return nums[length - k]; } 3 参考文献 #    215. 数组中的第 K 个最大元素。  通过 partition 减治 + 优先队列（Java、C++、Python）。  "},{"id":125,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.3-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/1.3.3-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/","title":"1.3.3 二叉树的最近公共祖先","section":"1.3 数据结构","content":"1 题目 #  给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个节点 p、q，最近公共祖先表示为一个节点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n示例 1：\n 输入：root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出：3 解释：节点 5 和节点 1 的最近公共祖先是节点 3 。 示例 2：\n 输入：root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 输出：5 解释：节点 5 和节点 4 的最近公共祖先是节点 5 。因为根据定义最近公共祖先节点可以为节点本身。 示例 3：\n输入：root = [1,2], p = 1, q = 2 输出：1 提示：\n 树中节点数目在范围 [2, 105] 内。 -109 \u0026lt;= Node.val \u0026lt;= 109 所有 Node.val 互不相同 。 p != q p 和 q 均存在于给定的二叉树中。  2 解题思路 #  2.1 前序遍历 #  2.1.1 问题分析 #   首先创建相关变量：  $stack$：存储根节点到遍历节点的遍历路径。 $res$、$res2$：分别存储 $p$、$q$ 的遍历节点信息。   然后对二叉树进行前序遍历：  将 $root$ 入栈。 如果 $root=p$：  将栈中的信息存入 $res$。 将 $res$ 中的信息存入 $stack$，用于后面的遍历。   如果 $root=q$：  将栈中的信息存入 $res2$。 将 $res2$ 中的信息存入 $stack$，用于后面的遍历。   递归遍历左子树。 递归遍历右子树。 如果栈的大小大于 0，则将栈中的元素弹出。   获取 $res$ 和 $res2$ 长度的最小值 $size$。 然后对 $size$ 从 $i=0$ 开始遍历：  如果 $res$ 的长度大于 $res2$：  如果 $res2$ 中第 $i$ 个元素等于 $res$ 中第 $res.size()-size+i$ 个元素，则返回 $res2$ 的第 $i$ 个元素。   如果 $res2$ 的长度大于 $res$：  如果 $res$ 中第 $i$ 个元素等于 $res2$ 中第 $res2.size()-size+i$ 个元素，则返回 $res$ 的第 $i$ 个元素。     如果上面的条件都不成立，则最后返回 $null$。   2.1.2 参考代码 #  Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); /** * 前序遍历 * * @param root 根节点 * @param p 其中一个需要查找节点 * @param q 另外一个需要查找的节点 */ public void preOrderRecur(TreeNode root, TreeNode p, TreeNode q, List\u0026lt;TreeNode\u0026gt; res, List\u0026lt;TreeNode\u0026gt; res2) { // base case  if (root == null) { return; } // 将根节点入栈  stack.push(root); if (root == p) { // 如果遍历到一个节点和 p 节点相同，就将栈中的数据添加到 res 中，即为从根节点到节点 p 的遍历路径  int size = stack.size(); for (int i = 0; i \u0026lt; size; i++) { res.add(stack.pop()); } // 从 res 中还原栈  for (int i = res.size() - 1; i \u0026gt;= 0; i--) { stack.push(res.get(i)); } } else if (root == q) { // 如果遍历到一个节点和 q 节点相同，就将栈中的数据添加到 res2 中，即为从根节点到节点 q 的遍历路径  int size = stack.size(); for (int i = 0; i \u0026lt; size; i++) { res2.add(stack.pop()); } // 从 res2 中还原栈  for (int i = res2.size() - 1; i \u0026gt;= 0; i--) { stack.push(res2.get(i)); } } // 递归遍历左子树  preOrderRecur(root.left, p, q, res, res2); // 递归遍历右子树  preOrderRecur(root.right, p, q, res, res2); // 如果栈的大小大于 0，则将栈中的元素弹出  if (stack.size() \u0026gt; 0) { stack.pop(); } } /** * 236. 二叉树的最近公共祖先（版本 1：前序遍历） * * @param root 根节点 * @param p 其中一个需要查找节点 * @param q 另外一个需要查找的节点 * @return 两个指定节点的最近公共祖先 */ public TreeNode lowestCommonAncestorV1(TreeNode root, TreeNode p, TreeNode q) { // 用于存储从根节点到节点 p 的遍历路径  List\u0026lt;TreeNode\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); // 用于存储从根节点到节点 q 的遍历路径  List\u0026lt;TreeNode\u0026gt; res2 = new ArrayList\u0026lt;\u0026gt;(); // 开始遍历  preOrderRecur(root, p, q, res, res2); int size = Math.min(res.size(), res2.size()); // 对 res 和 res2 进行遍历，寻找二者相同的节点，即为二叉树的公共祖先  for (int i = 0; i \u0026lt; size; i++) { if (res.size() \u0026gt; res2.size()) { if (res2.get(i) == res.get(res.size() - size + i)) { return res2.get(i); } } else if (res.size() \u0026lt;= res2.size()) { if (res.get(i) == res2.get(res2.size() - size + i)) { return res.get(i); } } } return null; } 2.2 后序遍历 #  2.2.1 问题分析 #   对二叉树进行中序遍历：  如果 $root$ 为空，则返回空。 如果 $root$ 等于 $p$ 或者 $q$，则这棵树最后一定返回 $p$ 或者 $q$。 递归遍历左节点，得到 $left$。 递归遍历右节点，得到 $right$。 若 $left$ 为空，则最终结果只能看到 $right$。 若 $right$ 为空，则最终结果只能看 $left$。 如果 $left$ 和 $right$ 都不为空，因为只给了 $p$ 和 $q$ 两个节点，都非空，说明一边一个，因此 $root$ 是他们的公共祖先。 如果上面的条件都不满足的话，直接返回 $null$。    后序遍历的演示动画可参考： 236. 二叉树的最近公共祖先（后序遍历 DFS ，清晰图解）。\n2.2.2 参考代码 #  /** * 236. 二叉树的最近公共祖先（版本 2：后序遍历） * * @param root 根节点 * @param p 其中一个需要查找节点 * @param q 另外一个需要查找的节点 * @return 两个指定节点的最近公共祖先 */ public TreeNode lowestCommonAncestorV2(TreeNode root, TreeNode p, TreeNode q) { // base case  if (root == null) {return null;} // 如果 root 等于 p 或者 q，那这棵树一定返回 p 或者 q  if (root == p || root == q) {return root;} // 递归遍历左节点  TreeNode left = lowestCommonAncestorV2(root.left, p, q); // 递归遍历右节点  TreeNode right = lowestCommonAncestorV2(root.right, p, q); // 如果 left 为空，则最终结果只要看 right  if (left == null) {return right;} // 如果 right 为空，则最终结果只要看 left  if (right == null) {return left;} // 如果 left 和 right 都不为空，因为只给了 p 和 q 两个节点，都非空，说明一边一个，因此 root 是他们的最近公共祖先  if (left != null \u0026amp;\u0026amp; right != null) {return root;} // 因为最后必须有返回值，所以如果上面的条件都不满足的话，直接返回 null  return null; } 3 参考文献 #    236. 二叉树的最近公共祖先。  236. 二叉树的最近公共祖先（后序遍历 DFS ，清晰图解）。  【C++ 经典递归】思路非常好理解 时间复杂度 O(n), 空间复杂度 O(n)。  "},{"id":126,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.5-%E4%BA%8C%E5%8F%89%E6%A0%91/1.5.1-%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/","title":"1.5.1 二叉树遍历","section":"1.5 二叉树","content":"二叉树遍历 #  1 算法模板 #  二叉树遍历算法分为两种，一种是深度优先遍历算法，例如前序遍历、中序遍历、后序遍历，另一种是广度优先遍历算法，例如层次遍历。\n1.1 深度优先遍历算法 #  1.1.1 递归解法 #  1.1.1.1 前序遍历 #  /** * 前序遍历（递归解法） * * @param head 头结点 */ public void preOrderRecur(TreeNode head) { // base case  if (head == null) { return; } // 访问根节点  System.out.println(head.val); // 遍历左节点  preOrderRecur(head.left); // 遍历右节点  preOrderRecur(head.right); } 1.1.1.2 中序遍历 #  /** * 中序遍历（递归解法） * * @param head 头结点 */ public void inOrderRecur(TreeNode head) { // base case  if (head == null) { return; } // 遍历左节点  preOrderRecur(head.left); // 访问根节点  System.out.println(head.val); // 遍历右节点  preOrderRecur(head.right); } 1.1.1.3 后序遍历 #  /** * 后序遍历（递归解法） * * @param head 头结点 */ public void postOrderRecur(TreeNode head) { // base case  if (head == null) { return; } // 遍历左节点  preOrderRecur(head.left); // 遍历右节点  preOrderRecur(head.right); // 访问根节点  System.out.println(head.val); } 1.1.2 迭代解法 #  1.1.2.1 前序遍历 #    初始时，将根节点入栈。\n  判断栈是否为空，如果栈不为空：\n 弹出栈顶元素 $node$，并将 $node.val$ 输出。 如果 $node$ 的右子树不为空，则将其对应的右子树 $node.right$ 入栈。 如果 $node$ 的左子树不为空，则将其对应的左子树 $node.right$ 入栈。     /** * 前序遍历（迭代解法） * * @param head 头结点 */ public void preOrderInter(TreeNode head) { // base case  if (head == null) { return; } // 用来模仿递归解法中的栈  Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); // 初始时将头结点入栈  stack.push(head); while (!stack.empty()) { // 访问根节点  TreeNode node = stack.pop(); System.out.println(node.val); // 将右节点入栈（左节点会先出栈，所以等价于遍历左节点）  if (node.right != null) { stack.push(node.right); } // 将左节点入栈（右节点会后出栈，所以等价于遍历右节点）  if (node.left != null) { stack.push(node.left); } } } 1.1.2.2 中序遍历 #   令当前指针 $cur=head$。 判断栈和当前 $cur$ 是否为空，如果二者有一个不为空：  判断当前 $cur$ 是否为空，如果 $cur$ 不为空（目的是尽量让当前节点的左子树入栈）：  将 $cur$ 入栈。 令 $cur=cur.left$。 重复步骤 A，直到 $cur$ 为空。   弹出栈中的节点 $node$，并输出弹出节点的值 $node.val$。 如果 $node$ 的右子树不为空，则令 $cur=cur.right$。     /** * 中序遍历（迭代解法） * * @param head 头结点 */ public void inOrderInter(TreeNode head) { // base case  if (head == null) { return; } TreeNode cur = head; // 用来模仿递归解法中的栈  Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while (!stack.empty() || cur != null) { // 尽可能将这个节点的左子树入栈，相当于访问左子树  while (cur != null) { stack.push(cur); cur = cur.left; } // 相当于访问根节点  TreeNode node = stack.pop(); System.out.println(node.val); // 相当于访问右节点  if (node.right != null) { cur = node.right; } } } 1.1.2.3 后序遍历 #  后序遍历和先序遍历类似，不过后序遍历是左子树先入栈，右子树后入栈。\n/** * 后序遍历（迭代解法） * * @param head 头结点 */ public void postOrderInter(TreeNode head) { // base case  if (head == null) { return; } // 用来模仿递归解法中的栈  Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); // 用于将根节点的值入栈  Stack\u0026lt;Integer\u0026gt; stack2 = new Stack\u0026lt;\u0026gt;(); // 初始时将头结点入栈  stack.push(head); while (!stack.empty()) { // 访问根节点  TreeNode node = stack.pop(); // 将根节点的值入栈  stack2.push(node.val); // 将右节点入栈（左节点会先出栈，所以等价于遍历左节点）  if (node.right != null) { stack.push(node.right); } // 将左节点入栈（右节点会后出栈，所以等价于遍历右节点）  if (node.left != null) { stack.push(node.left); } } // 遍历第二个栈中的值  while (!stack2.isEmpty()) { System.out.println(stack2.pop()); } } 1.2 广度优先遍历算法 #  1.2.1 层次遍历 #  1.2.1.1 输出一维数组 #  和前序遍历类似，不过层次遍历采用的是队列来存储相应的节点。\n/** * 层次遍历（迭代解法，输出一维数组） * * @param head 头结点 */ public void levelOrderInterV1(TreeNode head) { // 用户保存根节点  Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); // 初始时将根节点加入队列  queue.add(head); while (!queue.isEmpty()) { // 将队列中的节点弹出，然后将其值加入到结果中  TreeNode node = queue.poll(); System.out.println(node.val); if (node.left != null) { // 如果左子树非空，则将左子树加入到队列中  queue.add(node.left); } if (node.right != null) { // 如果右子树非空，则将右子树加入到队列中  queue.add(node.right); } } } 1.2.1.2 输出二维数组 #   创建结果数组 $res$， 初始时，将根节点加入队列。 判断队列是否为空，如果队列不为空：  创建临时结果数组 $tempRes$，然后计算队列的大小为 $n$，然后将队列中的元素依次出队：  弹出队首元素 $node$，并将 $node.val$ 添加到 $tempRes$ 中。 如果 $node$ 的右子树不为空，则将其对应的右子树 $node.right$ 加入队列。 如果 $node$ 的左子树不为空，则将其对应的左子树 $node.left$ 加入队列。   将 $tempRes$ 添加到中 $res$。     /** * 层次遍历（迭代解法，输出二维数组） * * @param head 头结点 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrderInterV2(TreeNode head) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); // 如果头结点为空，则直接返回 res  if (head == null) {return res;} // 用户保存根节点  Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); // 初始时将根节点加入队列  queue.add(head); while (!queue.isEmpty()) { // 一次性将同一层的节点都遍历一下  List\u0026lt;Integer\u0026gt; tempRes = new ArrayList\u0026lt;\u0026gt;(); int n = queue.size(); for (int i = 0; i \u0026lt; n; i++) { // 将队列中的节点弹出，然后将其值加入到结果中  TreeNode node = queue.poll(); tempRes.add(node.val); if (node.left != null) { // 如果左子树非空，则将左子树加入到队列中  queue.add(node.left); } if (node.right != null) { // 如果右子树非空，则将右子树加入到队列中  queue.add(node.right); } } // 将当前层的遍历结果添加到最终结果中  res.add(tempRes); } return res; } 1.2.1.3 输出锯齿形二维数组 #   该方法是在输出二维数组的基础上加上一个当前遍历层数的判断，如果当前遍历的层数为偶数层，则将当前的结果进行反转一下，然后再添加到最后的结果中，这样便可以实现输出锯齿形二维数组，具体的演示过程可参考前面的 输出二维数组解法。  /** * 层次遍历（迭代解法，输出锯齿形二维数组） * * @param head 头结点 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; zigzagLevelOrderInter(TreeNode head) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); // 记录当前遍历的层数  int level = 0; // 如果头结点为空，则直接返回 res  if (head == null) {return res;} // 用户保存根节点  Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); // 初始时将根节点加入队列  queue.add(head); // 遍历的层数加 1  level++; while (!queue.isEmpty()) { // 一次性将同一层的节点都遍历一下  List\u0026lt;Integer\u0026gt; tempRes = new ArrayList\u0026lt;\u0026gt;(); int n = queue.size(); for (int i = 0; i \u0026lt; n; i++) { // 将队列中的节点弹出，然后将其值加入到结果中  TreeNode node = queue.poll(); tempRes.add(node.val); if (node.left != null) { // 如果左子树非空，则将左子树加入到队列中  queue.add(node.left); } if (node.right != null) { // 如果右子树非空，则将右子树加入到队列中  queue.add(node.right); } } // 如果遍历的层数是偶数层，则将当前层的结果反转一下，变为从右往左遍历  if (level % 2 == 0) { List\u0026lt;Integer\u0026gt; tempRes2 = new ArrayList\u0026lt;\u0026gt;(); for (int i = tempRes.size() - 1; i \u0026gt;= 0; i--) { tempRes2.add(tempRes.get(i)); } tempRes = tempRes2; } // 将当前层的遍历结果添加到最终结果中  res.add(tempRes); level++; } // 遍历的层数加 1  return res; } 2 典型题目 #  2.1 翻转二叉树 #  2.1.1 问题分析 #   这道题目主要有两种解法，分别是递归（深度优先遍历）、迭代（广度优先遍历），分别套用前面的相应的模板即可。 需要注意的是：  递归时需要定义一个临时变量来保存左子树的数据，然后再进行下面的递归。 迭代时直接在原来访问根节点的地方添加节点替换逻辑即可。    2.1.2 参考代码 #  2.1.2.1 递归 #  /** * 226. 翻转二叉树（版本 1：递归） * @param root 根节点 * @return 反转后的二叉树 */ public TreeNode invertTreeV1(TreeNode root) { return dfs(root); } /** * 递归翻转二叉树 * @param root 根节点 * @return 反转后的二叉树 */ public TreeNode dfs(TreeNode root) { if (root == null) {return null;} // 保留左子树的数据  TreeNode leftTemp = root.left; root.left = dfs(root.right); root.right = dfs(leftTemp); return root; } 2.1.2.2 迭代 #  /** * 226. 翻转二叉树（版本 2：层序遍历） * @param root 根节点 * @return 反转后的二叉树 */ public TreeNode invertTreeV2(TreeNode root) { if (root == null) {return null;} Queue\u0026lt;TreeNode\u0026gt; queue = new ArrayDeque\u0026lt;\u0026gt;(); queue.add(root); while(!queue.isEmpty()) { TreeNode node = queue.poll(); // 交换节点信息  TreeNode leftTemp = node.left; node.left = node.right; node.right = leftTemp; if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } return root; } 2.2 对称二叉树 #  2.2.1 问题分析 #    对于二叉树的题目，一般可通过递归和迭代两种方法来求解：\n 递归时一般需先找出题目中蕴含的递归关系，然后确定递归函数的含义，一般递归采用的都是深度优先遍历，递归算法也是在原有的深度优先遍历算法上的改进。 迭代时一般采用的是广度优先遍历，迭代算法也是在原有的广度优先遍历算法上的改进。    如果一棵树的左子树和右子树对称，那么这个树是对称的。\n  因此，这个问题可以转化为两棵树在什么情况下互为镜像，如果同时满足下面的条件，两棵树互为镜像：\n 他们的两个根节点具有相同的值。 每个树的左子树都与另一个树的右子树镜像对称。 每个树的右子树都与另一个树的左子树镜像对称。    我们可以实现这样一个递归函数，通过同步移动两个指针的方法来遍历这棵树：\n $left$ 指针和 $right$ 指针一开始都指向这棵树的根。 随后 $left$ 左移时，$right$右移，$left$右移时，$right$ 左移。 每次检查当前 $left$ 和 $right$ 节点的值是否相等，如果相等再判断左右子树是否对称。     2.2.2 参考代码 #  2.2.2.1 递归 #  /** * 101. 对称二叉树（版本 1：递归） * * @param root 根节点 * @return 二叉树是否镜像对称 */ public boolean isSymmetricV1(TreeNode root) { if (root == null) {return true;} return dfs(root.left, root.right); } /** * 递归判断二叉树是否镜像对称 * * @param left 左子树 * @param right 右子树 * @return 二叉树是否镜像对称 */ public boolean dfs(TreeNode left, TreeNode right) { if (left == null \u0026amp;\u0026amp; right == null) {return true;} if (left == null || right == null) {return false;} return left.val == right.val \u0026amp;\u0026amp; dfs(left.left, right.right) \u0026amp;\u0026amp; dfs(left.right, right.left); } 2.2.2.2 迭代 #  /** * 101. 对称二叉树（版本 2：迭代） * * @param root 根节点 * @return 二叉树是否镜像对称 */ public boolean isSymmetricV2(TreeNode root) { if (root == null) {return true;} // 使用一个队列模拟同时存在两棵相同的二叉树，然后判断这两棵二叉树是否镜像对称  Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offer(root); queue.offer(root); while (!queue.isEmpty()) { TreeNode left = queue.poll(); TreeNode right = queue.poll(); if (left == null \u0026amp;\u0026amp; right == null) {continue;} if (left == null || right == null) {return false;} if (left.val != right.val) {return false;} queue.offer(left.left); queue.offer(right.right); queue.offer(left.right); queue.offer(right.left); } return true; } 2.3 验证二叉搜索树 #  2.3.1 问题分析 #   对于二叉树的题目，我们一般可以考虑一下看是否能利用二叉树的三种深度优先遍历和一种广度优先遍历方法来解决。 在该题目中，我们可以利用中序遍历，只要判断每一个遍历的节点的值是否都大于前一个节点，进而就可以判断该二叉树是否为二叉搜索树。  2.3.2 参考代码 #  2.3.2.1 递归 #  long pre = Long.MIN_VALUE; /** * 98. 验证二叉搜索树（版本 1：递归（中序遍历）） * @param root 根节点 * @return 是否为有效的二叉搜索树 */ public boolean isValidBSTV1(TreeNode root) { if (root == null) {return true;} if (!isValidBSTV1(root.left)) {return false;} /*访问左子树：判断左子树是否为二叉搜索树*/ if (root.val \u0026lt;= pre) {return false;} /*访问根节点：判断根节点的值是否大于等于中序遍历的前一个节点的值*/ pre = root.val; /*将当前节点保存为前一个节点*/ return isValidBSTV1(root.right); /*访问右子树：判断右子树是否为二叉搜索树*/ } 2.3.2.2 迭代 #  long pre = Long.MIN_VALUE; /** * 98. 验证二叉搜索树（版本 2：迭代（中序遍历）） * @param root 根节点 * @return 是否为有效的二叉搜索树 */ public boolean isValidBSTV2(TreeNode root) { if (root == null) {return true;} Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); TreeNode cur = root; while (!stack.empty() || cur != null) { while(cur != null) { /*尽可能将这个节点的左子树入栈，相当于访问左子树*/ stack.push(cur); cur = cur.left; } TreeNode node = stack.pop(); /*相当于访问根节点，然后判断根节点的值是否大于等于中序遍历的前一个节点的值*/ if (node.val \u0026lt;= pre) {return false;} pre = node.val; if (node.right != null) {cur = node.right;} /*相当于访问右子树*/ } return true; } 2.4 二叉树的完全性检验 #  2.4.1 问题分析 #   对于二叉树的题目，我们一般可以考虑一下看是否能利用二叉树的三种深度优先遍历和一种广度优先遍历方法来解决。 该题目可以通过 层序遍历来解决，判断的依据就是，如果一层中出现了一个为空的节点，并且后面还有节点，那么该二叉树就不是一个完全二叉树，如果后面没有节点，那么该二叉树就是一个完全二叉树。  2.4.2 参考代码 #  // 判断是否到达了二叉树末尾，只要层序遍历时遇到一个空节点，就认为到达了二叉树末尾 boolean reachEnd = false; /** * 958. 二叉树的完全性检验（层序遍历） * @param root 根节点 * @return 当前二叉树是否为完全二叉树 */ public boolean isCompleteTree(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(root); while (!queue.isEmpty()) { TreeNode node = queue.poll(); // 如果到达了二叉树末尾，但后面还有非空节点，则该二叉树不是完全二叉树  if (reachEnd \u0026amp;\u0026amp; node != null) { return false; } // 只要层序遍历时遇到一个空节点，就认为到达了二叉树末尾，令 reachEnd 为 true，然后进行下一个节点的遍历  if (node == null) { reachEnd = true; continue; } queue.add(node.left); queue.add(node.right); } return true; } 2.5 二叉搜索树的第k大节点 #  2.5.1 问题分析 #   该题目解法基于二叉树中序遍历为递增序列这一性质，可以得到二叉树中序遍历倒序为递减序列。 因此，求二叉树第$k$大的节点可以转化为求二叉树的中序遍历倒序的第$k$个节点。  2.5.2 参考代码 #  /** * 剑指 Offer 54. 二叉搜索树的第k大节点（中序遍历倒序） * @param root 根节点 * @param k 最值序号 * @return 二叉搜索树的第k大节点 */ public int kthLargest(TreeNode root, int k) { // base case  if (root == null) { return -1; } TreeNode cur = root; // 用来模仿递归解法中的栈  Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while (!stack.empty() || cur != null) { // 尽可能将这个节点的右子树入栈，相当于访问右子树  while (cur != null) { stack.push(cur); cur = cur.right; } // 相当于访问根节点  TreeNode node = stack.pop(); if (--k == 0) {return node.val;} // 相当于访问左节点  if (node.left != null) { cur = node.left; } } return -1; } 参考文献 #    图解 二叉树的四种遍历。  BFS 的使用场景总结：层序遍历、最短路径问题。  103. 二叉树的锯齿形层序遍历。  226. 翻转二叉树。  动画演示 两种实现 226. 翻转二叉树。  101. 对称二叉树。  对称二叉树。  958. 二叉树的完全性检验。  层序遍历。  剑指 Offer 54. 二叉搜索树的第k大节点。  面试题54. 二叉搜索树的第 k 大节点（中序遍历 + 提前返回，清晰图解）。  "},{"id":127,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.5-%E4%BA%8C%E5%8F%89%E6%A0%91/1.5.2-%E5%B2%9B%E5%B1%BF%E9%97%AE%E9%A2%98/","title":"1.5.2 岛屿问题","section":"1.5 二叉树","content":"1 前言 #   我们所熟悉的 DFS（深度优先搜索）问题通常是在树或者图结构上进行的，而我们今天要讨论的 DFS 问题，是在一种【网格】结构中进行的，岛屿问题是这类网格 DFS 问题的典型代表。 网格结构遍历起来要比二叉树复杂一些，如果没有掌握一定的方法，DFS 代码容易写得冗长繁杂。 本文将以岛屿问题为例，展示网格类问题 DFS 通用思路，以及如何让代码变得简洁。  2 网格问题的基本概念 #   我们首先要明确一下岛屿问题中的网格结构是如何定义的，以方便我们后面的讨论。 网格问题是由 $m \\times n$ 个小方格组成一个网格，每个小方格与其上下左右四个方格认为是相邻的，要在这样的网格上进行某种搜索。 岛屿问题是一类典型的网格问题，每个格子中的数字可能是 0 或者 1，我们把数字为 0 的格子看成海洋格子，数字为 1 的格子看成陆地格子，这样相邻的陆地格子就连接成一个岛屿。   在这样一个设定下，就出现了各种岛屿问题的变种，包括岛屿的数量、面积、周长等，不过这些问题，基本都可以用 DFS 遍历来解决。  3 网格 DFS 的基本结构 #  3.1 基本结构 #    网格结构要比二叉树结构稍微复杂一些，他其实是一种简化版的图结构，要写好网格上的 DFS 遍历，我们首先要理解二叉树上的 DFS 遍历方法，再类比写出网格结构上的 DFS 遍历，我们写的 二叉树 DFS 遍历一般是这样的：\nvoid traverse(TreeNode root) { // 判断 base case  if (root == null) { return; } // 访问两个相邻结点：左子结点、右子结点  traverse(root.left); traverse(root.right); }   可以看到，二叉树的 DFS 遍历有两个要素，分别为判断 base case和访问相邻接点：\n 第一个要素是判断 base case：  一般来说，二叉树遍历的base case 是root == null，这样一个条件判断其实有两个含义：  一方面，这表示root指向的子树为空，不需要再往下遍历了。 另一方面，在 root == null 的时候及时返回，可以让后面的 root.left 和 root.right 操作不会出现空指针异常。     第二个要素是访问相邻接点：  二叉树的相邻接点非常简单，只有左子树和右子树两个。 二叉树本身就是一个递归定义的结构：一棵二叉树，他的左子树和右子树也是一棵二叉树，那么我们的 DFS 遍历只需要递归调用左子树和右子树即可。      对于网格上的 DFS，我们完全可以参考二叉树的 DFS，写出网格 DFS 的两个要素：\n  首先，网格 DFS 中的 base case 从二叉树的 base case 对应过来，应该是网格中不需要继续遍历、$grid[r][c]$会出现数组下标越界异常的格子，也就是那些超出网格范围的格子：   其次，网格结构中的格子有上下左右四个相邻的节点，对于格子 $(r,c)$ 来说（$r$ 和 $c$ 分别代表行坐标和列坐标），四个相邻的格子分别是：\n 上：$(r+1,c)$。 下：$(r-1,c)$。 左：$(r,c-1)$。 右：$(r,c+1)$。       这样，我们得到了网格 DFS 遍历的框架代码：\n/** * 判断是否在方格范围内 * @param grid 方格数组 * @param r 横坐标 * @param c 纵坐标 * @return (r, c) 是否在方格范围内 */ public boolean inArea(int[][] grid, int r, int c) { return r \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; grid.length \u0026amp;\u0026amp; c \u0026gt;= 0 \u0026amp;\u0026amp; c \u0026lt; grid[0].length; } /** * 方格 DFS 遍历算法（版本 1：未加重复遍历的判断） * @param grid 方格数组 * @param r 横坐标 * @param c 纵坐标 */ public void dfsV1(int[][] grid, int r, int c) { // base case  // 如果坐标 (r, c) 超出了网格范围，直接返回  if (!inArea(grid, r, c)) { return; } // 访问上、下、左、右四个相邻接点  dfsV1(grid, r + 1, c); dfsV1(grid, r - 1, c); dfsV1(grid, r, c - 1); dfsV1(grid, r, c + 1); }   3.2 完善后的结构（避免重复遍历） #    网格结构的 DFS 与二叉树的 DFS 最大的不同之处在于遍历中可能遇到遍历过的节点，这是因为网格结构本质上是一个图，我们可以把每个格子看成图中的节点，每个节点有上下左右的四条边，在图中遍历时，自然可能遇到重复遍历节点，这时候，DFS 可能会不停地兜圈子，永远停不下来，如下图所示：   我们可以通过标记已经遍历过的格子来避免重复遍历问题，以岛屿问题为例：\n 我们需要在所有值为 1 的陆地格子上做 DFS 遍历。 每走过一个陆地格子，就把格子的值改为 2，这样当我们遇到 2 的时候，就知道这是遍历过的格子了。 也就是说，每个格子可能取三个值：  0：海洋格子。 1：陆地格子（未遍历过）。 2：陆地格子（已遍历过）。      我们在框架中加入避免重复遍历的语句，具体如下：\n/** * 判断是否在方格范围内 * @param grid 方格数组 * @param r 横坐标 * @param c 纵坐标 * @return (r, c) 是否在方格范围内 */ public boolean inArea(int[][] grid, int r, int c) { return r \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; grid.length \u0026amp;\u0026amp; c \u0026gt;= 0 \u0026amp;\u0026amp; c \u0026lt; grid[0].length; } /** * 方格 DFS 遍历算法（版本 2：加入重复遍历的判断） * @param grid 方格数组 * @param r 横坐标 * @param c 纵坐标 */ public void dfsV2(int[][] grid, int r, int c) { // base case  // 如果坐标 (r, c) 超出了网格范围，直接返回  if (!inArea(grid, r, c)) { return; } // 如果这个格子不是 未遍历过的陆地，则直接返回  if (grid[r][c] != 1) { return; } // 将当前陆地标记为 已遍历过  grid[r][c] = 2; // 访问上、下、左、右四个相邻接点  dfsV2(grid, r + 1, c); dfsV2(grid, r - 1, c); dfsV2(grid, r, c - 1); dfsV2(grid, r, c + 1); }   加入重复遍历的判断之后的示意图如下所示：\n   这样，我们就得到了一个岛屿问题、乃至各种网格问题的通用 DFS 遍历方法，以下所讲的几个例题，其实都只需要在 DFS 遍历框架上稍加修改而已。\n  4 相关题目 #  4.1 岛屿数量 #  4.1.1 题目 #  给你一个由 \u0026lsquo;1\u0026rsquo;（陆地）和 \u0026lsquo;0\u0026rsquo;（水）组成的的二维网格，请你计算网格中岛屿的数量。\n岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。\n此外，你可以假设该网格的四条边均被水包围。\n示例 1：\n输入：grid = [ [\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;], [\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;], [\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;], [\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;] ] 输出：1 示例 2：\n输入：grid = [ [\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;], [\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;], [\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;], [\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;1\u0026#34;] ] 输出：3 提示：\n m == grid.length n == grid[i].length 1 \u0026lt;= m, n \u0026lt;= 300 grid[i][j] 的值为 \u0026lsquo;0\u0026rsquo; 或 \u0026lsquo;1\u0026rsquo;  4.1.2 问题分析 #   这个直接根据模板进行修改一下即可，同时需要注意的是：  只要当遍历到的点是陆地的话才会开始执行 dfs() 函数，因此每当遍历一个陆地的点，岛屿数量看定会加 1。 在 dfs() 函数中，会将已经遍历过的陆地标记为已遍历，因此下一次遍历到这块已遍历过的陆地时，会直接返回。    4.1.3 参考代码 #  /** * 200. 岛屿数量 * @param grid 网格 * @return 网格中岛屿的数量 */ public int numIslands(char[][] grid) { int res = 0; for (int i = 0; i \u0026lt; grid.length; i++) { for (int j = 0; j \u0026lt; grid[i].length; j++) { if (grid[i][j] == \u0026#39;1\u0026#39;) { dfs(grid, i, j); res++; /*将当前岛屿数量加 1*/ } } } return res; } /** * 判断点 (r, c) 是否在网格范围内 * @param grid 网格 * @param r 横坐标 * @param c 纵坐标 * @return (r, c) 是否在网格范围内 */ public boolean inArea(char[][] grid, int r, int c) { return r \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; grid.length \u0026amp;\u0026amp; c \u0026gt;= 0 \u0026amp;\u0026amp; c \u0026lt; grid[0].length; } /** * 网格深度优先遍历 * @param grid 网格 * @param r 横坐标 * @param c 纵坐标 */ public void dfs(char[][] grid, int r, int c) { if (!inArea(grid, r, c)) {return;} if (grid[r][c] != \u0026#39;1\u0026#39;) {return;} grid[r][c] = \u0026#39;2\u0026#39;; dfs(grid, r + 1, c); dfs(grid, r - 1, c); dfs(grid, r, c - 1); dfs(grid, r, c + 1); } 4.2 岛屿的最大面积 #  4.2.1 题目 #  给定一个包含了一些 0 和 1 的非空二维数组 grid 。\n一个 岛屿 是由一些相邻的 1 (代表土地) 构成的组合，这里的「相邻」要求两个 1 必须在水平或者竖直方向上相邻。你可以假设 grid 的四个边缘都被 0（代表水）包围着。\n找到给定的二维数组中最大的岛屿面积。(如果没有岛屿，则返回面积为 0 。)\n示例 1:\n[[0,0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,1,1,0,1,0,0,0,0,0,0,0,0], [0,1,0,0,1,1,0,0,1,0,1,0,0], [0,1,0,0,1,1,0,0,1,1,1,0,0], [0,0,0,0,0,0,0,0,0,0,1,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,0,1,1,0,0,0,0]] 对于上面这个给定矩阵应返回 6。注意答案不应该是 11 ，因为岛屿只能包含水平或垂直的四个方向的 1 。\n示例 2:\n[[0,0,0,0,0,0,0,0]] 对于上面这个给定的矩阵, 返回 0。\n注意: 给定的矩阵 grid 的长度和宽度都不超过 50。\n4.2.2 问题分析 #    这道题目只需对每个岛屿做 DFS 遍历，求出每个岛屿的面积就可以了。\n  求岛屿面积的方法也很简单，代码如下，每遍历到一个格子，就把面积加 1：\n/** * 采用 DFS 遍历求陆地所在岛屿的面积 * @param grid 岛屿数组 * @param r 陆地横坐标 * @param c 陆地纵坐标 * @return 陆地所在岛屿的面积 */ public int dfs(int[][] grid, int r, int c) { // base case  // 如果坐标（r, c）超出岛屿范围，直接返回 0  if (!inArea(grid, r, c)) { return 0; } // 如果当前格子不是 未遍历陆地，则直接返回 0  if (grid[r][c] != 1) { return 0; } // 将当前陆地标记为 已遍历陆地  grid[r][c] = 2; // 访问上、下、左、右四个相邻接点  return 1 + dfs(grid, r - 1, c) + dfs(grid, r + 1, c) + dfs(grid, r, c - 1) +dfs(grid, r, c + 1); }   4.2.3 参考代码 #  /** * 判断陆地是否超出岛屿范围 * @param grid 岛屿数组 * @param r 陆地横坐标 * @param c 陆地纵坐标 * @return 陆地是否超出岛屿范围 */ public boolean inArea(int[][] grid, int r, int c) { return r \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; grid.length \u0026amp;\u0026amp; c \u0026gt;= 0 \u0026amp;\u0026amp; c \u0026lt; grid[0].length; } /** * 采用 DFS 遍历求陆地所在岛屿的面积 * @param grid 岛屿数组 * @param r 陆地横坐标 * @param c 陆地纵坐标 * @return 陆地所在岛屿的面积 */ public int dfs(int[][] grid, int r, int c) { // base case  // 如果坐标（r, c）超出岛屿范围，直接返回 0  if (!inArea(grid, r, c)) { return 0; } // 如果当前格子不是 未遍历陆地，则直接返回 0  if (grid[r][c] != 1) { return 0; } // 将当前陆地标记为 已遍历陆地  grid[r][c] = 2; // 访问上、下、左、右四个相邻接点  return 1 + dfs(grid, r - 1, c) + dfs(grid, r + 1, c) + dfs(grid, r, c - 1) +dfs(grid, r, c + 1); } /** * 695. 岛屿的最大面积 * @param grid 岛屿数组 * @return 岛屿的最大面积 */ public int maxAreaOfIsland(int[][] grid) { int res = 0; for (int i = 0; i \u0026lt; grid.length; i++) { for (int j = 0; j \u0026lt; grid[i].length; j++) { if (grid[i][j] == 1) { int a = dfs(grid, i, j); res = Math.max(res, a); } } } return res; } 4.2.4 题目扩展 #  4.2.4.1 最大正方形 #  4.2.4.1.1 问题分析 #    该题目也属于网格中寻找最值类型的题目，可以用DFS来实现，也可以用动态规划来实现，本文选择的是动态规划的方式来实现。\n  该题目中 $dp$ 数组的含义为 $dp[i][j]$表示以点 $matrix[i][j]$ 为起点的只包含 '1' 的最大正方形的面积，且：\n$$ dp\\lbrack i\\rbrack\\lbrack j\\rbrack=\\left{\\begin{array}{lc}0\u0026amp;matrix\\lbrack i\\rbrack\\lbrack j\\rbrack=\u0026lsquo;0\u0026rsquo;\\min(dp\\lbrack i\\rbrack\\lbrack j;+;1\\rbrack,;dp\\lbrack i;+;1\\rbrack\\lbrack j;+;1\\rbrack,;dp\\lbrack i;+;1\\rbrack\\lbrack j\\rbrack)\u0026amp;matrix\\lbrack i\\rbrack\\lbrack j\\rbrack=\u0026lsquo;1\u0026rsquo;\\end{array}\\right. $$\n  4.2.4.1.2 参考代码 #  /** * 221. 最大正方形 * * @param matrix 二维矩阵 * @return 二维矩阵中只包含 \u0026#39;1\u0026#39; 的最大正方形的面积 */ public int maximalSquare(char[][] matrix) { int m = matrix.length; int n = (m \u0026gt;= 1 ? matrix[0].length : 0); // dp 数组，其中 dp[i][j] 表示以 matrix[i][j] 为起点的只包含 \u0026#39;1\u0026#39; 的最大正方形的面积  int[][] dp = new int[m + 1][n + 1]; int res = 0; for (int i = m - 1; i \u0026gt;= 0; i--) { for (int j = n - 1; j \u0026gt;= 0; j--) { // base case  // 对矩阵右下角的元素进行初始化  if (matrix[i][j] == \u0026#39;0\u0026#39;) { dp[i][j] = 0; } else if ( (i == m - 1 \u0026amp;\u0026amp; j == n - 1) || (i == m - 2 \u0026amp;\u0026amp; j == n - 1) || (i == m - 1 \u0026amp;\u0026amp; j == n - 2) ) {dp[i][j] = 1;} else { // 分别以当前点 matrix[i][j] 的左边（matrix[i][j + 1]）、对角线（matrix[i + 1][j + 1]）、下边（matrix[i + 1][j]）为起点的只包含 \u0026#39;1\u0026#39; 的最大正方形的面积的的最小值  int min = Math.min( Math.min(dp[i][j + 1], dp[i + 1][j + 1]), dp[i + 1][j] ); // dp[i][j] 等于上面求得的最大面积的最小值对应边长加 1 后对应的正方形的面积  dp[i][j] = (int) Math.pow(Math.sqrt(min) + 1, 2); } // res 等于 dp[i][j] 中的最大值  res = Math.max(res, dp[i][j]); } } // 返回最后结果  return res; } 4.3 填海造陆问题 #  4.3.1 题目 #  给你一个大小为 n x n 二进制矩阵 grid 。最多 只能将一格 0 变成 1 。\n返回执行此操作后，grid 中最大的岛屿面积是多少？\n岛屿 由一组上、下、左、右四个方向相连的 1 形成。\n** 示例 1:**\n输入: grid = [[1, 0], [0, 1]] 输出: 3 解释: 将一格 0 变成 1，最终连通两个小岛得到面积为 3 的岛屿。 示例 2:\n输入: grid = [[1, 1], [1, 0]] 输出: 4 解释: 将一格 0 变成 1，岛屿的面积扩大为 4。 示例 3:\n输入: grid = [[1, 1], [1, 1]] 输出: 4 解释: 没有 0 可以让我们变成 1，面积依然为 4。 提示：\n n == grid.length n == grid[i].length 1 \u0026lt;= n \u0026lt;= 500 grid[i][j] 为 0 或 1  4.3.2 问题分析 #   这道题是 岛屿最大面积问题的升级版，现在我们有填海造陆的能力，可以把一个海洋格子变成陆地格子，进而让两块岛屿连成一块，然后求出填海造陆之后最大可能构成的岛屿的最大面积。 我们可以先计算出所有岛屿的面积，然后在所有的格子上标记出岛屿的最大面积，最后搜索出哪个海洋格子相邻的两个岛屿面积最大。 例如下图中红色方框内的海洋格子，上边、左边都与岛屿相邻，我们可以计算出他变成陆地之后可以连接成的岛屿面积为 7 + 1 + 2 = 10。  然而，这种做法可能遇到一个问题，如下图中红色方框内的海洋格子，他的上边、左边都与岛屿相邻，这时候连接成的岛屿面积并不是 7 + 1 + 7，因为这两个 7 来自同一个岛屿，所以填海造陆之后得到的岛屿面积应该只有 7 + 1 = 8。  可以看到，要让算法正确，我们需要区分一个海洋格子相邻的 7 是不是来自同一个岛屿，那么我们可以不在方格中标记岛屿的面积，而应该标记岛屿的索引（下标），另外用一个数组记录每个岛屿的面积，如下图所示，这样我们就可以发现红色方框内的海洋格子的两个相邻岛屿实际上是一个。  可以看到，这道题实际上对网络做了两遍 DFS：  第一遍 DFS 遍历陆地格子，计算每个岛屿的面积并标记岛屿。 第二遍 DFS 遍历海洋格子，观察每个海洋格子相邻的陆地格子。   这道题的基本思路就是这样，具体的代码还有一些需要注意的细节，但和本文的主题已经联系不大，这个会在后面有时间的时候在完善具体的代码实现。  4.4 岛屿的周长 #  4.4.1 题目 #  给定一个 row x col 的二维网格地图 grid ，其中：grid[i][j] = 1 表示陆地， grid[i][j] = 0 表示水域。\n网格中的格子 水平和垂直 方向相连（对角线方向不相连）。整个网格被水完全包围，但其中恰好有一个岛屿（或者说，一个或多个表示陆地的格子相连组成的岛屿）。\n岛屿中没有“湖”（“湖” 指水域在岛屿内部且不和岛屿周围的水相连）。格子是边长为 1 的正方形。网格为长方形，且宽度和高度均不超过 100 。计算这个岛屿的周长。\n示例 1：\n 输入：grid = [[0,1,0,0],[1,1,1,0],[0,1,0,0],[1,1,0,0]] 输出：16 解释：它的周长是上面图片中的 16 个黄色的边 示例 2：\n输入：grid = [[1]] 输出：4 示例 3：\n输入：grid = [[1,0]] 输出：4 提示：\n row == grid.length col == grid[i].length 1 \u0026lt;= row, col \u0026lt;= 100 grid[i][j] 为 0 或 1  4.4.2 问题分析 #    实际上，岛屿的周长是计算岛屿全部的边缘，而这些边缘就是我们在 DFS 遍历中 dfs 函数返回的位置。\n  我们可以将岛屿的周长分为两类：\n 与网格边界相邻的周长：dfs 函数因为坐标 $(r,c)$ 超出网格范围而返回的时候就经过了一条相应的边。 与海洋格子相邻的周长：dfs 函数因为当前格子是海洋格子而返回的时候就经过了一条相应的边。     这样，我们就把岛屿周长和 DFS 遍历联系起来了，具体的代码如下：\n/** * 采用 DFS 遍历求陆地所在岛屿的周长 * @param grid 岛屿数组 * @param r 陆地横坐标 * @param c 陆地纵坐标 * @return 陆地所在岛屿的面积 */ public int dfs(int[][] grid, int r, int c) { // base case  // 坐标（r, c）超出岛屿范围，对应一条 与网格边界相邻的边  if (!inArea(grid, r, c)) { return 1; } // 当前格子是 海洋格子，对应一条 与海洋格子相邻的边  if (grid[r][c] == 0) { return 1; } // 当前格子是 已遍历陆地，与 周长 无关  if (grid[r][c] != 1) { return 0; } // 将当前陆地标记为 已遍历陆地  grid[r][c] = 2; // 访问上、下、左、右四个相邻接点  return dfs(grid, r - 1, c) + dfs(grid, r + 1, c) + dfs(grid, r, c - 1) +dfs(grid, r, c + 1); }   4.4.3 参考代码 #  /** * 判断陆地是否超出岛屿范围 * @param grid 岛屿数组 * @param r 陆地横坐标 * @param c 陆地纵坐标 * @return 陆地是否超出岛屿范围 */ public boolean inArea(int[][] grid, int r, int c) { return r \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; grid.length \u0026amp;\u0026amp; c \u0026gt;= 0 \u0026amp;\u0026amp; c \u0026lt; grid[0].length; } /** * 采用 DFS 遍历求陆地所在岛屿的周长 * @param grid 岛屿数组 * @param r 陆地横坐标 * @param c 陆地纵坐标 * @return 陆地所在岛屿的面积 */ public int dfs(int[][] grid, int r, int c) { // base case  // 坐标（r, c）超出岛屿范围，对应一条 与网格边界相邻的边  if (!inArea(grid, r, c)) { return 1; } // 当前格子是 海洋格子，对应一条 与海洋格子相邻的边  if (grid[r][c] == 0) { return 1; } // 当前格子是 已遍历陆地，与 周长 无关  if (grid[r][c] != 1) { return 0; } // 将当前陆地标记为 已遍历陆地  grid[r][c] = 2; // 访问上、下、左、右四个相邻接点  return dfs(grid, r - 1, c) + dfs(grid, r + 1, c) + dfs(grid, r, c - 1) +dfs(grid, r, c + 1); } /** * 463. 岛屿的周长 * @param grid 岛屿 * @return 岛屿的周长 */ public int islandPerimeter(int[][] grid) { for (int i = 0; i \u0026lt; grid.length; i++) { for (int j = 0; j \u0026lt; grid[i].length; j++) { if (grid[i][j] == 1) { // 题目限制只有一个岛屿，计算一个即可  return dfs(grid, i, j); } } } return -1; } 4.5 单词搜索 #  4.5.1 题目 #  给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。\n单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。\n示例 1：\n 输入：board = [[\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;E\u0026#34;],[\u0026#34;S\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;S\u0026#34;],[\u0026#34;A\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;E\u0026#34;]], word = \u0026#34;ABCCED\u0026#34; 输出：true 示例 2：\n 输入：board = [[\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;E\u0026#34;],[\u0026#34;S\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;S\u0026#34;],[\u0026#34;A\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;E\u0026#34;]], word = \u0026#34;SEE\u0026#34; 输出：true 示例 3：\n 输入：board = [[\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;E\u0026#34;],[\u0026#34;S\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;S\u0026#34;],[\u0026#34;A\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;E\u0026#34;]], word = \u0026#34;ABCB\u0026#34; 输出：false 提示：\n m == board.length n = board[i].length 1 \u0026lt;= m, n \u0026lt;= 6 1 \u0026lt;= word.length \u0026lt;= 15 board 和 word 仅由大小写英文字母组成  进阶： 你可以使用搜索剪枝的技术来优化解决方案，使其在 board 更大的情况下可以更快解决问题？\n4.5.2 问题分析 #   该题目为网格搜索类题目，可以利用网格DFS的框架来解决。 需要注意的是，该题目在遍历的时候涉及到回退的操作，因此需要建立一个数组，用来标记当前节点是否使用过，如果使用过则直接跳过，但是最后如果当前遍历得到的字符串和原来的字符串不相等，需要进行回退，把刚才标记为使用过的节点标记为未使用，相当于原来的节点没有使用。  4.5.3 参考代码 #  // 最后结果 boolean res = false; /** * 79. 单词搜索 * @param board 二维字符网格 * @param word 字符串单词 * @return 字符串单词是否存在于二维字符网络中 */ public boolean exist(char[][] board, String word) { int index = 0; for (int i = 0; i \u0026lt; board.length; i++) { for (int j = 0; j \u0026lt; board[i].length; j++) { int[][] used = new int[board.length][board[i].length]; dfs(board, word, i, j, index, used); if (res) {return true;} } } return false; } /** * 深度优先遍历判断字符串单词是否存在于二维字符网络中 * @param board 二维字符网格 * @param word 字符串单词 * @param i 当前遍历的二维字符网络中字符的横坐标 * @param j 当前遍历的二维字符网络中字符的纵坐标 * @param index 当前遍历的字符串单词的下标 * @param used 当前遍历的二维字符网络中字符是否使用过 */ public void dfs(char[][] board, String word, int i, int j, int index, int[][] used) { if (!inArea(board, word, i, j, index, used)) {return;} if (index == word.length() - 1 \u0026amp;\u0026amp; board[i][j] == word.charAt(index) \u0026amp;\u0026amp; used[i][j] != -1) { res = true; return; } // 标记当前字符为已经使用过  used[i][j] = -1; if (!res) {dfs(board, word, i, j + 1, index + 1, used);} if (!res) {dfs(board, word, i + 1, j, index + 1, used);} if (!res) {dfs(board, word, i, j - 1, index + 1, used);} if (!res) {dfs(board, word, i - 1, j, index + 1, used);} if (!res) { // 如果字符串单词不在于二维字符网络中，则撤销原来的标记  used[i][j] = 0; } } /** * 判断当前遍历的二维字符网络中字符是否在边界内 * @param board 二维字符网格 * @param word 字符串单词 * @param i 当前遍历的二维字符网络中字符的横坐标 * @param j 当前遍历的二维字符网络中字符的纵坐标 * @param index 当前遍历的字符串单词的下标 * @param used 当前遍历的二维字符网络中字符是否使用过 * @return 判断当前遍历的二维字符网络中字符是否在边界内 */ public boolean inArea(char[][] board, String word, int i, int j, int index, int[][] used) { if ((i \u0026lt; 0 || i \u0026gt;= board.length || j \u0026lt; 0 || j \u0026gt;= board[i].length)) {return false;} if (index \u0026lt; 0 || index \u0026gt;= word.length()) {return false;} if (used[i][j] == -1) {return false;} if (board[i][j] != word.charAt(index)) {return false;} return true; } 参考文献 #    200. 岛屿数量。  岛屿类问题的通用解法、DFS 遍历框架。  695. 岛屿的最大面积。  827. 最大人工岛。  463. 岛屿的周长。  221. 最大正方形。  79. 单词搜索。  "},{"id":128,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.5-%E4%BA%8C%E5%8F%89%E6%A0%91/1.5.3-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"1.5.3 平衡二叉树","section":"1.5 二叉树","content":"平衡二叉树 #  1 题目 #  给定一个二叉树，判断它是否是高度平衡的二叉树。\n本题中，一棵高度平衡二叉树定义为：\n一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1 。\n示例 1：\n 输入：root = [3,9,20,null,null,15,7] 输出：true 示例 2：\n 输入：root = [1,2,2,3,3,null,null,4,4] 输出：false 示例 3：\n输入：root = [] 输出：true 提示：\n 树中的节点数在范围 [0, 5000] 内 -104 \u0026lt;= Node.val \u0026lt;= 104  2 解题思路 #  2.1 自顶向下（前序遍历） #  2.1.1 问题分析 #   自顶向下判断是否为平衡二叉树的基本思路为：  按照类似于先序遍历的方法判断当前节点的左右子树的高度差是否小于 2。 判断当前节点的左右子树是否为平衡二叉树。   算法的演示动画可参考 平衡二叉树。 该算法存在的缺点是计算左右子树的高度差和判断左右子树是否为平衡二叉树的过程中可能存在节点重复遍历的情况。  2.1.2 参考代码 #  /** * 110. 平衡二叉树（版本 1：自顶向下（前序遍历）） * @param root 根节点 * @return 二叉树是否为平衡二叉树 */ public boolean isBalancedV1(TreeNode root) { // base case  if (root == null) {return true;} // 平衡二叉树的条件：  // 1. 左右子树的高度差小于 2.  // 2. 左右子树均为平衡二叉树.  return Math.abs(heightV1(root.left) - heightV1(root.right)) \u0026lt; 2 \u0026amp;\u0026amp; isBalancedV1(root.left) \u0026amp;\u0026amp; isBalancedV1(root.right); } /** * 计算一个二叉树的高度 * @param root 根节点 * @return 二叉树的高度 */ public int heightV1(TreeNode root) { // base case  if (root == null) {return 0;} // 二叉树的高度等于左右子树高度的最大值加 1  return Math.max(heightV1(root.left), heightV1(root.right)) + 1; } 2.2 自底向上（后序遍历） #  2.2.1 问题分析 #   注：下面所说的高度指距离子树底部的高度。\n  自底向上判断二叉树是否为平衡二叉树的基本思路为：  按照类似于后序遍历的方法判断当前节点是否为平衡二叉树，判断标准为：  当前节点的左右子树均为平衡二叉树。 当前节点的左右子树高度差小于 2。   如果当前节点不是平衡二叉树，则直接返回-1。 如果当前节点是平衡二叉树，则返回其左右子树高度的最大值加 1 作为当前节点的高度。   算法的演示动画可参考 平衡二叉树。 该算法不存在自顶向下算法中的重复遍历问题。  2.2.2 参考代码 #  /** * 110. 平衡二叉树（版本 2：自底向上（后序遍历）） * @param root 根节点 * @return 二叉树是否为平衡二叉树 */ public boolean isBalancedV2(TreeNode root) { if (root == null) {return true;} return heightV2(root) != -1; } /** * 计算一个二叉树的高度 * @param root 根节点 * @return 二叉树的高度 */ public int heightV2(TreeNode root) { // base case  if (root == null) {return 0;} // 左子树高度  int leftHeight = heightV2(root.left); // 右子树高度  int rightHeight = heightV2(root.right); // 如果左子树不是平衡二叉树或者右子树不是平衡二叉树或者左右子树的高度差大于等于 2，则当前子树不是平衡二叉树  if (leftHeight == -1 || rightHeight == -1 || Math.abs(leftHeight - rightHeight) \u0026gt;= 2) { return -1; } // 如果当前子树是平衡二叉树，则返回当前子树的高度  return Math.max(leftHeight, rightHeight) + 1; } 3 参考文献 #    110. 平衡二叉树。  平衡二叉树。  "},{"id":129,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.5-%E4%BA%8C%E5%8F%89%E6%A0%91/1.5.4-%E4%BA%8C%E5%8F%89%E6%A0%91%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/","title":"1.5.4 二叉树路径问题","section":"1.5 二叉树","content":"二叉树路径问题 #  1 问题分类 #  二叉树路径的问题大致可以分为两类，分别是自顶向下和非自顶向下。\n1.1 自顶向下 #  1.1.1 概述 #   自顶向下就是从某一个节点（不一定是根节点）出发，从上向下寻找路径，到某一个节点（不一定是叶节点）结束，继续细分的话，还可以分为一般路径和定和路径。 这类题通常用 深度优先搜索（DFS）和 广度优先搜索（BFS）解决，BFS 一般比 DFS 更为繁琐，这里为了简洁只展现 DFS 代码。 这类题型需要注意以下几点：  如果是找路径和等于给定 $target$ 的路径的，那么可以不用新增一个临时变量来判断当前路径和，只需要用给定和 $target$ 减去节点值，最终结束条件判断 $target == 0$ 即可。 二叉树的问题大部分是不需要回溯的，因为：  二叉树的递归部分（dfs(root -\u0026gt; left)、dfs(root -\u0026gt; right)）已经把可能的路径穷尽了，因此到任意节点的路径只可能有一条，绝对不可能出现另外的路径也到这个满足条件的叶节点的。 而对比二维数组（例如 岛屿问题）的 DFS，for循环向四个方向查找每次只能朝向一个方向，并没有穷尽路径，因此某一个满足条件的点可能是多条路径到该点的，并且 visited数组标记已经走过的路径是会受到另外路径是否访问的影响，这时候必须回溯。   至于找到路径后是否需要 return，这取决于是否要求找到叶节点满足条件的路径：  如果必须找到叶节点，那么就要 return。 如果是到任意节点都可以，那么必不能 return，因为这条路径下面还可能有更深的路径满足条件，还要在此基础上继续递归。   至于是否需要双重递归（即调用根节点的 dfs 函数后，继续调用根左右节点的 pathsum 函数），需要看题目是要求从根节点开始，还是从任意节点开始。    1.1.2 解题模板 #  1.1.2.1 一般路径 #  /** * 自顶向下（版本 1：一般路径） * @param root * @param path */ public void dfsFromTopToBottomV1(TreeNode root, List\u0026lt;Integer\u0026gt; path) { // 根节点为空直接返回  if (root == null) {return;} // 做出选择（将节点的值添加到路径中）  List\u0026lt;Integer\u0026gt; pathTemp = new ArrayList\u0026lt;\u0026gt;(path); pathTemp.add(root.val); // 叶节点（左子树和右子树均为空），将路径添加到结果中，然后返回  if (root.left == null \u0026amp;\u0026amp; root.right == null) { res.add(path); return; } // 遍历左子树  dfsFromTopToBottomV1(root.left, pathTemp); // 遍历右子树  dfsFromTopToBottomV1(root.right, pathTemp); } 1.1.2.2 给定和的路径 #  /** * 自顶向下（版本 2：给定和的路径） * @param root * @param path */ public void dfsFromTopToBottomV2(TreeNode root, int sum, List\u0026lt;Integer\u0026gt; path) { // 根节点为空直接返回  if (root == null) {return;} // 做出选择（将 sum 值减去当前节点的值，然后将当前节点的值添加到路径中）  sum -= root.val; List\u0026lt;Integer\u0026gt; pathTemp = new ArrayList\u0026lt;\u0026gt;(path); pathTemp.add(root.val); // 叶节点（左子树和右子树均为空），且满足给定的路径和，将路径添加到结果中，然后返回  if (root.left == null \u0026amp;\u0026amp; root.right == null \u0026amp;\u0026amp; sum == 0) { res.add(path); return; } // 遍历左子树  dfsFromTopToBottomV1(root.left, pathTemp); // 遍历右子树  dfsFromTopToBottomV1(root.right, pathTemp); } 1.2 非自顶向下 #  1.2.1 概述 #    非自顶向下就是从任意节点到任意节点的路径，不需要自顶向下。\n  这类题目的一般解题思路如下：\n 设计一个辅助函数 maxPath()，调用自身求出以一个节点为根节点的左侧最长路径 $left$ 和右侧最长路径 $right$，那么经过该节点的最长路径就是 $left + right$。 接着只需要从根节点开始 DFS，不断更新比较全局变量即可。    这类题型 DFS 需要注意的地方：\n $left$ 和 $right$ 代表的含义要根据题目所求设置，比如最长路径、最大路径和等等。 全局变量 $res$ 的初值是 0 还是 INT_MIN 要看题目节点是否存在负值，如果存在就用 INT_MIN，否则就是 0。    1.2.2 解题模板 #  Integer resInt = 0; public int maxPath(TreeNode root) { /*以 root 为路径起始点的最长路径*/ if (root == null) {return 0;} int left = maxPath(root.left); int right = maxPath(root.right); resInt = Math.max(resInt, left + right + root.val); /*更新全局变量*/ return Math.max(left, right) + root.val; /*返回左右路径较长者*/ } 2 题目分析 #  2.1 自顶向下 #  2.1.1 一般路径 #  2.1.1.1 二叉树的所有路径 #  2.1.1.1.1 问题分析 #   该题目属于 自顶向下类型中的 一般路径，直接套用 相应模板即可。 该题目有两点需要注意的地方：  在 dfs() 方法中新建一个变量存储当前路径的值，防止对原来路径的值造成影响。 添加前缀时在遍历左子树和右子树前面添加，这样可以把前缀的处理变得更为简单。    2.1.1.1.2 参考代码 #  // 路径列表 List\u0026lt;String\u0026gt; path = new ArrayList(); /** * 257. 二叉树的所有路径 * @param root 根节点 * @return 二叉树的所有路径 */ public List\u0026lt;String\u0026gt; binaryTreePaths(TreeNode root) { dfs(root, \u0026#34;\u0026#34;); return path; } /** * 深度优先遍历查找当前节点到叶子节点的所有路径 * @param root 当前节点 * @param subPath 当前节点到叶子节点的路径 */ public void dfs(TreeNode root, String subPath) { // base case  if (root == null) {return;} // 新建一个变量存储 subPath 的值，防止对原来 subPath 的值造成影响  StringBuffer subPathBuffer = new StringBuffer(subPath); subPathBuffer.append(root.val); if (root.left == null \u0026amp;\u0026amp; root.right == null) { // 遍历到了叶子节点，将当前节点到叶子节点的路径添加到结果路径列表中  path.add(subPathBuffer.toString()); return; } // 添加前缀  subPathBuffer.append(\u0026#34;-\u0026gt;\u0026#34;); // 遍历左子树  dfs(root.left, subPathBuffer.toString()); // 遍历右子树  dfs(root.right, subPathBuffer.toString()); } 2.1.1.2 从叶结点开始的最小字符串 #  2.1.1.2.1 问题分析 #   该题目属于 自顶向下类型中的 一般路径，直接套用 相应模板即可。  2.1.1.2.2 参考代码 #  // 最小字符串 String res = \u0026#34;\u0026#34;; /** * 988. 从叶结点开始的最小字符串 * @param root 根节点 * @return 从叶结点开始的最小字符串 */ public String smallestFromLeaf(TreeNode root) { dfs(root, \u0026#34;\u0026#34;); return res; } /** * 深度优先遍历查找从叶结点开始的最小字符串 * @param root 当前节点 * @param path 从叶节点到当前节点的最小字符串 */ public void dfs(TreeNode root, String path) { if (root == null) {return;} StringBuffer newPath = new StringBuffer(path); newPath.append(String.valueOf((char)(root.val - 0 + \u0026#39;a\u0026#39;))); if (root.left == null \u0026amp;\u0026amp; root.right == null) { String pathReverseStr = newPath.reverse().toString(); if (res == \u0026#34;\u0026#34;) { res = pathReverseStr; } else { res = (res.compareTo(pathReverseStr) \u0026lt;= 0 ? res : pathReverseStr); } return; } dfs(root.left, newPath.toString()); dfs(root.right, newPath.toString()); } 2.1.2 给定路径和的路径 #  2.1.2.1 路径总和 II #  2.1.2.1.1 问题分析 #   该题目属于 自顶向下类型中的 给定路径和的路径，直接套用 相应模板即可。  2.1.2.1.2 参考代码 #  // 路径列表 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); /** * 113. 路径总和 II * @param root 根节点 * @param targetSum 目标路径和 * @return 所有从根节点到叶子节点路径总和等于给定目标和的路径 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; pathSum(TreeNode root, int targetSum) { List\u0026lt;Integer\u0026gt; path = new ArrayList\u0026lt;\u0026gt;(); dfs(root, targetSum, path); return res; } /** * 深度优先遍历查找所有从根节点到叶子节点路径总和等于给定目标和的路径 * @param root * @param targetSum * @param path */ public void dfs(TreeNode root, int targetSum, List\u0026lt;Integer\u0026gt; path) { if (root == null) {return;} targetSum -= root.val; List\u0026lt;Integer\u0026gt; pathTemp = new ArrayList\u0026lt;\u0026gt;(path); pathTemp.add(root.val); if (root.left == null \u0026amp;\u0026amp; root.right == null \u0026amp;\u0026amp; targetSum == 0) { res.add(pathTemp); return; } dfs(root.left, targetSum, pathTemp); dfs(root.right, targetSum, pathTemp); } 2.1.2.2 路径总和 III #  2.1.2.2.1 问题分析 #   该题目属于 自顶向下类型中的 给定路径和的路径，可以采用双重递归来实现：  先调用 dfs() 函数从 root 开始查找路径和等于给定值的路径。 然后再调用 pathSum() 函数从 root 左右子树开始查找。   需要注意的是：  在 dfs() 函数中当 $sum = 0$ 时不要直接 $return$，因为题目中不要求到叶子节点结束，所以一条路径下面可能还有另外一条。    2.1.2.2.2 参考代码 #  // 路径数目  int count = 0; /** * 437. 路径总和 III * @param root 根节点 * @param targetSum 目标值 * @return 二叉树里节点值之和等于 targetSum 的 路径 的数目 */ public int pathSum(TreeNode root, int targetSum) { if (root == null) {return 0;} // 查找以当前节点为起点，路径和等于给定值的路径  dfs(root, targetSum); // 查找当前节点的左子树节点值之和等于 targetSum 的路径的数目  pathSum(root.left, targetSum); // 查找当前节点的右子树节点值之和等于 targetSum 的路径的数目  pathSum(root.right, targetSum); return count; } /** * 深度优先遍历查找所有从根节点到叶子节点路径总和等于给定目标和的路径 * @param root * @param targetSum */ public void dfs(TreeNode root, int targetSum) { if (root == null) {return;} targetSum -= root.val; if (targetSum == 0) {count++;} dfs(root.left, targetSum); dfs(root.right, targetSum); } 2.2 非自顶向下 #  2.2.1 二叉树中的最大路径和 #  2.2.1.1 问题分析 #   该题目属于 非自顶向下类型，直接套用 相应模板即可。 需要注意的是：  最大路径和小于 0，意味着该路径和对总路径和做负贡献，因此不要计入到总路径中，将他设置为 0。    2.2.1.2 参考代码 #  // 二叉树中的最大路径和 int res = Integer.MIN_VALUE; /** * 124. 二叉树中的最大路径和 * @param root 根节点 * @return 二叉树中的最大路径和 */ public int maxPathSum(TreeNode root) { dfs(root); return res; } /** * 当前节点的最大贡献值 * @param root 当前节点 * @return 当前节点的最大贡献值 */ public int dfs(TreeNode root) { // base case  if (root == null) {return 0;} // 计算左右子树的最大贡献值  // 只有最大贡献值大于 0 时，才会选择对应子节点  int leftGain = Math.max(dfs(root.left), 0); int rightGain = Math.max(dfs(root.right), 0); // 当前节点的路径和，当其比已有最大路径和大时更新最大路径和为当前节点的路径和  // 当前节点的路径和 = 当前节点的值 + 左子树的最大贡献值 + 右子树的最大贡献值  res = Math.max(res, root.val + leftGain + rightGain); // 返回当前节点的最大贡献值  // 当前节点的最大贡献值 = 当前节点值 + 左右子树中最大的最大贡献值  return Math.max(leftGain, rightGain) + root.val; } 2.2.2 最长同值路径 #  2.2.2.1 问题分析 #   该题目属于 非自顶向下类型，直接套用 相应模板即可。 需要注意的是：  对当前节点的值和其左右子节点的值的对比应该放在递归遍历完左右子树后进行。    2.2.2.2 参考代码 #  // 同值路径的最大长度 int res = 0; // 路径 List\u0026lt;Integer\u0026gt; path = new ArrayList\u0026lt;\u0026gt;(); /** * 687. 最长同值路径 * @param root 根节点 * @return 同值路径的最大长度 */ public int longestUnivaluePath(TreeNode root) { dfs(root); return res; } /** * 深度优先遍历求同值路径的最大长度 * @param root 当前节点 * @return 同值路径的最大长度 */ public int dfs(TreeNode root) { if (root == null) {return 0;} int left = dfs(root.left); int right = dfs(root.right); // 如果存在左子节点，并且当前节点的值和左子节点的值相同，则更新左最长路径，否则，令左最长路径为 0  if (root.left != null \u0026amp;\u0026amp; root.val == root.left.val) { left++; } else { left = 0; } // 如果存在右子节点，并且当前节点的值和右子节点的值相同，则更新右最长路径，否则，令右最长路径为 0  if (root.right != null \u0026amp;\u0026amp; root.val == root.right.val) { right++; } else { right = 0; } res = Math.max(res, left + right); return Math.max(left, right); } 2.2.3 二叉树的直径 #  2.2.3.1 问题分析 #   该题目属于 非自顶向下类型，直接套用 相应模板即可。  2.2.3.2 参考代码 #  // 二叉树的直径 int res = 0; // 路径 List\u0026lt;Integer\u0026gt; path = new ArrayList\u0026lt;\u0026gt;(); /** * 543. 二叉树的直径 * @param root 根节点 * @return 二叉树的直径 */ public int diameterOfBinaryTree(TreeNode root) { dfs(root); return res; } /** * 深度优先遍历求二叉树的直径 * @param root 当前节点 * @return 二叉树的直径 */ public int dfs(TreeNode root) { if (root == null) {return 0;} int left = dfs(root.left); int right = dfs(root.right); res = Math.max(res, left + right); return Math.max(left, right) + 1; } 参考文献 #    一篇文章解决所有二叉树路径问题（问题分析 + 分类模板 + 题目剖析）。  257. 二叉树的所有路径。  113. 路径总和 II。  437. 路径总和 III。  687. 最长同值路径。  543. 二叉树的直径。  "},{"id":130,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.5-%E4%BA%8C%E5%8F%89%E6%A0%91/1.5.5-%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"1.5.5 构造二叉树","section":"1.5 二叉树","content":"构造二叉树 #  1 根据前序遍历和中序遍历构造二叉树 #  1.2 题目 #  给定一棵树的前序遍历 preorder 与中序遍历 inorder。请构造二叉树并返回其根节点。\n示例 1:\n Input: preorder = [3,9,20,15,7], inorder = [9,3,15,20,7] Output: [3,9,20,null,null,15,7] 示例 2:\nInput: preorder = [-1], inorder = [-1] Output: [-1] 提示:\n 1 \u0026lt;= preorder.length \u0026lt;= 3000 inorder.length == preorder.length -3000 \u0026lt;= preorder[i], inorder[i] \u0026lt;= 3000 preorder 和 inorder 均无重复元素 inorder 均出现在 preorder preorder 保证为二叉树的前序遍历序列 inorder 保证为二叉树的中序遍历序列  1.3 解题思路 #  1.3.1 递归 #  1.3.1.1 问题分析 #    对于任意一棵树而言，前序遍历的形式总是：\n$$ [根节点, [左子树的前序遍历结果],[右子树的前序遍历结果]] $$\n即根节点总是前序遍历中的第一个节点，而中序遍历的形式总是：\n$$ [[左子树的中序遍历结果], 根节点, [右子树的中序遍历结果]] $$\n  只要我们在中序遍历中定位到根节点，那么我们就可以分别知道左子树和右子树的节点数目，由于同一棵子树的前序遍历和中序遍历的长度是相同的，因此我们就可以对应到前序遍历的结果中，对上述形式中的所有左右括号进行定位，这样以来，我们就知道了左子树的前序遍历和中序遍历结果，以及右子树的前序遍历和中序遍历结果，我们就可以递归地构造出左子树和右子树，再将这两棵子树接到根节点的左右位置。\n  在中序遍历中对根节点进行定位时，一种简单的方法是直接扫描整个中序遍历的结果并找出根节点，但这样做的时间复杂度较高，我们可以考虑使用哈希表来帮助我们快速地定位根节点，对于哈希映射中的每个键值对，键表示一个元素（节点的值），值表示其在中序遍历中出现的位置，在构造二叉树的过程之前，我们就可以对中序遍历的列表进行一遍扫描，就可以构造出这个哈希树，在此后构造二叉树的过程中，我们就只需要 $O(1)$ 的时间对根节点进行定位了。 ![](../../../media/202108/105-从前序与中序遍历序列构造二叉树 003_1627997895.jpeg)\n  1.3.1.2 参考代码 #  /** * 105. 从前序与中序遍历序列构造二叉树 * @param preorder 前序遍历序列 * @param inorder 中序遍历序列 * @return 根据前序遍历序列和后续遍历序列构造的二叉树 */ public TreeNode buildTree(int[] preorder, int[] inorder) { // 存储前序遍历中每个元素在中序遍历中的位置  Map\u0026lt;Integer, Integer\u0026gt; inOrderIndexMap = new HashMap\u0026lt;\u0026gt;(); int m = inorder.length; for (int i = 0; i \u0026lt; m; i++) { inOrderIndexMap.put(inorder[i], i); } return myBuildTree(preorder, inorder, 0, m - 1, 0, m - 1, inOrderIndexMap); } /** * 根据前序遍历序列和中序遍历序列递归构建二叉树 * @param preorder 前序遍历序列 * @param inorder 中序遍历序列 * @param preOrderLeft 前序遍历序列左边界 * @param preOrderRight 前序遍历序列右边界 * @param inOrderLeft 中序遍历序列左边界 * @param inOrderRight 中序遍历序列右边界 * @param inOrderIndexMap 前序遍历序列中元素在中序遍历序列中的位置映射 * @return 根据前序遍历序列和后续遍历序列构造的二叉树 */ public TreeNode myBuildTree(int[] preorder, int[] inorder, int preOrderLeft, int preOrderRight, int inOrderLeft, int inOrderRight, Map\u0026lt;Integer, Integer\u0026gt; inOrderIndexMap) { // base case  if (preOrderLeft \u0026gt; preOrderRight) {return null;} // 前序遍历中的第一个节点就是根节点  int preOrderRoot = preorder[preOrderLeft]; // 在中序遍历中定位根节点  int inOrderRoot = inOrderIndexMap.get(preOrderRoot); // 根节点左子树中节点的数目  int inOrderLeftSubtreeSize = inOrderRoot - inOrderLeft; // 创建根节点  TreeNode root = new TreeNode(preOrderRoot); // 递归构建左子树，并连接到根节点  // 先序遍历中【从 左边界 +1 开始的 inOrderLeftSubtreeSize 个元素】就对应了中序遍历中【从 左边界 开始到 根节点定位-1 的元素】  root.left = myBuildTree(preorder, inorder, preOrderLeft + 1, preOrderLeft + inOrderLeftSubtreeSize, inOrderLeft, inOrderRoot - 1, inOrderIndexMap); // 递归构建右子树，并连接到根节点  // 先序遍历中【从 左边界 +1+inOrderLeftSubtreeSize 开始到 右边界 的元素】就对应了中序遍历中【从 根节点定位 +1 开始到 右边界 的元素】  root.right = myBuildTree(preorder, inorder, preOrderLeft + inOrderLeftSubtreeSize + 1, preOrderRight, inOrderRoot + 1, inOrderRight, inOrderIndexMap); // 返回根节点  return root; } 参考文献 #    105. 从前序与中序遍历序列构造二叉树。  从前序与中序遍历序列构造二叉树。  "},{"id":131,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.6-%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/1.6.1-%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E8%A7%A3%E9%A2%98%E6%A1%86%E6%9E%B6/","title":"1.6.1 回溯算法解题框架","section":"1.6 回溯算法","content":"1 含义 #    回溯算法建立在 DFS 基础之上，与 DFS 的主要不同在于：\n DFS 是一个劲的往某一个方向搜索，等到到达一个方向的终点时，才恢复状态，回溯上一层。 回溯算法在达到结束条件后，就恢复状态，回溯上一层。    当问题需要回头，以此来查出所有的解的时候，使用回溯算法，即满足结束条件或者发现不是正确路径的时候（走不通），要撤销选择，回退到上一个状态，继续尝试，直到找出所有解为止。\n  解决一个回溯算法时主要按照以下步骤：\n 画出递归树，找到状态变量（回溯函数的参数），这一部非常重要。 根据题意，确定结束条件。 找准选择列表（与函数参数相关），与第一步紧密关联。 判断是否需要剪枝。 做出选择，递归调用，进入下一层。 撤销选择。    回溯算法的核心就是for 循环里面的递归，即在递归调用之前【做选择】，在递归调用之后【撤销选择】，解题框架如下：\nresult = [] def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) return for 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择   2 应用场景 #  回溯算法的应用场景主要包括以下几个方面：\n  子集、组合。  全排列。 搜索。   需要注意的是，子集、组合与排列是不同性质的概念，子集、组合是无关顺序的，而排列是和元素顺序有关的，例如 [1, 2] 和 [2, 1] 是同一个组合（子集），但是是两种不一样的排列，因此被分为两类问题。\n 2.1 子集、组合 #  2.1.1 子集 #  2.1.1.1 问题分析 #   递归树：   观察上图可得选择列表里的数，都是选择路径（红色框）后面的数，比如 [1] 这条路径，他后面的选择列表只有 2、3，[2] 这条路径后面只有 3 这个选择，那么这个时候，就应该使用一个参数 start，来标识当前的选择列表的起始位置，也就是标识每一层的状态，因为被形象的称为状态变量，最终函数签名如下：\n// nums 为题目中给的数组 // track 为路径结果，要把每一条 path 加入结果集 public void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track, int start)    找结束条件：   此题非常特殊，所有路径都应该加入结果集，所以不存在结束条件。\n  当 start 参数越过数组边界的时候，程序就自己跳过下一层递归了，因此不需要手写结束条件，直接加入结果集：\n// res 为结果集，是全局变量，到时候需要返回 LinkedList list = new LinkedList\u0026lt;\u0026gt;(track); res.add(list);    找选择列表：   在 1 中已经提过，子问题的选择列表，是上一条选择路径之后的数，即\nfor (int i = start; i \u0026lt; nums.length; i++)    判断是否需要剪枝：  从递归树中看到，路径没有重复的，也没有不符合条件的，所以不需要剪枝。   做出选择：   即将节点添加到路径中：\ntrack.add(nums[i]);    撤销选择：   即将节点从路径中移除：\ntrack.removeLast();     2.1.1.2 参考代码 #  // 最后结果 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); /** * 78. 子集 * @param nums 数组 * @return 该数组所有可能的子集 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets(int[] nums) { LinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); backtrack(nums, track, 0); return res; } /** * 回溯法求解子集问题 * @param nums 数组 * @param track 路径 * @param start 起始位置 */ public void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track, int start) { // 将路径添加到结果列表中  LinkedList list = new LinkedList\u0026lt;\u0026gt;(track); res.add(list); for (int i = start; i \u0026lt; nums.length; i++) { // 做选择：将节点添加到路径中  track.add(nums[i]); backtrack(nums, track, i + 1); // 撤销选择：将节点从路径中移除  track.removeLast(); } } 2.1.2 子集 II #  2.1.2.1 问题分析 #   递归树：  从图中可以发现，树中出现了大量重复的集合，找结束条件和选择列表与 2.1.1 子集一样，不再赘述，我们直接看判断是否需要剪枝。   判断是否需要剪枝   因为数组中有重复元素，而这些重复元素可能并不在一起（回溯算法中时通过数组中前后元素是否一致来判断子集是否重复的），从而可能导致后面结果出现重复的子集，因此需要先对子集进行排序，使得重复的元素都在一起：\nArrays.sort(nums);   我们需要取出重复的集合，即需要剪枝，把递归树上的某些分支减掉，观察上图不难发现，应该去除当前选择列表中，与上一个数重复的那个数，引出的分支，如 2、2 这个选择列表，第二个 2 是最后重复的，应该去除这个 2 引出的分值，即下图中红色大框中的分支：\n   因此，在遍历时需要对当前遍历的节点进行判断，如果 $i \u0026gt; start$ 并且 $nums[i] == nums[i - 1]$，那么就需要进行下一个遍历：\nif (i \u0026gt; start \u0026amp;\u0026amp; nums[i] == nums[i - 1]) {continue;}    做出选择和撤销选择与 2.1.1 子集一样，这里不再赘述。  2.1.2.2 参考代码 #  // 最后结果 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); /** * 90. 子集 II * @param nums 数组 * @return 该数组所有可能的子集 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsetsWithDup(int[] nums) { LinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); // 因为数组中有重复元素，而这些重复元素可能并不在一起（回溯算法中时通过数组中前后元素是否一致来判断子集是否重复的），从而可能导致后面结果出现重复的子集，因此需要先对子集进行排序，使得重复的元素都在一起  Arrays.sort(nums); backtrack(nums, track, 0); return res; } /** * 回溯法解决子集问题 * @param nums 数组 * @param track 路径 * @param start 起始位置 */ public void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track, int start) { LinkedList list = new LinkedList\u0026lt;\u0026gt;(track); res.add(list); for (int i = start; i \u0026lt; nums.length; i++) { if (i \u0026gt; start \u0026amp;\u0026amp; nums[i] == nums[i - 1]) {continue;} // 做选择：将节点添加到路径中  track.add(nums[i]); backtrack(nums, track, i + 1); // 撤销选择：将节点从路径中移除  track.removeLast(); } } 2.1.3 组合总和 #  2.1.3.1 问题分析 #    递归树（绿色箭头上面的是路径，红色框的为结果，黄色框的为选择列表）：   从上面可以看出，组合问题和子集问题一样，1、2 和 2、1 是同一个组合，因此需要引入 start 参数标识，表示每个状态中选择列表的起始位置，另外，每个状态还需要一个 sum 变量，来记录当前路径的和，函数签名如下：\npublic void backtrack(int[] candidates, LinkedList\u0026lt;Integer\u0026gt; track, int start, int sum, int target)     找结束条件：\n  由题意可得，当路径总和等于 target 的时候，就应该把路径加入结果集，并返回，当路径总和大于 target 的时候，直接返回：\nif (sum == target) { LinkedList\u0026lt;Integer\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(track); res.add(list); return; } if (sum \u0026gt; target) { return; }     找选择列表：\nfor (int i = start; i \u0026lt; candidates.length; i++)   判断是否需要剪枝：\n  从 1 中的递归树中发现，当前状态的 sum 大于 target 的时候，就应该剪枝，不用再递归下去了：\nif (sum \u0026gt; target) { return; }     做出选择：\n  题目中说可以无限次被选择，那么 i就不用 +1，即下一层的选择列表，从自身开始，并且需要更新当前状态的 sum：\ntrack.add(candidates[i]); backtrack(candidates, track, i, sum + candidates[i], target);     撤销选择：\ntrack.removeLast();   2.1.3.2 参考代码 #  // 最后结果 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); /** * 39. 组合总和 * * @param candidates 数组 * @param target 目标和 * @return candidates 中所有可以使数字和为目标数 target 的唯一组合 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { LinkedList\u0026lt;Integer\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); backtrack(candidates, list, 0, 0, target); return res; } /** * 回溯法求解组合总和问题 * * @param candidates 数组 * @param track 路径 * @param start 起始位置 * @param sum 路径中的元素和 * @param target 目标和 */ public void backtrack(int[] candidates, LinkedList\u0026lt;Integer\u0026gt; track, int start, int sum, int target) { /** * 判断结束条件: * 如果当前路径中元素和与目标和相等，则将路径添加到结果列表中 * 如果当前路径中元素和大于目标和，则直接返回 */ if (sum == target) { LinkedList\u0026lt;Integer\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(track); res.add(list); return; } if (sum \u0026gt; target) { return; } for (int i = start; i \u0026lt; candidates.length; i++) { // 做选择：将节点添加到路径中 \ttrack.add(candidates[i]); backtrack(candidates, track, i, sum + candidates[i], target); // 撤销选择：将节点从路径中移除 \ttrack.removeLast(); } } 2.2 全排列 #  2.2.1 全排列 #  2.2.1.1 问题分析 #    递归树（最下面的叶子节点，红色框中的就是要求的结果）：   绘制递归树的过程中，如果我们选择了某个数，那么他的下一层的选择列表就是除去这个数以外的其他数，比如，第一次选择了 2，那么他的下一层的选择列表只有 1 和 3；如果选择了 3，那么他的下一层的选择列表只有 1 和 2，那么这个时候就要引入一个 used 数组来记录使用过的数字，算法签名如下：\npublic void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track, boolean[] used)     找结束条件：\nif (nums.length == track.size()) { res.add(new LinkedList\u0026lt;\u0026gt;(track)); return; }   找准选择列表：\nfor (int i = 0; i \u0026lt; nums.length; i++)   判断是否需要剪枝：\n  如果当前节点已经使用过，则直接遍历下一个节点：\nif (used[i]) { continue; }     做出选择：\ntrack.add(nums[i]); used[i] = true; backtrack(nums, track, used);   撤销选择：\ntrack.removeLast(); used[i] = false;   2.2.1.2 参考代码 #  // 最后结果 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); /** * 回溯算法解决全排列问题 * @param nums 数组 * @param track 路径 * @param used 是否使用过标识 */ public void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track, boolean[] used) { if (nums.length == track.size()) { res.add(new LinkedList\u0026lt;\u0026gt;(track)); return; } for (int i = 0; i \u0026lt; nums.length; i++) { /** * 剪枝： * 1. 如果使用过，则遍历下一个节点 */ if (used[i]) { continue; } track.add(nums[i]); used[i] = true; backtrack(nums, track, used); track.removeLast(); used[i] = false; } } /** * 46. 全排列 * * @param nums 数组 * @return 数组元素的全排列 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { // 记录【路径】 \tLinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); boolean[] used = new boolean[nums.length]; backtrack(nums, track, used); return res; } 2.2.2 全排列 II #  2.2.2.1 问题分析 #   递归树：  可以看到，有两组是重复的，这是因为在选了第二个 2 后，又选了第一个 2，从而导致最右边整条分支都是重复的：  找结束条件和选择列表和前面差不多，这里不再赘述。   判断是否需要剪枝：   有了前面 子集、组合问题的判重经验，同样首先要对题目中给出的 $nums$ 数组排序，让重复的元素并列排在一起，在 if(i \u0026gt; start \u0026amp;\u0026amp; nums[i]==nums[i-1]) 基础上修改为 if(i \u0026gt; 0 \u0026amp;\u0026amp; nums[i]==nums[i-1] \u0026amp;\u0026amp; !used[i-1])，语义为当 $i$ 可以选第一个元素之后的元素时，判断当前元素是否和上一个元素相同，如果相同，再判断上一个元素是否能用，如果上一个元素不能用，那么该分支一定是重复的，应该剪去：\nif (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1] \u0026amp;\u0026amp; used[i - 1]) { continue; }   做出选择和撤销选择和上面类似，这里不再赘述。\n    2.2.2.2 参考代码 #  // 最终结果 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); /** * 47. 全排列 II * * @param nums 数组 * @return 所有不重复的全排列 */ public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permuteUnique(int[] nums) { LinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); boolean[] used = new boolean[nums.length]; // 因为数组中有重复元素，而这些重复元素可能并不在一起（回溯算法中时通过数组中前后元素是否一致来判断子集是否重复的），从而可能导致后面结果出现重复的子集，因此需要先对子集进行排序，使得重复的元素都在一起 \tArrays.sort(nums); backtrack(nums, track, used); return res; } /** * 回溯法解决全排列问题 * * @param nums 数组 * @param track 路径 * @param used 是否使用过标识 */ public void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track, boolean[] used) { if (track.size() == nums.length) { List\u0026lt;Integer\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(track); res.add(list); return; } for (int i = 0; i \u0026lt; nums.length; i++) { /** * 剪枝： * 1. 如果使用过，则遍历下一个节点 * 2. 如果当前节点和上一个节点的值相等，且上一个节点已经使用过，则当前排列全排列一定为重复的全排列，直接遍历下一个节点 */ if (used[i]) { continue; } if (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1] \u0026amp;\u0026amp; used[i - 1]) { continue; } track.add(nums[i]); used[i] = true; backtrack(nums, track, used); used[i] = false; track.removeLast(); } } 2.2.2.3 扩展题目 #  2.2.2.3.1 字符串的排列 #   这个题目和 全排列 II类似，需要注意的是字符串转数组和List 转数组的问题：   字符串转数组：\nchar[] arr = s.toCharArray();   List 转数组：\nList\u0026lt;String\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); res.toArray(new String[res.size()]);    参考代码： // 最后结果 List\u0026lt;String\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); /** * 剑指 Offer 38. 字符串的排列 * * @param s 字符串 * @return 字符串中字符的所有排列 */ public String[] permutation(String s) { StringBuffer buffer = new StringBuffer(); boolean[] used = new boolean[s.length()]; // 因为数组中有重复元素，而这些重复元素可能并不在一起（回溯算法中时通过数组中前后元素是否一致来判断子集是否重复的），从而可能导致后面结果出现重复的子集，因此需要先对子集进行排序，使得重复的元素都在一起  char[] arr = s.toCharArray(); Arrays.sort(arr); backtrack(arr, buffer, used); return res.toArray(new String[res.size()]); } /** * 回溯法解决字符串的排列问题 * * @param arr 数组 * @param buffer 路径 * @param used 是否使用过标识 */ public void backtrack(char[] arr, StringBuffer buffer, boolean[] used) { if (buffer.length() == arr.length) { res.add(buffer.toString()); return; } for (int i = 0; i \u0026lt; arr.length; i++) { /** * 剪枝： * 1. 如果使用过，则遍历下一个节点 * 2. 如果当前节点和上一个节点的值相等，且上一个节点已经使用过，则当前排列全排列一定为重复的全排列，直接遍历下一个节点 */ if (used[i]) { continue; } if (i \u0026gt; 0 \u0026amp;\u0026amp; arr[i] == arr[i - 1] \u0026amp;\u0026amp; used[i - 1]) { continue; } buffer.append(arr[i]); used[i] = true; backtrack(arr, buffer, used); used[i] = false; buffer.deleteCharAt(buffer.length() - 1); } }   参考文献 #    回溯算法解题套路框架。  C++ 总结了回溯问题类型 带你搞懂回溯算法(大量例题)。  78. 子集。  90. 子集 II。  39. 组合总和。  46. 全排列。  47. 全排列 II。  剑指 Offer 38. 字符串的排列。  "},{"id":132,"href":"/school-recruitment/docs/algorithm/1%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6/1.6-%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/1.6.2-N%E7%9A%87%E5%90%8E/","title":"1.6.2 N皇后","section":"1.6 回溯算法","content":"N皇后 #  1 题目 #  n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。\n给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。\n每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 \u0026lsquo;Q\u0026rsquo; 和 \u0026lsquo;.\u0026rsquo; 分别代表了皇后和空位。\n示例 1：\n 输入：n = 4 输出：[[\u0026#34;.Q..\u0026#34;,\u0026#34;...Q\u0026#34;,\u0026#34;Q...\u0026#34;,\u0026#34;..Q.\u0026#34;],[\u0026#34;..Q.\u0026#34;,\u0026#34;Q...\u0026#34;,\u0026#34;...Q\u0026#34;,\u0026#34;.Q..\u0026#34;]] 解释：如上图所示，4 皇后问题存在两个不同的解法。 示例 2：\n输入：n = 1 输出：[[\u0026#34;Q\u0026#34;]] 提示：\n 1 \u0026lt;= n \u0026lt;= 9 皇后彼此不能相互攻击，也就是说：任何两个皇后都不能处于同一条横行、纵行或斜线上。  2 解题思路 #  2.1 回溯算法 #  2.1.1 问题分析 #   N 皇后的问题本质上跟 全排列问题差不多，决策树的每一层表示棋盘的每一行，每个节点可以做出的选择是在该行的任意一列放置一个皇后。 函数backtrack 依然像个在决策树上游走的指针，通过row 和col 就可以表示函数遍历到的位置，通过isValid 函数可以将不符合条件的情况进行剪枝。   2.1.2 参考代码 #  /** * 将字符数组转化为字符串 * @param array 字符数组 * @return 字符数组对应的字符串 */ public static String charArrayToString(char[] array) { return Arrays.toString(array).replaceAll(\u0026#34;[\\\\[\\\\]\\\\s,]\u0026#34;, \u0026#34;\u0026#34;); } /** * 判断是否可以在 track[row][col] 放置皇后 * * @param track 路径 * @param row 行 * @param col 列 * @return 是否可以在 track[row][col] 放置皇后 */ private boolean isValid(ArrayList\u0026lt;String\u0026gt; track, int row, int col) { int rowLen = track.size(); char[] rowArr = track.get(row).toCharArray(); // 检查所在行是否有冲突  for (int i = 0; i \u0026lt; rowArr.length; i++) { if (i != col \u0026amp;\u0026amp; rowArr[i] == \u0026#39;Q\u0026#39;) { return false; } } // 检查所在列是否有冲突  for (int i = 0; i \u0026lt; rowLen; i++) { if (i != row \u0026amp;\u0026amp; track.get(i).charAt(col) == \u0026#39;Q\u0026#39;) { return false; } } // 判断左上角所对应的斜线上是否有冲突  // 左上  int rowTemp = row, colTemp = col; while (rowTemp \u0026gt;= 0 \u0026amp;\u0026amp; colTemp \u0026gt;= 0) { if (rowTemp != row \u0026amp;\u0026amp; colTemp != col \u0026amp;\u0026amp; track.get(rowTemp).charAt(colTemp) == \u0026#39;Q\u0026#39;) { return false; } rowTemp--; colTemp--; } // 右下  rowTemp = row; colTemp = col; while (rowTemp \u0026lt; rowLen \u0026amp;\u0026amp; colTemp \u0026lt; rowLen) { if (rowTemp != row \u0026amp;\u0026amp; colTemp != col \u0026amp;\u0026amp; track.get(rowTemp).charAt(colTemp) == \u0026#39;Q\u0026#39;) { return false; } rowTemp++; colTemp++; } // 判断右上角所对应的斜线上是否有冲突  // 右上  rowTemp = row; colTemp = col; while (rowTemp \u0026gt;= 0 \u0026amp;\u0026amp; colTemp \u0026lt; rowLen) { if (rowTemp != row \u0026amp;\u0026amp; colTemp != col \u0026amp;\u0026amp; track.get(rowTemp).charAt(colTemp) == \u0026#39;Q\u0026#39;) { return false; } rowTemp--; colTemp++; } // 左下  rowTemp = row; colTemp = col; while (rowTemp \u0026lt; rowLen \u0026amp;\u0026amp; colTemp \u0026gt;= 0) { if (rowTemp != row \u0026amp;\u0026amp; colTemp != col \u0026amp;\u0026amp; track.get(rowTemp).charAt(colTemp) == \u0026#39;Q\u0026#39;) { return false; } rowTemp++; colTemp--; } return true; } /** * 回溯算法 * 【路径】：track 中小于 row 的那些行都已经成功放置了皇后 * 【选择列表】：第 row 行的所有列都是放置皇后的选择 * 【结束条件】：row 超过 track 的最后一行 * @param row * @param track * @param res */ public void backtrack(int row, ArrayList\u0026lt;String\u0026gt; track, List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; res) { // 触发结束条件  if (row == track.size()) { ArrayList\u0026lt;String\u0026gt; trackTemp = new ArrayList\u0026lt;\u0026gt;(); trackTemp.addAll(track); res.add(trackTemp); return; } int rowLen = track.get(row).length(); for (int col = 0; col \u0026lt; rowLen; col++) { // 排除不合法选择  if (!isValid(track, row, col)) { continue; } // 做选择  char[] array = track.get(row).toCharArray(); array[col] = \u0026#39;Q\u0026#39;; track.set(row, charArrayToString(array)); // 进入下一行决策  backtrack(row + 1, track, res); // 撤销选择  array[col] = \u0026#39;.\u0026#39;; track.set(row, charArrayToString(array)); } } /** * 51. N 皇后 * * @param n 皇后个数 * @return 所有不同的 n 皇后问题 的棋子放置方案 */ public List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; solveNQueens(int n) { List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); // 初始化棋盘，“.”表示空，“Q”表示皇后  ArrayList\u0026lt;String\u0026gt; track = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { track.add(String.join(\u0026#34;\u0026#34;, Collections.nCopies(n, \u0026#34;.\u0026#34;))); } backtrack(0, track, res); return res; } 参考文献 #    51. N 皇后。  回溯算法解题套路框架。  "},{"id":133,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.1-%E6%95%B0%E5%AD%A6/2.1.1-%E5%BF%AB%E9%80%9F%E6%A8%A1%E5%B9%82%E7%AE%97%E6%B3%95/","title":"2.1.1 快速模幂算法","section":"2.1 数学","content":"快速模幂算法 #  今天来聊一道与数学运算有关的题目，LeetCode 372 题 Super Pow，需要我们进行巨大的幂运算，然后求余数。\nint superPow(int a, vector\u0026lt;int\u0026gt;\u0026amp; b); 要求我们的算法返回幂运算 a^b 的计算结果与 1337 取模后的结果。首先我们需要计算 a^b，但是这个 b 会非常大，所以 b 是用数组的形式表示的。\n2.1.1 处理数组指数 #  我们可以发现这样一个规律： 通过公式发现我们可以用递归来解决这个问题，因此问题的规模缩小了：\nsuperPow(a, [1,5,6,4]) =\u0026gt; superPow(a, [1,5,6]) 根据这个规律，我们可以写出如下代码框架：\n/** * 返回模幂运算 a^[b] 的计算结果与 base 取模后的结果 * 1. a^[1024] = a^4 * a^[1020] * = a^4 * (a^[102])^10 * @param a 底数 * @param b 指数 * @param base 取模数 * @return 模幂运算 a^b 的计算结果与 base 取模后的结果 */ public static int superPow(int a, Vector\u0026lt;Integer\u0026gt; b, int base) { if (b.size() == 0) {return 1;} Integer last = b.lastElement(); b.removeElementAt(b.size() - 1); int part1 = myPow(a, last, base); int part2 = myPow(superPow(a, b, base), 10, base); // 每次乘法都要求模  return (part1 * part2) % base; } 2.1.2 处理模运算 #  由于计算机的编码方式，形如 (a * b) % base 这样的运算，乘法的结果可能导致溢出，因此，我们需要找到一种技巧，能够简化这种表达式，以此来避免溢出同时得到结果。\n模运算具有如下技巧：\n(a * b) % k = (a % k)(b % k) % k 证明如下：\n(a * b) % k = (a % k)(b % k) % k a = Ak+B b = Ck+D a * b = (Ak + B) * (Ck+D) = ACk^2 + (BC + AD)k + BD (a * b) % k = (ACk^2 + (BC + AD)k + BD) % k = ACk^2 % k + (BC + AD)k % k + BD % k = BD % k a % k = B b % k = D (a % k)(b % k) % k = BD % k (a * b) % k = (a % k)(b % k) % k 因此，对乘法的结果求模，等价于先对每个因子都求模，然后对因子相乘的结果再求模。对于该题来说，求一个数的幂就是对这个数连乘，通过将该思路进行扩展可以得到 a 的 k 次方与 base 取模  的算法，程序如下：\n/** * 计算 a 的 b 次方与 base 取模后的结果 * 1. (a * b) % k = (a % k) * (b % k) % k * @param a 底数 * @param k 指数 * @param base 取模数 * @return a 的 b 次方与 base 取模后的结果 */ public static int myPow(int a, int k, int base) { int tmpRes = a % base; int res = 1; for (int i = 0; i \u0026lt; k; i++) { // 这里有乘法，是潜在的溢出点  res *= tmpRes; // 对乘法结果求模: 可以保证 res * tmpRes 这句代码执行时两个因子都是小于 base 的，也就一定不会造成溢出  res %= base; } return res; } 完整程序如下：\npackage com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.Arrays; import java.util.Collections; import java.util.Vector; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/18 16:20 * @Description 高效模幂算法 */ public class SuperPow { /** * 计算 a 的 b 次方与 base 取模后的结果 * 1. (a * b) % k = (a % k) * (b % k) % k * @param a 底数 * @param k 指数 * @param base 取模数 * @return a 的 b 次方与 base 取模后的结果 */ public static int myPow(int a, int k, int base) { int tmpRes = a % base; int res = 1; for (int i = 0; i \u0026lt; k; i++) { // 这里有乘法，是潜在的溢出点  res *= tmpRes; // 对乘法结果求模: 可以保证 res * tmpRes 这句代码执行时两个因子都是小于 base 的，也就一定不会造成溢出  res %= base; } return res; } /** * 返回模幂运算 a^[b] 的计算结果与 base 取模后的结果 * 1. a^[1024] = a^4 * a^[1020] * = a^4 * (a^[102])^10 * @param a 底数 * @param b 指数 * @param base 取模数 * @return 模幂运算 a^b 的计算结果与 base 取模后的结果 */ public static int superPow(int a, Vector\u0026lt;Integer\u0026gt; b, int base) { if (b.size() == 0) {return 1;} Integer last = b.lastElement(); b.removeElementAt(b.size() - 1); int part1 = myPow(a, last, base); int part2 = myPow(superPow(a, b, base), 10, base); // 每次乘法都要求模  return (part1 * part2) % base; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); Vector\u0026lt;Integer\u0026gt; vector = new Vector\u0026lt;\u0026gt;(); vector.addAll(Arrays.asList(3)); int base = 3; int res = superPow(2, vector, base); stopWatch.stop(); System.out.println(String.format(\u0026#34;结果为: %s, 执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); vector.removeAllElements(); vector.addAll(Arrays.asList(3)); base = 7; res = superPow(4, vector, base); stopWatch.stop(); System.out.println(String.format(\u0026#34;结果为: %s, 执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); vector.removeAllElements(); vector.addAll(Arrays.asList(1, 0, 2, 4)); base = 1337; res = superPow(2, vector, base); stopWatch.stop(); System.out.println(String.format(\u0026#34;结果为: %s, 执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } "},{"id":134,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.1-%E6%95%B0%E5%AD%A6/2.1.2-%E6%90%9C%E7%B4%A2%E4%BA%8C%E7%BB%B4%E7%9F%A9%E9%98%B5/","title":"2.1.2 搜索二维矩阵","section":"2.1 数学","content":"搜索二维矩阵 #  1 题目 #  编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性：\n每行中的整数从左到右按升序排列。 每行的第一个整数大于前一行的最后一个整数。 示例 1：\n 输入：matrix = [[1,3,5,7],[10,11,16,20],[23,30,34,60]], target = 3 输出：true 示例 2：\n 输入：matrix = [[1,3,5,7],[10,11,16,20],[23,30,34,60]], target = 13 输出：false 提示：\n m == matrix.length n == matrix[i].length 1 \u0026lt;= m, n \u0026lt;= 100 -104 \u0026lt;= matrix[i][j], target \u0026lt;= 104  2 解题思路 #  2.1 普通遍历 #  2.1.1 问题分析 #  由于矩阵每行中的整数从左到右按升序排列，每行的第一个整数大于前一行的最后一个整数，因此我们可以按照如下方法来遍历：\n 先按行遍历，看每行的第一个元素是否等于目标元素，如果不等于，则找出第一个元素小于等于目标元素，最后一个元素大于等于目标元素的行。  如果能找到这样的行的话，则直接返回 true。 如果找不到这样的行的话，则直接返回 false。    上面的遍历方法时间复杂度较高，可以使用二分查找的方法代替普通查找，这就是下面要用的二分查找法。\n2.1.2 参考代码 #  /** * 74. 搜索二维矩阵（版本 1） * 编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性： * 1. 每行中的整数从左到右按升序排列。 * 2. 每行的第一个整数大于前一行的最后一个整数。 * @param matrix 矩阵 * @param target 目标值 * @return 矩阵中是否存在目标值 */ public boolean searchMatrixV1(int[][] matrix, int target) { // 如果矩阵的长度小于 1，则直接返回 false  if (matrix.length \u0026lt; 1) {return false;} // 如果矩阵第一个数组的长度小于 1，则直接返回 false  if (matrix[0].length \u0026lt; 1) {return false;} // 如果矩阵的第一个元素大于目标值，最后一个元素小于目标值，则直接返回 false  if (matrix[0][0] \u0026gt; target || matrix[matrix.length - 1][matrix[matrix.length - 1].length - 1] \u0026lt; target) {return false;} for (int i = 0; i \u0026lt; matrix.length; i++) { // 如果目标值在这一行矩阵中，则直接遍历这一行即可  if (matrix[i][0] \u0026lt;= target \u0026amp;\u0026amp; matrix[i][matrix[i].length - 1] \u0026gt;= target) { for (int i1 = 0; i1 \u0026lt; matrix[i].length; i1++) { if (matrix[i][i1] == target) {return true;} } } } return false; } 2.2 二分查找（一） #  2.2.1 问题分析 #  由于涉及到查找元素，所以我们可以考虑使用二分查找法来判断矩阵中是否含有目标元素，具体如下：\n 通过 寻找左侧边界的二分查找法找到不大于目标元素的一行。 然后在该行中通过二分查找法判断这一行中是否含有目标元素。  2.2.2 参考代码 #  /** * 74. 搜索二维矩阵（版本 2：二分法） * 编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性： * 1. 每行中的整数从左到右按升序排列。 * 2. 每行的第一个整数大于前一行的最后一个整数。 * @param matrix 矩阵 * @param target 目标值 * @return 矩阵中是否存在目标值 */ public boolean searchMatrixV2(int[][] matrix, int target) { // 先采用二分法搜索所有行  int left = 0, right = matrix.length - 1; while (left \u0026lt;= right) { // 为了防止 (left + right) 太大导致溢出  int mid = left + (right - left) / 2; if (matrix[mid][0] \u0026lt; target) {left = mid + 1;} else if (matrix[mid][0] \u0026gt; target) {right = mid -1;} else if (matrix[mid][0] == target) {return true;} } // 然后采用二分法搜索当前行的所有列  int col = left - 1; if (col == -1) {return false;} left = 0; right = matrix[col].length - 1; while (left \u0026lt;= right) { // 为了防止 (left + right) 太大导致溢出  int mid = left + (right - left) / 2; if (matrix[col][mid] \u0026lt; target) {left = mid + 1;} else if (matrix[col][mid] \u0026gt; target) {right = mid -1;} else if (matrix[col][mid] == target) {return true;} } // 目标值不存在矩阵中，返回 false  return false; } 2.3 二分查找（二） #  2.3.1 问题分析 #  我们还可以把这个二维矩阵当做一维矩阵，然后通过二分查找的方法来判断矩阵中是否包含目标元素。\n2.3.2 参考代码 #  /** * 74. 搜索二维矩阵（版本 3：二分法 2） * 编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性： * 1. 每行中的整数从左到右按升序排列。 * 2. 每行的第一个整数大于前一行的最后一个整数。 * @param matrix 矩阵 * @param target 目标值 * @return 矩阵中是否存在目标值 */ public boolean searchMatrixV3(int[][] matrix, int target) { // 将二阶矩阵当做一阶矩阵，然后按照二分法来查找即可  int m = matrix.length, n = matrix[0].length; int left = 0, right = m * n - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; // 当前遍历到的元素所在列  int col = mid % n; // 当前遍历到的元素所在行  int row = (mid - col) \u0026lt; n ? 0 : (mid - col) / n; if (matrix[row][col] \u0026lt; target) {left = mid + 1;} else if (matrix[row][col] \u0026gt; target) {right = mid - 1;} else if (matrix[row][col] == target) {return true;} } // 矩阵中不存在目标元素，返回 false  return false; } 2.4 BST #  2.4.1 问题分析 #   我们可以将二维矩阵抽象成以右上角为根的 BST，那么我们可以从根（右上角）开始搜索，如果当前节点不等于目标值，可以按照树的搜索顺序进行：\n 当前节点大于目标值，搜索当前节点的左子树，也就是当前矩阵位置的左方格子，即 y--。 当前节点小于目标值，搜索当前节点的右子树，也就是当前矩阵位置的右方格子，即 x++。  2.4.2 参考代码 #  int m, n; /** * 递归二叉树判断二维矩阵中是否含有目标元素 * @param matrix 二维矩阵 * @param x 横坐标 * @param y 纵坐标 * @param target 目标值 * @return 二维矩阵中是否含有目标元素 */ public boolean check(int[][] matrix, int x, int y, int target) { if (x \u0026lt; 0 || x \u0026gt;= m || y \u0026lt; 0 || y \u0026gt;= n) {return false;} if (matrix[x][y] == target) {return true;} boolean res = check(matrix, x + 1, y, target); if (!res) {res = check(matrix, x, y - 1, target);} return res; } /** * 74. 搜索二维矩阵（版本 4：二叉树） * 编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性： * 1. 每行中的整数从左到右按升序排列。 * 2. 每行的第一个整数大于前一行的最后一个整数。 * @param matrix 矩阵 * @param target 目标值 * @return 矩阵中是否存在目标值 */ public boolean searchMatrixV4(int[][] matrix, int target) { m = matrix.length; n = matrix[0].length; int x = 0, y = n - 1; return check(matrix, x, y, target); } 3 参考文献 #    74. 搜索二维矩阵  【宫水三叶】一题双解：「二分」\u0026amp;「抽象 BST」解法。 解题思路 2.1、2.2、2.3 来自于 @宫水三叶。  "},{"id":135,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.1-%E6%95%B0%E5%AD%A6/2.1.3-%E8%9E%BA%E6%97%8B%E7%9F%A9%E9%98%B5/","title":"2.1.3 螺旋矩阵","section":"2.1 数学","content":"螺旋矩阵 #  1 题目 #  给你一个 m 行 n 列的矩阵 matrix ，请按照 顺时针螺旋顺序 ，返回矩阵中的所有元素。\n示例 1：\n 输入：matrix = [[1,2,3],[4,5,6],[7,8,9]] 输出：[1,2,3,6,9,8,7,4,5] 示例 2：\n 输入：matrix = [[1,2,3,4],[5,6,7,8],[9,10,11,12]] 输出：[1,2,3,4,8,12,11,10,9,5,6,7] 提示：\n m == matrix.length n == matrix[i].length 1 \u0026lt;= m, n \u0026lt;= 10 -100 \u0026lt;= matrix[i][j] \u0026lt;= 100  2 问题分析 #  解决这个问题的基本思路是：\n 计算需要遍历的圈数$totalRing$：  矩阵的行数$rowNum = matrix.length$。 矩阵的列数$colNum$：  如果 $rowNum \u0026gt; 0$，则 $colNum = matrix[0].length$。 如果 $rowNum \\le 0$，则 $colNum = 0$。   令 $totalRingTemp = Math.min(rowNum / 2.0, colNum / 2.0)$，$totalRingTempInt = (int)(Math.min(rowNum / 2.0, colNum / 2.0))$：  如果 $totalRingTemp - totalRingTempInt \u0026gt; 0$$，则 $$totalRing = (int)(totalRingTemp + 1)$。 如果 $totalRingTemp = 1$，则 $totalRing = (int)(totalRingTemp)$。     对矩阵进行逐圈遍历，假设当前圈为$n$（$n$ 从 0 开始）：  遍历上面的元素：横坐标 $row = n$，纵坐标 $col \\in [n, colNum - n - 1]$。 遍历右面的元素：横坐标 $row \\in [n + 1]$，纵坐标 $col = colNum - n - 1$。 遍历下面的元素：横坐标 $row = rowNum - n - 1$，纵坐标 $col \\in [n, colNum - n - 2]$。 遍历左边的元素：横坐标 $row \\in [n + 1, rowNum - n - 2]$，纵坐标 $col = n$。     3 参考代码 #  /** * 获取矩阵第 n 圈顺时针遍历的元素 * * @param matrix 矩阵数组 * @param n 圈数（从 0 开始） * @param rowNum * @param orders */ public void getTargetRing(int[][] matrix, int n, int rowNum, int colNum, List\u0026lt;Integer\u0026gt; orders) { // 判断是否需要继续打印  boolean res = false; // 打印上面的元素  int rowIndexTop = n; for (int i = n; i + n \u0026lt; colNum; i++) { orders.add(matrix[n][i]); res = true; } // 打印右边的元素  int colIndexRight = colNum - n - 1; if (res) { for (int i = n + 1; i + n \u0026lt; rowNum; i++) { orders.add(matrix[i][colIndexRight]); res = true; } } // 打印下边的元素  int rowIndexBottom = rowNum - n - 1; if (res \u0026amp;\u0026amp; rowIndexBottom \u0026gt; rowIndexTop) { for (int i = colNum - n - 2; i \u0026gt;= n; i--) { orders.add(matrix[rowIndexBottom][i]); res = true; } } // 打印左边的元素  int colIndexLeft = n; if (res \u0026amp;\u0026amp; colIndexLeft \u0026lt; colIndexRight) { for (int i = rowNum - n - 2; i \u0026gt; n; i--) { orders.add(matrix[i][colIndexLeft]); } } } /** * 54. 螺旋矩阵 * * @param matrix 矩阵数组 * @return 按照 顺时针螺旋顺序 ，返回矩阵中的所有元素 */ public List\u0026lt;Integer\u0026gt; spiralOrder(int[][] matrix) { List\u0026lt;Integer\u0026gt; orders = new ArrayList\u0026lt;\u0026gt;(); int rowNum = matrix.length; int colNum = rowNum \u0026gt; 0 ? matrix[0].length : 0; double totalRingTemp = Math.min(rowNum / 2.0, colNum / 2.0); int totalRingTempInt = (int)(Math.min(rowNum / 2.0, colNum / 2.0)); // 总圈数  int totalRing = -1; if (totalRingTemp - totalRingTempInt \u0026gt; 0) { totalRing = (int)(totalRingTemp + 1); } else { totalRing = (int)(totalRingTemp); } for (int i = 0; i \u0026lt; totalRing; i++) { getTargetRing(matrix, i, rowNum, colNum, orders); } return orders; } 4 参考文献 #    54. 螺旋矩阵。  "},{"id":136,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.1-%E6%95%B0%E5%AD%A6/2.1.4-x-%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9/","title":"2.1.4 X 的平方根","section":"2.1 数学","content":"x 的平方根 #  1 题目 #  实现 int sqrt(int x) 函数。\n计算并返回 x 的平方根，其中 x 是非负整数。\n由于返回类型是整数，结果只保留整数的部分，小数部分将被舍去。\n示例 1:\n输入: 4 输出: 2 示例 2:\n输入: 8 输出: 2 说明: 8 的平方根是 2.82842..., 由于返回类型是整数，小数部分将被舍去。 2 解题思路 #  2.1 暴力解法 #  2.1.1 问题分析 #   暴力解法的基本思路是因为 $i * i = x$，因此 $x / i \u0026gt;= i$ 并且 $x / (i + 1) \u0026lt; (i + 1)$。  2.1.2 参考代码 #  /** * 69. x 的平方根（版本 1：暴力解法） * @param x 待求整数 * @return x 的平方根的整数部分 */ public int mySqrtV1(int x) { if (x == 0) {return 0;} if (x == 1) {return 1;} for (int i = 1; i \u0026lt;= x / 2; i++) { if (x / i \u0026gt;= i \u0026amp;\u0026amp; x / (i + 1) \u0026lt; (i + 1)) { return i; } } return -1; } 2.2 袖珍计算器算法 #  2.2.1 问题分析 #    袖珍计算器算法是一种用指数函数$exp$和对数函数$ln$代替平方根函数的方法，我们通过有限的可以使用的数学函数，得到我们想要的计算结果。\n  我们将 $\\sqrt x$ 写成幂的形式 $x^{\\frac12}$，再使用自然对数 $e$ 进行换底，即可得到\n$$ \\sqrt x = x^{\\frac12} = (e^{lnx})^{\\frac12} $$\n  由于计算机无法存储浮点数的精确值，而指数函数和对数函数的参数返回值均为浮点数，因此运算过程中会存在误差：\n 例如当 $x=2147395600$ 时，$e^{\\frac12{lnx}}$ 的计算结果与正确值 46340 相差 $ 10^{-11} $，这样在对结果取整数部分时，会得到 46399 这个错误的结果。    因此在得到结果的整数部分 $ans$ 时后，我们应当找出 $ans$ 与 $ans + 1$ 中哪一个是真正的答案。\n  2.2.2 参考代码 #  /** * 69. x 的平方根（版本 2：袖珍计算器算法） * @param x 待求整数 * @return x 的平方根的整数部分 */ public int mySqrtV2(int x) { // 袖珍计算器公式：x^(1/2) = e^(0.5 * lnx)  int res = (int)Math.exp(0.5 * Math.log(x)); return x / (res + 1) \u0026lt; (res + 1) ? res : (res + 1); } 2.3 二分查找法 #  2.3.1 问题分析 #    题目实际上是找到一个数 $num$ 使得：\n$$ num * num \\le x $$\n并且\n$$ (num + 1) * (num + 1) \\gt x $$\n而这个数一定不大于 $\\frac12 x$。\n  因此题目实际上可以看做查找一个 $num \\in [1, \\frac12 x]$ 符合上述条件，因此可以采用二分法来进行查找。\n  2.3.2 参考代码 #  /** * 69. x 的平方根（版本 3：二分查找法） * @param x 待求整数 * @return x 的平方根的整数部分 */ public int mySqrtV3(int x) { if (x == 1) {return 1;} int left = 1, right = x / 2, res = 0; while(left \u0026lt;= right) { int mid = left + (right - left) / 2; if ((long)mid * mid \u0026lt;= x) { res = mid; left = mid + 1; } else { right = mid - 1; } } return res; } 2.4 牛顿迭代法 #  2.4.1 问题分析 #    牛顿迭代法是一种可以用来快速求解函数零点的方法。\n  为了叙述方便，我们用 $C$ 表示待求出平方根的那个整数，显然，$C$ 的平方根就是函数\n$$ y = f(x) = x^2 - C $$\n的零点。\n  牛顿迭代法的本质是借助泰勒级数，从初始值快速向零点逼近：\n  我们任取一个 $x_0$ 作为初始值，在每一步的迭代中，我们找到函数图像上的点 $(x_i, f(x_i))$，过该点做一条斜率为该点导数 $f^{'}(x_i)$ 的直线，与横轴的交点记为 $x_{i + 1}$，$x_{i + 1}$ 相较于 $x_i$ 而言距离零点更近。\n  在经过多次迭代后，我们就可以得到一个距离零点非常接近的交点，下图给出了从 $x_0$ 开始迭代两次，得到 $x_1$ 和 $x_2$ 的过程。\n     我们选择 $x_0 = C$ 作为初始值，在每一步迭代中，我们通过当前交点 $x_i$，找到图像上的点 $(x_i, x_i^2 - C)$，作一条斜率为 $f^{'}(x_i) = 2x_i$ 的直线，直线的方程为\n$$ y - (x_i^2 - C) = 2x_i(x - x_i) $$\n化简后得\n$$ y = 2x_ix - (x_i^2 + C) $$\n与横轴的交点为方程\n$$ 2x_ix - (x_i^2 + C) = 0 $$\n的解，即为新的迭代结果 $x_{i + 1}$：\n$$ x_{i + 1} = \\frac12 (x_i + \\frac{C}{x_i}) $$\n在进行 $k$ 次迭代后，$x_k$ 的值与真实的零点 $\\sqrt C$ 足够接近，即可作为答案。\n   细节：\n 为什么选择 $ x_0 = C $ 作为初始值？ 因为 $y = x^2 - C$ 有两个零点 $-\\sqrt C$ 和 $\\sqrt C$，如果我们取得初始值较小，可能会迭代到 $-\\sqrt C$ 这个零点，而我们希望找到的是 $\\sqrt C$ 这个零点，因此选择 $x_0 = C$ 作为初始值，每次迭代均有 $x_{i + 1} \u0026lt; x_i$，零点 $\\sqrt C$ 在其左侧，所以我们一定会迭代到这个零点。 迭代到何时才算结束？ 每一次迭代后，我们都会距离零点更进一步，所以当相邻两次迭代得到的交点非常接近时，我们就可以断定，此时的结果已经足够我们得到答案了，一般来说，可以判断相邻两次迭代的结果的差值是否小于一个极小的非负数 $\\varepsilon$，其中 $\\varepsilon$ 一般可以取 $ 10^{-6} $ 或 $ 10^{-7} $。   2.4.2 参考代码 #  /** * 69. x 的平方根（版本 4：牛顿迭代法） * @param x 待求整数 * @return x 的平方根的整数部分 */ public int mySqrtV4(int x) { if (x == 0) {return 0;} double err = 1e-7; return (int)sqrt(x, x, err); } 3 参考文献 #    69. x 的平方根。  x 的平方根。  "},{"id":137,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.1-%E6%9E%84%E9%80%A0-K-%E4%B8%AA%E5%9B%9E%E6%96%87%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"2.4.1 构造 K 个回文字符串","section":"2.4 字符串","content":"构造 K 个回文字符串 #  1 题目 #  给你一个字符串 s 和一个整数 k 。请你用 s 字符串中 所有字符 构造 k 个非空 回文串 。\n如果你可以用 s 中所有字符构造 k 个回文字符串，那么请你返回 True ，否则返回 False 。 示例 1：\n输入：s = \u0026quot;annabelle\u0026quot;, k = 2 输出：true 解释：可以用 s 中所有字符构造 2 个回文字符串。 一些可行的构造方案包括：\u0026quot;anna\u0026quot; + \u0026quot;elble\u0026quot;，\u0026quot;anbna\u0026quot; + \u0026quot;elle\u0026quot;，\u0026quot;anellena\u0026quot; + \u0026quot;b\u0026quot; 示例 2：\n输入：s = \u0026quot;leetcode\u0026quot;, k = 3 输出：false 解释：无法用 s 中所有字符构造 3 个回文串。 示例 3：\n输入：s = \u0026quot;true\u0026quot;, k = 4 输出：true 解释：唯一可行的方案是让 s 中每个字符单独构成一个字符串。 示例 4：\n输入：s = \u0026quot;yzyzyzyzyzyzyzy\u0026quot;, k = 2 输出：true 解释：你只需要将所有的 z 放在一个字符串中，所有的 y 放在另一个字符串中。那么两个字符串都是回文串。 示例 5：\n输入：s = \u0026quot;cr\u0026quot;, k = 7 输出：false 解释：我们没有足够的字符去构造 7 个回文串。 提示：\n 1 \u0026lt;= s.length \u0026lt;= 10^5 s 中所有字符都是小写英文字母。 1 \u0026lt;= k \u0026lt;= 10^5  2 解题思路 #  2.1 定理证明 #  先来证明两个定理：\n对于一个字符串 $s$，能构成的最小回文串的个数为 $min$，能构成的最大回文串的个数为 $max$：\n $min$ 等于 $s$ 中出现奇数次字符的个数，$max$ 等于 $s$ 中字符的个数。 $s$ 能构成 $min$ 和 $max$ 之间任意个数的回文串。  具体证明如下：\n定理 1 证明：\n 因为出现奇数次的字符每一个都可以单独构成一个回文串，所以 $s$ 中能构成的最小回文串的个数就等于出现奇数次字符的个数。 由于每一个字符都能单独构成一个回文串，所以 $d$ 中能构成的最大回文串的个数就等于 $s$ 中字符的个数。  定理 2 证明：\n 回文串分两种，一种是奇数个字符，一种是偶数个字符。 对于奇数个字符的回文串，每次拿出一个字符单独构成一个回文串，整个字符串能构成的回文串的个数会加 1。 对于偶数个字符的回文串，可以随便拿出一个字符单独构成一个回文串，整个字符串能构成的回文串的个数会加 1，同时生预测回文串会成为奇数个字符，这就相当于情况 1 了。 因此 $s$ 能构成 $min$ 和 $max$ 之间任意个数的回文串。  2.2 问题解析 #  通过以上定理我们可知只要判断 $k$ 是否在 $min$ 和 $max$ 之间即可。\n2.3 参考代码 #  代码中有一点需要注意，就是采用****整数数组替代 $HashMap$ 来保存字符串中每个字符出现的次数，这样可以减少算法的时间复杂度和空间复杂度。\npackage com.grayson.top; import java.util.HashMap; import java.util.Map; /** * @author peng.wei * @version 1.0 * @date 2021/4/9 16:15 * @Description 构造 K 个回文字符串 */ public class L1400 { /** * 1400. 构造 K 个回文字符串 * 给你一个字符串 s 和一个整数 k 。请你用 s 字符串中 所有字符 构造 k 个非空 回文串 。 * 如果你可以用 s 中所有字符构造 k 个回文字符串，那么请你返回 True ，否则返回 False 。 * @param s 字符串 * @param k 回文串的个数 * @return 能否用字符串中的所有字符构建 k 个回文字符串 */ public boolean canConstruct(String s, int k) { // 字符串中出现奇数次字符的个数  int odd = 0; // 字符串可以构建的最少和做多回文串的个数  int min = 0, max = s.length(); // 字符串中每个字符出现的次数，使用整数数组来存，而不用 HashMap，从而减少算法的时间和空间复杂度  int[] chars = new int[26]; for (int i = 0; i \u0026lt; max; i++) { ++chars[s.charAt(i) - \u0026#39;a\u0026#39;]; } // 查找字符串中出现奇数次字符的个数  for (int i = 0; i \u0026lt; 26; i++) { if (chars[i] % 2 != 0) {odd++;} } min = Math.max(odd, 1); // 如果 k 在 [min, max] 之间，则该字符串可以构造 k 个回文字符串，否则不可以  return k \u0026gt;= min \u0026amp;\u0026amp; k \u0026lt;= max; } } 3 参考文献 #    1400. 构造 K 个回文字符串。  构造 K 个回文字符串「官方题解」。  "},{"id":138,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.2-%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E6%9C%80%E5%9B%9E%E4%B8%B2/","title":"2.4.2 如何寻找最 回 串","section":"2.4 字符串","content":"如何寻找最⻓回⽂⼦串 #  1 题目 #  给你一个字符串 s，找到 s 中最长的回文子串。\n示例 1：\n输入：s = \u0026quot;babad\u0026quot; 输出：\u0026quot;bab\u0026quot; 解释：\u0026quot;aba\u0026quot; 同样是符合题意的答案。 示例 2：\n输入：s = \u0026quot;cbbd\u0026quot; 输出：\u0026quot;bb\u0026quot; 示例 3：\n输入：s = \u0026quot;a\u0026quot; 输出：\u0026quot;a\u0026quot; 示例 4：\n输入：s = \u0026quot;ac\u0026quot; 输出：\u0026quot;a\u0026quot; 2 问题分析 #  寻找回文串的问题核心思想是：从中间开始向两边扩散来判断回文串。 可以通过双指针的方法来解决。\n3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/23 15:05 * @Description 最长回文子串 */ public class L5 { /** * 寻找回文子串 * * @param s 字符串 * @param start 起始字符的下标 * @param end 结束字符的下标 * @return 偶数回文子串的长度 */ public static String palindrome(String s, int start, int end) { int slow = start, fast = end; while (slow \u0026gt;= 0 \u0026amp;\u0026amp; fast \u0026lt; s.length() \u0026amp;\u0026amp; s.charAt(slow) == s.charAt(fast)) { slow--; fast++; } return s.substring(slow + 1, fast); } /** * 5.最长回文子串 * 给你一个字符串 s，找到 s 中最长的回文子串。 * * @param s 字符串 * @return 最长回文子串 */ public static String longestPalindrome(String s) { if (s.length() == 0) {return null;} else if (s.length() == 1) {return s;} String palindrome = \u0026#34;\u0026#34;; for (int i = 0; i \u0026lt; s.length(); i++) { String oddPalindrome = palindrome(s, i - 1, i + 1); String evenPalindrome = palindrome(s, i - 1, i); palindrome = oddPalindrome.length() \u0026gt; palindrome.length() ? oddPalindrome : palindrome; palindrome = evenPalindrome.length() \u0026gt; palindrome.length() ? evenPalindrome : palindrome; } return palindrome; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); String s = \u0026#34;babad\u0026#34;; String palindrome = longestPalindrome(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, palindrome, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); s = \u0026#34;cbbd\u0026#34;; palindrome = longestPalindrome(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, palindrome, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); s = \u0026#34;a\u0026#34;; palindrome = longestPalindrome(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 3 结果为：%s，执行用时：%s 微秒\u0026#34;, palindrome, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); s = \u0026#34;ac\u0026#34;; palindrome = longestPalindrome(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 4 结果为：%s，执行用时：%s 微秒\u0026#34;, palindrome, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } "},{"id":139,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.3-%E5%A6%82%E4%BD%95%E5%88%A4%E5%AE%9A%E6%8B%AC%E5%8F%B7%E7%9A%84%E5%90%88%E6%B3%95%E6%80%A7/","title":"2.4.3 如何判定括号的合法性","section":"2.4 字符串","content":"如何判定括号的合法性 #  1 题目 #  给定一个只包括 \u0026lsquo;('，')'，'{'，'}'，'['，']\u0026rsquo; 的字符串 s ，判断字符串是否有效。\n有效字符串需满足：\n左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。\n示例 1：\n输入：s = \u0026quot;()\u0026quot; 输出：true 示例 2：\n输入：s = \u0026quot;()[]{}\u0026quot; 输出：true 示例 3：\n输入：s = \u0026quot;(]\u0026quot; 输出：false 示例 4：\n输入：s = \u0026quot;([)]\u0026quot; 输出：false 示例 5：\n输入：s = \u0026quot;{[]}\u0026quot; 输出：true 2 问题解析 #  栈是一种后进先出的结构，处理括号的问题时尤其有用。\n我们可以遍历字符串，然后遇到左括号就入栈，遇到右括号就去栈中寻找最近的左括号，看是否匹配。\n3 参考代码 #  package com.grayson.top; import org.apache.commons.lang3.time.StopWatch; import javax.xml.stream.events.Characters; import java.util.Stack; import java.util.concurrent.TimeUnit; /** * @author peng.wei * @version 1.0 * @date 2021/3/25 15:58 * @Description 有效的括号 */ public class L20 { /** * 返回同类型的括号 * @param c 右侧括号 * @return 左侧括号 */ public static char sameType(char c) { if (c == \u0026#39;)\u0026#39;) {return \u0026#39;(\u0026#39;;} else if (c == \u0026#39;]\u0026#39;) {return \u0026#39;[\u0026#39;;} else if (c == \u0026#39;}\u0026#39;) {return \u0026#39;{\u0026#39;;} return Character.MIN_VALUE; } /** * 20. 有效的括号 * 题目： * 给定一个只包括 \u0026#39;(\u0026#39;，\u0026#39;)\u0026#39;，\u0026#39;{\u0026#39;，\u0026#39;}\u0026#39;，\u0026#39;[\u0026#39;，\u0026#39;]\u0026#39; 的字符串 s ，判断字符串是否有效。 * * 思路： * 1. 依次遍历字符串中的每个字符： * 1.1 如果是\u0026#34;(\u0026#34;、\u0026#34;[\u0026#34;、\u0026#34;{}\u0026#34;中的任意一个，则将其压入堆栈。 * 1.2 如果是其他字符，并且堆栈不为空，当前字符与堆栈中最上面的字符相同，则继续查看下一个字符。 * 1.3 否则的话，则直接返回 false * 2. 如果程序中间没有结束，并且堆栈为空，则直接返回 true。 * @param s 字符串 * @return 字符串的括号是否有效 */ public static boolean isValid(String s) { // 创建一个堆栈  Stack\u0026lt;Character\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; s.length(); i++) { char c = s.charAt(i); if (c == \u0026#39;(\u0026#39; || c == \u0026#39;[\u0026#39; || c == \u0026#39;{\u0026#39;) {stack.push(c);} else if (!stack.empty() \u0026amp;\u0026amp; sameType(c) == stack.pop()) {continue;} else {return false;} } return stack.empty() \u0026amp;\u0026amp; true; } public static void main(String[] args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); String s = \u0026#34;()\u0026#34;; boolean res = isValid(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 1 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); s = \u0026#34;()[]{}\u0026#34;; res = isValid(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); stopWatch.reset(); stopWatch.start(); s = \u0026#34;[\u0026#34;; res = isValid(s); stopWatch.stop(); System.out.println(String.format(\u0026#34;测试用例 2 结果为：%s，执行用时：%s 微秒\u0026#34;, res, stopWatch.getTime(TimeUnit.MICROSECONDS))); } } "},{"id":140,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.4-%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0-atoi/","title":"2.4.4 字符串转换整数 (Atoi)","section":"2.4 字符串","content":"字符串转换整数 (atoi) #  1 题目 #  请你来实现一个 myAtoi(string s) 函数，使其能将字符串转换成一个 32 位有符号整数（类似 C/C++ 中的 atoi 函数）。\n函数 myAtoi(string s) 的算法如下：\n 读入字符串并丢弃无用的前导空格 检查下一个字符（假设还未到字符末尾）为正还是负号，读取该字符（如果有）。 确定最终结果是负数还是正数。 如果两者都不存在，则假定结果为正。 读入下一个字符，直到到达下一个非数字字符或到达输入的结尾。字符串的其余部分将被忽略。 将前面步骤读入的这些数字转换为整数（即，\u0026ldquo;123\u0026rdquo; -\u0026gt; 123， \u0026ldquo;0032\u0026rdquo; -\u0026gt; 32）。如果没有读入数字，则整数为 0 。必要时更改符号（从步骤 2 开始）。 如果整数数超过 32 位有符号整数范围 [−231, 231 − 1] ，需要截断这个整数，使其保持在这个范围内。具体来说，小于 −231 的整数应该被固定为 −231 ，大于 231 − 1 的整数应该被固定为 231 − 1 。 返回整数作为最终结果。  注意：\n 本题中的空白字符只包括空格字符 ' ' 。 除前导空格或数字后的其余字符串外，请勿忽略 任何其他字符。  示例 1：\n输入：s = \u0026#34;42\u0026#34; 输出：42 解释：加粗的字符串为已经读入的字符，插入符号是当前读取的字符。 第 1 步：\u0026#34;42\u0026#34;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026#34;42\u0026#34;（当前没有读入字符，因为这里不存在 \u0026#39;-\u0026#39; 或者 \u0026#39;+\u0026#39;） ^ 第 3 步：\u0026#34;42\u0026#34;（读入 \u0026#34;42\u0026#34;） ^ 解析得到整数 42 。 由于 \u0026#34;42\u0026#34; 在范围 [-231, 231 - 1] 内，最终结果为 42 。 示例 2：\n输入：s = \u0026#34; -42\u0026#34; 输出：-42 解释： 第 1 步：\u0026#34; -42\u0026#34;（读入前导空格，但忽视掉） ^ 第 2 步：\u0026#34; -42\u0026#34;（读入 \u0026#39;-\u0026#39; 字符，所以结果应该是负数） ^ 第 3 步：\u0026#34; -42\u0026#34;（读入 \u0026#34;42\u0026#34;） ^ 解析得到整数 -42 。 由于 \u0026#34;-42\u0026#34; 在范围 [-231, 231 - 1] 内，最终结果为 -42 。 示例 3：\n输入：s = \u0026#34;4193 with words\u0026#34; 输出：4193 解释： 第 1 步：\u0026#34;4193 with words\u0026#34;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026#34;4193 with words\u0026#34;（当前没有读入字符，因为这里不存在 \u0026#39;-\u0026#39; 或者 \u0026#39;+\u0026#39;） ^ 第 3 步：\u0026#34;4193 with words\u0026#34;（读入 \u0026#34;4193\u0026#34;；由于下一个字符不是一个数字，所以读入停止） ^ 解析得到整数 4193 。 由于 \u0026#34;4193\u0026#34; 在范围 [-231, 231 - 1] 内，最终结果为 4193 。 示例 4：\n输入：s = \u0026#34;words and 987\u0026#34; 输出：0 解释： 第 1 步：\u0026#34;words and 987\u0026#34;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026#34;words and 987\u0026#34;（当前没有读入字符，因为这里不存在 \u0026#39;-\u0026#39; 或者 \u0026#39;+\u0026#39;） ^ 第 3 步：\u0026#34;words and 987\u0026#34;（由于当前字符 \u0026#39;w\u0026#39; 不是一个数字，所以读入停止） ^ 解析得到整数 0 ，因为没有读入任何数字。 由于 0 在范围 [-231, 231 - 1] 内，最终结果为 0 。 示例 5：\n输入：s = \u0026#34;-91283472332\u0026#34; 输出：-2147483648 解释： 第 1 步：\u0026#34;-91283472332\u0026#34;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026#34;-91283472332\u0026#34;（读入 \u0026#39;-\u0026#39; 字符，所以结果应该是负数） ^ 第 3 步：\u0026#34;-91283472332\u0026#34;（读入 \u0026#34;91283472332\u0026#34;） ^ 解析得到整数 -91283472332 。 由于 -91283472332 小于范围 [-231, 231 - 1] 的下界，最终结果被截断为 -231 = -2147483648 。提示：0 \u0026lt;= s.length \u0026lt;= 200 s 由英文字母（大写和小写）、数字（0-9）、\u0026#39; \u0026#39;、\u0026#39;+\u0026#39;、\u0026#39;-\u0026#39; 和 \u0026#39;.\u0026#39; 组成 2 解题思路 #  2.1 有限状态机 #  2.1.1 含义 #   有限状态机是一种用来进行对象行为建模的工具，其作用主要是描述对象在他的生命周期内所经历的状态序列，以及如何响应来自外界的各种事件。 在计算机科学中，有限状态机被广泛用于建模应用行为、硬件电路系统设计、软件工程、编译器、网络协议、计算机与语言的研究，比如下图非常有名的 TCP 协议状态机：  我们在编程时实现相关业务逻辑时经常需要处理各种事件和状态切换，写各种 switch/case 和 if/else，所以我们其实可能一直都在跟有限状态机打交道，只是可能没有意识到。 在处理一些业务逻辑比较复杂的需求时，可以先看看是否适用于用一个有限状态机来描述，如果可以把业务模型抽象成一个有限状态机，那么代码逻辑就会特别清晰，结构特别规整。 状态机主要包括四个要素，分别是现态、条件、动作、次态，现态和条件是因，动作和次态是果，具体如下：  现态：指当前所处状态。 条件：又称事件，当一个条件满足时，就会触发一个动作，或者执行一次状态的迁移。 动作：指条件满足后执行的动作，动作执行完毕后，可以迁移到新的状态，也可以仍旧保持原状态，动作不是必需的，当条件满足后，也可以不执行任何动作，直接迁移到新状态。 次态：指条件满足后要迁往的新状态，次态是相对于现态而言的，次态一旦被激活，就转变成新的现态了。   我们可以用状态表表示整个过程，如下图所示：   2.1.2 问题分析 #   该题目如果要用 if ... else 或 switch ... case 来判断的话将会非常麻烦，而且情况我们可能也不会考虑的那么全面，因此有可能我们调试了很长时间还是无法提交通过。 这时候我们可以采用一种叫做 有限状态机的方式来解决，具体的状态表如下表所示：      \u0026quot; \u0026quot; +/- number other     start start signed in_number end   signed end end in_number end   in_number end end in_number end   end end end end end    下面以第一行为例说明一下上面表格表示的含义，最左边的 start 表示初始状态：  当遇到\u0026quot; \u0026ldquo;时，状态更新为 start。 当遇到 +/- 时，状态更新为 signed。 当遇到 number 时，状态更新为 in_number。 当遇到 other 时，状态更新为 end。   因此可以对字符串 $s$ 中的字符 $c$ 进行逐个遍历，然后根据字符的值确定当前状态 $state$，然后根据相应的状态完成相应的动作，假设用 $ans$表示最后的结果，初始值为 0，$sign$表示最后的结果的符号，1 表示正号，-1 表示负号：  当 $state$ 为 in_number 时，说明当前字符 $c$ 为数字，则：  ans = 10 * ans + c - '0'。 ans = (sign == 1 ? Math.min(ans, Integer.MAX_VALUE) : Math.min(ans, -(long) Integer.MIN_VALUE))。   当 $state$ 为 signed 时，表示当前字符 $c$ 为符号，则：  sign = (c == '-' ? -1 : 1)。     当字符串遍历完成后，直接返回符号和结果的乘积$sign * ans$ 即可。  2.1.3 参考代码 #  /** * @author peng.wei * @version 1.0 * @date 2021/7/12 20:41 * @Description 字符串转换整数 (atoi) */ public class L8 { /** * 8. 字符串转换整数 (atoi) * * @param s 字符串 * @return 字符串对应的整数 */ public int myAtoi(String s) { // 定义有限状态机  Automaton automaton = new Automaton(); for (int i = 0; i \u0026lt; s.length(); i++) { // 执行有限状态机中的方法，完成相应的状态转移  automaton.get(s.charAt(i)); } // 将符号和数值相乘，作为最后的结果返回  return (int) (automaton.sign * automaton.ans); } /** * 有限状态机 */ class Automaton { public int sign = 1; /*符号*/ public long ans = 0; /*数值*/ private String state = \u0026#34;start\u0026#34;; /*状态*/ private Map\u0026lt;String, String[]\u0026gt; table = new HashMap\u0026lt;String, String[]\u0026gt;() { { put(\u0026#34;start\u0026#34;, new String[]{\u0026#34;start\u0026#34;, \u0026#34;signed\u0026#34;, \u0026#34;in_number\u0026#34;, \u0026#34;end\u0026#34;}); put(\u0026#34;signed\u0026#34;, new String[]{\u0026#34;end\u0026#34;, \u0026#34;end\u0026#34;, \u0026#34;in_number\u0026#34;, \u0026#34;end\u0026#34;}); put(\u0026#34;in_number\u0026#34;, new String[]{\u0026#34;end\u0026#34;, \u0026#34;end\u0026#34;, \u0026#34;in_number\u0026#34;, \u0026#34;end\u0026#34;}); put(\u0026#34;end\u0026#34;, new String[]{\u0026#34;end\u0026#34;, \u0026#34;end\u0026#34;, \u0026#34;end\u0026#34;, \u0026#34;end\u0026#34;}); } }; /*状态表*/ /** * 状态转移方法 * @param c 当前字符 */ public void get(char c) { state = table.get(state)[get_col(c)]; if (\u0026#34;in_number\u0026#34;.equals(state)) { // 当前状态为数值，因此进行数值相应的计算  ans = 10 * ans + c - \u0026#39;0\u0026#39;; // 当 sign == 1 时，说明此时为正数，因此取 ans 和 Integer.MAX_VALUE 的最小值  // 否则，说明此时为负数，因此取 ans 和 -Integer.MIN_VALUE 的最小值（现在求的只是数值大小，符号后面会统一加）  ans = (sign == 1 ? Math.min(ans, Integer.MAX_VALUE) : Math.min(ans, -(long) Integer.MIN_VALUE)); } else if (\u0026#34;signed\u0026#34;.equals(state)) { // 当前状态为符号，因此判断最后是否需要乘以 -1  sign = (c == \u0026#39;-\u0026#39; ? -1 : 1); } } /** * 获取有限状态表中的状态 * @param c 字符 * @return 字符对应于有限状态表中的状态 */ public int get_col(char c) { if (c == \u0026#39; \u0026#39;) { // start  return 0; } else if (c == \u0026#39;+\u0026#39; || c == \u0026#39;-\u0026#39;) { // signed  return 1; } else if (Character.isDigit(c)) { // in_number  return 2; } else { // end  return 3; } } } } 3 参考文献 #    8. 字符串转换整数 (atoi)。  字符串转换整数 (atoi)。  深入浅出理解有限状态机。  "},{"id":141,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.5-%E9%95%BF%E5%BA%A6%E4%B8%BA3%E7%9A%84%E4%B8%8D%E5%90%8C%E5%9B%9E%E6%96%87%E5%AD%90%E5%BA%8F%E5%88%97/","title":"2.4.5 长度为3的不同回文子序列","section":"2.4 字符串","content":"长度为3的不同回文子序列 #  1 题目 #  给你一个字符串 s ，返回 s 中 长度为 3 的不同回文子序列 的个数。\n即便存在多种方法来构建相同的子序列，但相同的子序列只计数一次。\n回文 是正着读和反着读一样的字符串。\n子序列 是由原字符串删除其中部分字符（也可以不删除）且不改变剩余字符之间相对顺序形成的一个新字符串。\n例如，\u0026ldquo;ace\u0026rdquo; 是 \u0026ldquo;abcde\u0026rdquo; 的一个子序列。 示例 1：\n输入：s = \u0026#34;aabca\u0026#34; 输出：3 解释：长度为 3 的 3 个回文子序列分别是： - \u0026#34;aba\u0026#34; (\u0026#34;aabca\u0026#34; 的子序列) - \u0026#34;aaa\u0026#34; (\u0026#34;aabca\u0026#34; 的子序列) - \u0026#34;aca\u0026#34; (\u0026#34;aabca\u0026#34; 的子序列) 示例 2：\n输入：s = \u0026#34;adc\u0026#34; 输出：0 解释：\u0026#34;adc\u0026#34; 不存在长度为 3 的回文子序列。 示例 3：\n输入：s = \u0026#34;bbcbaba\u0026#34; 输出：4 解释：长度为 3 的 4 个回文子序列分别是： - \u0026#34;bbb\u0026#34; (\u0026#34;bbcbaba\u0026#34; 的子序列) - \u0026#34;bcb\u0026#34; (\u0026#34;bbcbaba\u0026#34; 的子序列) - \u0026#34;bab\u0026#34; (\u0026#34;bbcbaba\u0026#34; 的子序列) - \u0026#34;aba\u0026#34; (\u0026#34;bbcbaba\u0026#34; 的子序列) 提示：\n 3 \u0026lt;= s.length \u0026lt;= 105 s 仅由小写英文字母组成  2 解题思路 #  2.1 两次遍历 #  2.1.1 问题分析 #    该题目的基本原理是两个字符之间字符的种类数即为这两个字符之间的字符所能组成的长度为 3 的回文序列的个数。\n  因此首先需要遍历字符串中的每个字符，然后使用 $map$存储每个字符在字符串中的位置。\n  然后遍历 $map$，当一个字符在字符串中出现两次的时候，统计这两个字符之间出现的字符的种类数，即为这两个字符之间的长度为 3 的回文序列的个数。\n  然后将所有的回文序列的个数相加，便是总的回文序列的个数。\n![](../../../media/202107/5809-长度为 3 的不同回文子序列（解法一：两次遍历）_1625987315.gif)\n  2.1.2 参考代码 #  /** * 5809. 长度为 3 的不同回文子序列 * @param s 字符串 * @return 字符串中长度为 3 的不同回文子序列 */ public int countPalindromicSubsequence(String s) { int n = s.length(); int count = 0; // 用于存储每个字符在字符串中的位置  Map\u0026lt;Character, List\u0026lt;Integer\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); // 遍历字符串，将每个字符对应的位置存储到 map 中  for (int i = 0; i \u0026lt; n; i++) { char c = s.charAt(i); if (!map.containsKey(c)) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(i); map.put(c, list); } else { List\u0026lt;Integer\u0026gt; list = map.get(c); list.add(i); map.put(c, list); } } // 遍历 map，当一个字符在字符串中出现大于等于两次时，则这两个字符之间字符的种类数就是这两个字符之间的回文序列个数，最后将所有的回文序列个数相加，便是总的回文序列个数  for (Map.Entry\u0026lt;Character, List\u0026lt;Integer\u0026gt;\u0026gt; entry: map.entrySet()) { List\u0026lt;Integer\u0026gt; values = entry.getValue(); if (values.size() \u0026gt;= 2) { int start = values.get(0); int end = values.get(values.size() - 1); Set\u0026lt;Character\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int j = start + 1; j \u0026lt;= end - 1; j++) { set.add(s.charAt(j)); } count += set.size(); } } // 返回最后的结果  return count; } 3 参考文献 #    5809. 长度为 3 的不同回文子序列。  "},{"id":142,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.6-%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/","title":"2.4.6 无重复字符的最长子串","section":"2.4 字符串","content":"无重复字符的最长子串 #  1 题目 #  给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\n示例 1:\n输入: s = \u0026quot;abcabcbb\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;abc\u0026quot;，所以其长度为 3。 示例 2:\n输入: s = \u0026quot;bbbbb\u0026quot; 输出: 1 解释: 因为无重复字符的最长子串是 \u0026quot;b\u0026quot;，所以其长度为 1。 示例 3:\n输入: s = \u0026quot;pwwkew\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;wke\u0026quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，\u0026quot;pwke\u0026quot; 是一个子序列，不是子串。 示例 4:\n输入: s = \u0026quot;\u0026quot; 输出: 0 提示：\n 0 \u0026lt;= s.length \u0026lt;= 5 * 104 s 由英文字母、数字、符号和空格组成  2 解题思路 #  2.1 暴力解法 #  2.1.1 问题解析 #   可以设计一个 Map，其中的 key 为每一个字符，value 为其对应的下标。 当 Map 中 value 的数值为连续的时候，说明此时的子串也是连续的。 然后每添加一个元素就修改一下该元素对应的下标，同时计算一下 Map 中连续值的个数，然后将它和历史的连续值个数进行比较，取其中的最大值。 当所有元素遍历完成后，Map 中最大的连续值个数即为最大不重复字串的长度。  2.1.2 参考代码 #  /** * 计算 Map 的 values 中连续数值的个数 * * @param map Map * @return Map 的 values 中连续数值的个数 */ public static int calculateMapAdjacentNumber(Map\u0026lt;Character, Integer\u0026gt; map) { int max = 1; List\u0026lt;Integer\u0026gt; values = map .values() .stream() .sorted() .collect(Collectors.toList()); for (int i = 1; i \u0026lt; values.size(); i++) { if (values.get(i) - values.get(i - 1) != 1) { max = 1; } else { max++; } } return max; } /** * 3. 无重复字符的最长子串（版本 1：暴力解法） * * @param s 字符串 * @return 无重复字符的最长子串 */ public int lengthOfLongestSubstringV1(String s) { Map\u0026lt;Character, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int length = s.length(); int max = 0; for (int i = 0; i \u0026lt; length; i++) { char key = s.charAt(i); map.put(key, i); max = Math.max(max, calculateMapAdjacentNumber(map)); } return max; } 2.2 滑动窗口 #  2.2.1 问题解析 #   定义一个窗口，窗口中的内容即为当前的不重复字串，其中窗口的左边界为 $left$，右边界为 $right$。 定义一个 $Map$，里面存储每个元素的下标，用于判断元素是否存在。 然后对元素进行遍历：  当遍历一个元素在窗口中已经存在时，将 $left$ 的值更新为 $max(left, map.get(key))$。 每遍历一个元素，$right$ 向右移动一位，同时计算当前不重复字串的最大长度 $max$。 然后更新 $map$ 中元素的下标。   当所有元素遍历完后，$max$ 的值即为不重复字串的最大长度。  2.2.2 参考代码 #  /** * 3. 无重复字符的最长子串（版本 2：滑动窗口） * * @param s 字符串 * @return 无重复字符的最长子串 */ public int lengthOfLongestSubstringV2(String s) { Map\u0026lt;Character, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int len = s.length(); int left = -1, right = left, max = 0, temp; for (int i = 0; i \u0026lt; len; i++) { char key = s.charAt(i); right = i; if (map.containsKey(key)) { left = Math.max(left, map.get(key)); } temp = right == left ? 1 : right - left; max = Math.max(max, temp); map.put(key, i); } return max; } 3 参考文献 #    无重复字符的最长子串。  无重复字符的最长子串『官方题解』。  滑动窗口。  "},{"id":143,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.7-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%8A%A0/","title":"2.4.7 字符串相加","section":"2.4 字符串","content":"字符串相加 #  1 题目 #  给定两个字符串形式的非负整数 num1 和 num2 ，计算它们的和。\n提示：\n num1 和 num2 的长度都小于 5100 num1 和 num2 都只包含数字 0-9 num1 和 num2 都不包含任何前导零 你不能使用任何內建 BigInteger 库， 也不能直接将输入的字符串转换为整数形式  2 解题思路 #  2.1 模拟竖式算法 #  2.1.1 问题分析 #   如果两个字符串的长度不同，则将位数较短的字符串补零，使其和较长的字符串长度相同。 将两个字符串各位进行相加：  两个字符串当前数字分别是 $a$、$b$，进位值 $plus$ 初始为 0。 将 $a$、$b$、$plus$ 相加得 $tempRes$，同时令当前位相加结果 $add$ 等于 $tempRes$：  如果 $tempRes\u0026gt;9$，说明当前位有进位，则令 $plus=tempRes/10$，$add=tempRes-plus/10$。 否则令 $plus=0$，说明当前位没有进位。   将当前位相加得结果 $add$ 添加到最终结果 $res$ 中。   如果最后 $plus$ 大于 0，说明最后一位又有进位，将进位结果添加到最终结果 $res$ 中。 前面得到的结果 $res$ 是反的，将其反转一下返回即可。   2.1.2 参考代码 #  /** * 将两个数字字符（均为个位数）相加 * @param a 第一个数字字符 * @param b 第二个数字字符 * @param plus 进位数字 * @return 两个数字字符之和 */ public int add(char a, char b, int plus) { return (a - \u0026#39;0\u0026#39;) + (b - \u0026#39;0\u0026#39;) + plus; } /** * 415. 字符串相加（版本 1：模拟竖式算法） * * @param num1 数字 1 * @param num2 数字 2 * @return 以字符串形式返回两个数字相加后的结果 */ public String addStringsV1(String num1, String num2) { int len1 = num1.length(); int len2 = num2.length(); int len = len1 \u0026gt;= len2 ? len1 : len2; int plus = 0; StringBuilder res = new StringBuilder(); String s = num1, s2 = num2; if (len1 != len2) { s = len1 \u0026gt; len2 ? num2 : num1; s2 = len1 \u0026gt; len2 ? num1 : num2; StringBuilder temp = new StringBuilder(); // 将位数较短的数字字符串补零，使其和较长的字符串长度相同  for (int i = 0; i \u0026lt; Math.abs(len1 - len2); i++) { temp.append(\u0026#34;0\u0026#34;); } s = temp.append(s).toString(); } // 将两个数字字符串各位相加  for (int i = len - 1; i \u0026gt;= 0; i--) { char a = s.charAt(i); char b = s2.charAt(i); int tempRes = add(a, b, plus); int add = tempRes; if (tempRes \u0026gt; 9) { plus = tempRes / 10; add = tempRes - plus * 10; } else { plus = 0; } res.append(add); } // 如果最后 plus 不为 0，说明最后一位又有进位，将进的位直接拼接到结果后面  if (plus \u0026gt; 0) { res.append(plus); } // 前面得到的结果是反的，将其进一步反转后返回  return res.reverse().toString(); } 2.2 双指针法 #  2.2.1 问题分析 #   双指针法实质是利用两个指针从后往前依次遍历字符串，然后将每次遍历的结果按照竖式算法进行相加，具体过程如下：  初始时令 $i=num1.length() - 1$，$j=num2.length()-1$，进位值 $plus$ 初始为 0。 如果 $i!=0$ 或者 $j!=0$：  如果 $i\u0026gt;=0$，则 $a=num1.charAt(i)$，否则，$a=\u0026lsquo;0\u0026rsquo;$。 如果 $j\u0026gt;=0$，则 $b=num1.charAt(j)$，否则，$b=\u0026lsquo;0\u0026rsquo;$。 将 $a$、$b$、$plus$ 相加得 $tempRes$，同时令当前位相加结果 $add$ 等于 $tempRes$： 如果 $tempRes\u0026gt;9$，说明当前位有进位，则令 $plus=tempRes/10$，$add=tempRes-plus/10$。 否则令 $plus=0$，说明当前位没有进位。 将当前位相加得结果 $add$ 添加到最终结果 $res$ 中。     如果最后 $plus$ 大于 0，说明最后一位又有进位，将进位结果添加到最终结果 $res$ 中。 前面得到的结果 $res$ 是反的，将其反转一下返回即可。   2.2.2 参考代码 #  /** * 将两个数字字符（均为个位数）相加 * @param a 第一个数字字符 * @param b 第二个数字字符 * @param plus 进位数字 * @return 两个数字字符之和 */ public int add(char a, char b, int plus) { return (a - \u0026#39;0\u0026#39;) + (b - \u0026#39;0\u0026#39;) + plus; } /** * 415. 字符串相加（版本 2：双指针） * * @param num1 数字 1 * @param num2 数字 2 * @return 以字符串形式返回两个数字相加后的结果 */ public String addStringsV2(String num1, String num2) { int i = num1.length() - 1; int j = num2.length() - 1; int plus = 0; StringBuilder res = new StringBuilder(); // 从右向左依次对两个字符串进行遍历，然后将两个对应的值按照竖式算法进行相加  while (i \u0026gt;= 0 || j \u0026gt;= 0) { char a = i \u0026gt;= 0 ? num1.charAt(i) : \u0026#39;0\u0026#39;; char b = j \u0026gt;= 0 ? num2.charAt(j) : \u0026#39;0\u0026#39;; int tempRes = add(a, b, plus); int add = tempRes; if (tempRes \u0026gt; 9) { plus = tempRes / 10; add = tempRes - plus * 10; } else { plus = 0; } res.append(add); i--; j--; } // 如果最后 plus 不为 0，说明最后一位又有进位，将进的位直接拼接到结果后面  if (plus \u0026gt; 0) { res.append(plus); } // 前面得到的结果是反的，将其进一步反转后返回  return res.reverse().toString(); } 3 参考文献 #    415. 字符串相加。  字符串相加 （双指针，清晰图解）。  "},{"id":144,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.4-%E5%AD%97%E7%AC%A6%E4%B8%B2/2.4.8-%E7%BF%BB%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%87%8C%E7%9A%84%E5%8D%95%E8%AF%8D/","title":"2.4.8 翻转字符串里的单词","section":"2.4 字符串","content":"翻转字符串里的单词 #  1 题目 #  给你一个字符串 s ，逐个翻转字符串中的所有 单词 。\n单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。\n请你返回一个翻转 s 中单词顺序并用单个空格相连的字符串。\n说明：\n 输入字符串 s 可以在前面、后面或者单词间包含多余的空格。 翻转后单词间应当仅用一个空格分隔。 翻转后的字符串中不应包含额外的空格。  示例 1：\n输入：s = \u0026#34;the sky is blue\u0026#34; 输出：\u0026#34;blue is sky the\u0026#34; 示例 2：\n输入：s = \u0026#34; hello world \u0026#34; 输出：\u0026#34;world hello\u0026#34; 解释：输入字符串可以在前面或者后面包含多余的空格，但是翻转后的字符不能包括。 示例 3：\n输入：s = \u0026#34;a good example\u0026#34; 输出：\u0026#34;example good a\u0026#34; 解释：如果两个单词间有多余的空格，将翻转后单词间的空格减少到只含一个。 示例 4：\n输入：s = \u0026#34; Bob Loves Alice \u0026#34; 输出：\u0026#34;Alice Loves Bob\u0026#34; 示例 5：\n输入：s = \u0026#34;Alice does not even like bob\u0026#34; 输出：\u0026#34;bob like even not does Alice\u0026#34; 提示：\n 1 \u0026lt;= s.length \u0026lt;= 104 s 包含英文大小写字母、数字和空格 ' ' s 中 至少存在一个 单词  进阶：\n 请尝试使用 O(1) 额外空间复杂度的原地解法。  2 解题思路 #  2.1 解法一：栈 #  2.1.1 问题分析 #    该方法的基本思想是依次遍历字符串中的字符，然后截取每个单词，并将截取后的单词放到栈中。\n  字符串遍历结束后，再依次遍历栈中的元素，并用 StringBuffer 以空格为分隔符将这些单词并接起来，最后直接返回即可。\n   2.1.2 参考代码 #  /** * 151. 翻转字符串里的单词（版本 1：栈） * * @param s 字符串 * @return 翻转后的字符串 */ public String reverseWordsV1(String s) { // 截取每个单词的起始位置  int start = 0, end = 0; // 判断单词的位置是否开始计数  boolean isBegin = false; // 存储截取后的单词  Stack\u0026lt;String\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); // 用于将栈中的单词转化为字符串  StringBuffer sb = new StringBuffer(); // 逐个遍历字符串中的每个字符，并截取相应的单词  for (int i = 0; i \u0026lt; s.length(); i++) { if (s.charAt(i) != \u0026#39; \u0026#39;) { // 当前字符不是空字符  if (isBegin) { // 已经开始对当前单词的位置进行计数，将对应的下标加 1  end++; } else { // 开始对当前单词的位置进行计数，使用 start 记录当前单词的起始位置  start = i; // 使用 end 记录当前单词的结束位置，其中 end 会一直累加，直到遇到空字符  end = start + 1; // 标记开始对当前单词的位置进行计数  isBegin = true; } } else { // 当前字符是空字符  if (isBegin) { // 已经开始对当前单词的位置进行计数，现在遇到了空字符，因此需要停止对当前单词的位置进行计数，然后截取字符串中 [start, end) 之间的字符，这些字符便构成了当前单词  stack.push(s.substring(start, end)); // 标记停止对当前单词的位置进行计数  isBegin = false; } } } // 如果整个字符串的最后一个不是空字符，那么如果没有这个判断就会导致最后一个单词漏记  if (isBegin) { String temp = s.substring(start, end); stack.push(temp); } // 将栈中的单词组成一个翻转字符串  while (stack.size() \u0026gt; 0) { sb.append(stack.pop()); sb.append(\u0026#34; \u0026#34;); } // 将反转后的字符串返回（上面 sb 后面会多加一个空格，因此在返回字符串时需去掉 ）  return sb.toString().substring(0, sb.length() - 1); } 2.2 解法二：两次翻转 #  2.2.1 问题分析 #   该方法的基本思想是首先将字符串去除首尾的空格后进行翻转。 然后遍历反转后的字符串，删除多余的空格，并将每个单词进行翻转。 最后转换为字符串，直接返回即可。   2.2.2 参考代码 #  /** * 翻转一个单词 * @param sb 字符串 * @param start 单词起始位置 * @param end 单词结束位置加 1 * @return 翻转相应单词后的字符串 */ public StringBuilder reverseWord(StringBuilder sb, int start, int end) { for (int i = start; i \u0026lt; (start + end) / 2; i++) { char temp = sb.charAt(i); int symmetricalPosition = start + end - i - 1; sb.setCharAt(i, sb.charAt(symmetricalPosition)); sb.setCharAt(symmetricalPosition, temp); } return sb; } /** * 151. 翻转字符串里的单词（版本 2：两次翻转） * * @param s 字符串 * @return 翻转后的字符串 */ public String reverseWordsV2(String s) { if (s == null || s.trim() == \u0026#34;\u0026#34;) {return \u0026#34;\u0026#34;;} // 去除字符串两边的空格  StringBuilder sb = new StringBuilder(s.trim()); // 将字符串翻转  sb = sb.reverse(); int len = sb.length(); // 是否遇到空格  boolean meetSpace = false; // 删除空格的数量  int deleteSpaceCount = 0; // 要翻转的单词的起始位置  int start = 0, end = 0; // 遍历字符串，删除空格，并将所有的单词翻转  for (int i = 0; i \u0026lt; len; i++) { if (sb.charAt(i - deleteSpaceCount) == \u0026#39; \u0026#39;) { // 遇到了一个空格  if (!meetSpace) { // 在这个单词前面没有遇到空格，说明这是但此后面的第一个空格，直接将 [start, end) 之间的单词进行翻转即可（注：start 和 end 为更正后的位置，即减去了删除空格的长度）  end = i - deleteSpaceCount; start = start - deleteSpaceCount; sb = reverseWord(sb, start, end); meetSpace = true; } else { // 在这个单词前面已经遇到了空格，说明这个空格是空格后面的空格，直接删掉即可  sb = sb.deleteCharAt(i - deleteSpaceCount); deleteSpaceCount++; meetSpace = true; } } else { // 不是空格  if (meetSpace) { // 单词前面遇到了空格，说明这个是单词的第一个字符，直接重置 start 的位置，然后标记为未遇到空格  start = i; meetSpace = false; } } } if (!meetSpace) { // 最后一个单词后面没有遇到空格，直接将最后一个单词进行翻转即可  end = len - deleteSpaceCount; sb = reverseWord(sb, start - deleteSpaceCount, end); } // 返回结果  return sb.toString(); } 3 参考文献 #    151. 翻转字符串里的单词。  原地翻转字符串里的单词(空间复杂度为 O(1))。  "},{"id":145,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.5-%E9%93%BE%E8%A1%A8/2.5.1-%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8-II/","title":"2.5.1 环形链表 Ii","section":"2.5 链表","content":"环形链表 II #  1 题目 #  给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。\n为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意，pos 仅仅是用于标识环的情况，并不会作为参数传递到函数中。\n说明： 不允许修改给定的链表。\n进阶：\n 你是否可以使用 O(1) 空间解决此题？  示例 1：\n 输入：head = [3,2,0,-4], pos = 1 输出：返回索引为 1 的链表节点 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2：\n 输入：head = [1,2], pos = 0 输出：返回索引为 0 的链表节点 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3：\n 输入：head = [1], pos = -1 输出：返回 null 解释：链表中没有环。 提示：\n 链表中节点的数目范围在范围 [0, 104] 内 -105 \u0026lt;= Node.val \u0026lt;= 105 pos 的值为 -1 或者链表中的一个有效索引  2 解题思路 #  2.1 哈希表 #  2.1.1 问题分析 #  当链表中有环时，在对链表进行遍历时环入口的节点一定会被重复遍历到，因此我们在对链表进行遍历时将每个节点的地址存到哈希表中，然后每访问一个节点，都看一下哈希表中是否存在该节点对应的地址：\n 若存在的话在该链表有环，直接返回即可。 如果访问到一个节点为 null 时，则该链表没有环，直接返回 null 即可。  2.1.2 参考代码 #  /** * 142. 环形链表 II（版本 1：哈希表） * 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 * 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意，pos 仅仅是用于标识环的情况，并不会作为参数传递到函数中。 * * @param head 链表头指针 * @return 链表是否有环 */ public static ListNode detectCycleV1(ListNode head) { ListNode p = head; // 定义一个 Map  Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); // 遍历链表，同时将链表中的数据存入栈中  while (p != null) { if (map.containsKey(p.hashCode())) { return p; } map.put(p.hashCode(), 1); p = p.next; } return null; } 2.2 双指针 #  2.2.1 问题分析 #   我们使用两个指针，fast 与 slow，他们都位于链表的头部。 随后，slow 指针每次向后移动一个位置，fast 指针向后移动两个位置。 如果链表中存在环，则 fast 指针将再次与 slow 指针相遇。 如下图所示，设链表中环外部分的长度为 a，slow 指针进入环后又走了 b 的距离与 fast 指针相遇。 此时，fast 指针已经走完了环的 n 圈，因此他走过的总距离为 a+n(b+c)+b=a+(n+1)b+nc。 根据题意，任意时刻 fast 指针走过的距离都为 slow 指针的 2 倍，因此，我们有 a+(n+1)b+nc=2(a+b)，即 a=c+(n-1)(b+c)。 根据这个等量关系我们可以发现：从相遇点到入环点的距离加上 n-1 圈的环长，恰好等于从链表头部到入环点的距离。 因此，当发现 slow 与 fast 相遇时，我们再额外使用一个指针 q：  开始的时候，q 指向链表头部，随后，他和 slow 每次向后移动一个位置，最终，他们会在入环点相遇。     2.2.2 参考代码 #  /** * 142. 环形链表 II（版本 2：双指针） * 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 * 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意，pos 仅仅是用于标识环的情况，并不会作为参数传递到函数中。 * * @param head 链表头指针 * @return 链表是否有环 */ public static ListNode detectCycleV2(ListNode head) { ListNode slow = head,fast = head; // 两种情况：  // 1. 链表没有环，fast 先走到链表结尾  // 2. 链表有环，slow 最终赶上 fast  while (true) { if (fast == null || fast.next == null) {return null;} // fast 每次走两步  fast = fast.next.next; // slow 每次走一步  slow = slow.next; if (slow == fast) { // 链表有环，且 slow 和 fast 相遇  // 再定义一个指针 q，从 head 开始，然后 q 和 slow 每次都走一步，则 q 会和 slow 在 环的入口处相遇  ListNode q = head; while (true) { if (q == slow) {return slow;} q = q.next; slow = slow.next; } } } } 3 参考文献 #    142. 环形链表 II。  环形链表 II（双指针法，清晰图解）。  环形链表 II。 解题思路 2.2 来自于 @力扣官方题解。  "},{"id":146,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.5-%E9%93%BE%E8%A1%A8/2.5.2-%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%88%A4%E6%96%AD%E5%9B%9E%E6%96%87%E9%93%BE%E8%A1%A8/","title":"2.5.2 如何高效判断回文链表","section":"2.5 链表","content":"如何高效判断回文链表 #  1 题目 #  请判断一个链表是否为回文链表。\n示例 1:\n输入: 1-\u0026gt;2 输出: false 示例 2:\n输入: 1-\u0026gt;2-\u0026gt;2-\u0026gt;1 输出: true 进阶： 你能否用 O(n) 时间复杂度和 O(1) 空间复杂度解决此题？\n2 不同解法 #  2.1 创建一个新的单链表 #  我们可以把原始链表反转存入一条新的链表，然后比较这两个链表是否相同即可。\n/** * Definition for singly-linked list. */ public static class ListNode { int val; ListNode next; ListNode() {} ListNode(int val) { this.val = val; } ListNode(int val, ListNode next) { this.val = val; this.next = next; } } /** * 反转链表 * @param head 单链表的头指针 * @return 反转后的链表 */ private static ListNode reverse(ListNode head) { ListNode p = null; // 遍历并反转链表  while (head != null) { ListNode tmpNode = new ListNode(head.val); tmpNode.next = p; p = tmpNode; head = head.next; } return p; } /** * 234. 回文链表（版本 1：创建一个新的单链表） * 请判断一个链表是否为回文链表。 * @param head 单链表的头指针 * @return 当前链表是否为回文链表 */ public static boolean isPalindromeV1(ListNode head) { ListNode q = reverse(head); ListNode p; ListNode r; // 一次比较两个链表的值是否相等，如果相等的话，就是回文链表，否则就不是  p = head; r = q; while (p != null \u0026amp;\u0026amp; r != null) { // 如果两个链表有一个值不相等，则该链表不是回文链表，直接返回  if (p.val != r.val) {return false;} p = p.next; r = r.next; } // 两个链表的值都相等，该链表是回文链表，返回 true  return true; } 2.2 双指针法 #   先通过双指针技巧中的快慢指针来找到链表的中点。   如果 fast 指针没有指向 null，说明链表长度为奇数，slow 还要再向前进一步。   从 slow 后面开始反转链表，然后就可以开始比较回文串了。  /** * Definition for singly-linked list. */ public static class ListNode { int val; ListNode next; ListNode() {} ListNode(int val) { this.val = val; } ListNode(int val, ListNode next) { this.val = val; this.next = next; } } /** * 反转链表 * @param head 单链表的头指针 * @return 反转后的链表 */ private static ListNode reverse(ListNode head) { ListNode p = null; // 遍历并反转链表  while (head != null) { ListNode tmpNode = new ListNode(head.val); tmpNode.next = p; p = tmpNode; head = head.next; } return p; } /** * 234. 回文链表（版本 3：双指针法） * 请判断一个链表是否为回文链表。 * @param head 单链表的头指针 * @return 当前链表是否为回文链表 */ public static boolean isPalindromeV3(ListNode head) { ListNode slow = head, fast = null; // 寻找中间节点  while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } // 如果 fast 不为 null，则说明链表的长度为奇数  if (fast != null) {slow = slow.next;} // 判断两个链表公共的部分是否相同  fast = reverse(slow); slow = head; while (fast != null \u0026amp;\u0026amp; slow != null) { // 当前链表不是回文链表，返回 false  if (fast.val != slow.val) {return false;} fast = fast.next; slow = slow.next; } // 当前链表是回文链表，返回 true  return true; } 2.3 栈 #  我们知道栈是一种后进先出的数据结构，这里可以用栈把双指针法中中间节点后面的节点的值放在栈中保存，然后将链表中间节点前面的部分和栈里面的中间节点后面的值进行比较即可。\n/** * Definition for singly-linked list. */ public static class ListNode { int val; ListNode next; ListNode() {} ListNode(int val) { this.val = val; } ListNode(int val, ListNode next) { this.val = val; this.next = next; } } /** * 234. 回文链表（版本 4：栈） * 请判断一个链表是否为回文链表。 * @param head 单链表的头指针 * @return 当前链表是否为回文链表 */ public static boolean isPalindromeV4(ListNode head) { // 将单链表的数据存储在栈中  Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); ListNode p = head; int len = 0; while (p != null) { len++; stack.push(p.val); p = p.next; } // 遍历栈  // 计算右侧开始比对的节点的位置：  // 1. 节点个数为偶数：len / 2 + 1  // 2. 节点个数为奇数：len / 2 + 2  if (len \u0026gt; 1) {len = (len % 2 == 0 ? len / 2 + 1 : len / 2 + 2);} p = head; while (len-- \u0026gt; 0) { if (p.val != stack.pop()) {return false;} p = p.next; } // 当前链表是回文链表，返回 true  return true; } 2.4 后序遍历 #  链表具有递归结构，其实也可以有前序遍历和后序遍历，例如通过后序遍历来倒序打印链表：\n/* 倒序打印单链表中的元素值 */ public static void traverse(ListNode head) { if (head == null) return; traverse(head.next); // 后序遍历代码  System.out.println(head.val); } 因此，我们可以将上面的程序稍作修改，模仿双指针实现回文判断功能，这么做的核心逻辑就是把链表节点放入一个栈，然后再拿出来，这时候元素顺序就是反的，只不过我们利用的是递归函数的堆栈而已。\n/** * Definition for singly-linked list. */ public static class ListNode { int val; ListNode next; ListNode() {} ListNode(int val) { this.val = val; } ListNode(int val, ListNode next) { this.val = val; this.next = next; } } /** * 判断一个链表是否是回文链表 * @param head 单链表的表头 * @return 当前链表是否是回文链表 */ public static boolean check(ListNode head) { if (head == null) {return true;} boolean res = check(head.next) \u0026amp;\u0026amp; temp.val == head.val; temp = temp.next; return res; } /** * 234. 回文链表（版本 5：后序遍历） * 请判断一个链表是否为回文链表。 * @param head 单链表的头指针 * @return 当前链表是否为回文链表 */ public static boolean isPalindromeV5(ListNode head) { temp = head; return check(head); } "},{"id":147,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.5-%E9%93%BE%E8%A1%A8/2.5.3-K-%E4%B8%AA%E4%B8%80%E7%BB%84%E7%BF%BB%E8%BD%AC%E9%93%BE%E8%A1%A8/","title":"2.5.3 K 个一组翻转链表","section":"2.5 链表","content":"K 个一组翻转链表 #  1 题目 #  给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。\n如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n进阶：\n 你可以设计一个只使用常数额外空间的算法来解决此问题吗？ 你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。  示例 1：\n 输入：head = [1,2,3,4,5], k = 2 输出：[2,1,4,3,5] 示例 2：\n 输入：head = [1,2,3,4,5], k = 3 输出：[3,2,1,4,5] 示例 3：\n输入：head = [1,2,3,4,5], k = 1 输出：[1,2,3,4,5] 示例 4：\n输入：head = [1], k = 1 输出：[1] 提示：\n 列表中节点的数量在范围 sz 内 1 \u0026lt;= sz \u0026lt;= 5000 0 \u0026lt;= Node.val \u0026lt;= 1000 1 \u0026lt;= k \u0026lt;= sz  2 解题思路 #   首先翻转链表中前 $K$ 个元素，翻转后的首节点为 $target$，末尾节点为 $head$，下一组元素的首节点为 $p$ 然后递归翻转下一组 $K$ 个元素，判断这一组元素的个数是否小于 $K$：  如果小于 $K$，将翻转后的元素再次翻转（相当于最后一组个数不为 $K$ 的元素按照原来的顺序进行排列）。 如果等于 $K$，则继续递归翻转下一组 $K$ 个元素，并将反转后的结果和上一组 $K$ 个元素反转后的结果进行拼接。   等到链表中所有元素递归结束后，将翻转后的链表（$target$）返回。     3 参考代码 #  package com.grayson.top.daily; import com.grayson.top.domain.ListNode; /** * @author peng.wei * @version 1.0 * @date 2021/5/18 21:03 * @Description K 个一组翻转链表 */ public class L25 { /** * 25. K 个一组翻转链表 * @param head 头结点 * @param k 指定节点个数 * @return 翻转后的链表 */ public ListNode reverseKGroup(ListNode head, int k) { return reverseK(head, k, k+1); } /** * 翻转链表 * @param head 头结点 * @return 反转后的链表 */ public ListNode reverse(ListNode head) { ListNode p = head, q = head, target = null; while (p != null) { q = p; p = p.next; q.next = target; target = q; } return target; } /** * 翻转链表的前 k 个元素 * @param head 头结点 * @param k 指定节点个数 * @param left 剩余节点个数 * @return 反转后的链表 */ public ListNode reverseK(ListNode head, int k, int left) { if (head == null || left \u0026lt;= k) {return head;} int num = 1; ListNode p = head, q = head, target = null; while (num \u0026lt;= k \u0026amp;\u0026amp; p != null) { q = p; p = p.next; q.next = target; target = q; num++; } if (num \u0026lt;= k) { return reverse(target); } else { head.next = reverseK(p, k, num); } return target; } public static void main(String[] args) { L25 l25 = new L25(); int k = 3; ListNode head = new ListNode(1); head.next = new ListNode(2); head.next.next = new ListNode(3); head.next.next.next = new ListNode(4); head.next.next.next.next = new ListNode(5); head.next.next.next.next.next = new ListNode(6); head.next.next.next.next.next.next = new ListNode(7); ListNode res = l25.reverseKGroup(head, k); // int k = 3; // ListNode head = new ListNode(1); // head.next = new ListNode(2); // head.next.next = new ListNode(3); // head.next.next.next = new ListNode(4); // head.next.next.next.next = new ListNode(5); // ListNode res = l25.reverseKGroup(head, k);  System.out.println(res); } } 4 参考文献 #    25. K 个一组翻转链表。  "},{"id":148,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.5-%E9%93%BE%E8%A1%A8/2.5.4-%E7%9B%B8%E4%BA%A4%E9%93%BE%E8%A1%A8/","title":"2.5.4 相交链表","section":"2.5 链表","content":"相交链表 #  1 题目 #  编写一个程序，找到两个单链表相交的起始节点。\n如下面的两个链表：\n 在节点 c1 开始相交。\n示例 1：\n 输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3 输出：Reference of the node with value = 8 输入解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2：\n 输入：intersectVal = 2, listA = [0,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1 输出：Reference of the node with value = 2 输入解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [0,9,1,2,4]，链表 B 为 [3,2,4]。在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3：\n 输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2 输出：null 输入解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。 解释：这两个链表不相交，因此返回 null。 注意：\n 如果两个链表没有交点，返回 null. 在返回结果后，两个链表仍须保持原有的结构。 可假定整个链表结构中没有循环。 程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。  2 解题思路 #  2.1 HashMap #  2.1.1 问题解析 #    将一个链表中的所有节点信息存入到HashMap中：\n key：当前节点的地址 Value：当前节点的父节点（头结点的父节点为null）    然后依次遍历另一个链表：\n 如果HashMap中包含当前节点的key，同时其对应的value不等于当前节点的父节点，则当前节点即为两个链表相交的起始节点。 如果遍历到最后不存在的话，则直接返回null。     2.1.2 参考代码 #  /** * 160. 相交链表（版本 1：HashMap） * @param headA 第一个链表的头结点 * @param headB 第二个链表的头结点 * @return */ public ListNode getIntersectionNodeV1(ListNode headA, ListNode headB) { if (headA == headB) {return headA;} Map\u0026lt;ListNode, ListNode\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); ListNode p = headA, prev = null, res = null; while (p != null) { map.put(p, prev); prev = p; p = p.next; } p = headB; prev = null; while (p != null) { if (map.containsKey(p) \u0026amp;\u0026amp; map.get(p) != prev) { res = p; break; } prev = p; p = p.next; } return res; } 2.2 双指针 #  2.2.1 问题解析 #   开始时令指针head1=headA，head2=headB。 然后两个指针分别从两个链表的起点开始遍历：  当其中一个指针到达链表的尾部时，则令其等于另一个链表的头部，然后继续开始遍历，例如head1遍历到headA的尾部时，令head1=headB。   在两次遍历的过程中，因为两个指针最终走的路程一样，所以：  如果两个链表相交时，两个指针一定会在两个链表相交的节点相遇。 如果两个链表不相交时，最后两个指针一定会在链表的尾部相遇，即两个指针最后的值都为null。     将二者的路径拼接到一起进行展示：\n 2.2.2 参考代码 #  /** * 160. 相交链表（版本 2：双指针） * @param headA 第一个链表的头结点 * @param headB 第二个链表的头结点 * @return */ public ListNode getIntersectionNodeV2(ListNode headA, ListNode headB) { // 如果 headA 或 headB 为 null，则两个链表肯定不相交，直接返回 null  if (headA == null || headB == null) {return null;} ListNode head1 = headA, head2 = headB; // 两次遍历：  // 如果两个链表有交点，则最后 head1 一定为二者的交点  // 如果两个链表没有交点，则一定是在最后到尾节点的时候二者相遇，此时 head1 为 null  while (head1 != head2) { if (head1 == null) { // 如果 head1 走到了尾节点，则令其等于 headB 的头结点  head1 = headB; } else { // 否则的话，继续遍历即可  head1 = head1.next; } if (head2 == null) { // 如果 head2 走到了尾节点，则令其等于 headA 的头结点  head2 = headA; } else { // 否则的话，继续遍历即可  head2 = head2.next; } } return head1; } 3 参考文献 #    160. 相交链表。  图解相交链表。  "},{"id":149,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.5-%E9%93%BE%E8%A1%A8/2.5.5-%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E8%8A%82%E7%82%B9/","title":"2.5.5 链表中倒数第k个节点","section":"2.5 链表","content":"链表中倒数第k个节点 #  1 题目 #  输入一个链表，输出该链表中倒数第 k 个节点。为了符合大多数人的习惯，本题从 1 开始计数，即链表的尾节点是倒数第 1 个节点。\n例如，一个链表有 6 个节点，从头节点开始，它们的值依次是 1、2、3、4、5、6。这个链表的倒数第 3 个节点是值为 4 的节点。\n示例：\n给定一个链表: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5, 和 k = 2. 返回链表 4-\u0026gt;5.  和该题目类似的题目还有：\n  19. 删除链表的倒数第 N 个结点。   2 解题思路 #  2.1 两次遍历 #  2.1.1 问题分析 #   先遍历一下链表，统计一下总的节点个数 $total$。 然后再遍历一下链表，将指针 $p$ 向前移动 $total - k$ 步，此时 $p$ 指向的节点即为倒数第 $k$ 个节点。  2.1.2 参考代码 #  /** * 剑指 Offer 22. 链表中倒数第 k 个节点（版本 1：两次遍历） * @param head 头结点 * @param k 倒数节点个数 * @return 链表中倒数第 k 个节点 */ public ListNode getKthFromEndV1(ListNode head, int k) { ListNode p = head; int total = 1, curr = 1; // 先遍历一下链表，统计一下链表总的节点个数  while (p != null) { p = p.next; total++; } // 然后再遍历一下链表，将指针向前移动 total - k 步，此时指针指向的节点即为倒数第 k 个节点  p = head; while (curr \u0026lt; total - k) { p = p.next; curr++; } return p; } 2.2 双指针 #  2.2.1 问题分析 #    定义两个指针，分别为慢指针 $slow$ 和快指针 $fast$。\n  让快指针先走 $k$ 步。\n  然后快慢指针同时移动，以后快慢指针的距离都为 $k$ 步，直到快指针为空，此时慢指针 $slow$ 即为倒数第 $k$ 个节点。\n   2.2.2 参考代码 #  /** * 剑指 Offer 22. 链表中倒数第 k 个节点（版本 2：双指针） * @param head 头结点 * @param k 倒数节点个数 * @return 链表中倒数第 k 个节点 */ public ListNode getKthFromEndV2(ListNode head, int k) { // 快慢指针  ListNode slow = head, fast = head; // 让快指针先走 k 步  while (k \u0026gt; 0) { fast = fast.next; k--; } // 然后快慢指针同时移动，以后快慢指针的距离都为 k 步，直到快指针为空，此时慢指针即为倒数第 k 个节点  while (fast != null) { slow = slow.next; fast = fast.next; } return slow; } 3 参考文献 #    剑指 Offer 22. 链表中倒数第 k 个节点。  19. 删除链表的倒数第 N 个结点。  面试题 22. 链表中倒数第 k 个节点（双指针，清晰图解）。  "},{"id":150,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.5-%E9%93%BE%E8%A1%A8/2.5.6-%E5%90%88%E5%B9%B6K%E4%B8%AA%E5%8D%87%E5%BA%8F%E9%93%BE%E8%A1%A8/","title":"2.5.6 合并 K个升序链表","section":"2.5 链表","content":"合并K个升序链表 #  1 题目 #  给你一个链表数组，每个链表都已经按升序排列。\n请你将所有链表合并到一个升序链表中，返回合并后的链表。\n示例 1：\n输入：lists = [[1,4,5],[1,3,4],[2,6]] 输出：[1,1,2,3,4,4,5,6] 解释：链表数组如下： [ 1-\u0026gt;4-\u0026gt;5, 1-\u0026gt;3-\u0026gt;4, 2-\u0026gt;6 ] 将它们合并到一个有序链表中得到。 1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4-\u0026gt;5-\u0026gt;6 示例 2：\n输入：lists = [] 输出：[] 示例 3：\n输入：lists = [[]] 输出：[] 提示：\n k == lists.length 0 \u0026lt;= k \u0026lt;= 10^4 0 \u0026lt;= lists[i].length \u0026lt;= 500 -10^4 \u0026lt;= lists[i][j] \u0026lt;= 10^4 lists[i] 按 升序 排列 lists[i].length 的总和不超过 10^4  2 解题思路 #  2.1 两两合并 #  2.1.1 问题分析 #    这道题目的基本思路是将链表数组中的链表进行两两合并，这样便可以得到最终合并后的链表。\n![](../../../media/202107/23-合并 K 个升序链表_1625748256.gif)\n  2.1.2 参考代码 #  /** * 将两个链表合并 * @param node1 第一个链表 * @param node2 第二个链表 * @return 合并后的链表 */ public ListNode merge(ListNode node1, ListNode node2) { ListNode p = node1, q = node2, res = new ListNode(), tempRes = res; while (p != null \u0026amp;\u0026amp; q != null) { if (p.val \u0026lt;= q.val) { // 如果 p 的值小于等于 q 的值，则将结果指针指向 p，并更新结果指针  tempRes.next = p; p = p.next; } else { // 如果 q 的值小于 p 的值，则将结果指针指向 q，并更新结果指针  tempRes.next = q; q = q.next; } tempRes = tempRes.next; } if (p != null) { // 如果最后 p 不为空，则将结果指针指向 p  tempRes.next = p; } else if (q != null) { // 如果最后 q 不为空，则将结果指针指向 q  tempRes.next = q; } return res.next; } /** * 23. 合并 K 个升序链表 * @param lists 链表数组 * @return 合并后的链表 */ public ListNode mergeKLists(ListNode[] lists) { if (lists.length == 0) {return null;} // 初始时 node1 指向 lists 中第一个元素  ListNode node1 = lists[0]; // 依次遍历 lists 中从第二个元素开始的每一个元素，然后分别将其与 node1 进行递归合并成一个更大的链表  for (int i = 1; i \u0026lt; lists.length; i++) { node1 = merge(node1, lists[i]); } // 最后 node1 便是 lists 中所有链表合并后的链表，直接返回即可  return node1; } 3 参考文献 #    23. 合并 K 个升序链表。  合并 K 个排序链表。  "},{"id":151,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.6-%E4%BA%8C%E5%8F%89%E6%A0%91/2.6.1-%E5%9C%A8%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%88%86%E9%85%8D%E7%A1%AC%E5%B8%81/","title":"2.6.1 在二叉树中分配硬币","section":"2.6 二叉树","content":"在二叉树中分配硬币 #  1 题目 #  给定一个有 N 个结点的二叉树的根结点 root，树中的每个结点上都对应有 node.val 枚硬币，并且总共有 N 枚硬币。\n在一次移动中，我们可以选择两个相邻的结点，然后将一枚硬币从其中一个结点移动到另一个结点。(移动可以是从父结点到子结点，或者从子结点移动到父结点。)。\n返回使每个结点上只有一枚硬币所需的移动次数。\n示例 1：\n 输入：[3,0,0] 输出：2 解释：从树的根结点开始，我们将一枚硬币移到它的左子结点上，一枚硬币移到它的右子结点上。 示例 2：\n 输入：[0,3,0] 输出：3 解释：从根结点的左子结点开始，我们将两枚硬币移到根结点上 [移动两次]。然后，我们把一枚硬币从根结点移到右子结点上。 示例 3：\n 输入：[1,0,2] 输出：2 示例 4：\n 输入：[1,0,0,null,3] 输出：4 ** 提示：**\n 1\u0026lt;= N \u0026lt;= 100 0 \u0026lt;= node.val \u0026lt;= N  2 解题思路 #  这里我们引入一个 过载量，用 $O$ 表示，他表示当前节点需要移出的金币数。\n例如一个节点 $T_1$ 有 2 枚金币，则他需要移出的金币数为 1，即该节点的过载量 $O(T_1)=1$。\n假如一个树 $T$ 有 $k$ 个节点，其中 $m$ 个非叶节点，分别为 $T_1,T_2,T_3,\u0026hellip;,T_m$，$k-m$ 个叶子节点，分别为 $T_{m+1},T_{m+2},T_{m+3},\u0026hellip;,T_k$，则：\n 叶子节点 $T_i$ 的过载量为：  $$ O(T_i)=N(T_i)-1 $$\n其中 $N(T_i)$ 为该叶子节点含有的金币个数。\n即一个叶子节点的过载量等于其含有的金币数减 1，该值可能为正，也可能为负，如果为正的话表示移出的金币个数，如果为负的话表示移入的金币个数。\n非叶节点 $T_j$ 的过载量为：  $$ O(T_j)=N(T_j)+O(T_{lj})+O(T_{rj})-1 $$\n其中 $O(T_{lj})$ 和 $O(T_{rj})$ 分别为该非叶节点 $O(T_j)$ 的 左孩子节点 和 右孩子节点 对应的过载量。\n即一个非叶节点的过载量等于=自身的金币个数 + 左孩子节点的过载量 + 右孩子节点的过载量-1。\n则该树对应的过载量为：  $$ O(T)=\\sum_{i=1}^kabs(O(T_i)) $$\n即一棵树的过载量等于所有节点的过载量的绝对值之和。\n具体实例如下：\n  该树 $T$ 共有 7 个节点，其中 4 个叶子节点，分别为 d、e、f、g，有 3 个非叶节点，分别为 a、b、c。 计算叶子节点的过载量：  $$ O(d)=N(d)-1=1-1=0 $$\n$$ O(e)=O(e)-1=0-1=-1 $$\n$$ O(f)=O(f)-1=1-1=0 $$\n$$ o(g)=O(g)=0-1=-1 $$\n计算非叶节点的过载量：  $$ O(b)=N(b)+O(d)+O(e)-1=3+0-1-1=1 $$\n$$ O(c)=N(c)+O(f)+O(g)-1=0+0-1-1=-2 $$\n$$ O(a)=N(a)+O(b)+O(c)-1=2+1-2-1=0 $$\n计算整棵树 $T$ 的过载量：  $$ O(T)=abs(O(a))+abs(O(b))+abs(O(c))+abs(O(d))+abs(O(e))+abs(O(f))+abs(O(g))=0+1+2+0+1+0+1=5 $$\n因此该树使每个结点上只有一枚硬币所需的移动次数为 5。\n3 参考代码 #  package com.grayson.top; import apple.laf.JRSUIUtils; import com.grayson.top.domain.TreeNode; /** * @author peng.wei * @version 1.0 * @date 2021/4/1 21:54 * @Description 在二叉树中分配硬币 */ public class L979 { int ans; /** * 采用深度优先搜索获取当前二叉树的过载量 * @param node 二叉树头结点 * @return 二叉树的过载量 */ public int dfs(TreeNode node) { if (node == null) {return 0;} int left = dfs(node.left); int right = dfs(node.right); ans += Math.abs(left) + Math.abs(right); return (node.val - 1) + left + right; } /** * 979. 在二叉树中分配硬币 * 给定一个有 N 个结点的二叉树的根结点 root，树中的每个结点上都对应有 node.val 枚硬币，并且总共有 N 枚硬币。 * 在一次移动中，我们可以选择两个相邻的结点，然后将一枚硬币从其中一个结点移动到另一个结点。(移动可以是从父结点到子结点，或者从子结点移动到父结点。)。 * 返回使每个结点上只有一枚硬币所需的移动次数。 * @param root 二叉树根节点 * @return 使二叉树每个结点上只有一枚硬币所需的移动次数 */ public int distributeCoins(TreeNode root) { ans = 0; dfs(root); return ans; } } 4 参考文献 #    979. 在二叉树中分配硬币。  在二叉树中分配硬币。  "},{"id":152,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.6-%E4%BA%8C%E5%8F%89%E6%A0%91/2.6.2-%E5%B0%86%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E8%BD%AC%E6%8D%A2%E4%B8%BA%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/","title":"2.6.2 将有序数组转换为二叉搜索树","section":"2.6 二叉树","content":"将有序数组转换为二叉搜索树 #  1 题目 #  给你一个整数数组 nums ，其中元素已经按 升序 排列，请你将其转换为一棵 高度平衡 二叉搜索树。\n高度平衡 二叉树是一棵满足「每个节点的左右两个子树的高度差的绝对值不超过 1 」的二叉树。\n示例 1：\n 输入：nums = [-10,-3,0,5,9] 输出：[0,-3,9,-10,null,5] 解释：[0,-10,5,null,-3,null,9] 也将被视为正确答案：  示例 2：\n 输入：nums = [1,3] 输出：[3,1] 解释：[1,3] 和 [3,1] 都是高度平衡二叉搜索树。 2 解题思路 #  2.1 问题分析 #  二叉搜索树的中序遍历是升序序列，题目中的数组也是按照生序排序的有序数组，因此可以判断数组时二叉搜索树的中序遍历序列。\n我们可以选择中间数字作为二叉搜索树的根节点，这样分给左右子树的数字个数相同或相差 1，这样可以使得树保持平衡，因此问题的实质可以转化为将有序数组按照中序遍历的方式构造为一棵二叉树即可。\n2.2 参考代码 #  package com.grayson.top; import com.grayson.top.domain.TreeNode; /** * @author peng.wei * @version 1.0 * @date 2021/4/9 19:32 * @Description 将有序数组转换为二叉搜索树 */ public class L108 { /** * 创建二叉搜索树 * * @param nums 有序数组 * @param left 左节点下标 * @param right 右节点下标 * @return 创建的二叉树的根节点 */ public TreeNode createBST(int[] nums, int left, int right) { // base case  if (left \u0026gt; right) { return null; } int mid = left + (right - left) / 2; TreeNode node = new TreeNode(); node.left = createBST(nums, left, mid - 1); node.right = createBST(nums, mid + 1, right); node.val = nums[mid]; return node; } /** * 108. 将有序数组转换为二叉搜索树 * 给你一个整数数组 nums ，其中元素已经按 升序 排列，请你将其转换为一棵 高度平衡 二叉搜索树。 * 高度平衡 二叉树是一棵满足「每个节点的左右两个子树的高度差的绝对值不超过 1 」的二叉树。 * * @param nums 有序数组 * @return 二叉搜索树 */ public TreeNode sortedArrayToBST(int[] nums) { int left = 0, right = nums.length - 1; return createBST(nums, left, right); } public static void main(String[] args) { int[] nums = {-10, -3, 0, 5, 9}; L108 l108 = new L108(); TreeNode node = l108.sortedArrayToBST(nums); System.out.println(node); } } 3 参考文献 #    108. 将有序数组转换为二叉搜索树。  将有序数组转换为二叉搜索树『官方题解』。  "},{"id":153,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.6-%E4%BA%8C%E5%8F%89%E6%A0%91/2.6.4-%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"2.6.4 重建二叉树","section":"2.6 二叉树","content":"重建二叉树 #  1 题目 #  输入某二叉树的前序遍历和中序遍历的结果，请重建该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。\n例如，给出\n前序遍历 preorder = [3,9,20,15,7] 中序遍历 inorder = [9,3,15,20,7] 返回如下的二叉树：\n 限制：\n0 \u0026lt;= 节点个数 \u0026lt;= 5000 2 解题思路 #  2.1 递归法 #  2.1.1 问题分析 #  前序遍历性质：节点按照 [根子树 | 左子树 | 右子树] 排序。\n中序遍历性质：节点按照 [左子树 | 根节点 | 右子树] 排序。\n根据以上性质，可得出以下结论：\n 前序遍历的首元素为树的根节点 node 的值。 在中序遍历中搜索根节点 node 的索引，可将中序遍历划分为 [左子树 | 根节点 | 右子树]。 根据中序遍历中的左/右子树的节点数量，可将前序遍历划分为 [根节点 | 左子树 | 右子树]。  通过以上三步，可确定三个节点：树的根节点、左子树根节点、右子树根节点。对于树的左、右子树，仍可使用以上步骤划分子树的左右子树。\n以上子树的递归性质是分治算法的体现，考虑通过递归对所有子树进行划分。\n2.1.1.1 分治算法解析 #   递推参数： 根节点在前序遍历的索引 root、子树在中序遍历的左边界left、子树在中序遍历的右边界right。 终止条件： 当 left \u0026gt; right 时，代表已经越过叶节点，此时返回 null。 递推工作：  建立根节点node：节点值为 preOrder[root]。 划分左右子树： 查找根节点在中序遍历 inOrder 中的索引 i。 构建左右子树： 开启左右子树递归。       根节点索引 中序遍历左边界 中序遍历右边界     左子树 root + 1 left   右子树 i - left + root + 1 i + 1      为了提升效率。本文使用哈希表 dic 存储中序遍历的值与索引的映射，查找时间复杂度为 $O(1)$。 i - left + root + 1 表示  根节点索引 + 左子树长度 + 1   返回值： 回溯返回 node，作为上一层递归中根节点的左/右子节点。   注：本方法只适用于无重复节点值的二叉树。\n 2.1.2 参考代码 #  package com.grayson.top.codinginterviews; import com.grayson.top.domain.TreeNode; import java.util.HashMap; /** * @author peng.wei * @version 1.0 * @date 2021/4/28 20:13 * @Description 重建二叉树 */ public class CI7 { // 二叉树前序遍历数组  int[] preOrder; // 哈希表，其中 key 为二叉树节点的值，value 为二叉树节点的值在后序遍历中的索引  HashMap\u0026lt;Integer, Integer\u0026gt; inValueAndIndexMap = new HashMap\u0026lt;\u0026gt;(); /** * 剑指 Offer 07. 重建二叉树 * * @param preorder 前序遍历数组 * @param inorder 中序遍历数组 * @return 二叉树 */ public TreeNode buildTree(int[] preorder, int[] inorder) { this.preOrder = preorder; // 构建哈希表，其中 key 为二叉树节点的值，value 为二叉树节点的值在后序遍历中的索引  int m = inorder.length; for (int i = 0; i \u0026lt; m; i++) { inValueAndIndexMap.put(inorder[i], i); } // 递归构建二叉树  return recur(0, 0, m - 1); } /** * 递归构建二叉树 * * @param root 根节点在前序遍历中的索引 * @param left 子树在中序遍历的左边界 * @param right 子树在中序遍历的右边界 * @return 二叉树 */ private TreeNode recur(int root, int left, int right) { // 已经越过叶节点，返回 null  if (left \u0026gt; right) { return null; } TreeNode node = new TreeNode(preOrder[root]); node.left = recur(root + 1, left, inValueAndIndexMap.get(preOrder[root]) - 1); node.right = recur(root + inValueAndIndexMap.get(preOrder[root]) - left + 1, inValueAndIndexMap.get(preOrder[root]) + 1, right); return node; } public static void main(String[] args) { CI7 ci7 = new CI7(); int[] preOrder = {3, 9, 20, 15, 7}; int[] inOrder = {9, 3, 15, 20, 7}; TreeNode res = ci7.buildTree(preOrder, inOrder); System.out.println(res); } } 3 参考文献 #    剑指 Offer 07. 重建二叉树。  面试题 07. 重建二叉树（递归法，清晰图解）。  "},{"id":154,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.6-%E4%BA%8C%E5%8F%89%E6%A0%91/2.6.5-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E7%9A%84%E6%9C%80%E5%A4%A7%E8%B7%AF%E5%BE%84%E5%92%8C/","title":"2.6.5 二叉树中的最大路径和","section":"2.6 二叉树","content":"二叉树中的最大路径和 #  1 题目 #  路径 被定义为一条从树中任意节点出发，沿父节点-子节点连接，达到任意节点的序列。同一个节点在一条路径序列中 至多出现一次 。该路径 至少包含一个 节点，且不一定经过根节点。\n路径和 是路径中各节点值的总和。\n给你一个二叉树的根节点 root ，返回其 最大路径和 。\n示例 1：\n输入：root = [1,2,3] 输出：6 解释：最优路径是 2 -\u0026gt; 1 -\u0026gt; 3 ，路径和为 2 + 1 + 3 = 6 示例 2：\n输入：root = [-10,9,20,null,null,15,7] 输出：42 解释：最优路径是 15 -\u0026gt; 20 -\u0026gt; 7 ，路径和为 15 + 20 + 7 = 42 提示：\n 树中节点数目范围是 [1, 3 * 104] -1000 \u0026lt;= Node.val \u0026lt;= 1000  2 解题思路 #  2.1 递归 #  2.1.1 问题分析 #   遇到二叉树的问题时，首先想到的是能不能用递归的方法来解决，递归时能不能用二叉树的三种遍历方法（前序遍历、中序遍历、后序遍历）。 解决该题目的核心思想是通过递归计算每个节点的最大贡献值，如果一个节点的最大贡献值大于 0，则把他放入到最大路径的节点中，其中每个节点的最大贡献值的计算公式如下： $$ 每个节点的贡献值 = max(左子树的最大贡献值，右子树的最大贡献值) + 当前节点的值 $$ 将最大路径和定义为全局变量，然后根据最大贡献值来计算当前节点的最大路径和，计算公式如下： $$ 当前节点的最大路径和 = 左子树的最大贡献值 + 右子树的最大贡献值 + 当前节点的值 $$ 如果当前节点的最大路径和大于已有的最大路径和，则将当前最大路径和更新为当前节点的最大路径和。   2.1.2 参考代码 #  // 二叉树中的最大路径和 int maxPathSumVal = Integer.MIN_VALUE; /** * 124. 二叉树中的最大路径和 * @param root 根节点 * @return 二叉树中的最大路径和 */ public int maxPathSum(TreeNode root) { maxGain(root); return maxPathSumVal; } /** * 当前节点的最大贡献值 * @param node 当前节点 * @return 当前节点的最大贡献值 */ public int maxGain(TreeNode node) { // base case  if (node == null) {return 0;} // 计算左右子树的最大贡献值  // 只有最大贡献值大于 0 时，才会选择对应子节点  int leftGain = Math.max(maxGain(node.left), 0); int rightGain = Math.max(maxGain(node.right), 0); // 当前节点的路径和，当其比已有最大路径和大时更新最大路径和为当前节点的路径和  // 当前节点的路径和 = 当前节点的值 + 左子树的最大贡献值 + 右子树的最大贡献值  int pathSum = node.val + leftGain + rightGain; maxPathSumVal = Math.max(maxPathSumVal, pathSum); // 返回当前节点的最大贡献值  // 当前节点的最大贡献值 = 当前节点值 + 左右子树中最大的最大贡献值  return node.val + Math.max(leftGain, rightGain); } 3 参考文献 #    124. 二叉树中的最大路径和。  二叉树中的最大路径和。  "},{"id":155,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.7-%E9%98%9F%E5%88%97/2.7.1-%E7%94%A8%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/","title":"2.7.1 用栈实现队列","section":"2.7 队列","content":"用栈实现队列 #  1 题目 #  请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty）：\n实现 MyQueue 类：\n void push(int x) 将元素 x 推到队列的末尾 int pop() 从队列的开头移除并返回元素 int peek() 返回队列开头的元素 boolean empty() 如果队列为空，返回 true ；否则，返回 false  说明：\n 你只能使用标准的栈操作 —— 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。  进阶：\n 你能否实现每个操作均摊时间复杂度为 O(1) 的队列？换句话说，执行 n 个操作的总时间复杂度为 O(n) ，即使其中一个操作可能花费较长时间。  示例：\n输入： [\u0026#34;MyQueue\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;peek\u0026#34;, \u0026#34;pop\u0026#34;, \u0026#34;empty\u0026#34;] [[], [1], [2], [], [], []] 输出： [null, null, null, 1, 1, false] 解释： MyQueue myQueue = new MyQueue(); myQueue.push(1); // queue is: [1] myQueue.push(2); // queue is: [1, 2] (leftmost is front of the queue) myQueue.peek(); // return 1 myQueue.pop(); // return 1, queue is [2] myQueue.empty(); // return false ** 提示：**\n 1 \u0026lt;= x \u0026lt;= 9 最多调用 100 次 push、pop、peek 和 empty 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作）  2 解题思路 #  2.1 问题解析 #  创建两个栈，分别为 $stack1$、$stack2$，二者的主要作用如下：\n $stack1$：作为容器，$push$ 进来的元素开始的时候都保存在 $stack1$ 中。 $stack2$：作为窗口，$pop$ 和 $peek$ 操作都是从 $stack2$ 取元素，如果 $stack2$ 为空，则将 $stack1$ 中的元素全部都 $push$ 到 $stack2$ 中，然后再从 $stack2$ 中 $pop$ 或 $peek$ 元素即可。   2.2 参考代码 #  class MyQueue { // stack1 作为容器，push 进来的元素开始的时候都保存在 stack1 中  public Stack\u0026lt;Integer\u0026gt; stack1; // stack2 作为窗口，pop 和 peek 操作都是从 stack1 取元素，如果 stack2 为空，则将 stack1 中的元素全部都 push 到 stack2 中，然后再从 stack2 中 pop 或 peek 元素即可  public Stack\u0026lt;Integer\u0026gt; stack2; /** * Initialize your data structure here. */ public MyQueue() { stack1 = new Stack\u0026lt;\u0026gt;(); stack2 = new Stack\u0026lt;\u0026gt;(); } /** * Push element x to the back of queue. */ public void push(int x) { stack1.push(x); } /** * Removes the element from in front of queue and returns that element. */ public int pop() { if (stack2.size() == 0) { // 如果 stack2 为空，则将 stack1 中的元素全部都 push 到 stack2 中  while (stack1.size() \u0026gt; 0) { stack2.push(stack1.pop()); } } // 从 stack2 中 pop 元素  return stack2.pop(); } /** * Get the front element. */ public int peek() { if (stack2.size() == 0) { // 如果 stack2 为空，则将 stack1 中的元素全部都 push 到 stack2 中  while (stack1.size() \u0026gt; 0) { stack2.push(stack1.pop()); } } // 从 stack2 中 peek 元素  return stack2.peek(); } /** * Returns whether the queue is empty. */ public boolean empty() { return stack1.empty() \u0026amp;\u0026amp; stack2.empty(); } } 3 参考文献 #    232. 用栈实现队列。  "},{"id":156,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.8-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/2.8.1-%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E8%A7%A3%E5%86%B3%E6%8E%A5%E9%9B%A8%E6%B0%B4%E9%97%AE%E9%A2%98/","title":"2.8.1 如何高效解决接雨水问题","section":"2.8 动态规划","content":"如何高效解决接雨水问题 #  2.3.1 题目 #  给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n示例 1：\n 输入：height = [0,1,0,2,1,0,1,3,2,1,2,1] 输出：6 解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例 2：\n输入：height = [4,2,0,3,2,5] 输出：9 2.3.2 核心思路 #  对于这种问题，我们不要想整体，而应该去想局部。这么一想，可以发现这道题的思路其实很简单。具体来说，仅仅对于位置 i，能装下能装 2 格水。\n 这是因为位置 i 能达到的水柱高度和其左边的最高柱子、右边的最高柱子有关，假设这两个柱子的高度分别为 leftMax 和 rightMax，则位置 i 的水柱高度为：\nwater[i] = min(leftMax, rightMax) - height[i]   2.3.3 解法 #  2.3.3.1 暴力解法 #  /** * 获取一个数组中指定下标范围内的最大值 * @param height 数组 * @param startIndex 起始坐标 * @param endIndex 结束坐标 * @return 该数组指定下标范围内的最大值 */ public static int getMax(int[] height, int startIndex, int endIndex) { int max = -1; if (startIndex \u0026lt; 0 || endIndex \u0026gt; height.length - 1 || startIndex \u0026gt; endIndex) {return -1;} for (int i = startIndex; i \u0026lt;= endIndex; i++) { max = Math.max(height[i], max); } return max; } /** * 42.接雨水（版本 1：暴力解法） * 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 * @param height 每个柱子的高度 * @return 总共可以接多少雨水 */ public static int trapV1(int[] height) { int trap = 0; for (int i = 0; i \u0026lt; height.length; i++) { int leftMax = getMax(height, 0, i); int rightMax = getMax(height, i, height.length - 1); trap += Math.min(leftMax, rightMax) - height[i]; } return trap; } 暴力解法的时间复杂度为 O(N^2)，空间复杂度为 O(1)，这种计算 leftMax 和 rightMax 的方法十分笨拙，一般的优化方法就是备忘录。\n2.3.3.2 备忘录优化 #  之前的暴力解法是在每个位置都要计算 leftMax 和 rightMax，现在可以把结果都缓存下来，这样时间复杂度就降下来了。我们可以用两个数组 leftMax 和 rightMax 充当备忘录，leftMax[i] 表示位置 i 左边最高的柱子高度，rightMax[i] 表示位置 i 右边最高的柱子高度。预先把这两个数组计算好，避免重复计算。\n2.3.3.2.1 优化一 #  /** * 获取一个数组中指定下标范围内的最大值 * @param height 数组 * @param startIndex 起始坐标 * @param endIndex 结束坐标 * @return 该数组指定下标范围内的最大值 */ public static int getMax(int[] height, int startIndex, int endIndex) { int max = -1; if (startIndex \u0026lt; 0 || endIndex \u0026gt; height.length - 1 || startIndex \u0026gt; endIndex) {return -1;} for (int i = startIndex; i \u0026lt;= endIndex; i++) { max = Math.max(height[i], max); } return max; } /** * 42.接雨水（版本 2：备忘录解法） * 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 * @param height 每个柱子的高度 * @return 总共可以接多少雨水 */ public static int trapV2(int[] height) { int trap = 0; int[] leftMax = new int[height.length]; int[] rightMax = new int[height.length]; for (int i = 0; i \u0026lt; height.length; i++) { leftMax[i] = getMax(height, 0, i); rightMax[i] = getMax(height, i, height.length - 1); } for (int i = 0; i \u0026lt; height.length; i++) { trap += Math.min(leftMax[i], rightMax[i]) - height[i]; } return trap; } 2.3.3.2.2 优化二-优化求最大值方法 #  /** * 42.接雨水（版本 3：备忘录解法-优化求最大值方法） * 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 * @param height 每个柱子的高度 * @return 总共可以接多少雨水 */ public static int trapV3(int[] height) { if (height.length == 0) {return 0;} int trap = 0; int[] leftMax = new int[height.length]; int[] rightMax = new int[height.length]; leftMax[0] = height[0]; rightMax[height.length - 1] = height[height.length - 1]; for (int i = 1; i \u0026lt; height.length; i++) { leftMax[i] = Math.max(height[i], leftMax[i - 1]); } for (int i = height.length - 2; i \u0026gt;= 0; i--) { rightMax[i] = Math.max(height[i], rightMax[i + 1]); } for (int i = 0; i \u0026lt; height.length; i++) { trap += Math.min(leftMax[i], rightMax[i]) - height[i]; } return trap; } 这个优化其实和暴力解法差不多，就是避免了重复计算，把时间复杂度降低为 O(N)，已经是最优了，但是空间复杂度是 O(N)，所以空间复杂度还需要进一步优化。\n2.3.3 双指针解法 #  这种解法的思路是完全相同的，但是实现手法上非常巧妙，我们这一次也不用备忘录提前计算了，而是用双指边走边算，节省下空间复杂度。\n/** * 42.接雨水（版本 4：双指针解法） * 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 * @param height 每个柱子的高度 * @return 总共可以接多少雨水 */ public static int trapV4(int[] height) { if (height.length == 0) {return 0;} int left = 0, right = height.length - 1, trap = 0; int leftMax = height[0], rightMax = height[height.length - 1]; while (left \u0026lt;= right) { leftMax = Math.max(leftMax, height[left]); rightMax = Math.max(rightMax, height[right]); if (leftMax \u0026lt; rightMax) { trap += leftMax - height[left]; left++; } else { trap += rightMax - height[right]; right--; } } return trap; } 核心思想和之前一样，但是细节上有差别。\n之前的备忘录解法，leftMax[i] 和 rightMax[i] 分别代表的是 height[0..i] 和 height[i..end] 的最高柱子高度。\n 但是双指针解法中， leftMax 和 rightMax 分别代表的是 height[0..left] 和 height[right..end] 的最高柱子高度。比如这段代码：\nif (leftMax \u0026lt; rightMax) { trap += leftMax - height[left]; left++; }  此时的 leftMax 是 left 指针  左边的最高柱子，由于我们只在乎 min(leftMax, rightMax)，并且此时 leftMax \u0026lt; rightMax，因此 rightMax 是不是右边最大的不重要，重要的是 height[i] 能装的水之和 leftMax 有关。\n "},{"id":157,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.8-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/2.8.2-%E6%9C%80%E5%B0%91%E4%BE%A7%E8%B7%B3%E6%AC%A1%E6%95%B0/","title":"2.8.2 最少侧跳次数","section":"2.8 动态规划","content":"最少侧跳次数 #  1 题目 #  给你一个长度为 n 的 3 跑道道路 ，它总共包含 n + 1 个 点 ，编号为 0 到 n 。一只青蛙从 0 号点第二条跑道 出发 ，它想要跳到点 n 处。然而道路上可能有一些障碍。\n给你一个长度为 n + 1 的数组 obstacles ，其中 obstacles[i] （取值范围从 0 到 3）表示在点 i 处的 obstacles[i] 跑道上有一个障碍。如果 obstacles[i] == 0 ，那么点 i 处没有障碍。任何一个点的三条跑道中 最多有一个 障碍。\n比方说，如果 obstacles[2] == 1 ，那么说明在点 2 处跑道 1 有障碍。 这只青蛙从点 i 跳到点 i + 1 且跑道不变的前提是点 i + 1 的同一跑道上没有障碍。为了躲避障碍，这只青蛙也可以在 同一个 点处 侧跳 到 另外一条 跑道（这两条跑道可以不相邻），但前提是跳过去的跑道该点处没有障碍。\n比方说，这只青蛙可以从点 3 处的跑道 3 跳到点 3 处的跑道 1 。 这只青蛙从点 0 处跑道 2 出发，并想到达点 n 处的 任一跑道 ，请你返回 最少侧跳次数 。\n注意： 点 0 处和点 n 处的任一跑道都不会有障碍。\n示例 1：\n 输入：obstacles = [0,1,2,3,0] 输出：2 解释：最优方案如上图箭头所示。总共有 2 次侧跳（红色箭头）。 注意，这只青蛙只有当侧跳时才可以跳过障碍（如上图点 2 处所示）。 示例 2：\n 输入：obstacles = [0,1,1,3,3,0] 输出：0 解释：跑道 2 没有任何障碍，所以不需要任何侧跳。 示例 3：\n 输入：obstacles = [0,2,1,0,3,0] 输出：2 解释：最优方案如上图所示。总共有 2 次侧跳。 提示：\n obstacles.length == n + 1 1 \u0026lt;= n \u0026lt;= 5 * 105 0 \u0026lt;= obstacles[i] \u0026lt;= 3 obstacles[0] == obstacles[n] == 0  2 解题思路 #  通过 最少侧跳次数 我们可以知道该问题属于动态规划问题，因此可以采用DP 数组来解决，DP 数组解决问题时主要由以下两个步骤：\n 找状态关系（通过数学归纳获得）。 定义 dp 数组（根据状态转移方程获得）。  2.1 找状态关系 #   第 $i$ 列的任意一个赛道的最少侧跳次数 $S_i$ 等于当前列其他赛道的最少侧跳次数的最少值，即：  $$ S_i=min(S_{i1},S_{i2},S_{i3}) $$\n 因此，为了求第 $i$ 列的最少侧跳次数，我们需要求每个赛道的最少侧跳次数，每个赛道的最少侧跳次数的求法主要分为两步：\n  根据前一列相同赛道更新当前赛道的最少侧跳次数： 由于同一赛道上不需要进行侧跳，所以如果当前赛道上没有障碍，则当前赛道的最少侧跳次数等于前一列的当前赛道的最少侧跳次数，即：\n$$ S_{ij}=S_{i(j-1)} $$\n  根据当前列更新当前赛道的最少侧跳次数： 由于到达同一列不同赛道上需要一次侧跳，所以如果当前赛道上没有障碍，则当前赛道的最少侧跳次数等于当前列所有赛道的最少侧跳次数的最小值（假设当前赛道为第一条赛道），即：\n$$ S_{i1}=min(S_{i1},S_{i2},S_{i3}) $$\n    2.2 定义 dp 数组 #  根据 2.1 找状态关系  中的状态关系，我们可以确定 dp 数组的定义为 $dp[i][j]$表示到达第 $i$ 列第 $j$ 条赛道需要的最少侧跳次数。\n3 参考代码 #  package com.grayson.top.competition.c20210411; import java.util.Arrays; /** * @author peng.wei * @version 1.0 * @date 2021/4/11 15:12 * @Description */ public class Third { /** * 5728. 最少侧跳次数 * @param obstacles 障碍数组 * @return 最少侧条次数 */ public int minSideJumps(int[] obstacles) { int ol = obstacles.length; // dp 数组  int[][] dp = new int[ol][3]; // base case  dp[0][0] = 1; dp[0][1] = 0; dp[0][2] = 1; // 将 dp 数组中其他数据初始化为 ol，即将其他赛道的最小侧条次数初始化为最大  for (int i = 1; i \u0026lt; ol; i++) { Arrays.fill(dp[i], ol); } for (int i = 1; i \u0026lt; ol; i++) { // 现根据与其平行的前一列的赛道的最小侧条次数求当前节点的最小侧条次数  if (obstacles[i] != 1) {dp[i][0] = dp[i-1][0];} if (obstacles[i] != 2) {dp[i][1] = dp[i-1][1];} if (obstacles[i] != 3) {dp[i][2] = dp[i-1][2];} // 根据同一列的其他节点求出当前节点的最小侧条次数  if (obstacles[i] != 1) {dp[i][0] = Math.min(dp[i][0], Math.min(dp[i][1], dp[i][2]) + 1);} if (obstacles[i] != 2) {dp[i][1] = Math.min(dp[i][1], Math.min(dp[i][0], dp[i][2]) + 1);} if (obstacles[i] != 3) {dp[i][2] = Math.min(dp[i][2], Math.min(dp[i][0], dp[i][1]) + 1);} } // dp 数组的第 ol - 1 列中的最小值即为该列中任意一条赛道的最小侧条次数  return Math.min( dp[ol-1][0], Math.min( dp[ol-1][1], dp[ol-1][2] ) ); } public static void main(String[] args) { Third third = new Third(); int[] obstacles = {0, 1, 2, 3, 0}; // int[] obstacles = {0,1,1,3,3,0};  int res = third.minSideJumps(obstacles); System.out.println(res); } } 4 参考文献 #    1824. 最少侧跳次数。  DP。  "},{"id":158,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.9-%E8%AE%BE%E8%AE%A1/2.9.1-%E5%AE%9E%E7%8E%B0-Trie-%E5%89%8D%E7%BC%80%E6%A0%91/","title":"2.9.1 实现 Trie (前缀树)","section":"2.9 设计","content":"实现 Trie (前缀树) #  1 题目 #  Trie（发音类似 \u0026ldquo;try\u0026rdquo;）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。\n请你实现 Trie 类：\nTrie() 初始化前缀树对象。 void insert(String word) 向前缀树中插入字符串 word 。 boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。 boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。\n示例：\n输入 [\u0026quot;Trie\u0026quot;, \u0026quot;insert\u0026quot;, \u0026quot;search\u0026quot;, \u0026quot;search\u0026quot;, \u0026quot;startsWith\u0026quot;, \u0026quot;insert\u0026quot;, \u0026quot;search\u0026quot;] [[], [\u0026quot;apple\u0026quot;], [\u0026quot;apple\u0026quot;], [\u0026quot;app\u0026quot;], [\u0026quot;app\u0026quot;], [\u0026quot;app\u0026quot;], [\u0026quot;app\u0026quot;]] 输出 [null, null, true, false, true, null, true]解释 Trie trie = new Trie(); trie.insert(\u0026quot;apple\u0026quot;); trie.search(\u0026quot;apple\u0026quot;); // 返回 True trie.search(\u0026quot;app\u0026quot;); // 返回 False trie.startsWith(\u0026quot;app\u0026quot;); // 返回 True trie.insert(\u0026quot;app\u0026quot;); trie.search(\u0026quot;app\u0026quot;); // 返回 True ** 提示：**\n 1 \u0026lt;= word.length, prefix.length \u0026lt;= 2000 word 和 prefix 仅由小写英文字母组成 insert、search 和 startsWith 调用次数 总计 不超过 3 * 104 次  2 解题思路 #  2.1 HashMap #  2.1.1 问题解析 #   第一种方法我们可以使用 HashMap，使用单词作为 key，value 默认为 1。 当 insert 一个元素时，先判断 HashMap 中是否包含该元素，如果不包含的话，则将该元素插入 HashMap。 当 search 一个元素时，直接看一下 HashMap 中是否含有该字符串对应的 key 即可。 当判断是否含有一个元素以某个字符串开头时，直接遍历 HashMap 的 key，看是否含有以该字符串开头的 key 即可。  2.1.2 参考代码 #  /** * 208. 实现 Trie (前缀树)（版本 1：HashMap） */ class TrieV1 { Map\u0026lt;String, Integer\u0026gt; pool; /** * Initialize your data structure here. */ public TrieV1() { pool = new HashMap\u0026lt;\u0026gt;(); } /** * Inserts a word into the trie. */ public void insert(String word) { if (!pool.containsKey(word)) { pool.put(word, 1); } } /** * Returns if the word is in the trie. */ public boolean search(String word) { return pool.containsKey(word); } /** * Returns if there is any word in the trie that starts with the given prefix. */ public boolean startsWith(String prefix) { for (String key : pool.keySet()) { if (key.startsWith(prefix)) { return true; } } return false; } } 2.2 TrieNode #  2.2.1 问题解析 #   相比 HashMap，更加常规的做法是建立 TrieNode 结构节点，具体如下：  package com.grayson.top.domain; /** * @author peng.wei * @version 1.0 * @date 2021/4/14 20:50 * @Description TrieNode 实体类 */ public class TrieNode { public boolean end; public TrieNode[] nodes = new TrieNode[26]; public TrieNode() { } } 随着数据的不断插入，根据需要不断创建 TrieNode 节点即可。   2.2.2 参考代码 #  /** * 208. 实现 Trie (前缀树)（版本 2：TrieNode） */ class TrieV2 { public class TrieNode { public boolean end; public TrieNode[] nodes = new TrieNode[26]; public TrieNode() {} } TrieNode root; /** * Initialize your data structure here. */ public TrieV2() { root = new TrieNode(); } /** * Inserts a word into the trie. */ public void insert(String word) { TrieNode p = root; for (int i = 0; i \u0026lt; word.length(); i++) { int u = word.charAt(i) - \u0026#39;a\u0026#39;; if (p.nodes[u] == null) {p.nodes[u] = new TrieNode();} p = p.nodes[u]; } p.end = true; } /** * Returns if the word is in the trie. */ public boolean search(String word) { TrieNode p = root; for (int i = 0; i \u0026lt; word.length(); i++) { int u = word.charAt(i) - \u0026#39;a\u0026#39;; if (p.nodes[u] == null) {return false;} p = p.nodes[u]; } return p.end; } /** * Returns if there is any word in the trie that starts with the given prefix. */ public boolean startsWith(String prefix) { TrieNode p = root; for (int i = 0; i \u0026lt; prefix.length(); i++) { int u = prefix.charAt(i) - \u0026#39;a\u0026#39;; if (p.nodes[u] == null) {return false;} p = p.nodes[u]; } return true; } } 3 参考文献 #    208. 实现 Trie (前缀树)。  【宫水三叶】一题双解：「二维数组」\u0026amp;「TrieNode」方式。  "},{"id":159,"href":"/school-recruitment/docs/algorithm/2%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%A2%98/2.9-%E8%AE%BE%E8%AE%A1/2.9.2-LRU-%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/","title":"2.9.2 Lru 缓存机制","section":"2.9 设计","content":"LRU 缓存机制 #  1 题目 #  运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制 。 实现 LRUCache 类：\n LRUCache(int capacity) 以正整数作为容量 capacity 初始化 LRU 缓存 int get(int key) 如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1 。 void put(int key, int value) 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字-值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。  进阶： 你是否可以在 O(1) 时间复杂度内完成这两种操作？\n示例：\n输入 [\u0026#34;LRUCache\u0026#34;, \u0026#34;put\u0026#34;, \u0026#34;put\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;put\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;put\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;get\u0026#34;] [[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]] 输出 [null, null, null, 1, null, -1, null, -1, 3, 4] 解释 LRUCache lRUCache = new LRUCache(2); lRUCache.put(1, 1); // 缓存是 {1=1} lRUCache.put(2, 2); // 缓存是 {1=1, 2=2} lRUCache.get(1); // 返回 1 lRUCache.put(3, 3); // 该操作会使得关键字 2 作废，缓存是 {1=1, 3=3} lRUCache.get(2); // 返回 -1 (未找到) lRUCache.put(4, 4); // 该操作会使得关键字 1 作废，缓存是 {4=4, 3=3} lRUCache.get(1); // 返回 -1 (未找到) lRUCache.get(3); // 返回 3 lRUCache.get(4); // 返回 4 提示：\n 1 \u0026lt;= capacity \u0026lt;= 3000 0 \u0026lt;= key \u0026lt;= 3000 0 \u0026lt;= value \u0026lt;= 104 最多调用 3 * 104 次 get 和 put  2 解题思路 #  2.1 PriorityQueue #  2.1.1 问题解析 #   这种方法是采用 $HashMap$ 和 $PriorityQueue$ 相结合的方式来完成的，其中创建了三个变量，分别为 $map$、$timeMap$、$queue$，其对应的具体含义如下：  $map$：类型为 $HashMap\u0026lt; Integer,String\u0026gt;$，数据格式为 $key:value_timestamp$，主要用来存储 $key$、$value$ 及其对应的时间戳（当前总的操作次数）。 $timeMap$：类型为 $HashMap\u0026lt; Integer,Integer\u0026gt;$，数据格式为 $timestamp:key$，主要用来存储 $timestamp$、$key$。 $queue$：类型为 $PriorityQueue\u0026lt; Integer\u0026gt;$，数据格式为 $timestamp$，主要用来存储时间戳。   当执行 $get(key)$ 方法时：  如果 $map$ 中不包含当前 $key$ 时，直接返回 $-1$。 当 $map$ 中包含当前 $key$ 时：修改 $map$ 中当前 $key$ 对应的 $timestamp$，同时将 $timestamp$ 和 $key$ 对应的存到 $timeMap$ 和 $queue$ 中，然后从 $map$ 中获取其对应的 $value$ 并返回。 将 $timestamp$ 加 1。   当执行 $put(key,value)$ 时：  将 $timestamp$ 加 1。 判断当前 $map$ 的容量是否大于等于 $capacity$：  如果是的话，则将 $queue$ 中的元素依次弹出，直到弹出的 $_timestamp$ 不小于当前的 $timestamp$，同时移除 $timeMap$ 中对应的 $timestamp$，然后获取 $timeMap$ 中 $timestamp$ 对应的 $key$，并从 $map$ 中移除对应的 $key$。 如果否的话，判断当前 $map$ 中是否存在对应的 $key$：  如果存在的话，更新 $map$ 中对应的 $key$、$value$ 和 $timestamp$，然后将 $key$ 和 $timestamp$ 添加到 $timeMap$ 和 $queue$ 中。 如果不存在的话，添加 $key$、$value$ 和 $timestamp$ 到 $map$，同时将 $key$ 和 $timestamp$ 添加到 $timeMap$ 和 $queue$ 中。        2.1.2 参考代码 #  class LRUCache { // 容量  int capacity; // 时间戳（每进行任何一次操作都 +1）  int timestamp = 0; // key:value_timestamp  Map\u0026lt;Integer, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); // timestamp:key  Map\u0026lt;Integer, Integer\u0026gt; timeMap = new HashMap\u0026lt;\u0026gt;(); // timestamp  Queue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;(); public LRUCache(int capacity) { this.capacity = capacity; } public int get(int key) { if (!map.containsKey(key)) { return -1; } timestamp++; String[] splits = map.get(key).split(\u0026#34;_\u0026#34;); int value = Integer.parseInt(splits[0]); // 修改时间戳  map.put(key, String.format(\u0026#34;%s_%s\u0026#34;, value, timestamp)); timeMap.put(this.timestamp, key); queue.add(timestamp); return value; } public void put(int key, int value) { timestamp++; if (map.size() \u0026gt;= capacity \u0026amp;\u0026amp; !map.containsKey(key)) { // 如果容量超限：  // 1. 移除 队列 中时间戳小于当前时间戳的所有元素。  // 2. 移除 timeMap 中第一步中队列中的时间戳。  // 3. 移除 前两步时间戳对应的 key。  while (true) { Integer _timestamp = queue.poll(); Integer _key = timeMap.get(_timestamp); timeMap.remove(_timestamp); int currTimestamp = Integer.parseInt(map.get(_key).split(\u0026#34;_\u0026#34;)[1]); if (_timestamp \u0026lt; currTimestamp) { continue; } map.remove(_key); break; } } // 添加元素及其对应的时间戳  map.put(key, String.format(\u0026#34;%s_%s\u0026#34;, value, timestamp)); timeMap.put(timestamp, key); queue.add(timestamp); } } 2.2 双链表 #  2.2.1 问题解析 #   这种方法采用的是将 $HashMap$ 和 $ 链表 $ 相结合的方式其中：   $HashMap$ 中存储的数据格式为:\n$$ HashMap\u0026lt; Integer,DlinkNode\u0026gt; $$\n  $ 链表 $ 的数据格式为：\nclass DLinkNode { int key; int value; // 前驱结点  DLinkNode prev; // 后驱节点  DLinkNode next; DLinkNode() { } DLinkNode(int _key, int _value) { this.key = _key; this.value = _value; } }    当执行 $get(key)$ 方法时：  如果 $map$ 中不存在当前 $key$，则直接返回-1。 否则，获取当前 $key$ 对应的节点，并将其移动到链表头部，然后返回当前节点对应的值。   当执行 $put(key,value)$ 方法时：  如果 $map$ 中的容量大于或等于 $capacity$，并且 $map$ 中不包含当前 $key$，则将尾部节点从链表中删除。 判断 $map$ 中是否包含当前 $key$：  如果不包含的话，则根据 $(key,value)$ 创建一个节点，并将当前节点添加到链表头部。 如果不包含的话，则更新链表中当前 $key$ 对应节点的 $value$，同时将该节点移动到链表头部。      2.2.2 参考代码 #  class LRUCache { class DLinkNode { int key; int value; // 前驱结点  DLinkNode prev; // 后驱节点  DLinkNode next; DLinkNode() { } DLinkNode(int _key, int _value) { this.key = _key; this.value = _value; } } // 模仿 链头 和 链尾  DLinkNode head, tail; // 存储 key 及其对应的链表  Map\u0026lt;Integer, DLinkNode\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int capacity; public LRUCache(int capacity) { this.capacity = capacity; // 初始化链表的头部和尾部  head = new DLinkNode(); tail = new DLinkNode(); head.next = tail; tail.prev = head; } public int get(int key) { if (!map.containsKey(key)) { return -1; } else { // 获取当前 key 对应的节点，并将其移动至链表头部  DLinkNode node = map.get(key); int value = node.value; moveToHead(key, node); return value; } } public void put(int key, int value) { if (map.size() \u0026gt;= capacity \u0026amp;\u0026amp; !map.containsKey(key)) { // 将链表尾部的节点删除  DLinkNode tmpNode = tail.prev; tmpNode.prev.next = tail; tail.prev = tmpNode.prev; map.remove(tmpNode.key); } if (!map.containsKey(key)) { // 将当前 key 对应的节点添加到链表头部  DLinkNode node = new DLinkNode(key, value); map.put(key, node); addToHead(node); } else { // 更新当前 key 对应的元素并将其添加到链表头部  moveToHeadAndUpdateValue(key, value); } } /** * 将当前节点添加到链表头部 * @param node 当前节点 */ public void addToHead(DLinkNode node) { node.prev = head; node.next = head.next; head.next = node; node.next.prev = node; } /** * 更新当前 key 对应节点的值，然后将该节点移动到链表头部 * @param key key * @param value value */ public void moveToHeadAndUpdateValue(int key, int value) { DLinkNode node = map.get(key); node.value = value; moveToHead(key, node); } /** * 将当前节点移动到链表头部 * @param key key * @param node 当前节点 */ public void moveToHead(int key, DLinkNode node) { node.next.prev = node.prev; node.prev.next = node.next; head.next.prev = node; node.next = head.next; node.prev = head; head.next = node; } } 3 参考文献 #    146. LRU 缓存机制。  LRU 缓存机制【官方题解】。  "},{"id":160,"href":"/school-recruitment/docs/design-pattern/2%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/2.1-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","title":"2.1 单例模式","section":"2、创建型模式","content":"单例模式 #  1 含义 #   单例模式（Singleton Design Pattern）是指保证一个类只能有一个实例，并且提供一个全局访问点。 单例模式的实现需要三个必要条件：   单例类的构造函数必须是私有的，这样才能将类的创建权控制在内部，从而使得类的外部不能创建类的实例。\n  单例类通过一个私有的静态变量来存储其唯一实例。\n  单例类通过提供一个公开的静态方法，使得外部使用者可以访问类的唯一实例。\n 因为单例类的构造函数是私有的，所以单例类不能被继承。\n    实现单例类时，需要考虑三个问题：  创建单例对象时，是否线程安全。 单例对象的创建，是否延时加载。 获取单例对象时，是否需要锁（锁会导致低性能）。    2 优缺点 #  2.1 优点 #   在单例模式中，活动的单例只有一个实例，对单例类的所有实例化得到的都是相同的一个实例，这样就可以防止其他对象对自己的实例化，确保所有的对象都访问一个实例，避免对共享资源的多重占用。 单例模式具有一定的伸缩性，类自己来控制实例化进程，因此在改变实例化进程上有相应的伸缩性。 由于在系统内存中只存在一个对象，因此可以节约系统资源，当需要频繁创建和销毁对象时，单例模式无疑可以提高系统的性能。  2.2 缺点 #   不适用于变化的对象，如果同一类型的对象总是要在不同的用例场景发生变化，单例就会引起数据的错误，不能保存彼此的状态。 由于单例模式没有抽象层，因此单例类的扩展有很大的困难。 滥用单例会带来一些负面影响，例如：  为了节省资源将数据库连接池对象设计为单例类，可能会导致共享连接池对象的程序过多而出现连接池溢出。 如果实例化的对象长时间不被利用，系统会认为是垃圾而被回收，这将导致对象状态的丢失。    3 使用场景 #  在下面几个场景中适合使用单例模式：\n 有频繁实例化然后销毁的情况，也就是频繁的new 对象，可以考虑单例模式。 创建对象耗时过多或者耗资源过多，但又经常用到的对象。 频繁访问 IO 资源的对象，例如数据库连接池或访问本地文件。  下面举几个例子来说明一下：\n3.1 网站在线人数统计 #    这个其实就是全局计数器，也就是说所有用户在相同的时刻获取到的在线人数数量都是一致的，要实现这个需求，计数器就要全局唯一，也就正好可以用单例模式来实现。\n  当然这里不包括分布式场景，因为计数是存在内存中的，并且还要保证线程安全。\n  下面代码是一个简单的计数器实现：\npublic class Counter { private static class CounterHolder{ private static final Counter counter = new Counter(); } private Counter(){ System.out.println(\u0026#34;init...\u0026#34;); } public static final Counter getInstance(){ return CounterHolder.counter; } private AtomicLong online = new AtomicLong(); public long getOnline(){ return online.get(); } public long add(){ return online.incrementAndGet(); } }   3.2 配置文件访问类 #    项目中经常需要一些环境相关的配置文件，比如短信通知相关的、邮件相关的。\n  这里就以读取一个 properties 文件配置为例：\n 如果我们使用的是Spring，可以用@PropertySource 注解实现，默认就是单例模式。 如果不用单例的话，每次都要new 对象，然后重新读一遍配置文件，很影响性能，如果用单例模式，则只需要读取一遍就好了。    以下是文件访问单例类简单实现：\npublic class SingleProperty { private static Properties prop; private static class SinglePropertyHolder{ private static final SingleProperty singleProperty = new SingleProperty(); } /** * config.properties 内容是 test.name=kite */ private SingleProperty(){ System.out.println(\u0026#34;构造函数执行\u0026#34;); prop = new Properties(); InputStream stream = SingleProperty.class.getClassLoader() .getResourceAsStream(\u0026#34;config.properties\u0026#34;); try { prop.load(new InputStreamReader(stream, \u0026#34;utf-8\u0026#34;)); } catch (IOException e) { e.printStackTrace(); } } public static SingleProperty getInstance(){ return SinglePropertyHolder.singleProperty; } public String getName(){ return prop.get(\u0026#34;test.name\u0026#34;).toString(); } public static void main(String[] args){ SingleProperty singleProperty = SingleProperty.getInstance(); System.out.println(singleProperty.getName()); } }   3.3 数据库连接池 #   数据库连接池的实现，也包括线程池，之所以做池化的原因，是因为新建连接很耗时，如果每次新任务来了都新建连接，那么对性能的影响就会很大，所以一般的做法是在一个应用内维护一个连接池，这样当任务进来时，如果有空闲连接，可以直接拿来用，省去了初始化的开销。 所以用单例模式正好可以实现一个应用内只有一个线程池的存在，所有需要连接的任务，都要从这个连接池来获取连接，如果不使用单例，那么应用内就会出现多个连接池，那也就没什么意义了。 如果我们使用Spring 的话，并集成了Druid 或C3P0，这些成熟开源的数据库连接池，一般也都是默认以单例模式实现的。  4 实现方法 #  4.1 饿汉式 #    饿汉式的单例实现比较简单，在类加载的时候，静态示例 instance 就已创建并初始化好了。\n  具体代码如下：\npublic class Singleton { private static final Singleton instance = new Singleton(); private Singleton () {} public static Singleton getInstance() { return instance; } }   优点：\n 单例对象的创建时线程安全的，因为instance作为类成员变量的实例化发生在类 Singleton 类加载的初始化阶段，初始化阶段是执行类构造器 \u0026lt;clinit\u0026gt;() 方法的过程：  \u0026lt;clinit\u0026gt;() 方法是由编译器自动收集类中的所有静态（static）类变量的赋值动作和静态语句块（static{}）中的语句合并产生的，因此private static final Singleton instance = new Singleton(); 也会被放入到这个方法中。 虚拟机会保证一个类的 \u0026lt;clinit\u0026gt;() 方法在多线程环境中被正确的加锁、同步，如果多线程同时去初始化一个类，那么只会有一个线程去执行这个类的\u0026lt;clinit\u0026gt;()方法，其他线程都需要阻塞等待，直到活动线程执行\u0026lt;clinit\u0026gt;()方法完毕。 需要注意的是，其他线程虽然会被阻塞，但如果执行\u0026lt;clinit\u0026gt;() 方法的那条线程退出\u0026lt;clinit\u0026gt;() 方法后，其他线程唤醒后不会再次进入\u0026lt;clinit\u0026gt;()方法，因此同一个类加载器下，一个静态类只会初始化一次。   获取单例对象时不需要加锁。    缺点：\n 单例对象的创建不是延时加载，因为当类加载完成后就会对 instance 进行初始化，然后被分配内存完成实例化，如果我们自始至终都没有使用过这个实例对象，这就会造成内存的浪费。    4.2 懒汉式 #    懒汉式为了支持延时加载，将对象的创建延迟到了获取对象的时候，但为了线程安全，不得不为获取对象的操作加锁，这就导致了低性能。\n  并且这把锁只有在第一次创建对象时有用，之后每次获取对象，这把锁都是一个累赘。\n  具体代码如下：\npublic class Singleton { private static final Singleton instance; private Singleton () {} public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } }   优点：\n 对象的创建是线程安全的。 支持延时加载。    缺点：\n 获取对象的操作被加上了锁，影响了并发度。 如果单例对象需要频繁使用，那这个缺点就是无法接受的。 如果单例对象不需要频繁使用，那这个缺点也无伤大雅。    4.3 双重检测 #    饿汉式和懒汉式的单例都有缺点，双重检测的实现方式解决了这两者的缺点。\n  双重检测将懒汉式中的 synchronized 方法改成了 synchronized 代码块，具体代码如下：\npublic class Singleton { private static Singleton instance; private Singleton () {} public static Singleton getInstance() { if (instance == null) { // 第一重检测，判断对象是否为空，主要为了当对象已经创建时不需要加锁，直接返回对象即可  synchronized(Singleton.class) { // 注意这里是类级别的锁  if (instance == null) { // 第二重检测，判断对象是否为空，主要为了避免多线程并发时多次创建对象  instance = new Singleton(); } } } return instance; } }   这种实现方式在 Java 1.4 及更早的版本中有些问题，就是 指令重排序，具体如下：\n  问题主要出现在创建对象的语句 instance = new Singleton(); 上，因为在 Java 中创建一个对象并非是原子操作，该操作可以被分解成三行伪代码：\n// 1：分配对象的内存空间 memory = allocate(); // 2：初始化对象 ctorInstance(memory); // 3：设置 instance 指向刚分配的内存地址 instance = memory;   上面三行伪代码中的 2 和 3 可能被重排序，重排序之后的伪代码是这样的：\n// 1：分配对象的内存空间 memory = allocate(); // 3：设置 instance 指向刚分配的内存地址 instance = memory; // 2：初始化对象 ctorInstance(memory);   在单线程程序下，重排序不会对最终结果产生影响，但是在并发的情况下，可能会导致某些线程访问到未初始化的变量，例如：\n   时间 线程 A 线程 B     $t_1$ $A_1$：分配对象内存空间    $t_2$ $A_3$：设置 $instance$ 指向内存空间    $t_3$  $B_1$：判断 $instance$ 是否为空   $t_4$  $B_2$：由于 $instance$ 不为 $null$，线程 $B$ 将访问 $instance$ 引用的对象   $t_5$ $A_2$：初始化对象    $t_6$ $A_4$：访问 $instance$ 引用的对象       按照这样的顺序执行，线程 $B$ 将会获得一个未初始化的对象，并且自始至终，线程 $B$ 无需获取锁。\n  要解决这个问题，需要给 instance 成员变量加上 volatile 关键字，从而禁止指令重排序，具体可参考 2.13 Volatile 原理，修改后的代码如下：\npublic class Singleton { private static volatile Singleton instance; private Singleton () {} public static Singleton getInstance() { if (instance == null) { // 第一重检测，判断对象是否为空，主要为了当对象已经创建时不需要加锁，直接返回对象即可  synchronized(Singleton.class) { // 注意这里是类级别的锁  if (instance == null) { // 第二重检测，判断对象是否为空，主要为了避免多线程并发时多次创建对象  instance = new Singleton(); } } } return instance; } }   优点：\n 对象的创建是线程安全的。 支持延时加载。 获取对象时不需要加锁。      4.4 静态内部类 #    用静态内部类的方式实现单例类，利用了 Java 静态内部类的特性：\n Java 加载外部类的时候，不会创建内部类的实例，只有在外部类使用到内部类的时候，才会创建内部类实例。 对于一个类，JVM 在仅用一个类加载器加载他时，静态变量的赋值在全局只会执行一次。    具体代码如下：\npublic class Singleton { private Singleton () {} private static class SingletonInner { private static final Singleton instance = new Singleton(); } public static Singleton getInstance() { return SingletonInner.instance; } }   SingletonInner 是一个静态内部类，当外部类 Singleton 被加载的时候，并不会创建 SingletonInner 实例对象，只有当调用 getInstance() 方法时，SingletonInner 才会被加载，这个时候才会创建 instance，这样就做到了延时加载。\n  instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证，具体可参考 3.4 类的生命周期和加载过程。\n  优点：\n 对象的创建时线程安全的。 支持延时加载。 获取对象时不需要加锁。    4.5 枚举 #    用枚举来实现单例，是最简单的方式，这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性：\n  当我们使用 enum 来定义一个枚举类型的时候，编译器会自动帮我们创建一个 final 类型的类继承 Enum 类，所以枚举类型不能被继承，同时这个类中的属性和方法都是 static 类型的，因此创建一个 enum 类型是线程安全的，具体原因可参考 3.1 饿汉式的优点中关于单例对象创建时是线程安全的原因的叙述。\n  定义的一个枚举类：\npublic enum T { SPRING,SUMMER,AUTUMN,WINTER; }   反编译后的部分信息：\npublic static final T SPRING; public static final T SUMMER; public static final T AUTUMN; public static final T WINTER; private static final T $VALUES[]; static { SPRING = new T(\u0026#34;SPRING\u0026#34;, 0); SUMMER = new T(\u0026#34;SUMMER\u0026#34;, 1); AUTUMN = new T(\u0026#34;AUTUMN\u0026#34;, 2); WINTER = new T(\u0026#34;WINTER\u0026#34;, 3); $VALUES = (new T[] { SPRING, SUMMER, AUTUMN, WINTER }); }     具体代码如下：\npublic enum Singleton { INSTANCE; // 该对象全局唯一 }   优点：\n 写法简单。 对象的创建时线程安全的。 获取对象时不需要加锁。    缺点：\n 不支持延时加载。    5 总结 #     实现方法 线程安全 延时加载     饿汉式 是 否   懒汉式 是 是   双重检测 是 是   静态内部类 是 是   枚举 是 否    参考文献 #    设计模式面试题（总结最全面的面试题！！！）。  单例模式的五种实现方式及优缺点。  单例模式的使用场景和 Java 静态块的使用。  设计模式【1.3】\u0026ndash; 为什么饿汉式单例是线程安全的？  【单例深思】饿汉式与类加载。  Hungry Chinese Style of Singleton Design Pattern.  单例陷阱——双重检查锁中的指令重排问题。  单例模式-静态内部类实现及原理剖析。  K：枚举的线程安全性及其序列化问题。  "},{"id":161,"href":"/school-recruitment/docs/design-pattern/2%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/2.2-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","title":"2.2 工厂模式","section":"2、创建型模式","content":"工厂模式 #  1 含义 #   工厂模式提供了一种创建对象的最佳方式， 在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，而是通过使用一个共同的接口来指向新创建的对象，实现了创建者和调用者的分离。 工厂模式可分为简单工厂模式、工厂方法模式、抽象工厂模式。  2 优缺点 #  2.1 优点 #   工厂模式是我们最常用的实例化对象模式，是用工厂方法代替 new操作的一种模式。 利用工厂模式可以降低程序的耦合性，为后期的维护修改提供很大的便利。 将选择实现类、创建对象统一管理和控制，从而将调用者跟我们的实现类解耦。  3 使用场景 #   首先，作为一种创建类模式，在任何需要生成复杂对象的地方，都可以使用工厂模式，而对于简单对象，特别是只需要 new就可以完成创建的对象，无需使用工厂模式，因为如果使用工厂模式，就需要引入一个工厂类，会增加系统的复杂度。 其次，工厂模式是一种典型的解耦模式，假如调用者自己组装产品需要增加依赖关系时，可以考虑使用工厂模式，这会大大降低对象之间的耦合度。 最后，由于工厂模式是依赖抽象架构的，他把实例化产品的任务交由实现类完成，扩展性比较好，也就是说，当需要系统有比较好的扩展性时，可以考虑工厂模式，不同的产品用不同的实现工厂来组装。  4 分类 #  4.1 简单工厂模式 #  4.1.1 含义 #   简单工厂模式相当于一个工厂中有各种产品，创建一个类时，客户无需知道具体产品的名称，只需要知道产品类所对应的参数即可。 但是工厂的职责过重，而且类型过多时不利于系统的扩展维护。  4.1.2 UML类图 #   4.1.3 实例 #    创建工厂：\npublic interface Car { void run(); }   创建工厂的产品（宝马）：\npublic class BMW implements Car { public void run() { System.out.println(\u0026#34;我是宝马汽车...\u0026#34;); } }   创建另外一种产品（奥迪）：\npublic class AoDi implements Car { public void run() { System.out.println(\u0026#34;我是奥迪汽车...\u0026#34;); } }   创建核心工厂类，由他决定具体调用哪种产品：\npublic class CarFactory { public static Car createCar(String name) { if (name.equals(\u0026#34;奥迪\u0026#34;)) { return new AoDi(); } if (name.equals(\u0026#34;宝马\u0026#34;)) { return new BMW(); } return null; } }   创建演示工厂的具体实例：\npublic class FactoryTest { public static void main(String[] args) { Car aodi = CarFactory.createCar(\u0026#34;奥迪\u0026#34;); aodi.run(); Car bmw = CarFactory.createCar(\u0026#34;宝马\u0026#34;); bmw.run(); } }   4.1.4 优缺点 #  4.1.4.1 优点 #   简单工厂模式能够根据外界给定的信息决定应该创建哪个具体类的对象，明确区分了各自的职责和权力，有利于整个软件体系结构的优化。  4.1.4.2 缺点 #   工厂类集中了所有实例的创建逻辑，违反了高内聚责任分配原则，将全部创建逻辑集中到了一个工厂类中，他所能创建的类只能是事先考虑到的，如果需要添加新的类，就需要改变工厂类了，不利于系统的维护和扩展。  4.2 工厂方法模式 #  4.2.1 含义 #   工厂方法模式又称多态性工厂模式，在工厂方法模式中，核心的工厂类不再负责所有产品的创建，而是将具体创建的工作交给子类去做，核心的工厂类成为一个抽象工厂角色，仅负责给出具体工厂子类必须实现的接口，而不接触哪一个产品类应当被实例化这种细节。  4.2.2 UML类图 #   4.2.3 实例 #    创建工厂：\npublic interface Car { void run(); }   创建工厂方法调用接口（所有的产品需要 new出来必须继承他来实现方法）：\npublic interface CarFactory { Car createCar(); }   创建工厂的产品（奥迪）：\npublic class AoDi implements Car { public void run() { System.out.println(\u0026#34;我是奥迪汽车...\u0026#34;); } }   创建工厂另外一种产品（宝马）：\npublic class BMW implements Car { public void run() { System.out.println(\u0026#34;我是宝马汽车...\u0026#34;); } }   创建工厂方法调用接口的实例（奥迪）：\npublic class AoDiFactory implements CarFactory{ @Override public Car createCar() { return new AoDi(); } }   创建工厂方法调用接口的实例（宝马）：\npublic class BMWFactory implements CarFactory { @Override public Car createCar() { return new BMW(); } }   演示创建工厂的具体实例：\npublic class FactoryTest { public static void main(String[] args) { Car aodi = new AoDiFactory().createCar(); aodi.run(); Car bmw = new BMWFactory().createCar(); bmw.run(); } }   4.2.4 优缺点 #  4.2.4.1 优点 #   具有 简单工厂模式的所有 优点。 扩展性高，如果想增加一个产品，只需要增加一个具体类和扩展一个工厂类即可。  4.2.4.2 缺点 #   每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。  4.3 抽象工厂模式 #  4.3.1 含义 #   抽象工厂简单地说就是工厂的工厂，抽象工厂可以创建具体工厂，由具体工厂来生产具体产品。  4.3.2 UML类图 #   4.3.3 实例 #    创建第一个工厂及其实现类：\npublic interface Car { void run(); } public class AoDi implements Car { public void run() { System.out.println(\u0026#34;我是奥迪汽车...\u0026#34;); } } public class BMW implements Car { public void run() { System.out.println(\u0026#34;我是宝马汽车...\u0026#34;); } }   创建第二个工厂及其实现类：\npublic interface Engine { void spin(); } public class AoDiEngine implements Engine{ @Override public void spin() { System.out.println(\u0026#34;转得快\u0026#34;); } } public class BMWEngine implements Engine{ @Override public void spin() { System.out.println(\u0026#34;转的慢\u0026#34;); } }   创建抽象方法调用接口及其实现类：\npublic interface AbstractFactory { Car createCar(); Engine createEngine(); } public class AoDiFactory implements AbstractFactory{ @Override public Car createCar() { return new AoDi(); } @Override public Engine createEngine() { return new AoDiEngine(); } } public class BMWFactory implements AbstractFactory{ @Override public Car createCar() { return new BMW(); } @Override public Engine createEngine() { return new BMWEngine(); } }   运行测试：\npublic class FactoryTest { public static void main(String[] args) { AoDiFactory aoDiFactory = new AoDiFactory(); Car aodi = aoDiFactory.createCar(); aodi.run(); Engine aodiEngine = aoDiFactory.createEngine(); aodiEngine.spin(); BMWFactory bmwFactory = new BMWFactory(); Car bmw = bmwFactory.createCar(); bmw.run(); Engine bmwEngine = bmwFactory.createEngine(); bmwEngine.spin(); } }   4.3.4 优缺点 #  4.3.4.1 优点 #   抽象工厂模式隔离了具体类的生成，使得客户并不需要知道什么时候被创建，由于这种隔离，更换一个具体的工厂就变得相对容易，所有的具体工厂都实现了抽象工厂中定义的那些公共接口，因此只需改变具体工厂的实例，就可以在某种程度上改变整个软件系统的行为。 当一个产品族中的多个对象被设计成一起工作时，他能够保证客户端始终只使用同一个产品族中的对象。 增加新的产品族很方便，只需增加一个新的具体工厂即可，无需修改已有系统。  4.3.4.2 缺点 #   增加新的产品结构很麻烦，因为需要修改所有的工厂角色，包括抽象工厂类，在所有的工厂类中都需要增加生产新产品的方法，这显然会带来较大不便。  参考文献 #    设计模式面试题（总结最全面的面试题！！！）。  工厂模式——这一篇真够了。  设计模式之工厂模式（factory pattern）。  "},{"id":162,"href":"/school-recruitment/docs/design-pattern/2%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/2.3-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","title":"2.3 代理模式","section":"2、创建型模式","content":"代理模式 #  1 含义 #    代理模式是指通过代理控制对象的访问，可以在这个对象调用方法之前、调用方法之后去处理/添加新的功能。\n  代码在原有代码乃至原业务流程都不修改的情况下，直接在业务流程中切入新代码，增加新功能。\n 关于代理，以下小例子可以帮助我们理解代理的含义：\n 游戏代练 游戏代练这件事就是一个代理模式，所谓代练，就是Proxy，也即代理类，代理的流程是我们把自己的账号交给代练人员，让他们帮我们打怪升级，而我们只需提供账号即可，代练人员那边，他所要做的就是登陆我们的账号，然后替我们打游戏，从第三者的角度看，我们这个角色在打怪升级，但这个第三者并不知道是不是我们本人在打游戏，他只能看到我们这个账号正在打怪升级，但并不需要知道后面打游戏的是谁，这就是代理模式，由他人代理玩游戏。 邀请明星 假设我们现在要邀请明星来上节目，我们应该先给他的经纪人打电话，然后再由经纪人通知到该明星，这里经纪人充当的就是代理的角色。     2 应用场景 #   当不想访问某个对象或访问某个对象存在困难时，就可以为这个对象创建一个代理，通过代理来间接的访问这个对象。 如果原始对象有不同的访问权限，可以使用代理控制对原始对象的访问，保护原始对象。 在访问原始对象时执行一些自己的附加条件。 为某个对象在不同的地址空间提供局部代理，使得系统可以将服务端的实现隐藏，客户端不必考虑服务端的存在。 具体应用场景主要包括Spring AOP、日志打印、异常处理、事务控制、权限控制。  3 分类 #  代理模式主要分为静态代理模式和动态代理模式两类，其中动态代理模式可分为 JDK 动态代理和 cglib 动态代理两种。\n3.1 静态代理 #  3.1.1 含义 #   静态代理是由程序员或工具生成代理类的源码，再编译代理类。 所谓静态也就是在程序运行之前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。  3.1.2 类图 #    Subject：抽象主题类，定义了代理对象和真实对象的共同接口方法，既可以是接口，也可以是抽象类。 RealSubject：真实主题类，该类可以称为被委托类或被代理类，该类定义了代理对象所表示的真实对象，实现了 Subject 接口，而Client端通过代理类间接的调用真实主题类的方法，由其执行真实的业务逻辑。 ProxySubject：代理类，该类也被称为委托类或代理类，该类中持有一个真实主题类的引用，同样实现了 Subject 接口，在其实现的接口方法中调用真实主题类中相应的接口方法，以此起到代理的作用。 Client：客户端，使用代理。   相关近义词：\n Subject = 公共接口。 ProxySubject = 代理对象 = 代理类 = 委托类 = 代理人。 RealSubject = 真实对象 = 被代理类 = 被委托类 = 被代理人。   3.1.3 实例 #   使用静态代理的基本步骤为：\n 定义代理对象和真实对象的公共接口。 真实对象实现公共接口中的方法。 代理对象实现公共接口中的方法，并把方法的逻辑转发给真实对象。   我们通过小明买房的例子来讲解一下静态代理，小明想要在大城市租房，但是他平时很忙没有时间去看房，于是他就找到一个房产中介，把自己的租房意愿告诉房产中介，让房产中介来替自己解决租房问题，很明显房产中介就是代理人，小明就是被代理的人，下面我们用静态代理来实现这个过程：\n  首先定义一个租房步骤的公共接口：\npublic interface IRoom { void seekRoom(); // 找房  void watchRoom(); // 看房  void room(); // 给钱租房  void finish(); // 完成租房 } 4 个步骤完成租房。\n  然后我们定义具体的想要租房的人，即小明：\npublic class XiaoMing implements IRoom { @Override public void seekRoom() { System.out.println(\u0026#34;找房\u0026#34;); } @Override public void watchRoom() { System.out.println(\u0026#34;看房\u0026#34;); } @Override public void room() { System.out.println(\u0026#34;给钱租房\u0026#34;); } @Override public void finish() { System.out.println(\u0026#34;完成租房\u0026#34;); } }   小明这个类实现了 IRoom 接口，实现了其中的具体逻辑，但是小明并会自己去租房，他委托房产中介去做，因此我们需要定义一个房产中介：\npublic class RoomAgency implements IRoom { private IRoom mRoom; // 持有一个被代理人（小明）的引用  public RoomAgency(final IRoom mRoom) { this.mRoom = mRoom; } @Override public void seekRoom() { mRoom.seekRoom(); } @Override public void watchRoom() { mRoom.watchRoom(); } @Override public void room() { mRoom.room(); } @Override public void finish() { mRoom.finish(); } } 在该类中会持有一个被代理人的引用，在这里指小明，可以看到房产中介所执行的方法实际上就是简单的调用被代理人中的方法。\n  下面我们定义一个客户端：\npublic class Client { public static void main(String[] args) { // 小明想租房  XiaoMing xiaoMing = new XiaoMing(); // 找一个代理人  RoomAgency roomAgency = new RoomAgency(xiaoMing); // 房产中介找房  roomAgency.seekRoom(); // 房产中介看房  roomAgency.watchRoom(); // 房产中介租房  roomAgency.room(); // 房产中介完成租房  roomAgency.finish(); } }   在上面的例子中，房产中介代理了小明的找房、看房、租房等过程，可以看到静态代理模式其实就是一种委托机制，真实对象将方法委托给代理对象，而且房产中介还可以继续代理其他人，比如 XiaoHong 也想租房，我们再定义一个 XiaoHong 实现 IRoom 接口，并在 Client 中给房产中介 RoomAgency 代理就行。\n  3.1.4 优缺点 #  3.1.4.1 缺点 #   只能为给定接口下的实现类做代理，如果接口不同，就需要定义不同的代理，例如上面小明是想要买房而不是租房，这是现在的房产中介就不能满足小明的需求了，因为现在的房产中介只有替人租房的能力，没有替人买房的能力，这时就需要更换租房接口为买房接口，再定义一个专门买房的房产中介，我们可以发现，每次更换接口，都需要更换代理类。  3.2 动态代理 #  动态代理模式可分为 JDK 动态代理和 cglib 动态代理两种。\n3.2.1 JDK 动态代理 #  3.2.1.1 含义 #   JDK 动态代理是利用 JDK 的 API，动态的在内存中构建代理对象（根据被代理的接口来动态生成代理类的 class 文件，并加载运行的过程）。  3.2.1.2 类图 #    上面就是动态代理的大概类图结构，其中Subject、ProxySubject、RealSubject 和Client 角色的作用和静态代理的一样。 与静态代理相比，多了一个InvocationHandler 角色和一个Proxy 角色：  InvocationHandler 是Java 提供的一个接口，我们需要定义一个类实现 InvocationHandler 接口，这里就叫DynamicProxy 角色。 Proxy 是Java 提供用于动态生成 ProxySubject 的一个类，它需要 ProxySubject 继承。   DynamicProxy 在ProxySubject 和RealSubject 之间起到了中间人的角色，ProxySubject 会把事情委托给DynamicProxy 来做，而DynamicProxy最终把事情委托给 RealSubject 来做，其中最重要的一点是ProxySubject是在代码运行时才动态生成的，这是和静态代理的最大区别。  3.2.1.3 实例 #   使用 JDK 动态代理的基本步骤为：\n 定义代理对象和真实对象的公共接口（与静态代理步骤相同）。 真实对象实现公共接口中的方法（与静态代理步骤相同）。 定义一个实现了 InvocationHandler 接口的动态代理类。 通过 Proxy 类的 newProxyInstance 方法创建代理对象，调用代理对象的方法。  与静态代理的区别是：\n 少了一个步骤：  代理对象实现公共接口的方法，因为动态代理的代理对象是代码运行时通过Proxy 动态创建的，所以不需要提前编写代理对象的类。   多了两个步骤：  定义一个实现了 InvocationHandler 接口的动态代理类。 通过 Proxy 类的 newProxyInstance 方法创建代理对象，调用代理对象的方法。       我们需要定义一个动态代理类，他用于执行真实对象的方法：\n// 实现了 InvocationHandler 的动态代理类 public class DynamicProxy implements InvocationHandler { private Object mObject; // 真实对象引用  public DynamicProxy(final Object mObject) { this.mObject = mObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 通过反射调用真实对象的方法  Object result = method.invoke(mObject, args); return result; } } 在该类中，我们声明了一个 Object 引用，该引用指向真实对象，真实对象在构造函数中传入，而在 invoke() 方法中，通过反射调用真实对象的具体方法，这里需要注意的是指向真实对象的引用类型最好定义为 Object 类型而不是真实对象的具体类型，如 XiaoMing，这样做的好处是当我们需要代理另外一个人时，例如 XiaoHong，我们只需在 DynamicProxy 的构造函数中传入 XiaoHong 引用而不用更改 DynamicProxy 的类结构，这样一个 DynamicProxy 就可以代理很多人。\n  通过 Proxy 类的 newProxyInstance() 方法创建代理对象，调用代理对象的方法：\npublic class Client { public static void main(String[] args) { // 构建一个小明  XiaoMing xiaoMing = new XiaoMing(); // 构造一个动态代理  DynamicProxy dynamicProxy = new DynamicProxy(xiaoMing); // 获取被代理类 小明 的 ClassLoader  ClassLoader classLoader = xiaoMing.getClass().getClassLoader(); // 1. 通过 Proxy 类的 newProxyInstance 方法动态构造一个代理人房产中介  IRoom roomAgency = (IRoom) Proxy.newProxyInstance(classLoader, new Class[]{IRoom.class}, dynamicProxy); // 2. 调用代理对象的方法  // 房产中介找房  roomAgency.seekRoom(); // 房产中介看房  roomAgency.watchRoom(); // 房产中介租房  roomAgency.room(); // 房产中介完成租房  roomAgency.finish(); } } Proxy 的 newProxyInstance() 方法会根据传入的类加载器动态生成代理对象实例，生成的代理对象会继承 Proxy 类并实现传入的接口列表：\n 这里的类加载器是小明的 ClassLoader，即真实对象的类加载器。 接口列表则是 IRoom，传入的为 IRoom 的 Class 对象。 除了这两个参数，还传入了动态代理类 InvocationHandler 实例，这样Proxy类在创建代理对象的实例时就会把这个 InvocationHandler 引用传给代理对象，接下来当我们调用代理对象的方法时，这个方法的处理逻辑就会委托给 InvocationHandler 实例的 invoke() 方法执行，invoke()方法中就会通过反射调用我们真实对象的方法。    3.2.1.4 源码分析 #  我们先看 Client 的注释 1：\n// 1. 通过 Proxy 类的 newProxyInstance 方法动态构造一个代理人房产中介 IRoom roomAgency = (IRoom) Proxy.newProxyInstance(classLoader, new Class[]{IRoom.class}, dynamicProxy);   Proxy 的 newProxyInstance() 方法会根据传入的类加载器动态生成代理对象的实例，我们可以看一下 Proxy 的 newProxyInstance() 方法的源码：\n//Proxy.java private static final Class\u0026lt;?\u0026gt;[] constructorParams = { InvocationHandler.class }; public static Object newProxyInstance(ClassLoader loader, Class\u0026lt;?\u0026gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ Objects.requireNonNull(h); //clone 一下传入的接口列表  final Class\u0026lt;?\u0026gt;[] intfs = interfaces.clone(); //getProxyClass 会把逻辑转发给 getProxyClass0，所以 getProxyClass 的作用 = getProxyClass0 的作用，它们的区别只是一个是 public，一个是 private 的  //1、调用 getProxyClass0，获得一个代理 Class 对象  Class\u0026lt;?\u0026gt; cl = getProxyClass0(loader, intfs); try { //constructorParams = InvocationHandler.class  //2、这里通过代理 Class 对象获取构造参数为 InvocationHandler 的 Constructor  final Constructor\u0026lt;?\u0026gt; cons = cl.getConstructor(constructorParams); //传入的 InvocationHandler 引用  final InvocationHandler ih = h; //这个 Constructor 是 protected 的，所以要设置为 Public  if (!Modifier.isPublic(cl.getModifiers())) { cons.setAccessible(true); } //3、通过构造参数为 InvocationHandler 的 Constructor 反射创建代理对象实例，并传入 InvocationHandler 引用给构造  return cons.newInstance(new Object[]{h}); } //...省略异常处理 }   这个方法里面的流程还是很简单的，首先注释 1，调用 getProxyClass0()，获得一个代理 Class 对象，getProxyClass0 等于 getProxyClass 的作用，主要用于在运行时根据 .class 的结构生成一个代理 Class 二进制流，并通过传入的 ClassLoader 去把代理 Class 二进制流加载成一个 Class 对象，该代理 Class 对象继承 Proxy 并实现了传入的第二个参数对应的 Interface 列表。\npublic static Class\u0026lt;?\u0026gt; getProxyClass(ClassLoader loader, Class\u0026lt;?\u0026gt;... interfaces)throws IllegalArgumentException { final Class\u0026lt;?\u0026gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) { checkProxyAccess(Reflection.getCallerClass(), loader, intfs); } return getProxyClass0(loader, intfs); }   我们来看一下这个动态生成的代理 Class 对象的真实面目：\n  首先在 Client 的 main 函数的开头填入下面的一段代码：\n// jdk8 及之前： System.setProperty(\u0026#34;sun.misc.ProxyGenerator.saveGeneratedFiles\u0026#34;, \u0026#34;true\u0026#34;); // jdk8 之后： System.setProperty(\u0026#34;jdk.proxy.ProxyGenerator.saveGeneratedFiles\u0026#34;, \u0026#34;true\u0026#34;);   填入后，运行 Client 的 main 函数，会在 idea 工作空间下的 com/sun/proxy/ 目录下生成一个 $Proxy0.class 文件，这个文件就是动态生成的代理 Class 对象，即代理对象，这个 .class 文件里面都是 JVM 才能看懂的二进制，用 idea 打开时，它会自动帮我们反编译成 .java 文件，具体内容如下：\npublic final class $Proxy0 extends Proxy implements IRoom{ private static Method m1; private static Method m3; private static Method m4; private static Method m2; private static Method m5; private static Method m6; private static Method m0; //调用父类 Proxy 的构造函数，传入 InvocationHandler 引用  public $Proxy0(InvocationHandler paramInvocationHandler){ super(paramInvocationHandler); } //下面四个方法都是实现自 IRoom 的方法，可以看到它们只是简单的调用了父类的 h 的 invoke 方法，并把代理对象 $Proxy0 实例、要调用的方法 method，还有参数传了进去  public final void watchRoom(){ try{ this.h.invoke(this, m3, null); return; } //...省略异常处理  } public final void room(){ try{ this.h.invoke(this, m4, null); return; } //...省略异常处理  } public final void seekRoom(){ try{ this.h.invoke(this, m5, null); return; } //...省略异常处理  } public final void finish(){ try{ this.h.invoke(this, m6, null); return; } //...省略异常处理  } //...我们只关注 IRoom 接口中的方法，所以我省略了 Object 中继承而来的 toSting，hashcode 方法等，里面逻辑都一样，都是调用父类的 h 的 invoke 方法  static{ try{ m0 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;hashCode\u0026#34;, new Class[0]); m1 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;equals\u0026#34;, new Class[] { Class.forName(\u0026#34;java.lang.Object\u0026#34;) }); m2 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;toString\u0026#34;, new Class[0]); //获取 IRoom 接口方法的 Method 对象  m3 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;watchRoom\u0026#34;, new Class[0]); m4 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;room\u0026#34;, new Class[0]); m5 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;seekRoom\u0026#34;, new Class[0]); m6 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;finish\u0026#34;, new Class[0]); return; } //...省略异常处理  } }   可以看到 Proxy 类的 getProxyClass0() 方法会替我们动态生成代理对象 $Proxy0.class，这个代理对象会继承 Proxy 类和实现接口列表，而这里传入的接口只有 IRoom，因此 $Proxy0 会实现 IRoom 的方法，这些方法里面的逻辑都是调用父类的 h 的 invoke() 方法，父类的 h 就是 InvocationHandler 的引用，是在通过反射创建 $Proxy0 实例时在构造中传入的。\n  我们在 $Proxy0 中还发现了很多 Method 对象，在 $Proxy0 的底部的 static 块中通过反射获取到我们 IRoom 接口所有方法的 Method 对象，当我们调用某个方法时，相应方法的 method、代理对象 $Proxy0 实例还有方法参数一起传进了父类的 h 的 invoke() 方法中，所以我们在 invoke() 方法中就可以根据 method 通过反射调用真实对象的相应方法，具体如下：\n// 实现了 InvocationHandler 的动态代理类 public class DynamicProxy implements InvocationHandler { private Object mObject; // 真实对象引用  public DynamicProxy(final Object mObject) { this.mObject = mObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 通过反射调用真实对象的方法  Object result = method.invoke(mObject, args); return result; } }       我们回到 Client 的注释 1，所以当我们调用 Proxy 类的 newProxyInstance() 方法时，这个方法在里面创建了代理对象，并返回代理对象 $Proxy0 实例，所以当我们调用代理对象的方法时，我们就是在调用 $Proxy0 相应的方法，这个方法处理逻辑就会委托给 InvocationHandler 实例的 invoke() 方法执行（代理对象的父类持有 InvocationHandler 引用），invoke() 方法中就会通过反射调用我们真实对象的方法（InvocationHandler 的实现类中持有真实对象的引用），这就是动态代理的整个过程。\n  3.2.1.5 原理 #   动态代理的原理就是通过反射机制动态生成代理类，这是由于 JVM 可以通过 .class 文件的二进制信息加载 class 对象的。 那么我们在代码运行时，遵循 .class 文件的格式和结构，生成相应的二进制数据，然后再把这些二进制数据通过 JVM 加载成对应的 class 对象。 有了class 对象，我们就可以在运行时通过反射创建出代理对象的实例，这样就完成了在代码运行时，动态的创建一个代理对象的能力，这就是动态代理的原理。  3.2.1.6 优缺点 #  3.2.1.6.1 优点 #   代理类在程序运行时由反射自动生成，无需我们手动编写代理类代码，简化编程工作。 一个动态代理类InvocationHandler 就能代理多个被代理类，较为灵活。  3.2.1.6.2 缺点 #   动态代理只能代理实现了接口的类，而不能代理实现抽象类的类。 通过反射调用被代理类的方法，效率低。  参考文献 #    设计模式面试题（总结最全面的面试题！！！）。  简说设计模式——代理模式。  设计模式（四）——搞懂什么是代理模式。  代理模式。  设计模式之代理模式。  静态和动态代理模式。  "},{"id":163,"href":"/school-recruitment/docs/design-pattern/3%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/3.1-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","title":"3.1 代理模式","section":"3、结构型模式","content":"1 含义 #    代理模式是指通过代理控制对象的访问，可以在这个对象调用方法之前、调用方法之后去处理/添加新的功能。\n  代码在原有代码乃至原业务流程都不修改的情况下，直接在业务流程中切入新代码，增加新功能。\n 关于代理，以下小例子可以帮助我们理解代理的含义：\n 游戏代练 游戏代练这件事就是一个代理模式，所谓代练，就是 Proxy，也即代理类，代理的流程是我们把自己的账号交给代练人员，让他们帮我们打怪升级，而我们只需提供账号即可，代练人员那边，他所要做的就是登陆我们的账号，然后替我们打游戏，从第三者的角度看，我们这个角色在打怪升级，但这个第三者并不知道是不是我们本人在打游戏，他只能看到我们这个账号正在打怪升级，但并不需要知道后面打游戏的是谁，这就是代理模式，由他人代理玩游戏。 邀请明星 假设我们现在要邀请明星来上节目，我们应该先给他的经纪人打电话，然后再由经纪人通知到该明星，这里经纪人充当的就是代理的角色。     2 应用场景 #   当不想访问某个对象或访问某个对象存在困难时，就可以为这个对象创建一个代理，通过代理来间接的访问这个对象。 如果原始对象有不同的访问权限，可以使用代理控制对原始对象的访问，保护原始对象。 在访问原始对象时执行一些自己的附加条件。 为某个对象在不同的地址空间提供局部代理，使得系统可以将服务端的实现隐藏，客户端不必考虑服务端的存在。 具体应用场景主要包括Spring AOP、日志打印、异常处理、事务控制、权限控制。  3 分类 #  代理模式主要分为静态代理模式和动态代理模式两类，其中动态代理模式可分为 JDK 动态代理和 cglib 动态代理两种。\n3.1 静态代理 #  3.1.1 含义 #   静态代理是由程序员或工具生成代理类的源码，再编译代理类。 所谓静态也就是在程序运行之前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。  3.1.2 类图 #    Subject：抽象主题类，定义了代理对象和真实对象的共同接口方法，既可以是接口，也可以是抽象类。 RealSubject：真实主题类，该类可以称为被委托类或被代理类，该类定义了代理对象所表示的真实对象，实现了 Subject 接口，而 Client端通过代理类间接的调用真实主题类的方法，由其执行真实的业务逻辑。 ProxySubject：代理类，该类也被称为委托类或代理类，该类中持有一个真实主题类的引用，同样实现了 Subject 接口，在其实现的接口方法中调用真实主题类中相应的接口方法，以此起到代理的作用。 Client：客户端，使用代理。   相关近义词：\n Subject = 公共接口。 ProxySubject = 代理对象 = 代理类 = 委托类 = 代理人。 RealSubject = 真实对象 = 被代理类 = 被委托类 = 被代理人。   3.1.3 实例 #   使用静态代理的基本步骤为：\n 定义代理对象和真实对象的公共接口。 真实对象实现公共接口中的方法。 代理对象实现公共接口中的方法，并把方法的逻辑转发给真实对象。   我们通过小明买房的例子来讲解一下静态代理，小明想要在大城市租房，但是他平时很忙没有时间去看房，于是他就找到一个房产中介，把自己的租房意愿告诉房产中介，让房产中介来替自己解决租房问题，很明显房产中介就是代理人，小明就是被代理的人，下面我们用静态代理来实现这个过程：\n  首先定义一个租房步骤的公共接口：\npublic interface IRoom { void seekRoom(); // 找房  void watchRoom(); // 看房  void room(); // 给钱租房  void finish(); // 完成租房 } 4 个步骤完成租房。\n  然后我们定义具体的想要租房的人，即小明：\npublic class XiaoMing implements IRoom { @Override public void seekRoom() { System.out.println(\u0026#34;找房\u0026#34;); } @Override public void watchRoom() { System.out.println(\u0026#34;看房\u0026#34;); } @Override public void room() { System.out.println(\u0026#34;给钱租房\u0026#34;); } @Override public void finish() { System.out.println(\u0026#34;完成租房\u0026#34;); } }   小明这个类实现了 IRoom 接口，实现了其中的具体逻辑，但是小明并会自己去租房，他委托房产中介去做，因此我们需要定义一个房产中介：\npublic class RoomAgency implements IRoom { private IRoom mRoom; // 持有一个被代理人（小明）的引用  public RoomAgency(final IRoom mRoom) { this.mRoom = mRoom; } @Override public void seekRoom() { mRoom.seekRoom(); } @Override public void watchRoom() { mRoom.watchRoom(); } @Override public void room() { mRoom.room(); } @Override public void finish() { mRoom.finish(); } } 在该类中会持有一个被代理人的引用，在这里指小明，可以看到房产中介所执行的方法实际上就是简单的调用被代理人中的方法。\n  下面我们定义一个客户端：\npublic class Client { public static void main(String[] args) { // 小明想租房  XiaoMing xiaoMing = new XiaoMing(); // 找一个代理人  RoomAgency roomAgency = new RoomAgency(xiaoMing); // 房产中介找房  roomAgency.seekRoom(); // 房产中介看房  roomAgency.watchRoom(); // 房产中介租房  roomAgency.room(); // 房产中介完成租房  roomAgency.finish(); } }   在上面的例子中，房产中介代理了小明的找房、看房、租房等过程，可以看到静态代理模式其实就是一种委托机制，真实对象将方法委托给代理对象，而且房产中介还可以继续代理其他人，比如 XiaoHong 也想租房，我们再定义一个 XiaoHong 实现 IRoom 接口，并在 Client 中给房产中介 RoomAgency 代理就行。\n  3.1.4 优缺点 #  3.1.4.1 缺点 #   只能为给定接口下的实现类做代理，如果接口不同，就需要定义不同的代理，例如上面小明是想要买房而不是租房，这是现在的房产中介就不能满足小明的需求了，因为现在的房产中介只有替人租房的能力，没有替人买房的能力，这时就需要更换租房接口为买房接口，再定义一个专门买房的房产中介，我们可以发现，每次更换接口，都需要更换代理类。  3.2 动态代理 #  动态代理模式可分为 JDK 动态代理和 cglib 动态代理两种。\n3.2.1 JDK 动态代理 #  3.2.1.1 含义 #   JDK 动态代理是利用 JDK 的 API，动态的在内存中构建代理对象（根据被代理的接口来动态生成代理类的 class 文件，并加载运行的过程）。  3.2.1.2 类图 #    上面就是动态代理的大概类图结构，其中 Subject、ProxySubject、RealSubject 和 Client 角色的作用和静态代理的一样。 与静态代理相比，多了一个 InvocationHandler 角色和一个 Proxy 角色：  InvocationHandler 是 Java 提供的一个接口，我们需要定义一个类实现 InvocationHandler 接口，这里就叫 DynamicProxy 角色。 Proxy 是 Java 提供用于动态生成 ProxySubject 的一个类，它需要 ProxySubject 继承。   DynamicProxy 在 ProxySubject 和 RealSubject 之间起到了中间人的角色，ProxySubject 会把事情委托给 DynamicProxy 来做，而 DynamicProxy最终把事情委托给 RealSubject 来做，其中最重要的一点是 ProxySubject是在代码运行时才动态生成的，这是和静态代理的最大区别。  3.2.1.3 实例 #   使用 JDK 动态代理的基本步骤为：\n 定义代理对象和真实对象的公共接口（与静态代理步骤相同）。 真实对象实现公共接口中的方法（与静态代理步骤相同）。 定义一个实现了 InvocationHandler 接口的动态代理类。 通过 Proxy 类的 newProxyInstance 方法创建代理对象，调用代理对象的方法。  与静态代理的区别是：\n 少了一个步骤：  代理对象实现公共接口的方法，因为动态代理的代理对象是代码运行时通过 Proxy 动态创建的，所以不需要提前编写代理对象的类。   多了两个步骤：  定义一个实现了 InvocationHandler 接口的动态代理类。 通过 Proxy 类的 newProxyInstance 方法创建代理对象，调用代理对象的方法。       我们需要定义一个动态代理类，他用于执行真实对象的方法：\n// 实现了 InvocationHandler 的动态代理类 public class DynamicProxy implements InvocationHandler { private Object mObject; // 真实对象引用  public DynamicProxy(final Object mObject) { this.mObject = mObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 通过反射调用真实对象的方法  Object result = method.invoke(mObject, args); return result; } } 在该类中，我们声明了一个 Object 引用，该引用指向真实对象，真实对象在构造函数中传入，而在 invoke() 方法中，通过反射调用真实对象的具体方法，这里需要注意的是指向真实对象的引用类型最好定义为 Object 类型而不是真实对象的具体类型，如 XiaoMing，这样做的好处是当我们需要代理另外一个人时，例如 XiaoHong，我们只需在 DynamicProxy 的构造函数中传入 XiaoHong 引用而不用更改 DynamicProxy 的类结构，这样一个 DynamicProxy 就可以代理很多人。\n  通过 Proxy 类的 newProxyInstance() 方法创建代理对象，调用代理对象的方法：\npublic class Client { public static void main(String[] args) { // 构建一个小明  XiaoMing xiaoMing = new XiaoMing(); // 构造一个动态代理  DynamicProxy dynamicProxy = new DynamicProxy(xiaoMing); // 获取被代理类 小明 的 ClassLoader  ClassLoader classLoader = xiaoMing.getClass().getClassLoader(); // 1. 通过 Proxy 类的 newProxyInstance 方法动态构造一个代理人房产中介  IRoom roomAgency = (IRoom) Proxy.newProxyInstance(classLoader, new Class[]{IRoom.class}, dynamicProxy); // 2. 调用代理对象的方法  // 房产中介找房  roomAgency.seekRoom(); // 房产中介看房  roomAgency.watchRoom(); // 房产中介租房  roomAgency.room(); // 房产中介完成租房  roomAgency.finish(); } } Proxy 的 newProxyInstance() 方法会根据传入的类加载器动态生成代理对象实例，生成的代理对象会继承 Proxy 类并实现传入的接口列表：\n 这里的类加载器是小明的 ClassLoader，即真实对象的类加载器。 接口列表则是 IRoom，传入的为 IRoom 的 Class 对象。 除了这两个参数，还传入了动态代理类 InvocationHandler 实例，这样 Proxy类在创建代理对象的实例时就会把这个 InvocationHandler 引用传给代理对象，接下来当我们调用代理对象的方法时，这个方法的处理逻辑就会委托给 InvocationHandler 实例的 invoke() 方法执行，invoke()方法中就会通过反射调用我们真实对象的方法。    3.2.1.4 源码分析 #  我们先看 Client 的注释 1：\n// 1. 通过 Proxy 类的 newProxyInstance 方法动态构造一个代理人房产中介 IRoom roomAgency = (IRoom) Proxy.newProxyInstance(classLoader, new Class[]{IRoom.class}, dynamicProxy);   Proxy 的 newProxyInstance() 方法会根据传入的类加载器动态生成代理对象的实例，我们可以看一下 Proxy 的 newProxyInstance() 方法的源码：\n//Proxy.java private static final Class\u0026lt;?\u0026gt;[] constructorParams = { InvocationHandler.class }; public static Object newProxyInstance(ClassLoader loader, Class\u0026lt;?\u0026gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ Objects.requireNonNull(h); //clone 一下传入的接口列表  final Class\u0026lt;?\u0026gt;[] intfs = interfaces.clone(); //getProxyClass 会把逻辑转发给 getProxyClass0，所以 getProxyClass 的作用 = getProxyClass0 的作用，它们的区别只是一个是 public，一个是 private 的  //1、调用 getProxyClass0，获得一个代理 Class 对象  Class\u0026lt;?\u0026gt; cl = getProxyClass0(loader, intfs); try { //constructorParams = InvocationHandler.class  //2、这里通过代理 Class 对象获取构造参数为 InvocationHandler 的 Constructor  final Constructor\u0026lt;?\u0026gt; cons = cl.getConstructor(constructorParams); //传入的 InvocationHandler 引用  final InvocationHandler ih = h; //这个 Constructor 是 protected 的，所以要设置为 Public  if (!Modifier.isPublic(cl.getModifiers())) { cons.setAccessible(true); } //3、通过构造参数为 InvocationHandler 的 Constructor 反射创建代理对象实例，并传入 InvocationHandler 引用给构造  return cons.newInstance(new Object[]{h}); } //...省略异常处理 }   这个方法里面的流程还是很简单的，首先注释 1，调用 getProxyClass0()，获得一个代理 Class 对象，getProxyClass0 等于 getProxyClass 的作用，主要用于在运行时根据 .class 的结构生成一个代理 Class 二进制流，并通过传入的 ClassLoader 去把代理 Class 二进制流加载成一个 Class 对象，该代理 Class 对象继承 Proxy 并实现了传入的第二个参数对应的 Interface 列表。\npublic static Class\u0026lt;?\u0026gt; getProxyClass(ClassLoader loader, Class\u0026lt;?\u0026gt;... interfaces)throws IllegalArgumentException { final Class\u0026lt;?\u0026gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) { checkProxyAccess(Reflection.getCallerClass(), loader, intfs); } return getProxyClass0(loader, intfs); }   我们来看一下这个动态生成的代理 Class 对象的真实面目：\n  首先在 Client 的 main 函数的开头填入下面的一段代码：\n// jdk8 及之前： System.setProperty(\u0026#34;sun.misc.ProxyGenerator.saveGeneratedFiles\u0026#34;, \u0026#34;true\u0026#34;); // jdk8 之后： System.setProperty(\u0026#34;jdk.proxy.ProxyGenerator.saveGeneratedFiles\u0026#34;, \u0026#34;true\u0026#34;);   填入后，运行 Client 的 main 函数，会在 idea 工作空间下的 com/sun/proxy/ 目录下生成一个 $Proxy0.class 文件，这个文件就是动态生成的代理 Class 对象，即代理对象，这个 .class 文件里面都是 JVM 才能看懂的二进制，用 idea 打开时，它会自动帮我们反编译成 .java 文件，具体内容如下：\npublic final class $Proxy0 extends Proxy implements IRoom{ private static Method m1; private static Method m3; private static Method m4; private static Method m2; private static Method m5; private static Method m6; private static Method m0; //调用父类 Proxy 的构造函数，传入 InvocationHandler 引用  public $Proxy0(InvocationHandler paramInvocationHandler){ super(paramInvocationHandler); } //下面四个方法都是实现自 IRoom 的方法，可以看到它们只是简单的调用了父类的 h 的 invoke 方法，并把代理对象 $Proxy0 实例、要调用的方法 method，还有参数传了进去  public final void watchRoom(){ try{ this.h.invoke(this, m3, null); return; } //...省略异常处理  } public final void room(){ try{ this.h.invoke(this, m4, null); return; } //...省略异常处理  } public final void seekRoom(){ try{ this.h.invoke(this, m5, null); return; } //...省略异常处理  } public final void finish(){ try{ this.h.invoke(this, m6, null); return; } //...省略异常处理  } //...我们只关注 IRoom 接口中的方法，所以我省略了 Object 中继承而来的 toSting，hashcode 方法等，里面逻辑都一样，都是调用父类的 h 的 invoke 方法  static{ try{ m0 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;hashCode\u0026#34;, new Class[0]); m1 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;equals\u0026#34;, new Class[] { Class.forName(\u0026#34;java.lang.Object\u0026#34;) }); m2 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;toString\u0026#34;, new Class[0]); //获取 IRoom 接口方法的 Method 对象  m3 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;watchRoom\u0026#34;, new Class[0]); m4 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;room\u0026#34;, new Class[0]); m5 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;seekRoom\u0026#34;, new Class[0]); m6 = Class.forName(\u0026#34;com.example.hy.designpatternDemo.proxy.IRoom\u0026#34;).getMethod(\u0026#34;finish\u0026#34;, new Class[0]); return; } //...省略异常处理  } }   可以看到 Proxy 类的 getProxyClass0() 方法会替我们动态生成代理对象 $Proxy0.class，这个代理对象会继承 Proxy 类和实现接口列表，而这里传入的接口只有 IRoom，因此 $Proxy0 会实现 IRoom 的方法，这些方法里面的逻辑都是调用父类的 h 的 invoke() 方法，父类的 h 就是 InvocationHandler 的引用，是在通过反射创建 $Proxy0 实例时在构造中传入的。\n  我们在 $Proxy0 中还发现了很多 Method 对象，在 $Proxy0 的底部的 static 块中通过反射获取到我们 IRoom 接口所有方法的 Method 对象，当我们调用某个方法时，相应方法的 method、代理对象 $Proxy0 实例还有方法参数一起传进了父类的 h 的 invoke() 方法中，所以我们在 invoke() 方法中就可以根据 method 通过反射调用真实对象的相应方法，具体如下：\n// 实现了 InvocationHandler 的动态代理类 public class DynamicProxy implements InvocationHandler { private Object mObject; // 真实对象引用  public DynamicProxy(final Object mObject) { this.mObject = mObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 通过反射调用真实对象的方法  Object result = method.invoke(mObject, args); return result; } }       我们回到 Client 的注释 1，所以当我们调用 Proxy 类的 newProxyInstance() 方法时，这个方法在里面创建了代理对象，并返回代理对象 $Proxy0 实例，所以当我们调用代理对象的方法时，我们就是在调用 $Proxy0 相应的方法，这个方法处理逻辑就会委托给 InvocationHandler 实例的 invoke() 方法执行（代理对象的父类持有 InvocationHandler 引用），invoke() 方法中就会通过反射调用我们真实对象的方法（InvocationHandler 的实现类中持有真实对象的引用），这就是动态代理的整个过程。\n  3.2.1.5 原理 #   动态代理的原理就是通过反射机制动态生成代理类，这是由于 JVM 可以通过 .class 文件的二进制信息加载 class 对象的。 那么我们在代码运行时，遵循 .class 文件的格式和结构，生成相应的二进制数据，然后再把这些二进制数据通过 JVM 加载成对应的 class 对象。 有了 class 对象，我们就可以在运行时通过反射创建出代理对象的实例，这样就完成了在代码运行时，动态的创建一个代理对象的能力，这就是动态代理的原理。  3.2.1.6 优缺点 #  3.2.1.6.1 优点 #   代理类在程序运行时由反射自动生成，无需我们手动编写代理类代码，简化编程工作。 一个动态代理类 InvocationHandler 就能代理多个被代理类，较为灵活。  3.2.1.6.2 缺点 #   动态代理只能代理实现了接口的类，而不能代理实现抽象类的类。 通过反射调用被代理类的方法，效率低。  参考文献 #    设计模式面试题（总结最全面的面试题！！！）。  简说设计模式——代理模式。  设计模式（四）——搞懂什么是代理模式。  代理模式。  设计模式之代理模式。  静态和动态代理模式。  "},{"id":164,"href":"/school-recruitment/docs/design-pattern/3%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/3.2-%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/","title":"3.2 装饰器模式","section":"3、结构型模式","content":"1 含义 #   装饰器模式又称为包装模式，主要以对客户端透明的方式，在不改变对象结构的情况下，可以动态地扩展器功能。  不同于继承，组合可以在运行时进行，所以称之为「动态扩展」，比如在按钮点击时进行一些 log 日志的打印，在绘制 text 文本框时，额外绘制一个滚动条和边框等。\n  装饰器模式是继承关系的一个替代方案，可以在不使用创造更多子类的情况下，扩展对象的功能，就增加功能来说，相比生成子类更加灵活。  组合比继承更加灵活是因为当可拓展的功能很多时，继承方案会产生大量的子类，而组合可以提前写好处理函数，在需要时动态构造，显然更加灵活。\n  具体的示例如下：  相框：  照片 + 相框 = 带相框的照片，这背后就是一种装饰器模式，照片具有看的功能，相框具有装饰功能，在我们看照片的基础上，还能看到精心设计的相框，增加了美感，同时相框还可以增加照片的保存时间与安全性。 相框与照片是一种组合关系，任何照片都可以放到相框中，而不是每个照片生成一个特定的相框，显然，组合的方式更加灵活。   带有缓存的文件读写：  假如我们有一个类 FileIO 用来读写文件，但是没有缓存能力，一眼看上去好像新建一个CachedFileIO 用起来更加方便，而CachedIO 的用法是new CachedIO(new FileIO()) 稍微麻烦些。 但如果我们增加一个网络读写类NetworkIO，一个数据库读写类DBIO，此时，继承的方式会使子类数量急速膨胀 ，而组合的方式则非常灵活，生成一个支持缓存的网络读写器，只需要 new CachedIO(new NetworkIO()) 即可，这就是组合灵活的地方。 当然，为了实现这个能力，CachedIO需要与 FileIO、CachedFileIO、CachedIO 继承自同一个类，具备相同的接口。   搭建平台的组件 wrapper：  装饰器模式别名也叫wrapper，wrapper 也经常在前端搭建场景中遇到，当搭建平台加载一个组件时，希望拓展其基础能力，一般会使用 wrapper 层对组件进行嵌套，wrapper 层就是在不改变 API 的基础上，对第三方组件进行增强。      2 结构 #    抽象构件（Component）角色：定义一个抽象接口以规范准备接收扩展功能的目标对象。 具体构件（ConrecteComponent）角色：实现抽象构件，负责具体的行为实现。 抽象装饰（Decorator）角色：实现抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecoratorA 和 ConcreteDecoratorB）角色：继承抽象装饰角色，传入具体构件对象给到父类，重写抽象构件的方法，在重写的方法里面给具体构件对象添加附加的责任。  3 示例 #    现在有这么一个场景：\n 有一批厨师，全是中国厨师，他们有一个共同的动作是做完饭。 这批厨师做晚饭前的习惯不同，有些人喜欢做晚饭前洗手，有些人喜欢做晚饭前洗头。    那么，按照装饰器模式，先定义抽象构件角色 Cook：\npublic interface Cook { /** * 做晚饭 */ public void cookDinner(); }   然后创建具体构件角色 ChineseCook：\npublic class ChineseCook implements Cook{ /** * 做晚饭 */ @Override public void cookDinner() { System.out.println(\u0026#34;中国人做晚饭\u0026#34;); } }   定义一个抽象装饰角色 FilterCook，实现 Cook 接口并持有 Cook 引用：\npublic abstract class FilterCook implements Cook{ /** * 厨师 */ protected Cook cook; }   定义具体装饰角色 WashHandsCook 和 WashHearCook：\npublic class WashHandsCook extends FilterCook{ public WashHandsCook(Cook cook) { this.cook = cook; } /** * 做晚饭 */ @Override public void cookDinner() { washHand(); this.cook.cookDinner(); } /** * 洗手「方法拓展」 */ private void washHand() { System.out.println(\u0026#34;洗手\u0026#34;); } } public class WashHearCook extends FilterCook{ public WashHearCook(Cook cook) { this.cook = cook; } /** * 做晚饭 */ @Override public void cookDinner() { washHear(); this.cook.cookDinner(); } /** * 洗头「方法拓展」 */ private void washHear() { System.out.println(\u0026#34;洗头\u0026#34;); } }   定义一个客户端 Client，对装饰器进行调用：\npublic class Client { public static void main(String[] args) { Cook cook1 = new WashHandsCook(new ChineseCook()); Cook cook2 = new WashHearCook(new ChineseCook()); cook1.cookDinner(); cook2.cookDinner(); } }   4 优缺点 #  4.1 优点 #   装饰器模式与继承关系的目的都是要扩展对象的功能，但是装饰器模式可以提供比继承更多的灵活性，允许系统动态决定贴上一个需要的装饰，或者除掉一个不需要的装饰，继承关系是静态的，在系统运行前就确定了。 通过使用装饰类及这些装饰类的排列组合，可以实现不同的效果，排列组合是指在有多个装饰器时，按排列组合，将具体装饰器作为其他装饰器的构造方法入参（当做具体部件），组成装饰器嵌套，实现复杂的附加功能。  4.2 缺点 #   装饰器的问题也是组合的问题，过多的组合会导致：  组合过程的复杂，要生成过多的对象。 包装器层次增多，会增加调试成本，我们比较难追溯到一个 bug 是在哪一层包装导致。    5 适用场景 #   需要扩展一个现有类的功能，或给一个类增加附加责任，而又不能采用生成子类的方法进行扩充时，例如，该类被隐藏，或者该类是终极类，或者采用继承方式会产生大量子类。 需要通过对现有的一组基本功能进行排列组合而产生非常大量的功能，采用继承关系很难实现，而采用装饰器模式又很好实现。 需要动态地给一个对象增加功能，可以再动态地撤销这些功能。  6 典型应用 #  6.1 在 JDK 中的应用 #    装饰器模式在 Java I/O 标准库（java.io）包中广泛应用，例如，基于字节流的 InputStream/OutputStream 和基于字符的 Reader/Writer 体系：\n InputStream 的子类FilterInputStream。 OutputStream 的子类FilterOutputStream。 Reader 的子类BufferedReader 以及FilterReader。 Writer 的子类BufferedWriter、FilterWriter 以及PrintWriter。    以 InputStream 为例，InputStream是所有字节输入流的基类，其下有众多的子类，如基于文件的 FileInputStream、基于对象的 ObjectInputStream、基于字节数组的 ByteArrayInputStream 等，有些时候，需要为这些流加一些其他的小特性，如缓冲、压缩，用装饰器模式就十分方便，相关的部分类图如下所示：\n  抽象构件：InputStream。 具体构件：FileInputStream、ObjectInputStream。 抽象装饰器：FilterInputStream，继承抽象构件 InputTsream，构造方法传入具体构件对象。 具体装饰器：FilterInputStream的所有子类，如BufferedInputStream。    参考文献 #    Java 设计模式 12：装饰器模式。  Decorator（装饰器模式）。  Java 设计模式\u0026mdash;-装饰器模式。  浅谈设计模式 - 装饰器模式（五）。  设计模式之装饰器模式。  设计模式(十六)：装饰器模式(Decorator)。  "},{"id":165,"href":"/school-recruitment/docs/design-pattern/4%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/4.1-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","title":"4.1 策略模式","section":"4、行为型模式","content":"1 模式动机 #   完成一项任务，往往可以有多种不同的方式，每一种方式称为一个策略，我们可以根据环境或者条件的不同选择不同的策略来完成该项任务。 在软件开发中也常常遇到类似的情况，实现某一个功能有多个路径，此时可以使用一种设计模式来使得系统可以灵活地选择解决路径，也能够方便地增加新的解决路径。 在软件系统中，有许多算法可以实现某一功能，如查找、排序等，一种常用的方法是硬编码在一个类中，如需要提供多种查找方法，可以将这些算法写到一个类中，在该类中提供多个方法，每一个方法对应一个具体的查找算法，当然也可以将这些查找算法封装在一个统一的方法中，通过if...else... 等条件判断语句来进行选择，这两种实现方法我们都称之为硬编码，如果需要增加一种新的查找算法，需要修改封装算法类的源代码，更换查找算法，也需要修改客户端调用代码，在这个算法类中封装了大量查找算法，该类代码将较复杂，维护较为困难。 除了提供专门的查找算法类之外，还可以在客户端程序中直接包含算法代码，这种做法更不可取，将导致客户端程序庞大而且难以维护，如果存在大量可供选择的算法时，问题将变得更加严重。 为了解决这些问题，可以定义一些独立的类来封装不同的算法，每一个类封装一个具体的算法，在这里，每一个封装算法的类我们都可以称之为策略（Strategy），为了保证这些策略的一致性，一般会用一个抽象的策略类来做算法的定义，而具体每种算法则对应于一个具体策略类。  2 模式定义 #   策略模式（Strategy Pattern）是指定义一系列算法，将每一个算法封装起来，并让他们可以相互替换。 策略模式让算法独立于使用他的客户而变化，也称为政策模式（Policy Pattern）。  3 模式结构 #  策略模式包含如下角色：\n 「抽象策略（Strategy）」角色：是「具体策略」的父类，定义所有支持的算法的公共接口。 「具体策略（ConcreteStrategy）」角色：封装了具体的算法或行为。 「环境（Context）」角色：用一个「具体策略」来配置，维护一个对抽象策略对象的引用。   4 使用步骤 #  策略模式的使用主要包含四步：\n 创建「抽象策略」类，并定义所有支持的算法的公共接口。 创建「具体策略」类，并定义具体的算法实现。 创建「环境」类，通过构造方法，传入具体的策略参数，并根据具体的策略对象来调用其算法。 外界通过调用「环境」类的方法，通过传入不同参数来实例化不同的策略，从而得到不同的结果。   5 示例 #  下面将通过一个商场的收银系统为例进行说明，商场收银系统需要能够处理正常收费、商品打折和节假日满减等各种活动，这里正常收费、打折和满减是该系统需要实现的具体算法，他们都共用一个收费接口。\n5.1 创建抽象策略类 #  创建 CashStrategy 的抽象策略类，并定义所支持的算法的公共接口 acceptCash()：\nabstract class CashStrategy { /** * 收费 * @param money 金额 * @return 最终收费金额 */ public abstract double acceptCash(double money); } 5.2 创建具体策略类 #  创建具体策略类（CashNormalStrategy、CashReturnStrategy、CashRebateStrategy 等），来继承 CashStrategy 的抽象策略类，并实现 acceptCash() 方法，来定义具体的算法：\npublic class CashNormalStrategy extends CashStrategy{ /** * 收费 * @param money 金额 * @return 最终收费金额 */ @Override public double acceptCash(double money) { System.out.println(String.format(\u0026#34;正常收费，原金额 %s.\u0026#34;, money)); return money; } } public class CashReturnStrategy extends CashStrategy{ private double moneyCondition = 0.0; private double moneyReturn = 0.0; public CashReturnStrategy(final double moneyCondition, final double moneyReturn) { this.moneyCondition = moneyCondition; this.moneyReturn = moneyReturn; } /** * 收费 * @param money 金额 * @return 最终收费金额 */ @Override public double acceptCash(double money) { double res = money; if (money \u0026gt;= moneyCondition) { res = money - Math.floor(money / moneyCondition) * moneyReturn; } System.out.println(String.format(\u0026#34;满 %s 返 %s，原金额 %s.\u0026#34;, moneyCondition, moneyReturn, money)); return res; } } public class CashRebateStrategy extends CashStrategy{ private double moneyRebate = 1.0; public CashRebateStrategy(final double moneyRebate) { this.moneyRebate = moneyRebate; } /** * 收费 * @param money 金额 * @return 最终收费金额 */ @Override public double acceptCash(double money) { double res = money * moneyRebate; System.out.println(String.format(\u0026#34;打折，折扣比率为 %s，原金额 %s.\u0026#34;, moneyRebate, money)); return res; } } 5.3 创建环境类 #  通过构造方法，传入具体的收费策略，并根据具体的策略对象来调用其算法，得到不同的计算结果：\npublic class CashContext { private CashStrategy cashStrategy; public CashContext(String type) { switch (type) { case \u0026#34;0\u0026#34;: cashStrategy = new CashNormalStrategy();break; case \u0026#34;满百返百\u0026#34;: cashStrategy = new CashReturnStrategy(300.0, 100.0); break; case \u0026#34;8 折\u0026#34;: cashStrategy = new CashRebateStrategy(0.8); break; } } /** * 获取最终的收费金额 * @param money 原始金额 * @return 最终的收费金额 */ public double getResult(double money) { return cashStrategy.acceptCash(money); } } 5.4 调用环境类的方法 #  在主函数中（客户端），只需调用环境类，传入收费类型，就可以得到收费的结果：\npublic class Client { public static void main(String[] args) throws InterruptedException { Scanner scanner = new Scanner(System.in); String type = null; double money = 0.0, res = 0.0; System.out.println(\u0026#34;Please input the discount(0/满百返百/8 折/结束): \u0026#34;); type = scanner.nextLine(); while (!type.equals(\u0026#34;结束\u0026#34;)) { CashContext cashContext = new CashContext(type); System.out.println(\u0026#34;Please input money: \u0026#34;); money = scanner.nextDouble(); res = cashContext.getResult(money); System.out.println(String.format(\u0026#34;最终消费金额为 %s\u0026#34;, res)); System.out.println(\u0026#34;Please input the discount(0/满百返百/8 折/结束): \u0026#34;); type = scanner.nextLine(); type = scanner.nextLine(); } } } 6 优缺点 #  6.1 优点 #   策略模式提供了对开闭原则的完美支持，用户可以在不修改原有系统的基础上选择算法或行为，也可以灵活地增加新的算法或行为。 策略模式提供了管理相关的算法族的办法。 策略模式提供了可以替换继承关系的办法。 使用策略模式可以避免使用多重条件转移语句。  6.2 缺点 #   客户端必须知道所有的策略类，并自行决定使用哪一个策略类。 策略模式将造成产生很多策略类，可以通过享元模式在一定程度上减少对象的数量。  7 适用环境 #  在以下情况下可以使用策略模式：\n 如果在一个系统里面有很多类，他们之间的区别仅在于他们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 一个系统需要动态地在几种算法中选择一种。 如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 不希望客户端知道复杂的、与算法相关的数据结构，在具体策略类中封装算法和相关的数据结构，提高算法的保密性与安全性。  参考文献 #    设计模式面试题（总结最全面的面试题！！！）。  5. 策略模式。  策略模式。  大话设计模式 ｜ 2. 策略模式。  "},{"id":166,"href":"/school-recruitment/docs/java/1Java%E5%9F%BA%E7%A1%80/1.1-StringStringBuffer%E5%92%8CStringBuilder%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"1.1 String、 String Buffer和 String Builder的区别","section":"1、 Java基础","content":"String、StringBuffer和StringBuilder的区别 #  1 各自的特点 #  1.1 String #    在 Java中字符串属于对象，Java提供了 String类来创建和操作字符串。\n  String的值是不可变的，这就导致每次对 String的操作都会生成新的 String对象，这样不仅效率低下，而且大量浪费有限的内存空间，下图是对 String操作时内存变化的图：\n  我们可以看到，初始String值是hello，然后在这个字符串后面加上新的字符串world，这个过程是需要在堆内存中开辟内存空间的，最终得到了hello world字符串也需要开辟相应的内存空间。 这样短短的两个字符串，却要开辟三次内存空间，这是对内存空间的极大浪费。    1.2 StringBuffer和StringBuilder #    StringBuffer始于Java 1.0，StringBuilder始于Java 1.5。 StringBuffer的所有方法都是同步（Synchronized）的，因此它是线程安全的，这就导致相比于StringBuilder，它在性能上会差一点；StringBuilder的方法不是同步的（Non-Synchronized），因此它是线程不安全的，因此相比于StringBuffer，他在性能上会好一点。 由于StringBuilder相较于StringBuffer有速度优势，所以大多数情况下建议使用 StringBuilder类，然而在应用程序要求线程安全的情况下，则必须使用 StringBuffer类。  2 三者的区别 #    字符串修改上的区别：\n String： 不可变字符串。 StringBuffer： 可变字符串，效率低，线程安全。 StringBuilder： 可变字符串，效率高，线程不安全。    初始化上的区别：\n String： 可以赋空值。 StringBuffer和StringBuilder： 不能赋空值。  ①String String s = null; String s = “abc”; ②StringBuffer StringBuffer s = null; //结果警告：Null pointer access: The variable result can only be null at this location StringBuffer s = new StringBuffer();//StringBuffer对象是一个空的对象 StringBuffer s = new StringBuffer(“abc”);//创建带有内容的StringBuffer对象,对象的内容就是字符串”   3 建议 #   如果要操作少量的数据用String。 多线程字符串缓冲区下操作大量数据用StringBuffer。 单线程字符串缓冲区下操作大量数据用StringBuilder。  4 参考文献 #    String vs StringBuffer vs StringBuilder。  图析:String,StringBuffer与StringBuilder的区别。  "},{"id":167,"href":"/school-recruitment/docs/java/1Java%E5%9F%BA%E7%A1%80/1.2-HashMap%E5%92%8CConcurrentHashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"1.2 Hash Map和 Concurrent Hash Map实现原理","section":"1、 Java基础","content":"HashMap和ConcurrentHashMap实现原理 #  1 HashMap #  1.1 简介 #   Java 为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、HashTable、LinkedHashMap和TreeMap，类继承关系如下图所示：  下面针对各个实现类的特点做一些说明：  HashMap：  根据键的 hashCode 值存储数据，大多数情况下可以直接定位到他的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 最多只允许一条记录的键为 null，允许多条记录为 null。 非线程安全，任一时刻可以有多个线程同时写 HashMap，可能会导致数据的不一致。 如果需要满足线程安全，可以用 Collections 的 synchronizedMap() 方法（该方法也是使用一个全局锁来同步多个线程间的并发访问）使 HashMap 具有线程安全能力，或者使用 ConcurrentHashMap。   HashTable：  HashTable 是遗留类，很多映射的常用功能与 HashMap 类似，不同的是他继承自 Dictionary 类，并且是线程安全的：  HashTable线程安全的策略实现代价比较大，简单粗暴，get/put所有相关的操作都是 synchronized 的，这相当于给整个哈希表加了一把大锁，多线程访问的时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞，相当于将所有的操作串行化，在竞争激烈的并发场景中性能就会非常差。    HashTable 不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换，因为ConcurrentHashMap 采用了分段锁，并发性比 HashTable 要高很多，具体可参考 2.1 实现原理。    LinkedHashMap：  LinkedHashMap 是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。   TreeMap：  TreeMap 实现了 SortedMap 接口，能够把保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的，如果使用排序的映射，建议使用 TreeMap。 在使用 TreeMap 时，key必须实现 Comparable 接口，或者在构造 TreeMap 传入自定义的 Comparator，否则会在运行时抛出 java.lang.ClassCastException 类型的异常。     对于上述四种 Map 类型的类，要求映射中的 key 是不可变对象，即该对象在创建后他的哈希值不会被改变，如果对象的哈希值发生变化，Map 对象很可能就定位不到映射的位置了。  1.2 内部实现 #  1.2.1 存储结构 #    从结构实现来讲，HashMap 是数组+链表+红黑树（JDK1.8 增加了红黑树部分）实现的，如下图所示：   从源码可知，HashMap 类中有一个非常重要的字段，就是 Node\u0026lt;K,V\u0026gt;[] table;，即哈希桶数组，他是一个Node 数组，Node 是HashMap 的一个内部类，实现了 Map.Entry 接口，本质就是一个映射（键值对），上图中的每个黑色圆点就是一个 Node 对象，Node 的源码如下：\nstatic class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash; final K key; V value; Node\u0026lt;K,V\u0026gt; next; Node(int hash, K key, V value, Node\u0026lt;K,V\u0026gt; next) {...} public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \u0026#34;=\u0026#34; + value; } public final int hashCode() {...} public final V setValue(V newValue) {...} public final boolean equals(Object o) {...} }   HashMap 就是使用哈希表来存储的，哈希表为解决冲突，可以采用开放地址法和链地址法，Java 中HashMap 采用了链地址法，即数组加链表的结合，在每个数组元素上都有一个链表结构，当数据被 Hash 后，得到数组下标，把数据放在对应下标元素的链表上。\n  HashMap 中其他几个字段如下：\nint threshold; // 所能容纳的 key-value 对极限 final float loadFactor; // 负载因子 int modCount; int size;  threshold：  HashMap所能容纳的最大数据量的 Node（键值对）的个数，计算公式为 $threshold = length * loadFactor$，其中 length为 table 的长度，默认为 16，而且length的大小必须为 2 的 $n$ 次方，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap 定位哈希桶索引位置时，也加入了高位参与运算的过程。 当 HashMap 中键值对的数量超过这个数目就需要重新扩容，扩容后的 HashMap 容量是之前容量的两倍。   loadFactor：  负载因子，默认为 0.75，该值是对空间和时间效率的一个平衡选择，建议不要修改。   modCount：  主要用来记录 HashMap 内部结构发生变化的次数，主要用于迭代的快速失败，其中内部结构发生变化指的是结构发生变化，例如put 新键值对，但是某个key 对应的value 值被覆盖不属于结构变化。   size：  HashMap 中实际存在的键值对数量。      即使负载因子和 Hash 算法设计的再合理，也免不了出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能，于是，在 JDK1.8 版本中，对数据结构做了进一步优化，引入了红黑树，当链表长度太长（默认超过 8）时，链表就转化为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能。\n  1.2.2 功能实现 #  HashMap 的内部功能实现很多，本文主要从根据 key 获取哈希桶数组索引位置、put方法的执行原理、扩容机制三个具有代表性的点深入展开讲解。\n1.2.2.1 确定哈希桶数组索引位置 #    HashMap 中使用 Hash 算法确定哈希桶数组索引位置，具体源码如下：\npublic V get(Object key) { Node\u0026lt;K,V\u0026gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } // 方法一： static final int hash(Object key) { // jdk1.8 \u0026amp; jdk1.7  int h; // h = key.hashCode() 1. 取 hashCode 值  // h ^ (h \u0026gt;\u0026gt;\u0026gt; 16) 2. 高位参与运算  return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } // 方法二： static int indexFor(int h, int length) { // jdk1.7 的源码，jdk1.8 没有这个方法，但是实现原理一样的  return h \u0026amp; (length-1); // 3. 取模运算 }   这里的 Hash 算法本质上就是三步，分比为取 key 的 hashCode 值、高位运算、取模运算。\n  对于任意给定的对象，只要他的 hashCode 返回值相同，那么程序调用方法一所计算得到的 Hash 码值总是相同的。\n  对于索引位置的确定，我们首先想到的是把 hash 值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的，但是模运算的消耗是比较大的，在 HashMap 中是通过调用方法二来计算该对象应该保存在 table 数组的哪个索引处，这个方法非常巧妙，他通过 h \u0026amp; (length - 1) 来得到该对象的保存位，而HashMap 底层数组的长度总是 2 的 $n$ 次方，此时 h \u0026amp; (length - 1)运算等价于对 length 取模，也就是 h % length，但是 \u0026amp;比 % 具有更高的效率。\n  在 JDK 1.8 的实现中，优化了高位运算的算法，通过 hashCode 的高 16 为异或低 16 位实现的，即 (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16)，这样做可以在数组 table 的长度比较小的时候，也能保证到高低 bit 都参与到 Hash 的计算中，同时不会有太大的开销，具体示例如下：\n   1.3.2.2 put 方法的执行原理 #    HashMap 的 put 方法执行过程如下图所示：  判断键值对数组 table[i] 是否为空或 null，如果是的话，则对 HashMap 进行扩容。 根据键值 key 计算 hash 值得到插入的数组索引 i：  如果 table[i] == null，直接新建节点添加，然后转向 6。 否则，转向 3。   判断 table[i] 的首个元素是否和 key 一样：  如果相同（hashCode 和equals 值均相同），直接覆盖 value。 否则，转向 4。   判断 table[i] 是否为 TreeNode，即红黑树：  如果是红黑树，则直接在树中插入键值对。 否则，转向 5。   遍历 table[i]，判断链表长度是否大于 8：  如果大于 8 的话，则把链表转换为红黑树，然后在红黑树中执行插入操作。 否则，进行链表的插入操作，遍历过程中，如果发现 key 已经存在，则直接覆盖 value 即可。   插入成功后，判断实际存在的键值对数量size是否超过了最大容量threshold：  如果超过了，则进行扩容。      JDK 1.8 中 HashMap 的 put 方法源码如下：\npublic V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // 1. 判断键值对数组 table 是否为空或者 table 的长度是否为 0  if ((tab = table) == null || (n = tab.length) == 0) // 如果是，则对 HashMap 进行扩容  n = (tab = resize()).length; // 2. 根据键值 key 计算 hash 值得到插入的数组索引 i，判断 table[i] 是否为空  if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) // 如果 table[i] 为空，则创建新的节点  tab[i] = newNode(hash, key, value, null); else { Node\u0026lt;K,V\u0026gt; e; K k; // 3. 判断 table[i] 的首个元素是否和 key 一样（hashCode 和 equals 值均相同）  if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) // 如果一样，则直接覆盖 value  e = p; // 4. 判断 table[i] 是否为红黑树  else if (p instanceof TreeNode) // 如果是红黑树，则直接在树中插入键值对  e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); else { // 5. 遍历 table[i]  for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 判断链表长度是否大于 8  if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st  // 如果链表长度大于 8，则把链表转换为红黑树，在红黑树中执行插入操作  treeifyBin(tab, hash); break; } // 进行链表的插入操作，如果 key 已经存在，则直接覆盖 value 即可  if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key  V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 6. 插入成功之后，判断实际存在的键值对数量 size 是否超过了最大容量 threshold，如果超过，则进行扩容  if (++size \u0026gt; threshold) resize(); afterNodeInsertion(evict); return null; }   1.2.2.3 扩容机制 #    扩容就是重新计算容量，当向 HashMap 对象中不同的添加元素，而HashMap 对象内部的数组无法承载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素，但是Java 里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。\n  下面我们分析下 resize 的源码，因为 JDK 1.8 融入了红黑树，较复杂，为了便于理解，我们仍使用 JDK 1.7 的源码，这样好理解一些，本质上区别不大，具体区别后文再说：\nvoid resize(int newCapacity) { /*传入新的容量*/ Entry[] oldTable = table; /*引用扩容前的 Entry 数组*/ int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { /*扩容前的数组大小如果已经达到最大（2^30）了*/ threshold = Integer.MAX_VALUE; /*修改阈值为 int 的最大值（2^31 - 1），这样以后就不会扩容了*/ return; } Entry[] newTable = new Entry[newCapacity]; /*初始化一个新的 Entry 数组*/ boolean oldAltHashing = useAltHashing; useAltHashing |= sun.misc.VM.isBooted() \u0026amp;\u0026amp; (newCapacity \u0026gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); boolean rehash = oldAltHashing ^ useAltHashing; transfer(newTable, rehash); /*将数据转移到新的 Entry 数组里*/ table = newTable; /*HashMap 的 table 属性引用新的 Entry 数组*/ threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); /*修改阈值*/ } 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有 Entry 数组的元素拷贝到新的 Entry 数组里，具体源码如下：\n5void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry\u0026lt;K,V\u0026gt; e : table) { /*遍历旧的 Entry 数组*/ while(null != e) { Entry\u0026lt;K,V\u0026gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); /*重新计算每个元素在数组中的位置*/ e.next = newTable[i]; /*标记*/ newTable[i] = e; /*将元素放在数组上*/ e = next; /*访问下一个 Entry 链上的元素*/ } } } newTable[i] 的引用赋给了 e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置，这样先放在一个索引上的元素终会被放到 Entry 链的尾部（如果发生了 hash 冲突的话），这一点和 JDK 1.8 有区别，下文讲解，在旧数组中同一条 Entry 链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。\n  下面举个例子说明下扩容过程，假设我们的 hash 算法就是简单的用 key 去 mod 一下表的大小（也就是数组的长度），负载因子 loadFactor 为 1，即当键值对的实际大小 size 大于 table 的实际大小时进行扩容，具体的过程如下图所示：\n   下面我们讲解下 JDK 1.8 做了哪些优化：\n  经过观测可以发现，我们使用的是2 次幂的扩展（指长度扩展为原来的 2 倍），所以，元素的位置要么是在原位置，要么是在原位置再移动 2 次幂的位置，看下图就可以明白这个意思，$n$ 为 table 的长度，图（a）表示扩容前的 key1 和 key2 两种 key 确定索引位置的示例，图（b）表示扩容后 key1 和 key2 两种 key 确定索引位置的示例，其中 hash1 是 key1 对应的哈希与高位运算结果：   元素在重新计算 hash 之后，因为 $n$变为 2 倍，那么 $n-1$的 mask 范围在高位多 1bit（红色），因此，新的 index 就会发生这样的变化：   因此，我们在扩充 HashMap 的时候，不需要像 JDK 1.7 那样重新计算 hash，只需要看看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，如果是 0 的话，索引没变，如果是 1 的话，索引变成原索引 + 扩容前 HashMap 的容量，下图为 16 扩充为 32 的扩容示意图：   这个设计确实非常巧妙，既省去了重新计算 hash 值的时间，而且同时，由于新增的 1bit 是 0 还是 1 可以认为是随机的，因此扩容的过程中，均匀的把之前冲突的节点分散到新的桶中了，这一块就是 JDK 1.8 新增的优化点。\n  有一点需要注意的是，JDK 1.7 中 rehash 的时候，如果在新表的数组的索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK 1.8 不会倒置。\n 为什么 HashMap 的数组长度一定是 2 的次幂？\n 这样可以保证扩容后与原容量相比只有一位差异，例如：  假设初始 HashMap 的长度 $length$ 为 16，用二进制表示为 10000，则 $length - 1$ 为 15，用二进制表示为 01111。 扩容后数组的长度 $newLength$ 为 32，用二进制表示为 100000，则 $newLength - 1$ 为 011111。 这样可以保证最低位全为 1，而且扩容后只有一位差异，也就是最左位的 1。   这样既省去了重新计算 hash 的时间，同时可以把之前冲突的节点均匀的分散到新的桶中，具体的原因可以参考 1.3.2.3 扩容机制中第 4 点的描述。        1.2.2.4 线程安全性 #   在多线程使用场景中，应该尽量避免使用线程不安全的 HashMap，而使用线程安全的 ConcurrentHashMap，因为：  HashMap 不是线程安全的，在多线程并发的情况下容易发生线程安全问题，在进行扩容时也会出现死循环问题，例如在并发的情况下，线程进行了put 操作，由于某种情况随后sleep 了两秒，在这两秒期间，线程 2 修改了线程 1 之前put 的值，等到线程 1 结束sleep 后再次get 到原来值的时候，就有可能取到的值已经不是原来的值了，就会存在问题。 在并发的多线程使用场景中使用 HashMap 可能会在扩容的时候形成环状链表，导致 get 操作时 CPU 空转。    1.3 HashMap 在 JDK 1.7 和 JDK 1.8 之间的区别 #   数据结构：  JDK 1.7 的底层结构是数组 + 链表。 JDK 1.8 的底层结构是数组 + 链表 + 红黑树，当单条链表的长度大于 8 时，将链表转换为红黑树，然后插入键值对。   扩容机制：  JDK 1.7 为先判断是否需要扩容，再插入。 JDK 1.8 为先插入，再判断是否需要扩容。   节点插入：  JDK 1.7 为头插法，存在多线程成环的问题。 JDK 1.8 为尾插法，存在数据丢失问题。    2 ConcurrentHashMap #  2.1 实现原理 #   如无特殊说明，下面关于 ConcurrentHashMap 原理的分析是基于 JDK 1.7 的。\n 2.1.1 Segment #    ConcurrentHashMap 采用了非常精妙的分段锁策略，COncurrentHashMap 的主干是个 Segment 数组：\nfinal Segment\u0026lt;K,V\u0026gt;[] segments;  分段锁的优缺点是什么？\n 优点：  保证在操作不同段Map的时候可以并发执行，操作同段Map的时候，进行锁的竞争和等待，这相对于对整个Map同步（Synchronized）是有优势的。   缺点：  分段锁每个锁控制的是一段，当每个Segment越来越大时，锁的粒度就变得有些大了。 分成很多段会比较浪费内存空间（不连续，碎片化）。 操作Map时竞争同一个分段锁的概率非常小，分段锁反而会造成更新操作的长时间等待。       Segment继承了 ReentrantLock，所以他就是一种 可重入锁（ReentrantLock），同时，一个 Segment 就是一个子哈希表（一些属性和 HashMap差不多，例如负载因子、阈值），Segment 里维护了一个 HashEntry 数组，并发环境下，对于不同 Segment 的数据进行操作是不用考虑锁竞争的，所以对于同一个 Segment 的操作才需要考虑线程同步，不同的 Segment 则无需考虑。\ntransient volatile HashEntry\u0026lt;K,V\u0026gt;[] table; /*链表数组，使用 volatile 修饰，保证可见性*/ transient int count; /*Segment 中元素的个数*/ transient int modCount; /*Segment 中可变操作的总次数*/ transient int threshold; /*阈值，当 Segment 的大小超过此阈值时，将对其进行 rehash 操作*/ final float loadFactor; /*负载因子，默认为 0.75*/ Segment(float lf, int threshold, HashEntry\u0026lt;K,V\u0026gt;[] tab) { this.loadFactor = lf; this.threshold = threshold; this.table = tab; }   2.1.2 构造方法 #    我们来看一下 ConcurrentHashMap 的构造方法：\npublic ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor \u0026gt; 0) || initialCapacity \u0026lt; 0 || concurrencyLevel \u0026lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel \u0026gt; MAX_SEGMENTS) /*MAX_SEGMENTS 为 1 \u0026lt;\u0026lt; 16 = 65536，即最大并发数为 65536*/ concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments  int sshift = 0; /*2 的 sshift 次方等于 ssize，例如 ssize = 16，ssift = 4；ssize = 32，sshit = 5*/ int ssize = 1; /*ssize 为 segments 数组的长度，根据 concurrentLevel 计算得出*/ while (ssize \u0026lt; concurrencyLevel) { ++sshift; ssize \u0026lt;\u0026lt;= 1; } this.segmentShift = 32 - sshift; /*segmentShift 和 segmentMask 这两个变量在定位 segment 时会用到，后面会详细讲*/ this.segmentMask = ssize - 1; if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; /*计算 cap 的大小，即 segment 中 HashEntry 的数组长度，cap 也一定为 2 的 n 次方*/ if (c * ssize \u0026lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap \u0026lt; c) cap \u0026lt;\u0026lt;= 1; // create segments and segments[0]  Segment\u0026lt;K,V\u0026gt; s0 = /*创建 segments 数组并初始化第一个 Segment，其余的 Segment 延迟初始化*/ new Segment\u0026lt;K,V\u0026gt;(loadFactor, (int)(cap * loadFactor), (HashEntry\u0026lt;K,V\u0026gt;[])new HashEntry[cap]); Segment\u0026lt;K,V\u0026gt;[] ss = (Segment\u0026lt;K,V\u0026gt;[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0]  this.segments = ss; }   初始化方法有三个参数，分别为 initialCapacity（默认为 16）、loadFactor（默认为 0.75）、concurrencyLevel（默认为 16），如果用户不指定，则会使用默认值。\n  Segment 数组的大小 ssize 是由 concurrencyLevel 决定，为大于或等于 concurrencyLevel 的最小的 2 的次幂，例如，默认情况下 concurrencyLevel 为 16，则 ssize 为 16；若 concurrencyLevel 为 14，则 ssize 为 16；若 concurrencyLevel 为 17，则 ssize 为 32。\n 为什么 Segment 的数组大小一定是 2 的次幂？\n 主要是便于通过按位与的散列算法来定位 Segment 的 Index，具体可参考 1.2.2.3 扩容机制。       2.1.3 get 方法 #    ConcurrentHashMap 的 get 方法的源码如下：\npublic V get(Object key) { Segment\u0026lt;K,V\u0026gt; s; // manually integrate access methods to reduce overhead  HashEntry\u0026lt;K,V\u0026gt;[] tab; int h = hash(key); long u = (((h \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask) \u0026lt;\u0026lt; SSHIFT) + SBASE; if ((s = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObjectVolatile(segments, u)) != null \u0026amp;\u0026amp; /*先定位到 Segment，再定位到 HashEntry*/ (tab = s.table) != null) { for (HashEntry\u0026lt;K,V\u0026gt; e = (HashEntry\u0026lt;K,V\u0026gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) \u0026amp; h)) \u0026lt;\u0026lt; TSHIFT) + TBASE); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == h \u0026amp;\u0026amp; key.equals(k))) return e.value; } } return null; }   get 方法无需加锁，由于其中涉及到的共享变量都使用 volatile修饰，volatile可以保证内存可见性，所以不会读取到过期的数据。\n  2.1.4 put 方法 #    ConcurrentHashMap 的 put 方法的源码如下：\npublic V put(K key, V value) { Segment\u0026lt;K,V\u0026gt; s; if (value == null) /*这里的 value 不能为空*/ throw new NullPointerException(); int hash = hash(key); /*计算 key 的 hash 值*/ int j = (hash \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask; /*根据 hash 值计算待插入对象在 segments 数组中的位置*/ if ((s = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObject /*检查当前数组中指定位置的 Segment 是否为空，如果为空，则先初始化 Segment 再进行 put，如果不为空，则直接执行 put 操作*/ (segments, (j \u0026lt;\u0026lt; SSHIFT) + SBASE)) == null) s = ensureSegment(j); return s.put(key, hash, value, false); }   SegmentShift 和 SegmentMask 主要用来定位 Segment，其中 int j = (hash \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask;：\n segmentShift：  2 的sshift 次方等于ssize，segmentShift = 32 - sshift，若segments 的长度为 16，，则segmentShift = 32 - 4 = 28。 计算得出的hash 值最大为 32 位，无符号右移segmentShift，则意味着只保留高几位（其余位是没用的），然后与段掩码进行与运算来定位 Segment。   segmentMask：  段掩码，等于segments 的长度减 1，即segmentMask = ssize - 1;，若segments 长度为 16，则segmentMask = 16 - 1 = 15。      从源码可以看出，put 的主要逻辑也就两步：\n  定位 Segment，并确保定位的 Segment 已经初始化。\n  调用 Segment 的 put 方法，Segment 的 put 方法源码如下：\nfinal V put(K key, int hash, V value, boolean onlyIfAbsent) { HashEntry\u0026lt;K,V\u0026gt; node = tryLock() ? null : /*先获取锁，获取到则返回 null，否则执行 scanAndLockForPut*/ scanAndLockForPut(key, hash, value); V oldValue; try { HashEntry\u0026lt;K,V\u0026gt;[] tab = table; /*此 table 被 volatile 修饰*/ int index = (tab.length - 1) \u0026amp; hash; /*计算在 HashEntry[] 中的位置*/ HashEntry\u0026lt;K,V\u0026gt; first = entryAt(tab, index); /*找到 HashEntry[] 中指定位置的第一个节点，即 first 指向桶中链表的第一个节点*/ for (HashEntry\u0026lt;K,V\u0026gt; e = first;;) { if (e != null) { /*如果当前节点不为空，则遍历该链表*/ K k; if ((k = e.key) == key || /*如果之前已经存在了该 key，则用新值替换旧值*/ (e.hash == hash \u0026amp;\u0026amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; /*来到下一个节点*/ } else { /*如果当前节点为空，则进入 else*/ if (node != null) node.setNext(first); else node = new HashEntry\u0026lt;K,V\u0026gt;(hash, key, value, first); /*采用链表的头插法新建一个节点*/ int c = count + 1; /*键值对数量加 1*/ if (c \u0026gt; threshold \u0026amp;\u0026amp; tab.length \u0026lt; MAXIMUM_CAPACITY) /*如果超过阈值则扩容*/ rehash(node); else setEntryAt(tab, index, node); /*没有超过阈值的话，则放在指定的位置*/ ++modCount; count = c; oldValue = null; /*桶中不存在相同 key 的节点，所以返回 null*/ break; } } } finally { unlock(); /*解锁操作*/ } return oldValue; /*put 成功，则返回旧值*/ }   首先会尝试获取锁，如果获取失败则肯定有其他线程存在竞争，则利用 scanAndLockForPut 自旋获取锁：\nHashEntry\u0026lt;K,V\u0026gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); private HashEntry\u0026lt;K,V\u0026gt; scanAndLockForPut(K key, int hash, V value) { HashEntry\u0026lt;K,V\u0026gt; first = entryForHash(this, hash); /*通过 Segment 和 hash 寻找匹配的 HashEntry*/ HashEntry\u0026lt;K,V\u0026gt; e = first; HashEntry\u0026lt;K,V\u0026gt; node = null; int retries = -1; // 重试次数 negative while locating node  while (!tryLock()) { /*不断循环，尝试获取锁*/ HashEntry\u0026lt;K,V\u0026gt; f; // to recheck first below  if (retries \u0026lt; 0) { if (e == null) { /*之前表中不存在当前 key*/ if (node == null) // speculatively create node  node = new HashEntry\u0026lt;K,V\u0026gt;(hash, key, value, null); /*新增一个节点*/ retries = 0; /*将重试次数置为 0*/ } else if (key.equals(e.key)) retries = 0; else e = e.next; /*第一个节点也不是，则继续来到下一个节点*/ } else if (++retries \u0026gt; MAX_SCAN_RETRIES) { /*尝试次数大于了最大次数（64）的话，则改为阻塞式获取，保证能获取成功*/ lock(); break; } else if ((retries \u0026amp; 1) == 0 \u0026amp;\u0026amp; /*在 MAX_SCAN_RETRIES 次过程中，key 对应的 entry 发生了变化，则从头开始*/ (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed  retries = -1; } } return node; }  假如当前环境下没有任何线程进行put 操作，此时如果线程 1 进行了put 操作，他首先会去尝试获得锁，由于之前没有任何一个线程持有锁，所以线程 1 是可以执行到tryLock() 并返回null 的，即线程 1成功的拿到了锁，然后根据计算找到对应桶的位置，新添加一个键值对。 如果此时在线程 1 还没有释放锁的情况下，线程 2 又执行了 put 操作，则假如线程 2 恰好也定位到了和线程 1 同一个段，然后尝试去进行put 操作，即尝试获取锁，但是线程 1 还没有释放锁，所以线程 2 在一开始的时候会执行 scanAndLockForPut 方法。 线程 2 虽然没有得到锁，但是也没有闲着，而是将准备存放的键值对在对应数组中相应的位置给计算了出来，一旦线程 2 获取到了锁，那么就可以利用等待获取锁的这段时间所做的工作，直接定位到具体的位置，从而节省了时间，提高了执行效率。 如果线程 2 在等待锁的期间，线程 1 将 key 对应的 entry 进行了修改，则线程 2 需要重新确定接下来要定位的位置。    如果获取锁成功，则将当前的 Segment 中的 table 通过 key 的 hash 值定位到 HashEntry。\n  然后遍历该 HashEntry：\n 如果当前节点不为空，则判断传入的 key 和当前遍历到的 key 是否相等，如果相等，则覆盖旧的 value。 如果当前节点为空，则新建一个 HashEntry 并加入到 Segment 中，再判断是否需要扩容。    最后在 finally 中解除之前获得的锁。\n       2.2 JDK 1.8中的ConcurrentHashMap #  2.2.1 与JDK 1.7中的ConcurrentHashMap的区别 #    抛弃了JDK 1.7中的Segment锁分段技术，而是采用CAS + Synchronized的方式保证并发的安全性，体现在 put操作的不同，在JDK 1.8 中则是对数组中单个位置加锁。\n 为什么要使用CAS + Synchronized取代Segment + ReentrantLock？\n 分段锁具有相应的缺点，具体可参考 2.1.1 Segment。 使用ReentrantLock需要节点继承AQS来获得同步支持，锁定的是一整个段，增加内存开销，而JDK 1.8中加锁的对象是每个链表的头结点，也就是锁定的是冲突的链表，因此提高了并发度，同时降低了内存开销。 Synchronized是JVM直接支持的，JDK 1.8之后进行了许多优化，能够在运行期间调整锁的粒度，而不需要在开始就是用重量级锁操作。     将JDK 1.7中用于存放数据的内部类HashEntry替换成了内部类Node，但作用相同。\n  JDK 1.8中的结构转换为数组 + 链表 + 红黑树。\n  参考文献 #    Java 8 系列之重新认识 HashMap。  第三天：HashMap 为什么是线程不安全的。  ConcurrentHashMap 实现原理及源码分析。  HashMap 实现原理及源码分析。  HashMap 1.7 和 1.8 的区别 \u0026ndash;答到面试官怀疑人生。  Java 容器之 ConcurrentHashMap。  JAVA8的ConcurrentHashMap为什么放弃了分段锁，有什么问题吗，如果你来设计，你如何设计。  "},{"id":168,"href":"/school-recruitment/docs/java/1Java%E5%9F%BA%E7%A1%80/1.3-ArrayList%E5%92%8CLinkedList%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"1.3 Array List和 Linked List实现原理","section":"1、 Java基础","content":"ArrayList和LinkedList实现原理 #   如无特殊说明，下面内容的叙述基于的 JDK 版本为 JDK 1.8.0_181。\n 1 ArrayList #  1.1 概述 #   ArrayList 的底层数据结构是数组，虽然对于用户来说 ArrayList 是个动态的数组，但是实际上底层是个定长数组，只是在必要的时候，对底层的数组进行扩容，每次扩容 1.5 倍，但是扩容、删除都是有代价的，极端情况下，需要将大量的元素进行移位。 ArrayList不是线程安全的，只能在单线程环境下使用，多线程环境下可以考虑用 Collections.synchronizedList(List\u0026lt;T\u0026gt; list) 返回一个线程安全的 ArrayList 类，也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 ArrayList实现了 Serializable 接口，因此他支持序列化，能够通过序列化传输，实现了 RandomAccess 接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了 Cloneable 接口，能被克隆。  1.2 实现原理 #  1.2.1 构造函数 #  ArrayList 的构造函数一共有三种，分别是无参构造、传入一个整数、传入一个集合。\n1.2.1.1 无参构造 #    ArrayList 的无参构造函数源码如下：\ntransient Object[] elementData; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; }   通过源码我们可以看出，ArrayList 的底层是一个 Object[]，添加到 ArrayList 中的数据都保存在了 elementData 属性中。\n  当调用 new ArrayList\u0026lt;\u0026gt;() 时，将一个空数组 {} 赋值给了 elementData，这个时候集合的长度 size为默认长度 0。\n  1.2.1.2 整数有参构造 #    ArrayList 的整数有参构造函数源码如下：\ntransient Object[] elementData; private static final Object[] EMPTY_ELEMENTDATA = {}; public ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } }   通过源码可知，当调用 new ArrayList\u0026lt;\u0026gt;(capacity) 时，ArrayList 会根据传入的长度，创建一个大小为 capacity 的 Object 数组赋值给 elementData，如果 capacity为 0 的话，会将一个空数组赋值给 elementData。\n  1.2.1.3 集合有参构造 #    ArrayList 的集合有参构造函数源码如下：\ntransient Object[] elementData; private static final Object[] EMPTY_ELEMENTDATA = {}; public ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652)  if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array.  this.elementData = EMPTY_ELEMENTDATA; } } public static \u0026lt;T,U\u0026gt; T[] copyOf(U[] original, int newLength, Class\u0026lt;? extends T[]\u0026gt; newType) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; }   通过源码可知：\n 当传递一个实现了 Collections 接口的类后，会将传递的集合调用 toArray() 方法转化为数组，然后将其赋值给 elementData。 如果传入的集合类型和我们定义用来保存添加到集合中值的 Object[] 类型不一致（例如List\u0026lt;String\u0026gt; setList = new ArrayList\u0026lt;\u0026gt;(new HashSet());）时，会定义一个新的 Object[]，然后调用 Arrays.copyOf() 将原数组中的数据拷贝到新数组中，最后再把新数组赋值给 elementData。    1.2.2 常用方法 #  1.2.2.1 add(E element) #    add(E element) 的源码如下：\npublic boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!!  elementData[size++] = e; return true; }   这里 ensureCapacityInternal()的作用为保证在不停的往 ArrayList 插入数据时，数组不会越界，并且实现自动扩容，源码如下：\n  private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); }  这里的 minCapacity 实际上就是在调用完当前这次 add 操作之后，数组中元素的数量，比如调用 add 之前，ArrayList 中有 3 个元素，那么此时这个 minCapacity 的值就为 4。\n  同时，可以看到将函数 calculateCapacity() 的返回值作为了 ensureExplicitCapacity() 的输入，calculateCapacity() 的功能为：\n 如果当前数组为空，则直接返回数组默认长度（10）和 minCapacity 的最大长度。 如果当前数组不为空，则直接返回 minCapacity。  源码如下：\nprivate static final int DEFAULT_CAPACITY = 10; private static int calculateCapacity(Object[] elementData, int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; }   接下来是 ensureExplicitCapacity()，源码如下：\nprivate void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code  if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); }  modCount表示该 ArrayList 被更改过多少次，这里的更改不只是新增，删除也是一种更改。 通过上面的了解我们知道：   如果添加一个元素后数组内的元素个数是小于等于数组长度的，则 minCapacity的值一定小于 elementData 的长度。\n  如果添加一个元素后数组内的元素个数是大于数组长度的，则 minCapacity的值一定大于 elementData 的长度，此时就会调用 grow() 函数来进行数组扩容，源码如下：\nprivate static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; private void grow(int minCapacity) { // overflow-conscious code  int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win:  elementData = Arrays.copyOf(elementData, newCapacity); }   扩容的核心逻辑很简单，即新的数组长度 = 旧数组长度 + 旧数组长度 / 2，即ArrayList 的扩容是每次扩容 1.5 倍。\n  这里可能会有一个疑问，因为上文提到扩容时 minCapacity 的值和数组长度应该是相等的，所以新数组长度减去 minCapacity 应该永远大于 0才对，为什么会有小于 0 的情况，这是因为 addAll()底层也会调用 ensureCapacityInternal()：\n add()是往数组中添加单个元素，而addAll() 则是往数组中添加整个数组。 例如我们传入的数组元素有 20 个，因为 ArrayList 的默认数组长度为 10，扩容一次之后长度为 15，因此即使扩容一次还是不足以放下所有的元素。 所以这时才会出现newCapacity（扩容之后的数组长度）小于 minCapacity（执行完当前操作之后的数组内元素数量）的情况。    如果扩容后的长度大于 MAX_ARRAY_SIZE，就会调用 hugeCapacity() 函数，源码如下：\nprivate static int hugeCapacity(int minCapacity) { if (minCapacity \u0026lt; 0) // overflow  throw new OutOfMemoryError(); return (minCapacity \u0026gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; }  如果扩容后的长度溢出了，则直接抛出 OOM 异常。 否则，保证其容量不会超过 Integer.MAX_VALUE。    最后是真正执行扩容的操作，调用了 java.util 包里的 Arrays.copyOf() 方法。\n        数组扩容完成后，就会将本次添加的元素写入 elementData 的末尾，即 elementData[size++] = e。\n   add() 方法可能会导致线程不安全：\n  多个线程进行 add 操作时可能会导致 elementData 数组越界：\n 假如 ArrayList 的大小为 9，即size = 9。 线程 A 开始进入add() 方法，这时他获得的size 的值为 9，调用ensureCapacityInternal() 方法进行容量判断。 线程 B 此时也进入add() 方法，他获得的size 值也为 9，也开始调用ensureCapacityInternal() 方法进行容量判断。 线程 A 发现需求大小为 10，而elementData 的大小就为 10，可以容纳，于是他不再扩容，返回。 线程 B 也发现需求大小为 10，也可以容纳，返回。 线程 A 开始进行设置值的操作，即elementData[size++] = e，此时size 变为 10。 线程 B 也开始进行设置值的操作，他尝试设置elementData[size++] = e，而elementData 没有进行过扩容，他的下标最大为 9，此时就会报出一个数组越界的异常。    多线程进行 elementData[size++] = e 操作时会导致线程不安全，因为 elementData[size++] = e 不是一个原子操作，他由如下两步操作构成：\nelementData[size] = e; size = size + 1; 在单线程执行着两条代码时没有任何问题，但是当多线程环境下执行时，可能会发生一个线程的值覆盖另一个线程的值：\n 假如 ArrayList 的大小为 0，即size = 0。 线程 A 开始添加一个元素，值为a，此时他执行第一条操作，将a 放在了elementData 下标为 0 的位置上。 接着线程 B 刚好也要开始添加一个值为b 的元素，且走到了第一步操作，此时线程 B 获取到size 的值依然为 0，于是他将b 也放在了elementData 下标为 0 的位置上。 线程 A 开始将size 的值增加为 1。 线程 B 开始将size 的值增加为 2。 这样线程 A、B 执行完毕后，理想情况为size 为 2，elementData 下标 0 的位置为a，下标1 的位置为b，而实际情况变成了size 为 2，elementData 下标为 0 的位置变成了b，下标 1 的位置上什么都没有，后续除非使用set 方法修改此位置的值，否则将一直为null，因为size 为 2，添加元素时会从下标为 2 的位置上开始。      2 LinkedList #  2.1 概述 #    LinkedList的类定义源码如下：\npublic class LinkedList\u0026lt;E\u0026gt; extends AbstractSequentialList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, Deque\u0026lt;E\u0026gt;, Cloneable, java.io.Serializable {}  LinkedList是一个继承于AbstractSequentialList的双向链表，可以被当做堆栈、队列或者双端队列进行操作。 LinkedList实现了List接口，能对他进行列表操作；实现了Deque接口，能当做双端队列使用；实现了Cloneable接口，能克隆；实现了java.io.Serializable接口，支持序列化，能通过序列化去传输。 LinkedList不是线程安全的，只能在单线程环境下使用，多线程环境下可以考虑用 Collections.synchronizedList(List\u0026lt;T\u0026gt; list) 返回一个线程安全的 LinkedList 类，也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。     LinkedList的优缺点如下：\n 优点：  添加和删除元素比较快，因为只是移动指针，并且不需要判断是否扩容。   缺点：  查询和遍历效率比较低。      LinkedList的结构图如下：   2.2 实现原理 #  2.2.1 整体结构 #    LinkedList类就包括三个属性，具体源码如下：\ntransient int size = 0; transient Node\u0026lt;E\u0026gt; first; transient Node\u0026lt;E\u0026gt; last;  size：用来记录双向链表的大小。 first：用来指向链表的头。 last：用来指向链表的尾。    其中 Node是内部类，表示LinkedList的每一个数据节点，具体源码如下：\nprivate static class Node\u0026lt;E\u0026gt; { E item; Node\u0026lt;E\u0026gt; next; Node\u0026lt;E\u0026gt; prev; Node(Node\u0026lt;E\u0026gt; prev, E element, Node\u0026lt;E\u0026gt; next) { this.item = element; this.next = next; this.prev = prev; } }  item：表示数据本身。 next：表示指向下一个节点的指针。 prev：表示指向上一个节点的指针。    LinkedList的构造方法主要有两种，一种是无参构造，另一种是有参构造，即调用 addAll()方法通过集合来构造LinkedList，具体源码如下：\npublic LinkedList() { } public LinkedList(Collection\u0026lt;? extends E\u0026gt; c) { this(); addAll(c); }   2.2.2 常用方法 #  2.2.2.1 add(E element) #    该方法是在LinkedList的尾部插入元素，因为有 last指向链表尾部，所以只需要简单修改几个相关引用即可，花费的时间是常数时间，具体源码如下：\npublic boolean add(E e) { linkLast(e); return true; } void linkLast(E e) { final Node\u0026lt;E\u0026gt; l = last; final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(l, e, null); last = newNode; if (l == null) first = newNode; /*原来链表为空，这是插入的第一个元素*/ else l.next = newNode; size++; modCount++; }   定义临时变量 l表示之前的 last。\n  调用Node的有参构造方法创建新的节点 newNode，其 prev指向 last，next为 null。\n  将 newNode赋值给 last。\n  将 l的  next指向 newNode。\n  将LinkedList的长度 size加1，同时将LinkedList的修改次数 modCount加1。\n     参考文献 #    Java 集合\u0026mdash;ArrayList 的实现原理。  Java 集合 ArrayList 原理及使用。  ArrayList 从源码角度剖析底层原理。  为什么说 ArrayList 是线程不安全的？  JDK8中LinkedList的工作原理剖析【我是攻城师】。  LinkedList 的实现原理浅析【OSC开源社区】。  深入LinkedList原理源码解析【与你同在架构之路】。  "},{"id":169,"href":"/school-recruitment/docs/java/1Java%E5%9F%BA%E7%A1%80/1.4-%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9D/","title":"1.4 深拷贝与浅拷贝","section":"1、 Java基础","content":"深拷贝与浅拷贝 #  1 数据类型 #  1.1 分类 #  Java 中的数据类型可以分为两大类，分别是基本类型和引用类型。\n1.1.1 基本类型 #  Java 中基本类型一共是四类八种，具体如下：\n 整型：byte、short、int、long。 浮点类型：float、double。 字符型：char。 逻辑型：boolean。  1.1.2 引用类型 #  除了上面四类八种基本类型外，其他的都是引用类型，包括数组。\n1.2 区别 #   基本类型的变量保存原始值，即他代表的值就是数值本身；而引用类型的变量保存引用值，引用值指向内存空间的地址，代表了某个对象的引用，而不是对象本身。 基本类型在声明时系统就给他分配了内存空间，引用类型在声明时只给变量分配了引用空间，而没有分配数据空间。 具体示例如下：   声明一个基本数据类型 int1 并赋值：\nint int1 = 100;    声明一个基本数据类型 int2，并赋值为 int1，对于基本数据类型来说，赋值（=号）就相当于拷贝了一份值，把 int1 的值 100，拷贝给 int2。：\nint int2 = int1;    将 int1 的值修改为 500：\nint1 = 500;    此时，分别打印 int1、int2 的值，会输出为 500、100。\n  然后，声明一个数组 arr1 并赋值，当执行到 new 这个关键字时，会在堆内存分配内存空间，并把该内存空间的地址赋值给 arr1：\nint[] arr1 = new int[]{1, 2, 3, 4, 5};    定义一个数组 arr2，并赋值为 arr1，因为 arr2初始化没有 new 关键字，所以并不会在堆内存里开辟一块空间，而是把 arr1 里存的堆内存地址直接赋值给了 arr2，对于引用类型来说，赋值（=号）就相当于拷贝了一份内存地址，也就是说 arr1和 arr2 现在指向了同一块堆内存：\nint[] arr2 = arr1;    然后将数组 arr1 下标位置为 3 的值修改为 8，虽然只是修改了 arr1 数组下标位置为 3 的值，但由于数组 arr1 和数组 arr2 指向同一块堆内存，因此当打印 arr1[3] 和 arr2[3] 的值时，二者都为 8：\narr1[3] = 8;    然后再看对象的初始化，定义一个类型为 Person 类的对象 per1 并将其实例化，其中 Person 类有两个属性，分别为 String 类型的 name、int 类型的 age，该对象实例化时，因为有 new，所以会在堆内存里开辟了一块内存空间：\npublic class Person { private int age; private String name; public Person(final int age, final String name) { this.age = age; this.name = name; } public int getAge() { return this.age; } public void setAge(final int age) { this.age = age; } public String getName() { return this.name; } public void setName(final String name) { this.name = name; } } Person per1 = new Person(\u0026#34;张三\u0026#34;, 21);    然后定义一个 Person 类型的对象 per2，并赋值为 per1，因为没有 new 关键字，所以 per2 不会在堆内存中开辟空间，和数组一样，也是把 per1 的内存地址直接赋值给了 per2：\nPerson per2 = per1;    然后将 per1 的 name 修改为 李四，age 修改为 35，给引用类型赋值是相当于引用重新指向一块堆内存，基本类型赋值是直接修改值：\nper1.setName(\u0026#34;李四\u0026#34;); per1.setAge(35);    此时，不管打印 per1 还是 per2 的 name、age，打印出来的结果都是 李四、35。\n    2 对象的拷贝分类 #  对象的拷贝主要有三种方式，分别为直接赋值、浅拷贝、深拷贝。\n2.1 直接赋值 #   用等号直接赋值是我们平时最常用的一种方式，它的特点就是直接引用等号右边的对象。 如果这些对象都是基本类型，当然没什么问题，但是如果都是引用类型，那么对一个对象的更改就会影响到另一个对象。 具体的示例如下可参考上面的 1.2 区别。  2.2 浅拷贝 #   浅拷贝是按位拷贝对象，他会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝：  如果属性是基本类型，拷贝的就是基本类型的值。 如果属性时引用类型，拷贝的就是内存地址，此时如果其中一个对象改变了这个地址，就会影响到另一个对象。   Java中使用浅拷贝需要类实现 Cloneable接口，然后重写 clone()方法。 具体的示例如下：   创建两个类，分别为 Person和 Friend，其中 Person实现了 Cloneable接口，并重写了 clone()方法（直接调用父类的 clone()方法）：\n@Data @AllArgsConstructor public class Friend { private String name; } @Data @AllArgsConstructor public class Person implements Cloneable{ private String name; private int age; private Friend friend; @Override protected Person clone() throws CloneNotSupportedException { return (Person)super.clone(); } }   然后进行测试：\npublic class ShallowCopyTest { public static void main(String[] args) throws CloneNotSupportedException { Person person1 = new Person(\u0026#34;张三\u0026#34;, 20, new Friend(\u0026#34;老王\u0026#34;)); Person person2 = person1.clone(); System.out.println(\u0026#34;person1: \u0026#34; + person1); System.out.println(\u0026#34;person2: \u0026#34; + person2 + \u0026#34;\\n\u0026#34;); person1.setName(\u0026#34;张四\u0026#34;); person1.setAge(25); person1.getFriend().setName(\u0026#34;小王\u0026#34;); System.out.println(\u0026#34;person1: \u0026#34; + person1); System.out.println(\u0026#34;person2: \u0026#34; + person2); } } 输出的结果为：\nperson1: Person(name=张三, age=20, friend=Friend(name=老王)) person2: Person(name=张三, age=20, friend=Friend(name=老王)) person1: Person(name=张四, age=25, friend=Friend(name=小王)) person2: Person(name=张三, age=20, friend=Friend(name=小王)) 可以看到 person1的值修改了之后，person2中的 name、age属性的值没有变，friend属性的值变了，这是因为：\n String类型为不可变对象，当需要修改不可变对象的值时，需要在内存中生成一个新的对象来存放新的值，然后将原来的引用指向新的地址。 我们修改了 person1对象的 name属性值，person1对象的 name字段指向了内存中新的 String对象，但是我们没有修改 person2对象的 name属性值，所以person2对象的 name字段还是指向内存中原来的 String地址。 同时，我们只修改了 person1对象的 age值，而没有修改 person2对象的 age值，因此person2对象的 age值不会发生变化。 我们修改了 friend对象的 name值，因此对象的 name值会指向新的 String对象，但是friend对象的地址并没有发生改变，而person1和 person2指向的是同一个 friend对象的地址，因此person1和 person2对象的 firend对象的 name值都会发生变化。        2.3 深拷贝 #   深拷贝是指无论是值类型还是引用类型都会完完全全的拷贝一份，在内存中生成一个新的对象，即把要复制的对象所引用的对象都复制一遍，拷贝对象和被拷贝对象没有任何关系，互不影响。 但是深拷贝相比于浅拷贝速度较慢并且花销较大。 深拷贝的实现方式有两种：  一种是在被引用的对象所在的类中实现 Cloneable接口，然后重写 clone()方法，接着在引用的类的 clone()方法中调用被引用类的 clone()方法，把被引用的类也复制一份。 将会被复制到的引用对象实现 Serializable接口，通过序列化的方式实现深拷贝，因为对象被序列化成流后，写在流里的是对象的一个拷贝，而原对象仍然存在虚拟机里面，探后通过反序列化就可以得到一个完全相同的拷贝。   具体的示例如下：   将 Person类和 Friend类都实现 Cloneable接口，然后重写 clone()方法：\n@Data @AllArgsConstructor public class Friend implements Cloneable{ private String name; @Override protected Friend clone() throws CloneNotSupportedException { return (Friend)super.clone(); } } @Data @AllArgsConstructor public class Person implements Cloneable{ private String name; private int age; private Friend friend; @Override protected Person clone() throws CloneNotSupportedException { Person person = (Person)super.clone(); person.friend = friend.clone(); return person; } }   然后进行测试：\npublic class DeepCopyTest { public static void main(String[] args) throws CloneNotSupportedException { Person person1 = new Person(\u0026#34;张三\u0026#34;, 20, new Friend(\u0026#34;老王\u0026#34;)); Person person2 = person1.clone(); System.out.println(\u0026#34;person1: \u0026#34; + person1); System.out.println(\u0026#34;person2: \u0026#34; + person2 + \u0026#34;\\n\u0026#34;); person1.setName(\u0026#34;张四\u0026#34;); person1.setAge(25); person1.getFriend().setName(\u0026#34;小王\u0026#34;); System.out.println(\u0026#34;person1: \u0026#34; + person1); System.out.println(\u0026#34;person2: \u0026#34; + person2); } } 输出的结果为：\nperson1: Person(name=张三, age=20, friend=Friend(name=老王)) person2: Person(name=张三, age=20, friend=Friend(name=老王)) person1: Person(name=张四, age=25, friend=Friend(name=小王)) person2: Person(name=张三, age=20, friend=Friend(name=老王)) 可以看到，这次真正独立了起来：\n    需要注意的是，如果 Friend类本身也存在引用类型，则需要在 Friend类中的 clone()也去调用其引用类型的 clone()方法，就像 Person类中的那样。\n  因此对于存在多层依赖关系的对象，实现 Cloneable接口重写 clone()方法就显得有些笨拙了，所以可以采用序列化实现深拷贝，具体示例如下：\n  修改 Person和 Friend，实现 Serializable接口：\n@Data @AllArgsConstructor public class Person implements Serializable { private String name; private int age; private Friend friend; public Person deepClone() throws IOException, ClassNotFoundException { // 序列化  ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); // 反序列化  ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return (Person)ois.readObject(); } } @Data @AllArgsConstructor public class Friend implements Serializable { private String name; }   然后进行测试：\npublic class DeepCopyTest { public static void main(String[] args) throws IOException, ClassNotFoundException { Person person1 = new Person(\u0026#34;张三\u0026#34;, 20, new Friend(\u0026#34;老王\u0026#34;)); Person person2 = person1.deepClone(); System.out.println(\u0026#34;person1: \u0026#34; + person1); System.out.println(\u0026#34;person2: \u0026#34; + person2 + \u0026#34;\\n\u0026#34;); person1.setName(\u0026#34;张四\u0026#34;); person1.setAge(25); person1.getFriend().setName(\u0026#34;小王\u0026#34;); System.out.println(\u0026#34;person1: \u0026#34; + person1); System.out.println(\u0026#34;person2: \u0026#34; + person2); } } 输出的结果为：\nperson1: Person(name=张三, age=20, friend=Friend(name=老王)) person2: Person(name=张三, age=20, friend=Friend(name=老王)) person1: Person(name=张四, age=25, friend=Friend(name=小王)) person2: Person(name=张三, age=20, friend=Friend(name=老王)) 可以看到，这次也真正独立了起来。\n      参考文献 #    Java 基本数据类型和引用类型。  基本类型和引用类型的区别。  深拷贝和浅拷贝的区别和与原理。  Java 轻松理解深拷贝与浅拷贝【Java 资料站】。  浅拷贝。  "},{"id":170,"href":"/school-recruitment/docs/java/4NIO/4.1-%E6%A6%82%E8%BF%B0/","title":"4.1 概述","section":"4、 Nio","content":"1 简介 #   NIO 中的 N 可以理解为 Non-Blocking，是一种同步非阻塞的 I/O 模型，在 JDK 1.4 中引入了 NIO 框架，对应于 java.nio 包，提供了 Channel、Selector、Buffer 等抽象。 他支持面向缓冲的、基于通道的 I/O 操作方法，提供了与传统 I/O 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现，这两种通道都支持阻塞和非阻塞两种模式：  阻塞模式就像传统中的支持一样，比较简单，但是性能和可靠性都不好，一般用于低负载、低并发的应用程序，以此来提升开发速度和可维护性。 非阻塞模式正好与之相反，一般用于高负载、高并发的（网络）应用，以此来提升高性能和可靠性。    2 BIO、NIO、AIO 的区别 #   底层原理可参考 五种 IO 模型，下面介绍的主要是 Java 中 BIO、NIO、AIO 的区别。\n 2.1 BIO 是阻塞的，NIO 是非阻塞的 #   BIO 的各种流是阻塞的，这就意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取，或数据完全写入，在此期间，该线程不能再干其他任何事，因此在任何时候都可能有大量的线程处于休眠状态，只是等待输入或者输出就绪，这可以算为对资源的一种浪费， NIO 使我们可以进行非阻塞IO 操作，比如说，单线程从通道读取数据到 Buffer，同时可以继续做别的事，当数据读取到 Buffer 中后，线程再进行处理数据，写数据也是一样的，一个线程请求写入一些数据到某通道，但不需要等待他完全写入，这个线程可以同时去做别的事情，这样可以提高线程的利用效率。  2.2 BIO 是面向流（Stream Oriented）的，NIO 是面向缓冲区（Buffer Oriented）的 #   Buffer 是一个可以读写数据的内存块，可以理解成一个容器对象（含数组），该对象提供了一组方法，可以更轻松地使用内存块，缓冲区对象内置了一些机制，能够跟踪和记录缓冲区的状态变化。 在 NIO 类库中加入 Buffer 对象，体现了 NIO 与 BIO 的一个重要区别：  在面向流的 BIO 中可以将数据直接写入或者直接读到 Stream 对象中，每次从流中读取一个或者多个字节，直至读取完所有的字节。 在 NIO 库中，所有数据都是用缓冲区处理的，在读取数据时，他是直接读缓冲区中的数据，在写入数据时，也是直接写到缓冲区中，任何时候访问 NIO 中的数据，都是通过缓冲区进行操作，因此在操作上更加灵活，读取速度也更加快。   NIO 的 Buffer 除了做缓冲块优化之外，还提供了一个可以直接访问物理内存的类 DirectBuffer：  普通的 Buffer 分配的是 JVM 堆内存，而DirectBuffer是直接分配物理内存。 数据要输出到外部设备，必须先从用户空间复制到内核空间，再复制到输出设备，而则是直接将步骤简化为从内核空间复制到外部设备，减少了数据拷贝。 但是由于 DirectBuffer 申请的是非 JVM 物理内存，所以创建和销毁的代价很高，不是直接由 JVM 负责垃圾回收，而是通过 Java 引用机制来释放该内存块。    2.3 NIO 通过 Channel 进行读写 #   传统 IO 的流都是单向的，因此他们需要分为Input Stream 和Output Stream。 而 NIO 中的 Channel 是双向的，数据可以从 Channel 读到 Buffer 中（使用read() 方法），也可以从 Buffer 写到 Channel（使用write() 方法）。 Channel 也可以设置为非阻塞模式，此时当 Channel 从 Buffer 中读取数据时，如果有待读取的数据，则返回该数据，如果没有待读取的数据，对应的方法也不会被阻塞，而是直接返回。   2.4 NIO 有 Selector 实现多路复用，而 BIO 没有 #    Java NIO实现了 IO 多路复用中的 Reactor 模型：\n  一个线程使用一个 Selector 通过轮询的方式去监听多个 Channel 上的事件（accept、read），如果某个 Channel 上面发生监听事件，这个 Channel 就处于就绪状态，然后进行 IO 操作。\n  可以将监听的 Channel 配置为非阻塞，这样当 Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮询其他 Channel，找到 IO 事件已经到达的 Channel 执行。\n  因为创建和切换线程的开销很大，所以使用一个线程来处理多个事件，减少了线程之间的切换，提高了系统的效率。\n 需要注意的是：\n 只有 SocketChannel 才能配置为非阻塞，而FileChannel不能，因为FileChannel配置非阻塞也没有意义。 目前操作系统的 IO 多路复用机制都是用了 epoll，相比传统的 select 机制，epoll 没有最大连接句柄 1024 的限制，所以Selector 在理论上可以轮询成千上万的客户端。        2.5 AIO 是真正意义上的异步 IO #   AIO 是一种异步非阻塞的通信模式，实现了真正意义上的异步 IO，直接将 IO 操作交给操作系统进行异步处理。  3 三种 IO 的适用场景 #   BIO 方式适用于连接数目少且固定的架构，程序直观简单易理解，是 JDK 1.4 以前的唯一选择。 NIO 方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，编程比较复杂，JDK 1.4 开始支持。 AIO 方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用操作系统参与并发操作，编程比较复杂，JDK 1.7 开始支持。  参考文献 #    offer 快到碗里来-Netty 核心面试题 15 连问。  Java NIO。  Java BIO NIO AIO。  Java NIO 核心组件全解。  "},{"id":171,"href":"/school-recruitment/docs/spring-family/1Spring/1.1-%E6%A6%82%E8%BF%B0/","title":"1.1 概述","section":"1、 Spring","content":"概述 #  "},{"id":172,"href":"/school-recruitment/docs/spring-family/1Spring/1.2-Spring-IOC/","title":"1.2 Spring Ioc","section":"1、 Spring","content":"Spring IOC #   如无特殊说明，本文使用的 Spring 版本为 5.3.9。\n 1 背景 #   传统的 Java 组件协作方式如下：   假如我们有一个在线书店的项目，通过 BookService 获取书籍：\npublic class BookService { private HikariConfig config = new HikariConfig(); private DataSource dataSource = new HikariDataSource(config); public Book getBook(long bookId) { try (Connection conn = dataSource.getConnection()) { ... return book; } } } 为了从数据库查询书籍，BookService 持有一个 DataSource，为了实例化一个 HikariDataSource，又不得不实例化一个 HikariConfig。\n  现在我们继续编写 UserService 获取用户：\npublic class UserService { private HikariConfig config = new HikariConfig(); private DataSource dataSource = new HikariDataSource(config); public User getUser(long userId) { try (Connection conn = dataSource.getConnection()) { ... return user; } } } 因为 UserService 也需要访问数据库，因此，我们不得不也实例化一个 HikariDataSource。\n  在处理用户购买的 CartServlet 中，我们需要实例化 UserService 和 BookService：\npublic class CartServlet extends HttpServlet { private BookService bookService = new BookService(); private UserService userService = new UserService(); protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { long currentUserId = getFromCookie(req); User currentUser = userService.getUser(currentUserId); Book book = bookService.getBook(req.getParameter(\u0026#34;bookId\u0026#34;)); cartService.addToCart(currentUser, book); ... } }   类似的，在购买历史 HistoryServlet 中，也需要实例化 UserService 和 BookService：\npublic class HistoryServlet extends HttpServlet { private BookService bookService = new BookService(); private UserService userService = new UserService(); }   上述每个组件都采用了一种简单的通过 new 创建实例并持有的方式，这种方式具有如下缺点：\n 实例化一个组件其实很难，例如，BookService 和UserService 要创建HikariDataSource，实际上需要读取配置，才能先实例化HikariConfig，再实例化HikariDataSource。 没有必要让BookService 和UserService 分别创建DataSource 实例，完全可以共享一个DataSource，但谁负责创建DataSource，谁负责获取其他组件已经创建的DataSource，不好处理。 很多组件需要销毁以便释放资源，例如DataSource，但如果该组件被多个组件共享，如何确保他的使用方都已经全部被销毁。 随着更多的组件被引入，例如，书籍评论，需要共享的组件写起来会更困难，这些组件的依赖关系会越来越复杂。 测试某个组件，例如BookService，是复杂的，因为必须要在真实的数据库环境下执行。    从上面的例子可以看出，如果一个系统有大量的组件，其生命周期和相互之间的依赖关系如果由组件自身来维护，不仅大大增加了系统的复杂性，而且会导致组件之间极为紧密的耦合，继而给测试和维护带来了极大地困难。\n  因此，核心问题是：\n 谁负责创建组件。 谁负责根据依赖关系组装组件。 销毁时，如何按依赖顺序正确销毁。  解决这一问题的核心方案就是IOC。\n  例如，BookService 自己并不会创建 DataSource，而是等待外部通过 setDataSource() 方法来注入一个 DataSource：\npublic class BookService { private DataSource dataSource; public void setDataSource(DataSource dataSource) { this.dataSource = dataSource; } }   因为IoC 容器要负责实例化所有的组件，因此，有必要告诉容器如何创建组件，以及各组件之间的依赖关系，一种最简单的配置是通过 XML 文件来实现：\n\u0026lt;beans\u0026gt; \u0026lt;bean id=\u0026#34;dataSource\u0026#34; class=\u0026#34;HikariDataSource\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;bookService\u0026#34; class=\u0026#34;BookService\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;userService\u0026#34; class=\u0026#34;UserService\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 上述 XML 配置文件指示 IoC 容器创建 3 个 JavaBean 组件，并把 id 为 dataSource 的组件通过属性 dataSource（即通过调用 setDataSource() 方法）注入到另外两个容器中。\n 在 Spring 的 IoC 容器中，我们把所有组件统称为 JavaBean，即配置一个组件就是配置一个 Bean。\n   从上面的代码中我们可以看到，依赖注入可以通过 set() 方法实现，但依赖注入也可以通过构造方法实现：\npublic class BookService { private DataSource dataSource; public BookService(DataSource dataSource) { this.dataSource = dataSource; } } Spring 的 IoC 容器同时支持属性注入和构造方法注入，并允许混合使用。\n    2 含义 #   IoC，全称是 Inverse of Control，即控制反转，就是把原先我们代码里面需要实现的对象创建、依赖的代码，反转给容器来帮忙实现，所有组件不再由应用程序自己创建和配置，而是由 IOC 容器负责，这样应用程序只需要直接使用已经创建并配置好的组件。 因此，IoC解决了一个最主要的问题，将组件的创建和配置与组件的使用相分离，由 IoC 容器负责管理组件的生命周期。  3 容器实现 #  Spring 中的容器可以分为两大类\n 一类是由 BeanFactory 接口定义的核心容器：  BeanFactory位于整个容器类体系结构的顶端，其基本实现类为 DefaultListableBeanFactory。 之所以称其为核心容器，是因为该类容器实现了 IoC 的核心功能，比如配置文件的加载解析、Bean 依赖的注入以及生命周期的管理等。 BeanFactory 作为Spring 框架的基础设施，面向 Spring 框架本身，一般不会被用户直接使用。   另一类是由 ApplicationContext 接口定义的容器：  通常译为应用上下文，不过称其为应用容器可能更形象些。 他在 BeanFactory 提供的核心 IoC 功能之上做了扩展：  通常 ApplicationContext 的实现类内部都持有一个 BeanFactory 的实例，IoC 的核心功能会交由他去完成。 而 ApplicationContext 本身，则专注于在应用层对 BeanFactory 做扩展，比如提供对国际化的支持、支持框架级的事件监听机制以及增加了很多对应用环境的适配。   ApplicationContext面向的是使用 Spring 框架的开发者，例如开发中经常使用的 ClassPathXmlApplicationContext 就是典型的 Spring 的应用容器。    3.1 BeanFactory #   BeanFactory，即生产 Bean 的工厂，他负责生产和管理各个 Bean 实例。 BeanFactory 的继承体系如下图所示： 先介绍一下里面比较重要的一些接口和类：  ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是通过这个接口，我们可以获取多个 Bean，但是通过源码我们可以发现，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 的意思为分层，也就是说我们可以在应用中起多个 BeanFactory，然后将各个 BeanFactory 设置为父子关系。 AutowireCapableBeanFactory 主要用来自动装配 Bean，ApplicationContext 虽然没有继承他，但是ApplicationContext 接口定义的最后一个方法 getAutowireCapableBeanFactory() 来获取 AutowireCapableBeanFactory。 ConfigurableListableBeanFactory 也是一个特殊的接口，特殊之处在于他继承了第二层所有的三个接口，而ApplicationContext 没有，主要用于 IoC 容器的定制性。    3.2 ApplicationContext #   ApplicationContext 的继承体系如下图所示： 我们重点了解一下比较重要的实现类：  ClassPathXmlApplicationContext：  主要用于在 ClassPath 中寻找 XML 配置文件，根据 XML 文件内容来构建 ApplicationContext 容器。   FileSystemXmlApplicationContext：  其构造函数需要一个 XML 配置文件在系统中的路径，其他和 ClassPathXmlApplicationContext 基本一样。   AnnotationConfigApplicationContext：  基于注解来使用，不需要配置文件，采用 Java 配置类和各种注解来配置，是比较简单的方式，也是大势所趋。   ConfigurableApplicationContext：  扩展于 ApplicationContext，新增加了两个主要的方法，分别为 refresh() 和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力：  在应用上下文关闭的情况下调用 refresh() 即可启动应用上下文。 在已经启动的状态下，调用 refresh() 则清除缓存并重新装载配置信息。 调用 close() 则关闭应用上下文。       此外，ApplicationContext 还通过其他接口扩展了 BeanFactory 的功能：   ApplicationEventPublisher：\n 让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。 实现了 ApplicationListener 事件监听接口的 Bean，可以接收容器事件，并对事件进行相应处理。 在 ApplicationContext 抽象实现类 AbstractApplicationContext 中，我们可以发现存在一个 ApplicationEventMulticaster，他负责保存所有监听器，以便在容器产生上下文事件时通知这些事件监听者。    MessageSource：\n  为应用提供 i18n 国际化消息访问的功能。\n 什么是 i18n？\n 在开发应用程序的时候，经常会遇到支持多语言的需求，这种支持多语言的功能称之为国际化，英文是Internationalization，缩写为i18n（因为首字母i 和末字母n 中间有 18 个字母）。 还有针对特定地区的本地化功能，英文是Localization，缩写为l10n，本地化是指根据地区调整类似姓名、日期的显示等。 也有把上面两者合称为全球化，英文是Globalization，缩写为g11n。       ResourcePatternResolver：\n ApplicationContext 实现类都实现了类似于 PathMatchingResourcePatternResolver 的功能，可以通过带前缀的 Ant 风格的资源文件路径装载 Spring 的配置文件。      3.3 WebApplicationContext #    在 ApplicationContext 下面还有一个实现类是WebApplicationContext，是专门为 Web 应用准备的容器，他允许从相对于 Web 根目录的路径中装载配置文件完成初始化工作。\n  从 WebApplicationContext 中可以获得 ServletContext 的引用，整个 Web 应用上下文对象将作为属性放置到 ServletContext 中，以便 Web 应用环境可以访问 Spring 应用上下文。\n  WebApplicationContext 定义了一个常量 ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE，在上下文启动时，WebApplicationContext 实例即以此为键放置在 ServletContext 的属性列表中，因此我们可以直接通过下面语句从 Web 容器中获取 WebApplicationContext：\nWebApplicationContext wac = (WebApplicationContext)servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE);    4 启动流程 #    Spring IoC 的启动时会进行以下步骤：\n 首先读取应用程序提供的 Bean 的配置信息，并在 Spring 容器中生成一份相应的 Bean 配置注册表。 然后根据注册表加载、实例化 Bean、建立 Bean 与 Bean 之间的依赖关系。 最后将这些准备就绪的 Bean 放到 Bean 缓冲池中，等待应用程序调用。     总结一下，我们可以把 IoC 的启动流程分为以下两个重要的阶段：\n  容器的启动阶段。\n  Bean 的实例化阶段。  需要注意的是：\n 在 Spring 中，最基础的容器接口方法是由 BeanFactory 定义的，而BeanFactory 的实现采用的是延迟加载，也就是说，容器启动时，只会进行第一个阶段的操作，当需要用到某个实例时，才会进行第二个阶段的操作。 ApplicationContext（另一个容器的实现类）在启动容器时就完成了所有初始化，这就需要更多的资源。 因此，我们需要根据不同的场景选择不同的容器实现类，下面介绍更多的是以 ApplicationContext 为主。       4.1 容器的启动阶段 #  在容器启动阶段，我们的 Spring 经历了很多事情，具体的话可以分为以下几个步骤：\n 加载配置信息。 解析配置信息。 装配 BeanDefinition。 后处理。  4.1.1 加载配置信息 #   加载配置信息主要分为两种，分别为：  类路径加载：ClassPathXmlApplicationContext。 文件系统加载：FileSystemXmlApplicationContext   IoC 容器读取配置文件的接口为BeanDefinitionReader，他会根据配置文件格式的不同给出不同的实现类，将配置文件中的内容读取并映射到 BeanDefinition，比如XML 文件就会用 XmlBeanDefinitionReader。  4.1.2 解析配置信息 #   解析配置信息就是将我们读取到的配置信息转换成一个 DOM 树，然后解析里面的配置信息装配到我们的 BeanDefinition。 我们在 processBeanDefinition() 中先将解析后的信息封装到一个 BeanDefinitionHolder，包含一个 BeanDefinition 的实例和他的beanName、alias（别名）这三个信息。  4.1.3 装配 BeanDefinition #   在上面我们将信息解析后，就会装配到一个 BeanDefinitionHolder，里面就包含了我们的 BeanDefinition。 然后装配 BeanDefinition，就是将这些 BeanDefinition 注册到 BeanDefinitionRegistry（核心是一个beanName 到beanDefinition 的Map）中，我们在获取 BeanDefinition 的时候，需要通过 beanName 获取别名，然后通过别名再一次重定向获取我们的 BeanDefinition。 Spring 的后续操作直接从 BeanDefinitionRegistry 中读取配置信息，具体的注册实现就是在我们上面介绍到的 DefaultListableBeanFactory 实现类里。  4.1.4 后处理 #    在我们的后续操作中，容器扫描 BeanDefinitionRegistry 中的 BeanDefinition，然后使用 Java 的 反射机制自动识别出 Bean 工厂后处理器（实现 BeanFactoryPostProcessor 接口）的 Bean，然后调用这些 Bean 工厂后处理器对 BeanDefinitionRegistry 中的 BeanDefinition 进行加工处理，主要完成以下两个工作：\n 对使用到占位符的元素标签进行解析，得到最终的配置值，这意味着对一些半成品式的 BeanDefinition 对象进行加工处理并得到成品的 BeanDefinition 对象。 对 BeanDefinitionRegistry 中的 BeanDefinition 进行扫描，通过 Java 反射机制找出所有属性编辑器的 Bean（实现java.beans.PropertyEditor 接口的 Bean），并自动将他们注册到 Spring 容器的属性编辑器注册表中（PropertyEditorRegistry）。    Spring 容器从 BeanDefinitionRegistry 中取出加工后的 BeanDefinition，并调用 InstantiationStrategy 着手进行 Bean 实例化的工作，在实例化 Bean 时，Spring 容器使用 BeanWrapper 对 Bean 进行封装，BeanWrapper 提供了很多以 Java 反射机制操作 Bean 的方法，他将结合该 Bean 的 BeanDefinition 以及容器中属性编辑器，完成 Bean 属性的设置工作。\n  在装配好 Bean 容器后，还要通过方法 prepareBeanFactory() 准备 Bean 容器，在准备阶段会注册一些特殊的 Bean，这里不做深究，在准备容器后，我们可能会对 Bean 进行一些加工，就需要用到 BeanPostProcessor 来进行一些后处理，我们利用容器中注册的 Bean 后处理器（实现 BeanPostProcessor 接口的 Bean）对已经完成属性设置工作的 Bean 进行后续加工，直接装配出一个准备就绪的 Bean。\n BeanPostProcessor 和 BeanFactoryPostProcessor 有什么区别？\n BeanPostProcessor：  对容器中的 Bean 进行后处理，对 Bean 进行额外的加强、加工。 是在我们单例 Bean 实例化过程中穿插执行的。   BeanFactoryPostProcessor：  对 Spring 容器本身进行后处理，增强容器的功能。 是在我们单例 Bean 实例化之前执行的。       4.2 Bean 的实例化阶段（Bean 的生命周期） #   在ApplicationContext 中，所有的 BeanDefinition 的 scope 默认是 singleton，针对 singleton 我们 Spring 容器采用的是预先实例化的策略，这样在我们获取实例的时候，就会直接从缓存里面拉取出来，提升了运行效率。 但是如果我们是懒加载的话，那么就不预先实例化，而是在我们第一次 getBean 的时候才会去实例化，不过我们大部分时候都不会去用懒加载，除非这个 Bean 比较特殊，例如非常耗费资源，在应用程序的生命周期里的使用概率比较小，在这汇总情况下我们可以将他设置为懒加载。  4.2.1 实例化过程 #   Spring 中 Bean 的实例化一般可以分为以下几个阶段：\n 实例化 Bean 对象：  Spring 对 Bean 进行实例化，默认 Bean 是单例。   设置对象属性：  Spring 对 Bean 进行依赖注入，比如有没有配置当前 depends-on 的依赖，有的话就去实例化依赖的 Bean。   检查 Aware 相关接口并设置相关依赖：  如果 Bean 实现了BeanNameAware 接口，Spring将 Bean 的 id 传给 setBeanName() 方法。 如果 Bean 实现了BeanFactoryAware 接口，Spring 将调用他的 setBeanFactory() 方法，将 BeanFactory 实例传进来。 如果 Bean 实现了ApplicationContextAware 接口，Spring 将调用他的 setApplicationContext() 方法，将应用上下文的引用传入到 Bean 中。   BeanPostProcessor 前置处理：  如果 Bean 实现了BeanPostProcessor 接口，Spring 将调用他的 postProcessBeforeInitialization() 方法。   注册必要的 Initialization 相关回调接口：  如果 Bean 实现了InitializingBean 接口，Spring 将调用他的 afterPropertiesSet() 方法。 如果 Bean使用 init-method 属性声明了初始化方法，Spring 将调用他的 afterPropertiesSet() 方法。   BeanPostProcessor 后置处理：  如果 Bean 实现了BeanPostProcessor 接口，Spring 将调用他的postProcessAfterInitialization()方法。   注册必要的 Destruction 相关回调接口：  如果 Bean 实现了DisposableBean 接口，Spring 将调用他的 distroy() 接口方法。 如果 Bean使用了 destriy-method 属性声明了销毁方法，Spring 将调用他的 distroy() 接口方法。   此时 Bean 已经准备就绪，可以被应用程序使用了，他们将一直驻留在应用程序上下文中，直到该应用上下文被销毁。  4.2.2 循环依赖问题 #    循环依赖问题是 Bean 在实例化过程中一个比较复杂的问题，例如我们有 A、B 两个类，A 的构造方法有一个参数是 B，B 的构造方法有一个参数是 A，这种 A 依赖于 B，B 依赖于 A 的问题就是循环依赖问题：\n@Service public class A { public A(B b) { } } @Service public class B { public B(A a) { } }   循环依赖可以分为三种：\n  单例构造器循环依赖，例如：\n@Service public class A { public A(B b) { } } @Service public class B { public B(C c) { } } @Service public class C { public C(A a) { } } 结果，项目启动失败，报了一个 cycle：\n   原型循环依赖，例如\n@Service @Scope(\u0026#34;prototype\u0026#34;) public class A1 { @Autowired private B1 b1; } @Service @Scope(\u0026#34;prototype\u0026#34;) public class B1 { @Autowired public C1 c1; } @Service @Scope(\u0026#34;prototype\u0026#34;) public class C1 { @Autowired public A1 a1; } 结果，项目启动失败，报了一个 cycle：\n   单例 setter 注入循环依赖，例如：\n@Service @Scope(\u0026#34;singleton\u0026#34;) // 默认就是单例方式 public class A1 { @Autowired private B1 b1; } @Service public class B1 { @Autowired public C1 c1; } @Service public class C1 { @Autowired public A1 a1; } 结果，项目启动成功：\n     4.2.3 循环依赖解决 #  4.2.3.1 单例构造器循环依赖 #   假如 A 依赖于 B、B 依赖于 A，在我们实例化 A 的时候就要去实例化 B，然后 B 又要去实例化 A，在这个过程中，我们会将 beanName 添加到一个 Set 结构中，当第二次添加 A 的时候，因为Set 已经存在 A 的 beanName 了，所以Spring 就会判断发生了循环依赖问题，抛出异常。 Spring 无法解决单例构造器循环依赖问题，只能抛出异常。  4.2.3.2 原型循环依赖 #   原型循环依赖的判断条件和构造器的差不多，最主要的区别就是 Set 的类型变成了 ThreadLocl 类型。 Spring 无法解决原型循环依赖问题，只能抛出异常。  4.2.3.3 单例 setter 注入循环依赖 #   Spring通过三级缓存来解决单例 setter 注入循环依赖问题。 三级缓存是指分别有三个缓存：  一级缓存：singletonObjects。 二级缓存：earlySingletonObjects。 三级缓存：singletonFactories。   我们以上面 A 依赖于 B，B 依赖于 A 的样例来分析一下setter 是如何通过三级缓存来解决循环依赖问题：  首先我们在实例化 A 的时候，通过 BeanDifinition 定义拿到 A 的无参构造方法，通过 反射创建了这个实例对象，这个 A 的实例对象是一个尚未进行依赖注入和 init-method 方法调用等等逻辑处理的早期实例，是我们业务无法使用的，然后在进行后续的包装处理前，我们会将他封装成一个 ObjectFactory 对象然后存入到我们的三级缓存中（key 是beanName，value 是ObjectFactory 对象），相当于一个早期工厂提前曝光。 然后呢我们会继续实例化 A，在实例化过程中发现 A 依赖于 B，我们通过 Setter 依赖注入的时候，通过getBean(B)去获取依赖对象 B，但是这个B 还没有实例化，所以我们就需要去创建 B 的实例。 然后我们就开始创建 B 的实例，同上面创建实例 A 的过程，在实例化 B 的过程中，因为B 依赖于 A，所以也会调用 getBean(A) 去获得 A 的实例，首先会去一级缓存访问，如果没有就去二级缓存，再没有就去三级缓存，然后在三级缓存中发现我们的早期实例 A，就直接拿来用了，然后完成 B 的依赖，再完成后面 B 实例化过程的一系列阶段，最后将实例化完成的 B 存放到一级缓存中，并将二三级缓存清理掉。 完成 B 的实例化后，我们就会回到 A 的实例化阶段，我们的A 在有了 B 的依赖后，也继续完成了后续的实例化过程，把一个早期的对象变成一个完整的对象，并将 A 存进到一级缓存中，清除二三级缓存。  为什么要有三级缓存，二级缓存不够用吗？\n 我们在上面分析的过程中，可能会感觉二级缓存的存在感不是特别强，为什么不去掉第二级的缓存然后变成一个二级缓存呢。 这是因为我们的B 在拿到 A 的早期实例后就会进行缓存升级，将 A 从三级缓存移到二级缓存中，之所以需要三级缓存，是因为在这一步我们的Bean 可能还需要一些其他的操作，比如被 Bean 后置处理器进行一些增强，或者做一些 AOP 的判断，如果只有二级缓存的话，那么返回的就是早期实例而不是我们增强后的实例。       参考文献 #    Spring 面试题（总结最全面的面试题）  IoC 原理。  Spring IOC 原理深层解析。  Spring：源码解读 Spring IOC 原理。  国际化。  Spring IOC 容器启动流程源码解析(一)——容器概念详解及源码初探。  Spring IOC 容器源码分析。  通过循环依赖问题彻底理解 Spring IOC 的精华。  循环依赖的三种方式。  Spring 容器如何解决循环依赖的原理。  "},{"id":173,"href":"/school-recruitment/docs/spring-family/1Spring/1.3-Spring-AOP/","title":"1.3 Spring Aop","section":"1、 Spring","content":"Spring AOP #   如无特殊说明，本文使用的 Spring 版本为 5.3.9。\n 1 背景 #   软件开发一直在寻求一种高效开发、扩展、维护的方式，从面向过程的开发实践中，前人将关注点抽象出来，对行为和属性进行聚合，形成了面向对象的开发思想，其在一定程度上影响了软件开发的过程。 鉴于此，我们在开发的过程中会对软件开发进行抽象、分割成各个模块或对象，例如，我们会对 API 进行抽象成四个模块，分别为Controller、Service、Gateway、Command，这很好地解决了业务级别的开发，但对于系统级别的开发我们很难聚焦，比如，对于每一个模块需要进行打印日志、代码监控、异常处理，以打日志为例，我们只能将日志代码嵌套在各个对象上，而无法关注日志本身，这种现象有偏离了 OOP（Object-Oriented Programming, 面向对象编程）思想。  为了能更好地将系统级别的代码抽离出来，去掉与对象的耦合，就产生了 AOP（Aspect-Oriented Programming, 面向切面编程），如下图所示，OOP 属于一种横向扩展，AOP 属于一种纵向扩展，AOP 依托于 OOP，进一步将系统级别的代码抽象出来，进行纵向排列，实现低耦合。   2 含义 #   AOP，全称为 Aspect-Oriented Programming，即面向切面编程。 它是一种新的模块化机制，用来描述分散在对象、类或函数中的横切关注点，从关注点中分离出横切关注点是面向切面程序设计的核心概念。 分离关注点使解决特定领域问题的代码从业务逻辑中独立出来，业务逻辑代码中不再含有针对特定领域问题代码的调用，业务逻辑同特定领域问题的关系通过切面来封装、维护，这样原本分散在整个应用程序中的变动就可以很好地管理起来。 Spring AOP 的核心技术是 JDK 动态代理，以动态代理技术为基础，设计出了一系列 AOP 的横切实现，比如前置通知、返回通知、异常通知，同时，Spring AOP 还提供了一系列的 Pointcut来匹配切入点，例如JdkRegexpMethodPointcut（正则切入点）、NameMatchMethodPointcut（方法名切入点），可以使用现有的切入点来设置横切面，也可以扩展相关的 Pointcut 方法来实现切入需求。 为了让 AOP 起作用，需要完成一系列的过程，比如：  需要为目标对象建立代理对象，这个代理对象可以通过使用 JDK 的 Proxy 来完成，也可以通过第三方的代理生成器 CGLIB 来完成。 需要启动代理对象的拦截器来完成各种横切面的织入，这一系列的织入是通过一系列的 Adapter 来实现的，通过这一系列 Adapter 的设计，可以把 AOP 的横切面设计和 Proxy 模式有机地结合起来，从而实现在 AOP 中定义好的各种织入方式。    3 家庭成员 #  Spring AOP 的家庭成员主要包括三个，分别为Advice（通知）、Pointcut（切点）、Advisor（通知器）。\n3.1 Advice #    Advice定义在连接点做什么，比如打日志、执行缓存、处理异常等，为切面增强提供织入接口。\n  Advice 是 AOP 联盟定义的一个接口，在 Spring AOP 的实现中，使用了这个统一接口，并通过这个接口，为 AOP 切面增强的织入功能做了更多的细化和扩展，比如提供了更具体的通知类型，如 BeforeAdvice、AfterAdvice、ThrowsAdvice 等，具体的切面增强可以通过这些接口集成到 AOP 框架中去发挥作用。\n   3.2 Pointcut #    Pointcut决定 Advice 通知应该作用于哪个连接点，也就是说通过 Pointcut 来定义需要增强的方法的集合，这些集合的选取可以按照一定的规则来完成，在这种情况下，Pointcut 通常意味着标识方法，例如，这些需要增强的地方可以由某个正则表达式进行标识或根据某个方法名进行匹配等。\n   3.3 Advisor #    将 Advice 和 Pointcut 结合起来，为即开即用地使用 AOP 基础设施提供了便利，下图为 DefaultPointcutAdvisor 的一个有参构造函数：\n   4 原理 #  4.1 建立 AopProxy 代理对象 #  4.1.1 设计原理 #    在 Spring 的 AOP 模块中，一个主要的部分是代理对象的生成，主要通过配置和调用 ProxyFactoryBean 来完成这个任务的，在 ProxyFactoryBean 中，封装了主要代理对象的生成过程，在这个生成过程中，可以使用 JDK 的 Proxy 和 CGLIB 两种生成方式。\n  以 ProxyFactoryBean 为中心，相关类的继承关系如下：\n   在这个类继承关系中，可以看到完成 AOP 应用的类，比如 AspectJAdvisorFactory、ProxyFactoryBean、ProxyFactory，都在同一个类的继承体系下，都是 ProxyConfig、AdvisedSupport、ProxyCreatorSupport 的子类。\n  ProxyConfig 可以看作是一个数据基类，主要提供了配置属性：\npublic class ProxyConfig implements Serializable { private boolean proxyTargetClass = false; private boolean optimize = false; boolean opaque = false; boolean exposeProxy = false; private boolean frozen = false; }   AdvisedSupport 主要封装了 AOP 对通知和通知器的相关操作，这些操作对于不同的 AOP 的代理对象的生成都是一样的，但对于具体的 AOP 代理对象的创建，则会交给他们的子类去完成：\n/**部分方法**/ public class AdvisedSupport extends ProxyConfig implements Advised { @Override public void addAdvice(Advice advice) throws AopConfigException { int pos = this.advisors.size(); addAdvice(pos, advice); } @Override public void addAdvisor(Advisor advisor) { int pos = this.advisors.size(); addAdvisor(pos, advisor); } @Override public final Advisor[] getAdvisors() { return this.advisors.toArray(new Advisor[0]); } }   ProxyCreatorSupport 可以看做是子类对象创建 AOP 代理对象的一个辅助类，通过继承以上提到的基类的功能实现，具体的 AOP 代理对象的生成，根据不同的需要，分别由 AspectJAdvisorFactory、ProxyFactoryBean、ProxyFactory 来完成：\n/**部分方法**/ public class ProxyCreatorSupport extends AdvisedSupport { public ProxyCreatorSupport() { this.aopProxyFactory = new DefaultAopProxyFactory(); } public ProxyCreatorSupport(AopProxyFactory aopProxyFactory) { Assert.notNull(aopProxyFactory, \u0026#34;AopProxyFactory must not be null\u0026#34;); this.aopProxyFactory = aopProxyFactory; } public AopProxyFactory getAopProxyFactory() { return this.aopProxyFactory; } protected final synchronized AopProxy createAopProxy() { if (!this.active) { activate(); } return getAopProxyFactory().createAopProxy(this); } }   AspectJAdvisorFactory 起到集成 Spring 和 AspectJ 的作用，可以应用于需要使用 AspectJ 的 AOP 应用。\n  ProxyFactoryBean、ProxyFactory都提供了 AOP 功能的封装，区别是：\n ProxyFactoryBean可以在 IoC 容器中完成声明式配置来使用 Spring AOP 的功能。 ProxyFactory需要使用编程式使用 Spring AOP 的功能。      4.1.2 配置 ProxyFactoryBean #    在分析 Spring AOP 的实现原理中，主要以 ProxyFactoryBean 的实现作为例子和实现的基本线索进行分析，这是因为ProxyFactoryBean 是在 Spring IoC 环境中创建 AOP 应用的底层方法，也是最灵活的方法，Spring通过他完成了对 AOP 使用的封装。\n  在基于 XML 配置 Spring 的 Bean 时，往往需要一系列的配置步骤来使用 ProxyFactoryBean 和 AOP：\n  定义使用的通知器 Advisor：\n 这个通知器定义了需要对目标对象进行增强的切面行为，也就是Advice 通知。    定义 ProxyFactoryBean：\n 封装 AOP 功能的主要类，配置时需要设定与 AOP 实现相关的重要属性，比如proxyInterface、intercetorNames 和target。    定义 target 属性：\n 需要用 AOP 通知器中的切面应用来增强的对象。       4.1.3 ProxyFactoryBean 生成 AopProxy 对象 #  AopProxy 对象的生成过程如下图所示：\n   从 FactoryBean 中获取对象，是以 getObject() 方法作为入口的，ProxyFactoryBean 实现中的 getObject() 方法，是 FactoryBean 需要实现的接口，这里面封装了需要对 target 目标对象增加的增强处理，该方法首先对通知器链进行初始化，通知器链封装了一系列的拦截器，这些拦截器都需要从配置中读取，然后为代理对象的生成做好准备，在生成代理对象时，因为 Spring 中有 singleton 类型和 prototype 类型两种不同的 Bean，所需要对代理对象的生成做一个区分。\n  ProxyFactoryBean 的 getObject() 方法的源码如下所示：\npublic Object getObject() throws BeansException { initializeAdvisorChain();\t/*初始化通知器链*/ if (isSingleton()) {\t/*对 singleton 和 prototype 的类型进行区分，生成对应的 proxy*/ return getSingletonInstance(); } else { if (this.targetName == null) { logger.info(\u0026#34;Using non-singleton proxies with singleton targets is often undesirable. \u0026#34; + \u0026#34;Enable prototype proxies by setting the \u0026#39;targetName\u0026#39; property.\u0026#34;); } return newPrototypeInstance(); } }  为 Proxy 代理对象配置 Advisor 链是在initializeAdvisorChain() 方法中完成的：   这个初始化过程有一个标志位 advisorChainInitialized，用来表示通知器链是否已经初始化，如果已经初始化，那么这里就不会再初始化，而是直接返回，也就是说，这个初始化的工作发生在应用第一次通过 ProxyFactoryBean 去获取代理对象的时候。\n  在完成初始化之后，接着会读取配置中出现的所有通知器，这个取得通知器的过程也比较简单，把通知器的名字交给容器的 getBean() 方法就可以了，这是通过对 IoC 容器实现的一个回调来完成的。\n  然后把从 IoC 容器中取得的通知器加入拦截器链中，这个动作是由 addAdvisorOnChainCreation 方法来实现的。\nprivate synchronized void initializeAdvisorChain() throws AopConfigException, BeansException { if (this.advisorChainInitialized) { return; } if (!ObjectUtils.isEmpty(this.interceptorNames)) { if (this.beanFactory == null) { throw new IllegalStateException(\u0026#34;No BeanFactory available anymore (probably due to serialization) \u0026#34; + \u0026#34;- cannot resolve interceptor names \u0026#34; + Arrays.asList(this.interceptorNames)); } // Globals can\u0026#39;t be last unless we specified a targetSource using the property...  if (this.interceptorNames[this.interceptorNames.length - 1].endsWith(GLOBAL_SUFFIX) \u0026amp;\u0026amp; this.targetName == null \u0026amp;\u0026amp; this.targetSource == EMPTY_TARGET_SOURCE) { throw new AopConfigException(\u0026#34;Target required after globals\u0026#34;); } // Materialize interceptor chain from bean names.  for (String name : this.interceptorNames) {\t/*添加 Advisor 链的调用，通过 interceptorNames 属性进行配置的*/ if (name.endsWith(GLOBAL_SUFFIX)) { if (!(this.beanFactory instanceof ListableBeanFactory)) { throw new AopConfigException( \u0026#34;Can only use global advisors or interceptors with a ListableBeanFactory\u0026#34;); } addGlobalAdvisors((ListableBeanFactory) this.beanFactory, name.substring(0, name.length() - GLOBAL_SUFFIX.length())); } else { // If we get here, we need to add a named interceptor.  // We must check if it\u0026#39;s a singleton or prototype.  Object advice;\t/*如果程序在这里被调用，那么需要加入命名的拦截器 advice，并且需要检查这个 Bean 是 singleton 还是 prototype 类型的*/ if (this.singleton || this.beanFactory.isSingleton(name)) { // Add the real Advisor/Advice to the chain.  advice = this.beanFactory.getBean(name);\t/*加入 advice 或 advisory*/ } else { // It\u0026#39;s a prototype Advice or Advisor: replace with a prototype.  // Avoid unnecessary creation of prototype bean just for advisor chain initialization.  advice = new PrototypePlaceholderAdvisor(name);\t/*对 prototype 类型 Bean 的处理*/ } addAdvisorOnChainCreation(advice); } } } this.advisorChainInitialized = true; }    生成 singleton 的代理对象是在 getSingletonInstance() 方法中完成的：   这个方法是 ProxyFactoryBean 生成 AopProxy 代理对象的调用入口，代理对象会封装对 target 目标对象的调用，也就是说针对 target 对象的方法调用行为会被这里生成的代理对象所拦截，具体的生成过程是：\n 首先读取 ProxyFactoryBean 中的配置，为生成代理对象做好必要的准备，比如设置代理的方法调用接口等。 Spring 通过 AopProxy 类来具体生成代理对象，AopProxy 是一个接口，由两个子类实现，一个是CglibAopProxy（最终使用的实际上是 ObjenesisCglibAopProxy，该类继承自 CglibAopProxy），另一个是JdkDynamicAopProxy，Spring 分别通过这两个子类来生成需要的 Proxy 代理对象。  private synchronized Object getSingletonInstance() { if (this.singletonInstance == null) { this.targetSource = freshTargetSource(); if (this.autodetectInterfaces \u0026amp;\u0026amp; getProxiedInterfaces().length == 0 \u0026amp;\u0026amp; !isProxyTargetClass()) { // Rely on AOP infrastructure to tell us what interfaces to proxy.  Class\u0026lt;?\u0026gt; targetClass = getTargetClass();\t/*根据 AOP 框架来判断需要代理的接口*/ if (targetClass == null) { throw new FactoryBeanNotInitializedException(\u0026#34;Cannot determine target class for proxy\u0026#34;); } setInterfaces(ClassUtils.getAllInterfacesForClass(targetClass, this.proxyClassLoader));\t/*设置代理对象的接口*/ } // Initialize the shared singleton instance.  super.setFrozen(this.freezeProxy); this.singletonInstance = getProxy(createAopProxy());\t/*使用 ProxyFactory 来生成需要的 Proxy*/ } return this.singletonInstance; } protected Object getProxy(AopProxy aopProxy) {\t/*通过 createAopProxy 返回的 AopProxy 来得到代理对象*/ return aopProxy.getProxy(this.proxyClassLoader); }    具体的代理对象的生成，是在 ProxyFactoryBean 的基类 AdvisedSupport 的实现中借助 AopProxyFactory 完成的：\n  这个代理对象要么从 JDK 中生成，要么借助 CGLIB 获得。\n  这里使用了 AopProxyFactory 来创建 AopProxy，AopProxyFactory 使用的是 DefaultAopProxyFactory，因此问题就转换为在 DefaultAopProxyFactory 中，AopProxy 是怎样生成的了：\n 关于 AopProxy 代理对象的生成，需要考虑使用哪种方式：  如果目标对象是接口类，那么适合使用 JDK 来生成目标对象的代理对象。 否则，会使用 CGLIB 来生成目标对象的代理对象。   DefaultAopProxyFactory 作为 AopProxy 对象的生产工厂，可以根据不同的需要生成这两种 AopProxy 对象，在 DefaultAopProxyFactory 创建 AopProxy 对象的过程中，对不同的 AopProxy 代理对象的生成所涉及的生成策略和场景做了相应的设计，但是对于具体的 AopProxy 代理对象的生成，最终并没有由 DefaultAopProxyFactory 来完成，而是交给了 JdkDynamicAopProxy 和 ObjenesisCglibAopProxy（该类继承自 CglibAopProxy）。  private AopProxyFactory aopProxyFactory; public ProxyCreatorSupport() { this.aopProxyFactory = new DefaultAopProxyFactory(); } public AopProxyFactory getAopProxyFactory() { return this.aopProxyFactory; } protected final synchronized AopProxy createAopProxy() { if (!this.active) { activate(); } return getAopProxyFactory().createAopProxy(this); } @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (!NativeDetector.inNativeImage() \u0026amp;\u0026amp; (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config))) { Class\u0026lt;?\u0026gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(\u0026#34;TargetSource cannot determine target class: \u0026#34; + \u0026#34;Either an interface or a target is required for proxy creation.\u0026#34;); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) {\t/*如果 targetClass 是接口类，使用 JDK 来生成 Proxy*/ return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config);\t/*如果不是接口类要生成 Proxy，那么使用 CGLIB 来生成*/ } else { return new JdkDynamicAopProxy(config); } }         4.1.4 JDK 生成 AopProxy 代理对象 #    在 JdkDynamicAopProxy 中，使用了 JDK 的 Proxy 类来生成代理对象。\n  首先需要从 advised 对象中取得代理对象的代理接口配置，然后调用 Proxy 的 newProxyInstance() 方法，最终得到对应的 Proxy 代理对象。\n  再生成代理对象时，需要指明三个参数，分别是：\n 类装载器。 代理接口。 Proxy 回调方法所在的对象。  这个对象需要实现 InvocationHandler 接口，该接口定义了 invoke() 方法，提供代理对象的回调入口，具体原理可参考 3.2.1 JDK 动态代理。 对于JdkDynamicAopProxy，他本身实现了 InvocationHandler 接口和 invoke() 方法，这个invoke() 方法是 Proxy 代理对象的回调方法，所以可以使用 this 来把 JdkDynamicAopProxy 指派给 Proxy 对象，也就是说 JdkDynamicAopProxy 对象本身，在 Proxy 代理的接口方法被调用时，会触发 invoke() 的回调，从而完成对目标对象方法调用的拦截或者说功能增强。     public Object getProxy(@Nullable ClassLoader classLoader) { if (logger.isTraceEnabled()) { logger.trace(\u0026#34;Creating JDK dynamic proxy: \u0026#34; + this.advised.getTargetSource()); } return Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this);\t/*调用 JDK 生成 Proxy*/ }   4.2 拦截器调用的实现 #  4.2.1 设计原理 #   在Spring AOP通过JDK的Proxy方式或者CGLIB方式生成代理对象的时候，相关的拦截器已经配置到代理对象中去了，拦截器在代理中起作用是通过对这些方法的回调来完成的：  如果使用JDK的Proxy来生成代理对象，那么需要通过InvocationHandler的 invoke()方法来设置拦截回调。 如果使用CGLIB来生成代理对象，就需要根据CGLIB的使用要求，通过DynamicAdvisedInterceptor的 intercept()方法来设置拦截回调。    4.2.2 JdkDynamicAopProxy的invoke拦截 #  JdkDynamicAopProxy中的 invoke()方法主要由以下功能：\n 设置Proxy对象，这些设置包括获取目标对象、拦截器链，同时把这些对象作为输入，创建了ReflectiveMethodInvocation对象，通过这个ReflectiveMethodInvocation对象来完成对AOP功能实现的封装。 包含了一个完整的拦截器链对目标对象的拦截过程，比如获得拦截器链并对拦截器链中的拦截器进行配置，逐个运行拦截器里的拦截增强，知道最后对目标对象方法的运行等。  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Object target = null; try { if (!this.equalsDefined \u0026amp;\u0026amp; AopUtils.isEqualsMethod(method)) { // The target does not implement the equals(Object) method itself.  return equals(args[0]);\t/*如果目标对象没有实现 Object 类的基本方法 equals()*/ } else if (!this.hashCodeDefined \u0026amp;\u0026amp; AopUtils.isHashCodeMethod(method)) { // The target does not implement the hashCode() method itself.  return hashCode();\t/*如果目标对象没有实现 Object 类的基本方法 hashCode()*/ } else if (method.getDeclaringClass() == DecoratingProxy.class) { // There is only getDecoratedClass() declared -\u0026gt; dispatch to proxy config.  return AopProxyUtils.ultimateTargetClass(this.advised); } else if (!this.advised.opaque \u0026amp;\u0026amp; method.getDeclaringClass().isInterface() \u0026amp;\u0026amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) { // Service invocations on ProxyConfig with the proxy config...  return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args);\t/*根据代理对象的配置来调用服务*/ } Object retVal; if (this.advised.exposeProxy) { // Make invocation available if necessary.  oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } // Get as late as possible to minimize the time we \u0026#34;own\u0026#34; the target,  // in case it comes from a pool.  target = targetSource.getTarget();\t/*得到目标对象*/ Class\u0026lt;?\u0026gt; targetClass = (target != null ? target.getClass() : null); // Get the interception chain for this method.  List\u0026lt;Object\u0026gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);\t/*获得定义好的拦截器链*/ // Check whether we have any advice. If we don\u0026#39;t, we can fallback on direct  // reflective invocation of the target, and avoid creating a MethodInvocation.  if (chain.isEmpty()) {\t/*如果没有设置拦截器，那么就直接调用 target 的对应方法*/ // We can skip creating a MethodInvocation: just invoke the target directly  // Note that the final invoker must be an InvokerInterceptor so we know it does  // nothing but a reflective operation on the target, and no hot swapping or fancy proxying.  Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); } else {\t/*如果有拦截器的设定，那么需要调用拦截器之后才调用目标对象的相应方法，通过构造一个 ReflectiveMethodInvocation 来实现*/ // We need to create a method invocation...  MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain.  retVal = invocation.proceed();\t/*沿着拦截器链继续前进*/ } // Massage return value if necessary.  Class\u0026lt;?\u0026gt; returnType = method.getReturnType(); if (retVal != null \u0026amp;\u0026amp; retVal == target \u0026amp;\u0026amp; returnType != Object.class \u0026amp;\u0026amp; returnType.isInstance(proxy) \u0026amp;\u0026amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) { // Special case: it returned \u0026#34;this\u0026#34; and the return type of the method  // is type-compatible. Note that we can\u0026#39;t help if the target sets  // a reference to itself in another returned object.  retVal = proxy; } else if (retVal == null \u0026amp;\u0026amp; returnType != Void.TYPE \u0026amp;\u0026amp; returnType.isPrimitive()) { throw new AopInvocationException( \u0026#34;Null return value from advice does not match primitive return type for: \u0026#34; + method); } return retVal; } finally { if (target != null \u0026amp;\u0026amp; !targetSource.isStatic()) { // Must have come from TargetSource.  targetSource.releaseTarget(target); } if (setProxyContext) { // Restore old proxy.  AopContext.setCurrentProxy(oldProxy); } } } 4.2.3 目标对象方法的调用 #   如果没有设置拦截器，那么会对目标对象的方法直接进行调用。 对于JdkDynamicAopProxy代理对象，这个对目标对象的方法调用是通过AopUtils使用反射机制在 AopUtils.invokeJoinpointUsingReflection()的方法中实现的，在这个调用中，首先得到调用方法的反射对象，然后使用 invoke()启动对方法反射对象的调用。  public static Object invokeJoinpointUsingReflection(@Nullable Object target, Method method, Object[] args) throws Throwable { // Use reflection to invoke the method.  try { ReflectionUtils.makeAccessible(method); return method.invoke(target, args);\t/*使用反射调用 target 对象*/ } catch (InvocationTargetException ex) { // Invoked method threw a checked exception.  // We must rethrow it. The client won\u0026#39;t see the interceptor.  throw ex.getTargetException();\t/*抛出 AOP 异常，对异常进行转换*/ } catch (IllegalArgumentException ex) { throw new AopInvocationException(\u0026#34;AOP configuration seems to be invalid: tried calling method [\u0026#34; + method + \u0026#34;] on target [\u0026#34; + target + \u0026#34;]\u0026#34;, ex); } catch (IllegalAccessException ex) { throw new AopInvocationException(\u0026#34;Could not access method [\u0026#34; + method + \u0026#34;]\u0026#34;, ex); } } 4.2.4 拦截器的调用 #   JdkDynamicAopProxy和CglibAopProxy虽然使用了不同的AopProxy代理对象，但他们对拦截器链的调用都是在ReflectiveMethodInvocation中通过 proceed()方法实现的。 proceed()方法的具体执行流程如下：  先进行判断：  如果现在已经运行到拦截器链的末尾，那么就会直接调用目标对象的实现方法。 否则，沿着拦截器链继续进行，得到下一个拦截器，通过这个拦截器进行 matches判断，判断是否适用于横切片增强的场合：  如果是，从拦截器中得到通知器，并启动通知器的 invoke()方法进行切面增强，在这个过程结束以后，会迭代调用 proceed()方法，直到拦截器链中的拦截器都完成以上的拦截过程为止。        public Object proceed() throws Throwable { // We start with an index of -1 and increment early.  /** * 从索引为 -1 的拦截器开始调用，并按序递增 * 如果拦截器链中的拦截器迭代调用完毕，就开始调用 target 的函数，这个函数是通过反射机制完成的，具体实现在 {@link AopUtils#invokeJoinpointUsingReflection} 方法中 */ if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) { return invokeJoinpoint(); } Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex);\t/*沿着定义好的 interceptorOrInterceptionAdvice 链进行处理*/ if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) {\t/*对拦截器进行动态匹配的判断，如果和定义的 Pointcut 匹配，那么这个 advice 将得到执行*/ // Evaluate dynamic method matcher here: static part will already have  // been evaluated and found to match.  InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; Class\u0026lt;?\u0026gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass()); if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) { return dm.interceptor.invoke(this); } else { // Dynamic matching failed.  // Skip this interceptor and invoke the next in the chain.  return proceed();\t/*如果不匹配，那么 proceed 会被递归调用，知道所有的拦截器都被运行过为止*/ } } else { // It\u0026#39;s an interceptor, so we just invoke it: The pointcut will have  // been evaluated statically before this object was constructed.  return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this);\t/*如果是一个 interceptor，直接调用这个 interceptor 对应的方法*/ } } 参考文献 #    Spring-aop 全面解析（从应用到原理）。 《Spring 技术内幕-深入解析 Spring 架构与设计原理（第 2 版）》  "},{"id":174,"href":"/school-recruitment/docs/spring-family/2SpringBoot/2.1-SpringSpringBootSpringCloud%E7%9A%84%E8%81%94%E7%B3%BB%E4%B8%8E%E5%8C%BA%E5%88%AB/","title":"2.1 Spring、 Spring Boot、 Spring Cloud的联系与区别","section":"2、 Spring Boot","content":"1 Spring #  1.1 含义 #   Spring是一个生态体系，包含了 Spring Framework、Spring Boot、Spring Cloud 等（还包括 Spring Cloud Data Flow、Spring Data、Spring Integration、Spring Batch、Spring Security、Spring Hateoas），是一站式的轻量级的 Java 开发框架，核心是 IoC（控制反转）和 AOP（面向切面），针对于开发的 WEB 层（SpringMVC）、业务层（IoC）、持久层（JdbcTemplate）等都提供了多种配置解决方案。  2 SpringBoot #  2.1 含义 #   SpringBoot使用了约定大于配置的理念，集成了多个快速开发的 Spring 插件，同时自动过滤不需要配置的多余的插件，简化了项目开发的配置流程，一定程度上取消了 XML 配置，是一套快速配置开发的脚手架，能快速开发单个微服务。  3 SpringCloud #  3.1 含义 #   SpringCloud是一套分布式服务治理的框架，本身不会提供具体功能性的操作，专注于服务之间的通讯、熔断、监控等，需要很多的 组件来支持一套功能。 SpringCloud依赖于 SpringBoot 开发，而 SpringBoot 可以独立开发。  3.2 组件 #  3.2.1 默默无闻服务 #   融合在每个微服务中，依赖其他组件并为其提供服务。\n  Ribbon：客户端负载均衡，特性有区域亲和、重试机制。 Hystrix：客户端容错保护，特性有服务降级、服务熔断、请求合并、依赖隔离。 Feign：声明式服务调用，本质上就是 Ribbon + Hystrix。 Stream：消息驱动，有 Sink、Source、Processor 三种通道，特性有订阅发布、消费组、消息分区。 Bus：消息总线，配合 Config 仓库修改的一种 Stream 实现。 Sleuth：分布式服务追踪，需要搞清楚 TraceID、SpanID、抽样、如何与 ELK 整合。  3.2.2 利刃独挑大梁 #   独自启动不需要依赖其他组件。\n  Euraka：服务注册中心，特性有失效剔除，服务保护。 Dashboard：Hystrix 仪表盘，监控集群模式和单点模式，其中集群模式需要收集器 Turbine 配合。 Zuul：API 服务网关，功能有路由分发和过滤。 Config：分布式配置中心，支持本地仓库、SVN、Git、Jar 包内配置等模式。  3.2.3 各司其职 #   每个组件都不是平白无故的产生的，是为了解决某一特定的问题而存在。\n  Eureka 和 Ribbon 是最基础的组件，一个注册服务，一个消费服务。 Hystrix 为了优化 Ribbon，防止整个微服务架构因为某个服务节点的问题导致崩溃，相当于一个保险丝。 Dashboard 给 Hystrix 统计和展示用的，而且监控服务节点的整体压力和健康情况。 Turbine 是集群收集器，服务于 Dashboard。 Feign 是方便我们程序员写更优美的代码的。 Zuul 是加在整个微服务最前沿的防火墙和代理器，隐藏微服务节点 IP 端口信息，加强安全保护。 Config 是为了解决所有微服务各自维护各自的配置，设置一个统一的配置中心，方便修改配置。 Bus 是因为 Config 修改完配置后各个节点都要 refresh 才能生效太麻烦，所以交给 Bus 来通知服务节点刷新配置。 Stream 是为了简化研发人员对 MQ 使用的复杂度，弱化 MQ 的差异性，达到程序和 MQ 松耦合。 Sleuth 是因为单次请求在微服务节点中跳转无法追溯，解决任务链日志追踪问题。  参考文献 #    Spring、SpringBoot、SpringCloud 的关系区别。  springboot，springcloud 的区别。  "}]